chi
2018
honourable
mention
chi
2018
april
21
26
2018
montréal
qc
canada
investigating
the
impact
of
gender
on
rank
in
resume
search
engines
le
chen
northeastern
university
leonchen@ccs.neu.edu
ruijun
ma
rutgers
university
rma@stat.rutgers.edu
anikó
hannák
central
european
university
ancsaaa@gmail.com
christo
wilson
northeastern
university
cbw@ccs.neu.edu
abstract
in
this
work
we
investigate
gender-based
inequalities
in
the
context
of
resume
search
engines
which
are
tools
that
allow
recruiters
to
proactively
search
for
candidates
based
on
keywords
and
filters
if
these
ranking
algorithms
take
demographic
features
into
account
directly
or
indirectly
they
may
produce
rankings
that
disadvantage
some
candidates
we
collect
search
results
from
indeed
monster
and
careerbuilder
based
on
35
job
titles
in
20
cities
resulting
in
data
on
855k
job
candidates
using
statistical
tests
we
examine
whether
these
search
engines
produce
rankings
that
exhibit
two
types
of
indirect
discrimination
individual
and
group
unfairness
furthermore
we
use
controlled
experiments
to
show
that
these
websites
do
not
use
inferred
gender
of
candidates
as
explicit
features
in
their
ranking
algorithms
acm
classification
keywords
3.5
online
information
services
web-based
services
social
and
behavioral
sciences
sociology
4.2
social
issues
employment
author
keywords
information
retrieval
algorithm
auditing
discrimination
introduction
the
internet
is
fundamentally
changing
the
labor
economy
millions
of
people
use
services
like
linkedin
indeed
monster
and
careerbuilder
to
find
employment
42
71
13
these
online
services
offer
innovative
mechanisms
for
recruiting
and
organizing
employment
often
driven
by
algorithmic
systems
that
rate
sort
and
recommend
workers
and
employers
there
is
potential
for
online
labor
markets
to
mitigate
some
of
the
mechanisms
that
cause
discrimination
in
traditional
labor
markets
in
online
contexts
workers
demographics
may
be
less
clear
or
even
anonymized
which
limits
the
potential
for
cognitive
biases
to
skew
recruiting
decisions
for
example
permission
to
make
digital
or
hard
copies
of
all
or
part
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
for
profit
or
commercial
advantage
and
that
copies
bear
this
notice
and
the
full
citation
on
the
first
page
copyrights
for
components
of
this
work
owned
by
others
than
the
author
must
be
honored
abstracting
with
credit
is
permitted
to
copy
otherwise
or
republish
to
post
on
servers
or
to
redistribute
to
lists
requires
prior
specific
permission
and
or
fee
request
permissions
from
permissions@acm.org
chi
2018
april
21
26
2018
montreal
qc
canada
2018
copyright
held
by
the
owner
author
publication
rights
licensed
to
acm
isbn
978
4503
5620
18
04
15.00
indeed
monster
and
careerbuilder
do
not
ask
job
seekers
to
input
their
demographics
or
upload
profile
image
yet
evidence
indicates
that
inequalities
persist
in
many
different
online
labor
contexts
scholars
have
uncovered
cases
of
unequal
opportunities
presented
to
women
in
online
ads
18
biases
in
social
feedback
for
gig-economy
workers
based
on
gender
and
race
37
and
discrimination
against
online
customers
based
on
socioeconomics
87
in
2017
the
illinois
attorney
general
sent
letters
to
six
major
hiring
websites
after
users
complained
about
age
discrimination
63
although
there
are
policies
and
best
practices
that
employers
may
adopt
to
address
biases
in
traditional
hiring
contexts
detecting
and
mitigating
these
issues
in
online
algorithmically-driven
contexts
remains
an
open
challenge
57
our
goal
in
this
work
is
to
investigate
gender-based
inequalities
in
the
ranking
algorithms
used
by
three
major
hiring
websites
indeed
monster
and
careerbuilder
common
feature
offered
by
these
and
many
other
hiring
websites
is
resume
search
engine
which
allows
recruiters
to
proactively
search
for
candidates
based
on
keywords
and
filters
like
any
search
engine
these
tools
algorithmically
rank
candidates
with
those
at
the
top
being
more
likely
to
be
seen
and
clicked
on
by
recruiters
79
17
56
however
if
the
ranking
algorithm
takes
demographic
features
into
account
explicitly
or
inadvertently
through
proxy
feature
it
may
produce
rankings
that
systematically
disadvantage
some
candidates
although
candidates
on
hiring
websites
rarely
self-report
their
gender
gender
can
be
inferred
with
high-accuracy
from
other
information
such
as
their
first
name
26
86
64
54
93
first
we
examine
indirect
discrimination
which
is
defined
as
correlations
between
the
output
of
system
and
sensitive
user
features
gender
even
if
those
features
are
not
explicitly
used
by
the
system
76
20
95
ranking
algorithm
that
exhibits
indirect
discrimination
may
cause
disparate
impact
on
the
individuals
being
ranked
to
facilitate
our
investigation
we
ran
queries
on
each
site
resume
search
engine
for
35
job
titles
across
20
american
cities
between
may
and
october
2016
and
recorded
the
search
results
our
final
dataset
includes
over
855k
job
candidates
intuitively
our
data
corresponds
to
recruiter
perspective
of
these
sites
including
the
candidates
profiles
and
their
rank
in
the
search
results
using
this
dataset
we
leverage
statistical
tests
to
quantify
whether
the
resume
search
engines
exhibit
individual
fair
doi
https://doi.org/10.1145/3173574.3174225
paper
651
page
chi
2018
honourable
mention
ness
which
is
defined
as
placing
candidates
with
similar
features
excluding
gender
at
similar
ranks
and
group
fairness
which
is
defined
as
assigning
similar
distributions
of
ranks
men
and
women
20
95
94
we
use
two
measures
of
fairness
because
they
correspond
to
different
assumptions
about
the
world
and
consequently
have
different
normative
consequences
27
if
we
assume
that
candidates
personal
profiles
and
resumes
accurately
reflect
their
intrinsic
skills
then
individual
fairness
is
the
appropriate
accountability
standard
for
search
engine
output
however
if
we
assume
that
candidates
data
is
impacted
by
structural
inequalities
in
society
then
the
raw
data
is
not
an
accurate
reflection
of
intrinsic
skills
and
therefore
group
fairness
is
more
appropriate
standard
we
make
no
assumptions
about
how
structural
inequalities
may
impact
our
dataset
and
thus
we
evaluate
both
types
of
fairness
our
statistical
tests
reveal
complicated
picture
with
respect
to
gender
fairness
on
the
three
hiring
websites
individual
fairness
by
fitting
mixed
linear
models
we
find
that
inferred
gender
is
significant
negative
feature
on
all
three
websites
0.05
indicating
that
feminine
candidates
appear
at
lower
ranks2
than
masculine
candidates
even
when
controlling
for
all
visible
candidate
features
however
the
size
of
the
gender
effect
is
small
on
careerbuilder
for
example
at
rank
30
men
appear
1.4
ranks
above
equally
qualified
women
on
average
95
ci
0.73
2.13
we
demonstrate
that
these
findings
are
robust
by
replicating
them
using
matched
subsets
of
candidates
39
varying
subsets
of
the
top
candidates
and
candidate
populations
that
include
search
filters
group
fairness
using
the
mann-whitney
test
we
find
that
8.5
13.2
of
job
title
city
pairs
exhibit
significant
group
unfairness
0.05
in
12
of
the
35
job
titles
the
search
results
consistently
favor
masculine
candidates
bartender
is
the
only
job
title
that
favors
women
second
we
examine
direct
discrimination
on
these
resume
search
engines
which
is
defined
as
the
explicit
use
of
inferred
gender
as
feature
when
ranking
candidates
76
we
performed
controlled
tests
using
resumes
uploaded
by
us
to
test
whether
the
ranking
algorithms
use
features
extracted
directly
from
the
resumes
to
rank
candidates
including
inferred
gender
almae
mater
and
unemployment
status
overall
our
examination
of
resume
search
engines
leads
to
mixed
conclusions
on
the
positive
side
we
find
that
the
ranking
algorithms
used
by
all
three
hiring
sites
do
not
use
candidates
inferred
gender
as
feature
furthermore
our
regressions
demonstrate
that
the
three
ranking
algorithms
are
for
the
most
part
individually
fair
with
respect
to
gender
the
small
significant
gender
effects
that
we
observe
are
likely
caused
by
some
ranking
feature
that
serves
as
weak
proxy
for
gender
on
the
negative
side
however
we
do
observe
significant
and
consistent
group
unfairness
against
feminine
dwork
et
al
originally
defined
these
terms
in
the
context
of
classification
algorithms
20
we
have
adapted
them
slightly
to
the
context
of
ranking
algorithms
in
this
paper
we
use
the
terms
top
and
high
to
refer
to
desirable
ranks
in
search
results
rank
this
is
the
standard
terminology
used
in
information
retrieval
literature
17
46
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
candidates
in
roughly
of
the
job
titles
we
examine
this
may
be
of
particular
concern
in
technical
professions
like
electrical
mechanical
network
and
software
engineering
that
are
known
to
be
gender-imbalanced
72
whether
the
hiring
websites
should
adopt
ranking
algorithms
that
strive
for
group
fairness
is
fraught
question
our
analysis
conclusively
shows
that
these
ranking
algorithms
do
not
create
group
unfairness
with
respect
to
gender
the
algorithms
are
mostly
gender
blind
rather
the
algorithms
are
likely
reflecting
structural
gender
inequalities
that
are
embedded
in
the
raw
data
ultimately
we
hope
that
our
work
furthers
the
dialog
about
the
adoption
of
algorithmic
affirmative
action
policies
that
benefit
marginalized
populations
limitations
there
are
several
limitations
of
our
work
first
there
are
no
user
studies
that
quantify
how
recruiters
use
resume
search
engines
including
how
many
results
they
view
and
click
on
or
how
they
construct
queries
and
filters
we
attempt
to
address
this
in
our
analysis
by
examining
gender
effects
under
large
variety
of
use
cases
35
different
job
titles
search
results
lists
of
differing
lengths
and
queries
with
and
without
filters
furthermore
it
is
unknown
what
fraction
of
online
recruiting
is
active
using
resume
search
engines
or
passive
using
advertisements
for
open
positions
we
leave
user
studies
of
recruiters
as
future
work
second
the
nature
of
our
dataset
restricts
us
to
inferring
binary
gender
labels
for
candidates
this
is
common
limitation
of
work
that
uses
observational
datasets
to
examine
gender
biases
in
online
contexts
55
37
related
work
labor
discrimination
is
long
standing
troubling
aspect
of
society
that
may
impact
workers
wages
or
opportunities
for
advancement
in
this
paper
we
specifically
focus
on
hiring
discrimination
which
occurs
when
discrimination
impacts
the
candidates
that
are
selected
to
fill
open
positions
hiring
discrimination
still
impacts
the
modern
job
market
75
and
may
be
based
on
gender
race
89
sexual
orientation
92
disability
29
or
age
25
unfortunately
it
is
one
of
the
most
difficult
types
of
discrimination
to
prove
in
court
31
one
of
the
key
tools
used
to
study
hiring
discrimination
is
the
audit
or
correspondence
study
in
this
methodology
researchers
probe
the
hiring
practices
of
target
by
submitting
carefully
crafted
resumes
or
by
sending
human
participants
in
for
interviews
78
by
carefully
constructing
the
treatments
to
only
differ
by
specific
demographic
features
gender
the
researchers
can
measure
the
correlation
between
these
variables
and
hiring
outcomes
74
scholars
and
regulators
have
begun
to
focus
on
the
ways
that
big
data
and
algorithms
can
create
hiring
discrimination
pauline
kim
thoroughly
catalogs
how
data-driven
systems
that
evaluate
job
seekers
may
introduce
new
forms
of
discrimination
against
members
of
protected
classes
57
one
potential
driver
of
algorithmic
discrimination
identified
by
kim
and
by
barocas
and
selbst
occurs
when
subpopulations
are
not
well-represented
in
training
data
this
issue
is
not
hypothetical
in
2017
the
illinois
attorney
general
page
chi
2018
honourable
mention
launched
an
investigation
against
six
major
online
job
boards
including
indeed
monster
and
careerbuilder
after
receiving
complaints
that
their
design
excluded
older
job
seekers
63
defining
fairness
defining
and
operationalizing
fair
and
non-discriminatory
algorithms
is
an
active
area
of
research
pedreshi
et
al
defined
direct
and
indirect
discrimination
where
the
former
refers
to
algorithms
that
explicitly
take
sensitive
features
as
input
76
direct
discrimination
is
analogous
to
disparate
treatment
when
the
use
of
sensitive
input
is
legally
proscribed
gender
and
race
an
algorithm
exhibits
indirect
discrimination
if
the
outputs
are
strongly
correlated
with
sensitive
features
even
if
those
sensitive
features
are
not
explicitly
used
as
inputs
76
this
is
analogous
to
disparate
impact
and
it
may
occur
in
practice
when
neutral
features
in
dataset
act
as
proxies
for
sensitive
features
classic
example
is
the
use
of
zipcode
in
place
of
race
to
implement
redlining
34
dwork
et
al
introduced
two
types
of
fairness
to
address
indirect
discrimination
individual
fairness
states
that
similar
individuals
should
be
treated
similarly
while
group
fairness
states
that
demographic
subsets
of
the
population
should
be
treated
the
same
as
the
entire
population
20
dwork
et
al
likened
group
fairness
to
fair
affirmative
action
as
it
equalizes
outcomes
across
protected
and
non-protected
groups
20
unfortunately
it
is
not
always
possible
to
achieve
both
types
of
fairness
simultaneously
if
the
base
rates
of
one
or
more
critical
features
are
different
across
subpopulations
thus
most
existing
fair
classification
systems
optimize
for
individual
50
53
66
52
20
or
group
fairness
48
49
11
96
51
10
34
24
zemel
et
al
present
classifiers
that
allow
the
operator
to
tune
the
tradeoff
between
individual
and
group
fairness
95
friedler
et
al
present
framework
for
grappling
with
the
assumptions
that
underly
individual
and
group
fairness
in
algorithmic
scenarios
27
if
we
assume
that
dataset
accurately
encodes
the
intrinsic
characteristics
of
population
then
it
is
appropriate
to
judge
fairness
at
the
individual-level
however
if
we
assume
that
dataset
is
influenced
by
systematic
or
structural
biases
then
it
is
no
longer
individually-accurate
and
we
should
instead
pursue
group
fairness
in
this
work
we
evaluate
whether
hiring
websites
exhibit
individual
and
group
fairness
since
we
do
not
make
assumptions
about
whether
the
data
on
the
websites
is
impacted
by
structural
bias
biases
in
online
systems
researchers
have
begun
to
empirically
investigate
whether
algorithmic
systems
may
inadvertently
cause
harm
to
users
the
process
of
examining
black-box
computer
system
has
become
known
as
an
algorithm
audit
as
the
methodology
draws
inspiration
from
classic
audit
studies
80
prior
algorithm
audits
have
examined
search
engines
35
58
online
maps
84
social
networks
23
e-commerce
69
70
36
15
and
online
advertising
33
61
to
our
knowledge
ours
is
the
first
audit
to
focus
on
online
job
boards
there
are
many
causes
for
bias
in
online
systems
some
cases
are
direct
manifestations
of
societal
biases
by
users
gender
biases
on
wikipedia
60
77
91
in
other
cases
biases
are
learned
by
an
algorithm
that
is
trained
on
biased
data
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
racist
ad
targeting
on
google
search
85
or
sexist
word
associations
in
language
models
12
finally
bias
may
arise
due
to
combination
of
user-driven
structural
bias
and
algorithm
design
47
in
this
work
we
examine
whether
volunteered
information
from
job
seekers
and
or
algorithm
design
decisions
cause
hiring
websites
to
produce
biased
search
results
closely
related
to
our
work
are
studies
that
have
uncovered
discrimination
on
gig
and
sharing-economy
services
studies
have
found
examples
of
workers
service
providers
discriminating
against
customers
on
taskrabbit
87
airbnb
21
and
uber
28
as
well
as
cases
where
customers
discriminate
against
workers
on
taskrabbit
and
fiverr
37
in
this
study
we
examine
three
the
importance
of
rank
websites
that
present
job
seekers
in
ranked
lists
in
response
to
queries
from
recruiters
if
these
ranking
algorithms
systematically
elevate
candidates
with
specific
demographic
attributes
this
may
recreate
real-world
social
inequality
in
an
online
context
because
the
top
items
of
ranked
lists
are
much
more
likely
to
be
clicked
by
users
30
32
79
17
for
example
keane
et
al
demonstrated
that
even
if
users
are
unknowingly
presented
with
an
inverted
list
of
google
search
results
they
still
overwhelming
click
on
the
top-ranked
items
56
our
work
falls
within
the
literature
search
engines
that
examines
social
harms
that
can
occur
when
search
engines
present
misleading
or
biased
information
researchers
have
examined
misinformation
about
vaccines
65
biased
scientific
information
73
and
climate
change
denial
90
epstein
et
al
use
controlled
experiments
to
show
that
biased
political
information
presented
by
search
engine
can
significantly
alter
users
voting
patterns
22
three
studies
have
specifically
examined
demographic
biases
on
search
engines
hannák
et
al
found
negative
correlations
between
race
and
rank
on
the
gig-economy
website
taskrabbit
even
after
controlling
for
all
other
worker-related
features
37
kay
et
al
and
magno
et
al
both
examined
gender
on
google
image
search
and
found
that
women
are
often
depicted
stereotypically
55
67
kay
et
al
work
is
particularly
relevant
since
they
examined
gendered
images
after
querying
for
occupations
they
found
that
users
preferred
images
containing
people
that
matched
an
occupation
gender
stereotype
male
ceo
and
that
over-representation
of
gender
in
the
search
results
shifted
user
perception
of
gender
balance
in
that
occupation
if
these
findings
apply
to
recruiters
then
this
suggests
that
hiring
websites
should
strive
for
group
fairness
in
search
results
as
this
would
improve
the
perceived
gender
balance
in
occupations
as
well
as
encourage
recruiters
to
engage
with
candidates
that
do
not
match
an
occupation
gender
stereotype
female
ceos
two
studies
have
proposed
metrics
for
quantifying
bias
in
search
results
kulshrestha
et
al
separately
quantify
the
amount
of
bias
in
search
engine
corpus
and
output
59
unfortunately
we
cannot
use
this
metric
because
we
do
not
have
the
entire
corpus
of
candidates
from
the
hiring
websites
yang
et
al
define
family
of
bias
metrics
that
are
related
to
normalized
discounted
cumulative
gain
ndcg
94
which
is
common
information
retrieval
metric
that
we
use
in
this
page
chi
2018
honourable
mention
limits
number
of
candidates
shown
per
page
maximum
number
of
candidates
per
query
maximum
resume
views
per
month
cost
per
month
chi
2018
april
21
26
2018
montréal
qc
canada
indeed
50
5000
no
limit
free
monster
20
1000
100
700
cb
20
30
5000
50
400
table
search
result
format
and
limits
for
all
three
sites
paper
however
the
yang
et
al
metrics
are
difficult
to
use
in
practice
since
they
rely
on
normalization
term
that
is
computed
stochastically
in
this
paper
we
rely
on
standard
statistical
tests
since
they
are
easier
to
interpret
provide
confidence
guarantees
and
have
been
used
successfully
by
prior
work
to
examine
inequalities
in
algorithmic
systems
81
background
in
this
section
we
introduce
the
three
websites
that
are
the
focus
of
our
study
we
discuss
how
recruiters
use
these
resume
search
engines
and
specific
details
about
their
user
interfaces
we
also
briefly
discuss
how
candidates
use
these
websites
hiring
websites
we
chose
three
hiring
sites
to
examine
in
this
study
indeed
monster
and
careerbuilder
we
chose
these
sites
for
three
reasons
first
they
are
three
of
the
most
popular
employment
websites
in
the
along
with
linkedin
and
glassdoor
each
of
these
websites
claims
to
serve
millions
of
unique
visitors
and
job
queries
per
month
42
71
13
second
all
three
sites
provide
economical
access
to
their
resume
search
engine
as
shown
in
table
contrast
this
to
linkedin
which
charges
9000
for
access
to
its
unrestricted
recruiter
tools
third
as
we
discuss
next
all
three
sites
have
similar
user
interfaces
for
candidates
and
recruiters
this
makes
the
sites
roughly
comparable
in
terms
of
usability
and
features
which
allows
us
to
contrast
our
results
across
the
sites
state
texas
iowa
wisconsin
louisiana
new
york
nebraska
utah
california
missouri
michigan
ohio
tennessee
illinois
cities
austin
des
moines
madison
milwaukee
new
orleans
new
york
city
buffalo
omaha
salt
lake
city
san
francisco
stockton
san
bernardino
los
angeles
springfield
detroit
toledo
cleveland
cincinnati
memphis
chicago
table
cities
used
in
our
queries
can
observe
in
search
results
for
each
site
besides
candidates
names
and
current
job
titles
are
given
in
table
scope
in
this
work
we
focus
on
the
ranking
of
candidates
rather
than
the
composition
of
search
results
which
candidates
are
deemed
relevant
although
examination
of
the
overall
candidate
pool
would
be
interesting
we
cannot
do
so
because
there
is
no
way
for
us
to
enumerate
all
candidates
in
hiring
website
corpus
candidate
perspective
indeed
monster
and
careerbuilder
are
very
similar
from
candidate
perspective
candidates
must
register
for
free
account
and
possibly
fill
out
personal
profile
and
upload
resume
the
amount
of
profile
information
that
is
mandatory
varies
across
the
sites
on
monster
users
must
provide
their
name
location
educational
attainment
and
previous
job
while
careerbuilder
allows
users
to
leave
their
profile
empty
however
all
three
sites
remind
users
to
upload
more
information
especially
resume
since
the
job
recommendation
functionality
on
the
sites
depends
on
this
information
once
candidate
has
created
profile
they
can
browse
open
positions
and
respond
to
solications
from
recruiters
recruiter
perspective
in
this
study
we
examine
the
resume
search
engines
provided
by
indeed
monster
and
careerbuilder
all
three
sites
offer
similar
search
engines
that
are
designed
to
help
recruiters
identify
and
recruit
new
employees
the
corpus
of
each
resume
search
engine
contains
resumes
and
personal
profiles
uploaded
by
candidates
seeking
employment
recruiters
query
the
corpus
by
entering
free-text
job
title
geographic
location
and
optional
filters
to
refine
the
results
years
of
experience
minimum
educational
attainment
etc
none
of
the
sites
allow
recruiters
to
filter
or
order
search
results
by
demographics
gender
ethnicity
but
proxy
variables
exist
in
some
cases
years
of
experience
as
an
indicator
of
age
the
resume
search
engines
use
rankings
algorithms
to
determine
which
candidates
are
relevant
to
given
query
and
their
ordering
subject
to
any
specified
filters
by
default
only
candidates
within
20
50
miles
of
the
specified
location
are
deemed
relevant
and
the
search
results
are
sorted
by
opaque
metrics
most
relevant
although
the
recruiter
may
re-sort
the
list
by
objective
metrics
like
years
of
experience
table
lists
details
about
the
search
result
format
and
data
access
restrictions
on
the
three
sites
candidate
features
we
paper
651
data
collection
in
this
section
we
describe
our
data
collection
methodology
and
the
specific
variables
we
extract
from
the
data
crawl
to
collect
data
for
this
study
we
use
an
automated
web
browser
to
search
for
candidates
on
indeed
monster
and
careerbuilder
intuitively
our
crawler
is
designed
to
emulate
how
recruiter
would
search
for
candidates
on
these
hiring
websites
we
ran
queries
for
35
job
titles
in
20
cities
described
below
on
all
three
sites
and
recorded
the
resulting
lists
of
candidates
we
also
queried
for
subset
of
490
700
and
700
job
title
city
pairs
with
one
two
and
three
search
filters
on
monster
indeed
and
careerbuilder
respectively
for
binary
filters
willingness
to
relocate
we
queried
with
both
options
for
non-binary
filters
minimum
years
of
experience
we
set
three
different
values
for
the
filter
chosen
such
that
the
values
select
from
33
33
66
and
66
100
of
the
candidate
population
on
indeed
we
recorded
candidates
resumes
but
not
on
monster
or
careerbuilder
since
they
only
allow
recruiters
to
view
100
resumes
we
calculate
these
filter
values
based
on
the
cumulative
density
function
of
the
corresponding
features
from
the
unfiltered
datasets
page
chi
2018
honourable
mention
observed
in
search
results
origin
inferred
feature
job
title
relevance
skills
relevance
education
level
job
popularity
last
modified
experience
relocate
skills
popularity
information
relevance
bio
relevance
skills
match
information
match
bio
match
gender
chi
2018
april
21
26
2018
montréal
qc
canada
description
relevance
of
the
searched
job
title
to
the
candidate
current
job
title
relevance
of
the
searched
job
title
to
the
candidate
skills
education
level
of
the
candidate
popularity
of
the
candidate
current
job
title
among
all
candidates
returned
for
the
searched
job
title
the
recency
of
the
candidate
profile
and
resume
the
experience
of
the
candidate
in
years
whether
the
candidate
is
willing
to
relocate
binary
popularity
of
the
candidate
skills
among
all
candidates
returned
for
the
searched
job
title
relevance
of
the
searched
job
title
to
additional
profile
info
relevance
of
the
searched
job
title
to
the
candidate
provided
description
of
the
working
experience
whether
the
entire
searched
job
title
is
present
in
the
candidate
skill
set
binary
whether
the
entire
searched
job
title
is
present
in
the
candidate
additional
profile
information
whether
the
entire
searched
job
title
is
present
in
the
candidate
bio
probability
of
the
candidate
being
masculine
indeed
present
on
monster
cb
table
per-candidate
features
we
extract
from
search
results
on
indeed
monster
and
careerbuilder
we
infer
gender
from
each
candidate
first
name
other
features
are
directly
observed
not
all
features
are
present
on
all
hiring
websites
per
month
all
of
our
data
was
collected
between
may
and
october
2016
and
the
crawling
took
two
months
on
each
site
100
to
obtain
broad
sample
of
candidates
we
ran
queries
for
35
job
titles
in
20
cities
table
lists
our
35
job
titles
19
were
chosen
because
they
are
the
most
commonly
searched
job
titles
14
while
the
remaining
16
do
not
require
highschool-level
education
which
adds
diversity
to
our
queries
table
shows
the
20
cities
we
focus
on
which
were
chosen
to
give
us
broad
geographic
and
demographic
variety
we
ultimately
gathered
521
783
265
172
and
67
580
candidates
on
indeed
monster
and
careerbuilder
after
running
our
queries
60
candidate
features
next
we
extract
information
about
candidates
in
the
search
results
we
focus
on
three
types
of
features
profile
data
experience
education
etc
inferred
gender
and
rank
in
search
results
table
lists
the
features
we
are
able
to
extract
on
each
site
we
normalize
all
features
in
table
to
be
between
and
for
consistency
details
of
how
we
encode
and
normalize
each
feature
can
be
found
in
the
supplementary
materials
inferring
gender
since
our
goal
is
to
examine
resume
search
engines
with
respect
to
gender
we
need
to
label
each
candidate
gender
however
none
of
the
sites
we
focus
on
collect
this
information
instead
we
infer
each
candidate
gender
based
on
their
first
name
which
is
common
method
to
infer
gender
in
western
societies
26
86
64
54
93
in
this
work
we
rely
on
the
baby
name
dataset
83
to
infer
candidates
gender
we
assign
probability
of
being
masculine
to
each
candidate
based
on
the
fraction
of
times
their
first
name
was
given
to
male
baby
in
the
name
dataset
we
represent
gender
as
probability
since
this
corresponds
to
how
recruiters
perceive
candidates
genders
for
example
candidate
named
john
is
almost
certainly
masculine
we
also
tried
inferring
candidates
gender
using
the
genni
and
sexmachine
datasets
82
88
however
this
resulted
in
13
and
19
candidates
having
unknown
gender
respectively
versus
for
our
method
additionally
we
fit
mixed
linear
models
to
the
genni
and
sexmachine
labeled
data
and
found
that
the
probability
of
being
masculine
was
significant
0.001
and
negative
on
all
three
hiring
websites
paper
651
indeed
careerbuilder
monster
cdf
80
40
20
0.2
0.4
0.6
probability
of
being
masculine
0.8
figure
inferred
probability
of
being
masculine
for
all
candidates
in
our
dataset
while
madison
is
ambiguous
we
assign
score
of
0.5
to
candidates
whose
names
do
not
appear
in
the
dataset
figure
shows
the
cdf
of
the
probability
of
being
masculine
for
all
candidates
on
our
dataset
only
of
candidates
have
ambiguous
gender
all
but
45
of
which
correspond
to
candidates
whose
names
do
not
appear
in
the
dataset
this
means
that
we
can
be
very
confident
in
the
vast
majority
of
our
gender
labels
the
plot
also
shows
that
the
masculine
feminine
ratio
is
approximately
on
all
three
sites
limitations
there
are
two
limitations
of
our
dataset
and
labeling
methodology
that
are
worth
noting
first
the
candidate
attributes
that
we
extract
from
search
results
may
not
match
candidates
true
attributes
fortunately
this
limitation
does
not
impact
our
analysis
since
the
ranking
algorithms
we
are
auditing
as
well
as
recruiters
that
rely
on
resume
search
engines
base
their
decisions
on
candidates
reported
attributes
thus
throughout
this
paper
when
we
compare
the
capabilities
of
candidates
we
are
referring
to
their
reported
attributes
rather
than
their
true
attributes
second
we
do
not
know
candidates
true
genders
as
above
this
limitation
does
not
impact
our
analysis
since
recruiters
and
ranking
algorithms
must
also
rely
on
inferred
gender
when
making
decisions
if
they
use
this
information
at
all
throughout
this
paper
when
we
refer
to
gender
we
are
referring
to
recruiters
do
not
learn
candidate
true
attributes
until
much
later
in
the
hiring
process
after
interviewing
the
candidate
and
checking
their
references
page
chi
2018
honourable
mention
inferred
gender
based
on
first
name
furthermore
as
noted
in
the
introduction
we
are
limited
to
analyzing
binary
genders
we
were
careful
to
conduct
our
study
in
an
ethical
ethics
manner
this
study
was
approved
under
northeastern
irb
1601
19
to
protect
the
contextual
privacy
of
candidates
we
will
not
be
releasing
our
crawled
data
furthermore
we
limited
the
impact
of
our
crawler
on
the
hiring
sites
by
restricting
it
to
one
query
every
30
seconds
and
at
no
point
did
we
contact
candidates
in
our
controlled
experiments
detailed
in
the
next
section
we
only
uploaded
two
resumes
at
time
to
the
hiring
sites
meaning
we
decreased
the
rank
of
other
candidates
by
at
most
two
although
all
three
sites
prohibit
crawling
in
their
terms
of
service
we
believe
our
study
is
acceptable
under
the
norms
that
have
been
established
by
prior
algorithm
audit
studies
80
37
analysis
in
this
section
we
investigate
the
rankings
of
candidates
produced
by
resume
search
engines
with
respect
to
inferred
gender
we
organize
our
analysis
around
three
questions
do
the
resume
search
engines
exhibit
individual
fairness
do
they
exhibit
group
fairness
do
they
explicitly
rank
candidates
based
on
inferred
gender
individual
fairness
in
this
section
we
investigate
whether
the
rankings
of
candidates
produced
by
resume
search
engines
are
individually
fair
with
respect
to
inferred
gender
recall
that
to
be
individually
fair
the
ranking
algorithms
must
rank
candidates
with
similar
features
excluding
gender
at
similar
ranks
20
95
94
to
investigate
individual
fairness
we
use
regression
tests
our
goal
is
to
examine
the
effect
of
inferred
gender
on
candidate
rank
while
controlling
for
all
other
observable
candidate
features
if
the
gender
feature
is
significant
and
has
non-zero
coefficient
in
the
fitted
model
this
indicates
that
the
ranking
algorithm
in
question
is
not
individually
fair
as
candidates
with
equivalent
features
but
different
inferred
genders
are
not
assigned
the
same
rank
throughout
this
section
we
focus
on
the
top
100
candidates
returned
in
search
results
since
recruiters
are
unlikely
to
browse
to
candidates
at
lower
ranks
79
17
however
to
ensure
the
robustness
of
our
results
and
avoid
making
assumptions
about
the
behavior
of
recruiters
we
fit
additional
models
to
many
different
subsets
of
candidates
which
we
describe
below
model
specification
we
adopt
mixed
linear
model
mlm
for
our
regressions
we
regress
on
individual
candiβ
is
vector
dates
specifying
the
model
as
xβ
of
the
responses
log2
rank
of
each
candidate
explained
below
is
the
design
matrix
and
the
predictors
include
the
features
from
table
is
the
vector
of
fixed-effects
parameters
including
the
global
intercept
is
the
vector
of
random-effects
intercepts
in
other
words
search
results
for
specific
job
title
and
city
have
shared
global
fixed-effects
intercept
and
an
individual
random-effects
intercept
we
choose
model
with
fixed
and
random
effects
because
it
agrees
with
two
fundamental
assumptions
about
our
data
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
each
hiring
website
has
single
ranking
algorithm
with
features
and
weights
do
not
vary
by
query
term
or
location
this
corresponds
to
fixed
effects
regardless
of
job
title
and
location
this
assumption
is
reasonable
because
it
is
impractical
for
hiring
website
to
implement
different
algorithms
or
feature
weight
vectors
for
an
unbounded
number
of
free-text
job
titles
and
locations
candidates
with
identical
features
that
appear
in
different
search
result
lists
may
be
assigned
dramatically
different
ranks
this
is
true
because
the
population
of
candidates
and
their
relative
qualifications
varies
across
locations
and
professions
this
corresponds
to
random-effects
group
inµ
that
vary
by
job
title
and
location
tercepts
we
use
log2
rank
as
the
dependent
variable
in
our
regressions
for
two
reasons
it
prioritizes
top-ranked
candidates
by
giving
them
higher
weight
while
decaying
the
importance
for
lower-ranked
candidates
the
log
function
is
monotonically
increasing
which
maintains
the
ordering
of
candidates
after
the
transformation
logarithms
have
been
found
to
be
widely
applicable
in
the
ir
literature
empirical
studies
79
17
backed
up
by
eyetracking
surveys
30
32
68
have
found
that
the
probability
of
search
result
items
being
clicked
decays
logarithmically
similarly
log2
is
used
to
calculate
normalized
discount
cumulative
gain
ndcg
which
is
standard
metric
used
to
quantify
the
similarity
of
search
result
lists
43
44
by
using
logarithmic
decay
ndcg
affords
higher
weight
to
the
important
items
at
the
top
of
the
search
result
lists
model
fitting
we
fit
three
mlms
one
for
each
hiring
website
on
the
top
100
candidates
in
each
search
result
list
negative
coefficients
signify
effects
with
higher
rank
we
conduct
the
variance
inflation
factors
vif
test
to
remove
multicollinearity
before
fitting
the
models
all
the
variables
remain
after
the
test
the
correlation
matrix
is
available
in
the
supplementary
materials
to
assess
how
well
the
mlms
fit
for
our
data
we
evaluate
their
predictive
power
to
do
this
we
treat
the
each
model
as
ranking
algorithm
we
input
the
ground-truth
feature
vectors
of
candidates
from
given
search
result
list
into
fit
model
which
then
outputs
predicted
log
ranking
corresponding
to
predicted
ranking
next
we
use
the
ndcg
metric
to
compute
the
similarity
between
and
the
original
ranking
ndcg
is
standard
metric
used
in
the
ir
literature
to
compare
ranked
lists
43
44
the
dcg
of
ranking
r1
r2
rk
is
calculated
as
dcg
r1
ri
log2
where
ri
is
the
gain
or
score
assigned
to
result
ri
ndcg
is
defined
as
dcg
dcg
where
is
the
ideal
ranking
of
the
items
in
in
our
case
is
the
original
ranking
produced
by
hiring
site
we
treat
the
original
ranking
as
the
baseline
and
is
ranking
predicted
by
the
model
an
page
chi
2018
honourable
mention
chi
2018
april
21
26
2018
montréal
qc
canada
100
80
cdf
60
40
indeed
monster
careerbuilder
20
0.7
0.8
0.9
ndcg
per
job
per
city
rank
increase
for
men
figure
ndcg
comparison
of
the
predicted
rankings
produced
by
our
mlms
versus
the
original
rankings
indeed
monster
careerbuilder
feature
fixed
effect
intercept
job
title
relevance
skills
relevance
skills
relevance
skills
relevance
education
level
job
popularity
last
modified
experience
relocate
skills
popularity
skills
popularity
skills
popularity
bio
relevance
information
relevance
skills
match
information
match
bio
match
random
effect
prob
of
being
masculine
dependent
variable
log2
rank
indeed
monster
cb
4.803
5.938
4.129
0.518
1.3
1.258
0.051
0.31
0.109
0.108
0.042
0.061
0.027
0.115
0.004
0.147
2.053
0.197
0.149
0.116
1.303
0.185
0.021
0.048
0.062
0.017
0.041
0.255
0.072
0.093
0.262
0.01
0.106
0.018
0.042
0.028
0.071
observations
20
40
60
80
100
rank
figure
effect
sizes
with
95
confidence
intervals
for
the
gender
coefficients
points
have
been
jittered
horizontally
to
prevent
overlap
ndcg
value
of
indicates
that
the
two
rankings
are
identical
in
essense
ndcg
for
rank
responses
is
analogous
to
r2
for
continous
responses
figure
shows
the
evaluation
of
the
predictive
power
of
our
mlms
we
observe
that
60
77
of
the
ndcg
scores
are
0.8
across
all
three
websites
to
put
this
in
perspective
state-of-the-art
learning-to-rank
algorithms
produce
ndcg
scores
in
the
0.4
0.8
range
depending
on
the
context
and
the
benchmark
dataset
that
is
used
40
19
45
this
demonstrates
that
our
mlms
reproduce
most
of
the
search
results
with
high
accuracy
and
that
the
models
are
good
fit
for
our
data
results
table
shows
the
results
of
our
mlm
regressions
on
the
top
100
candidates
in
search
results
we
observe
that
the
majority
of
features
have
significant
negative
effect
on
log
rank
on
all
three
hiring
sites
such
as
job
title
relevance
and
job
popularity
on
indeed
last
modified
has
the
largest
coefficient
by
far
which
matches
their
documentation
stating
that
they
tend
to
rank
candidates
by
resume
update
time
41
interestingly
education
level
and
experience
have
significant
positive
effects
on
log
rank
on
indeed
which
suggest
that
there
are
more
candidates
with
lower
degrees
and
less
experience
in
the
top
ranks
possibly
newly
graduated
students
in
contrast
monster
and
careerbuilder
both
exhibit
significant
negative
effects
of
experience
on
log
rank
lastly
we
observe
that
the
probability
of
being
masculine
feature
is
significant
0.05
in
all
cases
and
negative
in
all
three
models
meaning
that
overall
men
rank
higher
than
women
with
equivalent
features
figure
shows
the
effect
sizes
and
95
confidence
intervals
for
the
gender
coefficient
in
our
models
the
effect
sizes
paper
651
67410
50813
28289
table
estimated
coefficients
and
standard
deviation
of
mixed
linear
regressions
on
the
top
100
candidates
in
search
results
from
each
hiring
website
grouped
by
city
and
job
title
significance
level
is
unavailable
for
random
effect
note
0.05
0.01
0.001
top
candidates
top
10
top
20
top
50
top
100
top
200
top
500
top
1000
indeed
gender
coef
0.023
0.009
0.036
0.042
0.029
0.022
0.019
monster
gender
coef
0.031
0.056
0.047
0.028
0.026
0.040
0.043
careerbuilder
gender
coef
0.027
0.023
0.053
0.071
0.071
0.055
0.039
table
estimated
probability
of
being
masculine
coefficient
from
mixed
linear
regressions
as
the
length
of
the
search
result
list
is
varied
note
0.05
0.01
0.001
increase
as
rank
decreases
due
to
our
log2
transformation
for
very
high
ranks
the
increase
in
rank
for
men
is
negligible
for
example
on
monster
at
rank
10
the
mean
increase
in
rank
is
0.2
95
ci
0.02
0.36
however
by
rank
50
on
monster
ties
between
men
and
women
are
consistently
broken
in
favor
of
men
mean
increase
0.96
95
ci
0.08
1.82
and
by
rank
80
the
increase
is
large
enough
to
push
men
across
pagination
boundaries
mean
increase
1.54
95
ci
0.14
2.91
indeed
and
careerbuilder
both
exhibit
larger
effects
than
monster
robustness
in
addition
to
the
mlms
we
fit
to
the
top
100
candidates
in
our
dataset
we
fit
hundreds
of
other
mlms
to
sub
and
supersets
of
our
candidate
population
to
guage
the
robustness
of
our
models
specifically
we
fit
models
to
the
top
candidates
in
search
results
as
is
varied
from
10
to
1000
chosen
because
it
is
the
maximum
number
of
candidates
returned
by
monster
the
top
100
and
1000
candidates
in
matched
subset
of
the
population
propensity
score
matching
is
technique
for
reducing
selection
bias
in
observational
datasets
38
the
top
100
candidates
in
search
results
that
include
combinations
of
one
two
and
three
filters
minimum
experience
page
chi
2018
honourable
mention
indeed
tdrc
20
0.05
0.06
15
0.00
0.07
25
0.05
10
0.01
5.5
0.03
0.6
0.04
20
0.15
0.03
22.8
0.53
0.01
6.1
0.16
10
0.03
0.35
20
0.36
10
0.16
25
0.03
5.5
0.01
16.1
0.01
47.6
0.48
7.5
0.04
0.04
5.5
0.01
35
0.08
15
0.02
0.06
0.03
0.09
28.3
0.01
25
0.59
0.07
28.3
0.04
20
0.15
15
0.50
13.2
3.68
monster
tdrc
15
0.02
0.05
0.00
0.6
0.04
0.02
0.06
4.1
0.01
0.19
20
0.02
20
0.13
6.1
0.02
0.15
0.17
40
0.36
9.3
0.22
0.10
0.08
0.29
0.27
0.01
0.00
0.21
0.09
11.7
0.04
0.3
0.15
0.9
0.01
55
0.05
30
0.42
0.02
0.02
15
0.12
9.3
0.49
8.5
3.48
7.5
11.7
37.1
8.3
36.7
70
20
25.8
11.7
45
13.2
cb
tdrc
0.01
0.26
0.02
0.06
0.09
0.03
0.17
0.00
0.11
0.02
0.19
0.00
0.00
0.00
0.04
0.03
0.00
0.05
0.00
0.04
0.00
0.00
0.00
1.45
table
evaluation
of
group
fairness
the
columns
show
the
percentage
of
cities
with
0.05
in
the
mann-whitney
test
on
the
rank
of
the
top
1000
women
and
men
for
given
job
title
and
hiring
website
we
subtract
from
each
percentage
with
floor
of
to
account
for
multiple
hypothesis
testing
the
tdrc
columns
show
the
total
difference
in
area
under
the
recall
curves
negative
positive
tdrc
indicates
that
feminine
masculine
candidates
are
ranked
higher
overall
marks
instances
where
there
are
no
cities
with
sufficiently
large
populations
to
test
we
also
remove
the
of
candidates
with
ambiguous
genders
job
titles
with
significant
unfairness
in
constant
direction
are
bolded
overall
these
models
exhibit
the
same
significance
sign
and
effect
sizes
for
the
probability
of
being
masculine
feature
as
the
top
100
unmatched
non-filtered
models
that
we
examine
in
table
for
example
the
gender
coefficients
from
our
top
models
are
shown
in
table
we
observe
that
once
the
sample
sizes
are
sufficiently
large
50
the
gender
coefficients
become
uniformly
significant
and
negative
taken
together
these
models
demonstrate
that
our
results
are
robust
to
under
wide
variety
of
conditions
detailed
results
for
our
top
1000
unmatched
and
matched
models
are
available
in
the
supplementary
materials
group
fairness
next
we
investigate
whether
the
resume
search
engines
exhibit
group
fairness
with
respect
to
inferred
gender
to
be
group
fair
the
ranking
algorithms
should
assign
similar
distribution
of
ranks
to
masculine
and
feminine
candidates
20
95
94
paper
651
100
recall
of
women
job
title
accountant
auditor
bartender
business
dev
manager
call
center
director
cashier
casino
manager
concierge
corrections
officer
customer
service
electrical
engineer
elevator
technician
financial
analyst
human
res
specialist
janitor
laborer
mail
carrier
manufacturing
engineer
marketing
manager
mechanical
engineer
network
engineer
occupational
therapist
payroll
specialist
personal
trainer
pharmacist
physical
therapist
real
estate
agent
registered
nurse
retail
sales
speech
pathologist
software
engineer
tax
manager
taxi
driver
technical
recruiter
truck
driver
overall
percentage
total
chi
2018
april
21
26
2018
montréal
qc
canada
less
men
than
expected
80
60
40
less
women
than
expected
20
20
40
60
of
search
result
list
80
100
figure
recall
of
women
when
searching
for
software
engineer
on
indeed
the
20
lines
correspond
to
different
cities
metrics
to
examine
group
fairness
we
use
two
metrics
first
we
use
the
mann-whitney
m-w
test
to
compare
the
distribution
of
ranks
for
men
and
women
in
given
list
of
search
results
16
the
m-w
test
is
nonparametric
test
of
the
null
hypothesis
that
there
is
no
tendency
for
ranks
of
one
class
to
be
significantly
different
from
the
other
we
omit
search
result
lists
where
the
number
of
feminine
or
masculine
candidates
is
less
than
20
as
the
result
of
m-w
are
not
reliable
when
samples
are
this
small
out
of
35
20
700
samples
on
each
hiring
website
648
421
and
181
are
suitable
for
analysis
on
indeed
monster
and
careerbuilder
respectively
we
also
remove
the
of
candidates
with
uncertain
gender
probability
of
being
masculine
0.2
and
0.8
table
shows
the
results
of
the
m-w
tests
on
our
search
result
samples
each
cell
in
the
columns
shows
the
percentage
of
samples
across
cities
where
the
m-w
test
was
significant
at
0.05
for
given
job
title
and
hiring
website
we
adjust
each
percentage
downward
by
to
correct
for
multiple
hypothesis
testing
we
assume
that
all
tests
are
independent
and
that
there
is
uniform
false
positive
rate
this
correction
approach
is
more
appropriate
for
our
scenario
than
bonferroni
correction
since
we
are
interested
in
the
overall
amount
of
positive
tests
not
the
specific
cities
that
exhibit
significant
differences
the
disadvantage
of
m-w
is
that
it
does
not
tell
us
the
direction
or
magnitude
of
group
unfairness
to
answer
these
questions
we
calculate
the
area
under
recall
curves
to
generate
recall
we
iterate
from
the
first
candidate
rank
to
the
last
candidate
in
given
search
result
list
at
rank
we
calculate
tuple
xi
yi
rf
rf
where
is
the
total
number
of
candidates
in
the
list
rf
is
the
total
number
of
feminine
candidates
in
and
rf
is
the
number
of
feminine
candidates
observed
between
ranks
and
as
an
illustrative
example
figure
shows
the
recall
for
candidates
on
indeed
when
we
search
for
software
engineer
each
red
line
corresponds
to
the
search
results
in
different
city
if
women
and
men
are
evenly
distributed
in
the
list
the
resulting
line
is
along
the
diagonal
in
this
case
we
see
that
most
of
the
lines
are
below
the
diagonal
meaning
that
women
are
under-represented
in
the
search
results
relative
to
their
overall
percentage
of
the
candidate
population
finally
we
calculate
the
area
under
the
recall
curves
and
subtract
the
area
under
the
diagonal
the
difference
between
page
chi
2018
honourable
mention
feature
inferred
gender
school
ranking
employment
number
of
keywords
resume
length
job
churn
contact
information
company
name
indeed
chi
2018
april
21
26
2018
montréal
qc
canada
monster
cb
table
features
tested
in
our
controlled
resume
experiments
green
check
marks
denote
cases
where
the
feature
is
taken
into
account
by
the
ranking
algorithm
the
recall
curves
drc
exists
in
the
range
0.5
0.5
with
negative
positive
values
indicating
that
feminine
masculine
candidates
are
favored
in
the
rankings
table
shows
the
total
drc
tdrc
for
each
job
title
summed
across
cities
results
table
shows
that
overall
8.5
13.2
of
job
title
city
pairs
exhibit
significant
group
unfairness
and
that
the
aggregate
directionality
favors
masculine
candidates
in
some
of
the
significant
job
titles
accountant
physical
therapist
retail
sales
and
technical
recruiter
the
direction
of
unfairness
is
inconsistent
suggesting
that
the
unfairness
may
be
due
to
natural
variations
in
the
candidate
populations
however
in
12
of
the
significant
job
titles
highlighted
in
bold
the
direction
of
unfairness
is
consistently
in
favor
of
men
the
magnitude
of
favor
towards
men
in
some
of
these
occupations
network
engineer
software
engineer
and
truck
driver
is
very
large
often
0.5
the
consistent
directionality
of
unfairness
in
these
job
titles
suggests
that
the
underlying
cause
is
structural
the
only
job
title
that
is
significant
and
approaches
uniform
unfairness
in
favor
of
women
is
bartender
but
the
magnitude
of
unfairness
is
relatively
small
direct
discrimination
and
hidden
features
up
to
now
our
analysis
has
focused
on
candidate
features
that
are
directly
observable
in
the
search
results
however
there
is
an
element
of
each
candidates
profile
that
we
cannot
observe
on
monster
and
careerbuilder
but
that
may
be
taken
into
account
by
the
ranking
algorithm
resumes
for
example
monster
and
careerbuilder
ask
candidates
to
enter
their
education
level
into
their
profile
but
actual
almae
matres
are
likely
stated
in
each
candidate
resume
the
ranking
algorithm
could
parse
this
additional
information
from
the
pdf-format
resume
and
use
it
when
ranking
candidates
parsing
resumes
makes
sense
from
design
standpoint
in
that
it
allows
the
websites
to
collect
detailed
information
about
candidates
without
having
to
ask
them
to
enter
it
manually
which
can
be
tedious
to
test
if
resume
content
influences
ranking
we
conducted
controlled
experiments
using
resumes
created
and
uploaded
by
us
we
create
two
user
accounts
and
in
that
temporal
order
with
identical
profile
information
and
resumes
we
then
verify
that
the
two
accounts
appear
directly
adjacent
in
search
results
in
the
order
next
we
delete
the
old
resumes
upload
two
new
resumes
starting
with
that
differ
by
exactly
one
feature
then
query
for
our
users
again
in
each
the
time
delay
between
uploading
resume
and
its
inclusion
in
search
results
varies
between
hours
so
we
query
periodically
paper
651
treatment
we
assign
the
stronger
value
for
the
feature
attended
an
ivy
league
school
while
attended
community
college
if
user
appears
before
user
it
means
the
treatment
variable
in
the
resumes
has
flipped
the
rank
ordering
thus
revealing
that
the
algorithm
takes
that
particular
resume
feature
into
account
we
repeat
this
process
on
all
three
hiring
websites
with
the
different
treatment
features
shown
in
table
furthermore
we
repeated
each
treatment
three
times
to
ensure
that
the
observed
result
was
consistent
table
shows
the
results
of
our
controlled
experiments
crucially
our
inferred
gender
treatment
did
not
influence
the
order
of
our
users
which
confirms
that
none
of
the
hiring
websites
engage
in
direct
discrimination
with
respect
to
inferred
gender
only
three
features
influenced
the
ranking
algorithms
monster
algorithm
ranks
users
by
the
strength
of
their
alma
mater
and
whether
they
are
currently
employed
while
careerbuilder
algorithm
takes
employment
status
and
frequency
of
job
changes
into
account
all
of
these
findings
were
consistent
across
repeated
trials
which
is
expected
if
we
assume
that
these
websites
use
deterministic
ranking
algorithms
hiring
websites
may
extract
features
from
limitations
resumes
that
are
not
covered
by
our
treatments
in
table
we
manually
examined
dozens
of
real
resumes
from
the
hiring
websites
to
inform
our
selection
of
features
and
were
surprised
by
how
consistent
the
types
of
informational
content
were
across
resumes
we
hypothesize
that
freely
available
templates
and
optimizers
may
encourage
resume
homogeneity
across
online
hiring
services
thus
we
believe
that
our
treatments
cover
the
most
salient
features
of
typical
resumes
concluding
discussion
in
this
study
we
examine
gender
inequality
on
the
resume
search
engines
provided
by
indeed
monster
and
careerbuilder
we
crawled
search
results
for
35
job
titles
across
20
cities
these
contain
data
on
855k
candidates
using
statistical
tests
we
examine
two
types
of
algorithmic
fairness
with
respect
to
inferred
gender
individual
fairness
we
find
statistically
significant
0.05
negative
correlations
between
rank
and
inferred
gender
in
our
dataset
this
means
that
even
when
controlling
for
all
other
visible
candidate
features
there
is
slight
penalty
against
feminine
candidates
these
results
are
robust
under
variety
of
conditions
however
the
effect
size
is
small
only
by
rank
30
50
depending
on
the
website
is
the
gender
effect
large
enough
that
masculine
candidates
receive
substantive
increase
in
rank
group
fairness
we
observe
that
8.5
13.2
of
job
title
city
pairs
show
statistically
significant
group
unfairness
in
12
of
35
job
titles
the
unfairness
benefits
men
using
controlled
experiments
we
find
that
none
of
the
hiring
sites
are
directly
discriminatory
with
respect
to
inferred
gender
this
concurs
with
the
design
of
these
websites
which
do
not
ask
candidates
to
input
their
gender
however
we
see
that
until
our
users
appear
thus
it
is
unlikely
that
our
users
receive
many
clicks
from
recruiters
before
we
measure
their
ranks
this
is
important
because
clicks
may
be
feature
used
to
rank
candidates
https://www.jobscan.co/
page
chi
2018
honourable
mention
other
hidden
features
unemployment
and
alma
mater
are
taken
into
account
why
is
there
unfairness
one
unsatisfying
aspect
of
our
study
is
that
we
are
not
able
to
say
definitively
why
there
is
unfairness
with
respect
to
inferred
gender
on
these
resume
search
engines
this
is
common
criticism
of
algorithm
audits
that
rely
on
observational
data
37
we
hypothesize
that
there
are
two
potential
causes
for
the
slight
individual
unfairness
we
observe
first
the
ranking
algorithms
may
rely
on
hidden
feature
that
is
extracted
from
resumes
that
is
weakly
correlated
with
gender
our
controlled
experiments
rule
out
direct
discrimination
as
cause
and
our
regressions
control
for
indirect
discrimination
that
might
be
caused
by
visible
candidate
features
unfortunately
we
cannot
isolate
the
hidden
feature
that
may
be
causing
individual
unfairness
because
we
do
not
have
access
to
all
candidate
resumes
on
monster
and
careerbuilder
second
possibility
is
that
small
amounts
of
individual
unfairness
occur
because
the
algorithms
adjust
the
rank
of
candidates
based
on
the
volume
of
clicks
they
receive
from
recruiters
so-called
learning-to-rank
approach
40
19
45
if
recruiters
are
biased
they
may
generate
more
clicks
on
candidates
with
desirable
demographic
traits
testing
this
hypothesis
is
challenging
since
it
would
require
uploading
many
resumes
with
varying
features
and
then
waiting
weeks
in
the
hope
of
collecting
sufficient
clicks
to
trigger
changes
in
rank
with
respect
to
group
unfairness
the
likely
cause
is
structural
inequality
it
is
unlikely
to
be
coincidence
that
the
job
titles
where
we
observe
the
largest
magnitudes
of
group
unfairness
include
technical
professions
software
engineer
truck
driver
and
laborer
all
professions
that
are
historically
gendered
thus
it
is
fair
to
say
that
the
ranking
algorithms
on
these
hiring
sites
are
not
increasing
group
unfairness
on
top
of
what
already
exists
at
large
in
society
rather
they
reflect
an
unfortunate
status
quo
that
persists
in
many
professions
interpretation
are
the
ranking
algorithms
on
these
hiring
sites
fair
and
if
so
who
is
responsible
for
addressing
the
situation
answering
these
complex
questions
requires
grappling
with
the
desired
goals
of
fairness
and
the
role
of
companies
in
society
when
john
kennedy
introduced
the
modern
usage
of
the
term
affirmative
action
he
asked
companies
to
take
affirmative
action
to
ensure
that
applicants
are
employed
without
regard
to
their
race
creed
color
or
national
origin
jfk
definition
of
affirmative
action
was
quite
conservative
in
that
he
was
calling
for
equal
treatment
for
equivalent
candidates
this
roughly
corresponds
to
individual
fairness
if
we
judge
these
three
resume
search
engines
output
by
jfk
definition
of
affirmative
action
then
we
conclude
that
they
are
largely
successful
although
we
do
observe
slightly
negative
gender
coefficients
in
our
models
the
effect
sizes
are
sufficiently
small
that
recruiters
would
need
to
browse
50
or
more
candidates
before
the
gender
effect
substantively
impacted
the
search
results
it
is
unclear
whether
recruiters
browse
this
deeply
into
results
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
however
there
are
other
interpretations
of
the
meaning
of
affirmative
action
lyndon
johnson
famously
advocated
for
active
recruitment
which
encouraged
companies
to
make
their
workforces
more
reflective
of
the
overall
population
by
actively
hiring
underrepresented
workers
as
noted
by
dwork
et
al
this
roughly
corresponds
to
group
fairness
20
if
lbj
definition
of
affirmative
action
is
our
barometer
then
we
conclude
that
these
three
resume
search
engines
are
less
successful
it
is
heartening
to
observe
that
the
search
results
for
the
majority
of
job
titles
we
investigate
are
group
fair
yet
there
are
professions
like
software
engineer
where
large
group
unfairness
to
women
remains
although
the
ranking
algorithms
are
not
responsible
for
creating
this
group
unfairness
we
have
to
consider
whether
it
is
appropriate
for
these
algorithms
to
perpetuate
widely
criticized
structural
inequalities
72
to
address
structural
inequalities
hiring
websites
will
need
to
adopt
ranking
algorithms
that
are
group
fair
by
design
this
would
ensure
that
recruiters
see
the
strongest
men
and
women
at
rate
that
reflects
the
underlying
population
distribution
admittedly
having
group
fair
search
results
does
not
necessarily
mean
that
candidates
from
the
minority
class
will
be
hired
more
frequently
but
it
does
correct
the
fundamental
problem
that
unseen
candidates
have
no
chance
of
being
interviewed
or
hired
furthermore
presenting
search
results
that
are
strictly
representative
of
population
demographics
may
have
the
positive
effect
of
combating
entrenched
stereotypes
that
discourage
hiring
in
some
professions
55
limitations
and
future
work
the
primary
limitation
of
our
work
is
that
we
do
not
know
exactly
how
recruiters
interact
with
resume
search
engines
it
is
safe
to
assume
that
well-documented
psychological
biases
ordering
effects
do
impact
how
recruiters
use
these
systems
30
32
79
17
56
however
there
are
differences
between
the
goals
of
recruiters
and
search
engine
users
in
general
that
may
cause
behavioral
changes
for
example
search
engine
users
often
want
single
result
while
recruiter
may
want
to
interview
multiple
candidates
we
advocate
for
future
user
studies
of
recruiters
epstein
et
al
and
kay
et
al
both
present
methodological
frameworks
that
could
be
adapted
to
quantify
recruiters
behavior
when
they
interact
with
resume
search
engines
as
well
as
the
causal
impacts
of
showing
unfair
search
results
22
55
furthermore
additional
work
is
needed
to
study
other
hiring
websites
especially
linkedin
and
glassdoor
studying
linkedin
will
be
complicated
due
to
the
monetary
cost
the
complexity
introduced
by
their
social
graph
and
their
history
of
litigation
against
third-parties
that
crawl
their
data
62
acknowledgments
we
thank
christoph
riedl
for
methodological
suggestions
vetle
torvik
for
help
inferring
genders
and
all
of
the
anonymous
reviewers
for
their
insightful
suggestions
this
research
was
supported
in
part
by
nsf
grants
iis-1408345
and
iis-1553088
any
opinions
findings
conclusions
or
recommendations
expressed
herein
are
those
of
the
authors
and
do
not
necessarily
reflect
the
views
of
the
nsf
page
10
chi
2018
honourable
mention
references
alexa
2016
ranking
of
emplyment
sites
alexa
2016
http://www.alexa.com/topsites/category/
business
employment
ahmed
allam
peter
johannes
schulz
and
kent
nakamoto
2014
the
impact
of
search
engine
selection
and
sorting
criteria
on
vaccination
beliefs
and
attitudes
two
experiments
manipulating
google
output
journal
of
medical
internet
research
16
2014
e100
terry
anderson
2004
the
pursuit
of
fairness
history
of
affirmative
action
oxford
university
press
solon
barocas
and
andrew
selbst
2016
big
data
disparate
impact
104
california
law
review
671
2016
marc
bendick
charles
jackson
and
victor
reinoso
1994
measuring
employment
discrimination
through
controlled
experiments
the
review
of
black
political
economy
23
1994
25
48
marianne
bertrand
and
sendhil
mullainathan
2004
are
emily
and
greg
more
employable
than
lakisha
and
jamal
field
experiment
on
labor
market
discrimination
the
american
economic
review
94
2004
991
1013
joshua
brustein
2014
linkedin
sues
unknown
hackers
in
an
attempt
to
find
out
who
they
are
bloomberg
businessweek
january
2014
https://www.bloomberg.com/news/articles/201401-08/linkedin-sues-unknown-hackers-in-anattempt-to-find-out-who-they-are.
quoctrung
bui
2014
the
most
common
jobs
for
the
rich
middle
class
and
poor
npr
october
2014
http://n.pr/2isviu0.
toon
calders
faisal
kamiran
and
mykola
pechenizkiy
2009
building
classifiers
with
independency
constraints
in
proc
of
icdm
workshops
10
toon
calders
asad
karim
faisal
kamiran
wesam
ali
and
xiangliang
zhang
2013
controlling
attribute
effect
in
linear
regression
in
proc
of
icdm
11
toon
calders
and
sicco
verwer
2010
three
naive
bayes
approaches
for
discrimination-free
classification
data
mining
and
knowledge
discovery
21
2010
277
292
12
aylin
caliskan
joanna
bryson
and
arvind
narayanan
2017
semantics
derived
automatically
from
language
corpora
contain
human-like
biases
science
356
6334
april
2017
183
186
13
careerbuilder
2016
careerbuilder
about
us
careerbuilder
2016
http://www.careerbuilder.com/share/aboutus/.
14
ceb
2011
most
common
job
titles
posted
online
ceb
august
2011
http://bit.ly/2isyb9h.
15
le
chen
alan
mislove
and
christo
wilson
2016
an
empirical
analysis
of
algorithmic
pricing
on
amazon
marketplace
in
proc
of
www
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
16
gregory
corder
and
dale
foreman
2014
nonparametric
statistics
step-by-step
approach
john
wiley
sons
17
nick
craswell
onno
zoeter
michael
taylor
and
bill
ramsey
2008
an
experimental
comparison
of
click
position-bias
models
in
proc
of
wsdm
18
amit
datta
michael
carl
tschantz
and
anupam
datta
2015
automated
experiments
on
ad
privacy
settings
tale
of
opacity
choice
and
discrimination
in
proc
of
pets
19
clebson
de
sá
marcos
gonçalves
daniel
sousa
and
thiago
salles
2016
generalized
broof-l2r
general
framework
for
learning
to
rank
based
on
boosting
and
random
forests
in
proc
of
sigir
20
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
and
richard
zemel
2012
fairness
through
awareness
in
proc
of
itcs
21
benjamin
edelman
michael
luca
and
dan
svirsky
2015
racial
discrimination
in
the
sharing
economy
evidence
from
field
experiment
2015
http://ssrn.com/abstract=2701902.
22
robert
epstein
and
ronald
robertson
2015
the
search
engine
manipulation
effect
seme
and
its
possible
impact
on
the
outcomes
of
elections
proceedings
of
the
national
academy
of
sciences
112
33
2015
e4512
e4521
23
motahhare
eslami
amirhossein
aleyasen
karrie
karahalios
kevin
hamilton
and
christian
sandvig
2015
feedvis
path
for
exploring
news
feed
curation
algorithms
in
proc
of
cscw
companion
24
michael
feldman
sorelle
friedler
john
moeller
carlos
scheidegger
and
suresh
venkatasubramanian
2015
certifying
and
removing
disparate
impact
in
proc
of
kdd
25
lisa
finkelstein
michael
burke
and
manbury
raju
1995
age
discrimination
in
simulated
employment
contexts
an
integrative
analysis
journal
of
applied
psychology
80
1995
652
26
kevin
fiscella
and
allen
fremont
2006
use
of
geocoding
and
surname
analysis
to
estimate
race
and
ethnicity
health
serv
res
27
sorelle
friedler
carlos
scheidegger
and
suresh
venkatasubramanian
2016
on
the
im
possibility
of
fairness
corr
abs
1609.07236
2016
28
yanbo
ge
christopher
knittel
don
mackenzie
and
stephen
zoepf
2016
racial
and
gender
discrimination
in
transportation
network
companies
working
paper
22776
national
bureau
of
economic
research
29
drew
gouvier
sara
sytsma-jordan
and
stephen
mayville
2003
patterns
of
discrimination
in
hiring
job
applicants
with
disabilities
the
role
of
disability
type
job
complexity
and
public
contact
rehabilitation
psychology
48
2003
175
page
11
chi
2018
honourable
mention
chi
2018
april
21
26
2018
montréal
qc
canada
30
laura
granka
thorsten
joachims
and
geri
gay
2004
eye-tracking
analysis
of
user
behavior
in
www
search
in
proc
of
sigir
44
kalervo
järvelin
and
jaana
kekäläinen
2002
cumulated
gain-based
evaluation
of
ir
techniques
acm
trans
inf
syst
oct
2002
31
steven
greenhouse
2013
supreme
court
raises
bar
to
prove
job
discrimination
new
york
times
june
2013
http://nyti.ms/2ppug3w.
45
shan
jiang
yuening
hu
changsung
kang
tim
daly
jr
dawei
yin
yi
chang
and
chengxiang
zhai
2016
learning
query
and
document
relevance
from
web-scale
click
graph
in
proc
of
sigir
32
zhiwei
guan
and
edward
cutrell
2007
an
eye
tracking
study
of
the
effect
of
target
rank
on
web
search
in
proc
of
chi
33
saikat
guha
bin
cheng
and
paul
francis
2010
challenges
in
measuring
online
advertising
systems
in
proc
of
imc
34
sara
hajian
and
josep
domingo-ferrer
2013
methodology
for
direct
and
indirect
discrimination
prevention
in
data
mining
ieee
transactions
on
knowledge
and
data
engineering
25
2013
1445
1459
35
anikó
hannák
piotr
sapiez
yn
ski
arash
molavi
kakhki
balachander
krishnamurthy
david
lazer
alan
mislove
and
christo
wilson
2013
measuring
personalization
of
web
search
in
proc
of
www
36
anikó
hannák
gary
soeller
david
lazer
alan
mislove
and
christo
wilson
2014
measuring
price
discrimination
and
steering
on
e-commerce
web
sites
in
proc
of
imc
37
anikó
hannák
claudia
wagner
david
garcia
alan
mislove
markus
strohmaier
and
christo
wilson
2017
bias
in
online
freelance
marketplaces
evidence
from
taskrabbit
and
fiverr
in
proc
of
cscw
38
daniel
ho
kosuke
imai
gary
king
and
elizabeth
stuart
2007
matching
as
nonparametric
preprocessing
for
reducing
model
dependence
in
parametric
causal
inference
political
analysis
15
2007
199
236
39
daniel
ho
kosuke
imai
gary
king
and
elizabeth
stuart
2011
matchit
nonparametric
preprocessing
for
parametric
causal
inference
journal
of
statistical
software
42
2011
28
40
muhammad
ibrahim
and
mark
carman
2016
comparing
pointwise
and
listwise
objective
functions
for
random-forest-based
learning-to-rank
acm
trans
inf
syst
34
2016
20
20
38
41
indeed
2016a
how
do
you
rank
search
results
indeed
2016
http://support.indeed.com/hc/en-us/articles/
204488980
how-do-you-rank-search-results
42
indeed
2016b
indeed
hits
record
200
million
unique
visitors
indeed
2016
http://blog.indeed.com/2016/02/08/indeed-200million-unique-visitors/.
43
kalervo
järvelin
and
jaana
kekäläinen
2000
ir
evaluation
methods
for
retrieving
highly
relevant
documents
in
proc
of
sigir
paper
651
46
thorsten
joachims
adith
swaminathan
and
tobias
schnabel
2017
unbiased
learning-to-rank
with
biased
feedback
in
proc
of
wsdm
47
isaac
johnson
connor
mcmahon
johannes
schöning
and
brent
hecht
2017
the
effect
of
population
and
structural
biases
on
social
media-based
algorithms
case
study
in
geolocation
inference
across
the
urban-rural
spectrum
in
proc
of
chi
48
faisal
kamiran
and
toon
calders
2009
classifying
without
discriminating
in
proc
of
conference
on
computer
control
and
communication
49
faisal
kamiran
and
toon
calders
2010
classification
with
no
discrimination
by
preferential
sampling
in
proc
of
machine
learning
conference
of
belgium
and
the
netherlands
50
faisal
kamiran
toon
calders
and
mykola
pechenizkiy
2010
discrimination
aware
decision
tree
learning
in
proc
of
icdm
51
faisal
kamiran
asad
karim
and
xiangliang
zhang
2012
decision
theory
for
discrimination-aware
classification
in
proc
of
icdm
52
toshihiro
kamishima
shotaro
akaho
hideki
asoh
and
jun
sakuma
2012
fairness-aware
classifier
with
prejudice
remover
regularizer
in
proc
of
ecml
pkdd
53
toshihiro
kamishima
shotaro
akaho
and
jun
sakuma
2011
fairness-aware
learning
through
regularization
approach
in
proc
of
icdm
workshops
54
fariba
karimi
claudia
wagner
florian
lemmerich
mohsen
jadidi
and
markus
strohmaier
2016
inferring
gender
from
names
on
the
web
comparative
evaluation
of
gender
detection
methods
www
16
companion
55
matthew
kay
cynthia
matuszek
and
sean
munson
2015
unequal
representation
and
gender
stereotypes
in
image
search
results
for
occupations
in
proc
of
chi
56
mark
keane
maeve
brien
and
barry
smyth
2008
are
people
biased
in
their
use
of
search
engines
commun
acm
51
feb
2008
49
52
57
pauline
kim
2017
data-driven
discrimination
at
work
william
mary
law
review
58
2017
58
chloe
kliman-silver
anikó
hannák
david
lazer
christo
wilson
and
alan
mislove
2015
location
location
location
the
impact
of
geolocation
on
web
search
personalization
in
proc
of
imc
page
12
chi
2018
honourable
mention
59
juhi
kulshrestha
motahhare
eslami
johnnatan
messias
muhammad
bilal
zafar
saptarshi
ghosh
krishna
gummadi
and
karrie
karahalios
2017
quantifying
search
bias
investigating
sources
of
bias
for
political
searches
in
social
media
in
proc
of
cscw
60
shyong
tony
lam
anuradha
uduwage
zhenhua
dong
shilad
sen
david
musicant
loren
terveen
and
john
riedl
2011
wp
clubhouse
an
exploration
of
wikipedia
gender
imbalance
in
proc
of
wikisym
61
mathias
lécuyer
guillaume
ducoffe
francis
lan
andrei
papancea
theofilos
petsios
riley
spahn
augustin
chaintreau
and
roxana
geambasu
2014
xray
enhancing
the
web
transparency
with
differential
correlation
in
proc
of
usenix
security
symposium
62
thomas
lee
2017
linkedin
hiq
spat
presents
big
questions
for
freedom
innovation
san
francisco
chronicle
july
2017
http://www.sfchronicle.com/
business
article
linkedin-hiq-spat-presentsbig-questions-for-11274133
php
63
lisa
madigan
2017
madigan
probes
national
job
search
sites
over
potential
age
discrimination
illinois
attorney
general
press
release
march
2017
http://www.illinoisattorneygeneral.gov/
pressroom
2017_03
20170302
html
64
wendy
liu
and
derek
ruths
2013
what
in
name
using
first
names
as
features
for
gender
inference
in
twitter
in
aaai
spring
symposium
series
65
ramona
ludolph
ahmed
allam
and
peter
schulz
2016
manipulating
google
knowledge
graph
box
to
counter
biased
information
processing
during
an
online
search
on
vaccination
application
of
technological
debiasing
strategy
journal
of
medical
internet
research
18
2016
66
binh
thanh
luong
salvatore
ruggieri
and
franco
turini
2011
k-nn
as
an
implementation
of
situation
testing
for
discrimination
discovery
and
prevention
in
proc
of
kdd
67
gabriel
magno
camila
souza
araujo
wagner
meira
jr
and
virgílio
almeida
2016
stereotypes
in
search
engine
answers
local
or
global
corr
abs
1609.05413
2016
68
mediative
2015
keeping
an
eye
on
google
eye
tracking
serps
through
the
years
mediative
2015
http://www.mediative.com/eye-tracking-googlethrough-the-years/.
69
jakub
mikians
lászló
gyarmati
vijay
erramilli
and
nikolaos
laoutaris
2012
detecting
price
and
search
discrimination
on
the
internet
in
proc
of
hotnets
70
jakub
mikians
lászló
gyarmati
vijay
erramilli
and
nikolaos
laoutaris
2013
crowd-assisted
search
for
price
discrimination
in
e-commerce
first
results
in
proc
of
acm
conext
paper
651
chi
2018
april
21
26
2018
montréal
qc
canada
71
monster
2016
monster
about
us
monster
2016
http://www.monster.com/about/.
72
liza
mundy
2017
why
is
silicon
valley
so
awful
to
women
the
atlantic
april
2017
https://www.
theatlantic
com
magazine
archive
2017
04
whyis-silicon-valley-so-awful-to-women
517788
73
alamir
novin
and
eric
meyers
2017
making
sense
of
conflicting
science
information
exploring
bias
in
the
search
engine
result
page
in
proceedings
of
the
2017
conference
on
human
information
interaction
and
retrieval
acm
175
184
74
devah
pager
2008
marked
race
crime
and
finding
work
in
an
era
of
mass
incarceration
university
of
chicago
press
75
devah
pager
and
hana
shepherd
2008
the
sociology
of
discrimination
racial
discrimination
in
employment
housing
credit
and
consumer
markets
annual
review
of
sociology
34
2008
181
76
dino
pedreshi
salvatore
ruggieri
and
franco
turini
2008
discrimination-aware
data
mining
in
proc
of
kdd
77
joseph
reagle
and
lauren
rhue
2011
gender
bias
in
wikipedia
and
britannica
international
journal
of
communication
2011
1138
1158
78
peter
riach
and
judith
rich
1991
measuring
discrimination
by
direct
experimental
methods
seeking
gunsmoke
journal
of
post
keynesian
economics
14
1991
143
150
79
matthew
richardson
2007
predicting
clicks
estimating
the
click-through
rate
for
new
ads
in
proc
of
www
80
christian
sandvig
kevin
hamilton
karrie
karahalios
and
cedric
langbort
2014
auditing
algorithms
research
methods
for
detecting
discrimination
on
internet
platforms
in
proceedings
of
data
and
discrimination
converting
critical
concerns
into
productive
inquiry
preconference
at
the
64th
annual
meeting
of
the
international
communication
association
81
jennifer
skeem
and
christopher
lowenkamp
2016
risk
race
recidivism
predictive
bias
and
disparate
impact
2016
https://ssrn.com/abstract=2687339.
82
brittany
smith
mamta
singh
and
vetle
torvik
2013
search
engine
approach
to
estimating
temporal
changes
in
gender
orientation
of
first
names
in
proceedings
of
the
13th
acm
ieee-cs
joint
conference
on
digital
libraries
jcdl
13
199
208
doi
http://dx.doi.org/10.1145/2467696.2467720
83
social
security
administration
2016
baby
names
from
social
security
card
applications-national
level
data
data
gov
2016
https://catalog.data.gov/dataset/baby-namesfrom-social-security-card-applicationsnational-level-data.
page
13
chi
2018
honourable
mention
chi
2018
april
21
26
2018
montréal
qc
canada
84
gary
soeller
karrie
karahalios
christian
sandvig
and
christo
wilson
2016
mapwatch
detecting
and
monitoring
international
border
personalization
on
online
maps
in
proc
of
www
90
sander
van
der
linden
anthony
leiserowitz
seth
rosenthal
and
edward
maibach
2017
inoculating
the
public
against
misinformation
about
climate
change
global
challenges
2017
85
latanya
sweeney
2013
discrimination
in
online
ad
delivery
2013
http://ssrn.com/abstract=2208240.
91
claudia
wagner
david
garcia
mohsen
jadidi
and
markus
strohmaier
2015
it
man
wikipedia
assessing
gender
inequality
in
an
online
encyclopedia
in
proc
of
icwsm
86
cong
tang
keith
ross
nitesh
saxena
and
ruichuan
chen
2011
what
in
name
study
of
names
gender
inference
and
gender
behavior
in
facebook
in
dasfaa
workshops
87
jacob
thebault-spieker
loren
terveen
and
brent
hecht
2015
avoiding
the
south
side
and
the
suburbs
the
geography
of
mobile
crowdsourcing
markets
in
proc
of
cscw
92
doris
weichselbaumer
2003
sexual
orientation
discrimination
in
hiring
labour
economics
10
2003
629
642
93
david
word
charles
coleman
robert
nunziata
and
robert
kominski
2000
demographic
aspects
of
surnames
from
census
2000
census
gov
2000
https://www2.census.gov/topics/genealogy/
2000surnames
surnames
pdf
88
vetle
torvik
and
sneha
agarwal
2016
ethnea
an
instance-based
ethnicity
classifier
based
on
geo-coded
author
names
in
large-scale
bibliographic
database
2016
http://hdl.handle.net/2142/88927
international
symposium
on
science
of
science
march
22
23
2016
library
of
congress
washington
dc
usa
94
ke
yang
and
julia
stoyanovich
2017
measuring
fairness
in
ranked
outputs
in
proc
of
ssdbm
89
margery
austin
turner
michael
fix
and
raymond
struyk
1991
opportunities
denied
opportunities
diminished
racial
discrimination
in
hiring
the
urban
insitute
96
indre
zliobaite
faisal
kamiran
and
toon
calders
2011
handling
conditional
discrimination
in
proc
of
icdm
paper
651
95
richard
zemel
yu
wu
kevin
swersky
toniann
pitassi
and
cynthia
dwork
2013
learning
fair
representations
in
proc
of
icml
page
14