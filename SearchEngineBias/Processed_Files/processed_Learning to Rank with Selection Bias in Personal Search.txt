learning
rank
selection
bias
personal
search
xuanhui
wang
michael
bendersky
donald
metzler
marc
najork
google
inc
mountain
view
ca
94043
xuanhui
bemike
metzler
najork
google
com
abstract
data
will
negatively
impact
downstream
applications
21
result
great
deal
research
extracting
reliable
signals
click-through
data
10
previous
work
typically
focused
click
modeling
estimate
relevance
individual
query-document
pairs
instance
craswell
et
al
11
proposed
cascade
model
conditional
probability
click
document
given
position
predicated
marginal
probability
document
relevant
query
marginal
probabilities
documents
positions
nonrelevant
query
order
estimate
marginals
click
models
often
assume
access
large
quantities
click
data
document
given
query
11
15
models
generally
proven
successful
context
web
search
assumption
holds
however
less
clear
can
applied
search
scenarios
click
data
highly
sparse
one
scenario
focus
paper
personal
search
personal
search
important
well
studied
information
retrieval
task
applications
email
search
desktop
search
13
recently
on-device
search
23
one
important
difference
personal
web
search
personal
search
scenario
user
access
private
document
corpus
emails
files
mobile
application
data
therefore
vast
majority
existing
click
models
learn
click
probabilities
large
quantities
clicks
individual
querydocument
pairs
applicable
personal
search
scenario
another
important
challenge
context
personal
search
collection
explicit
relevance
judgments
collection
trec-like
document
relevance
judgments
third
party
raters
commonly
used
information
retrieval
tasks
letor
data
set
29
difficult
obtain
due
privacy
restrictions
addition
since
user
will
unique
set
information
needs
documents
evolve
time
new
emails
arrive
every
day
explicit
relevance
judgments
may
prohibitively
costly
maintain
therefore
development
ranking
models
general
specifically
learning-to-rank
models
26
utilize
click-through
data
noisy
biased
source
relevance
data
becomes
essential
personal
search
paper
study
problem
learning-torank
click
data
personal
search
different
majority
past
click
modeling
work
whose
focus
estimating
relevance
individual
query-document
pairs
propose
novel
selection
bias
problem
context
learning-to-rank
click
data
basic
idea
click-through
data
proven
critical
resource
improving
search
ranking
quality
though
large
amount
click
data
can
easily
collected
search
engines
various
biases
make
difficult
fully
leverage
type
data
past
many
click
models
proposed
successfully
used
estimate
relevance
individual
query-document
pairs
context
web
search
click
models
typically
require
large
quantity
clicks
individual
pair
makes
difficult
apply
systems
click
data
highly
sparse
due
personalized
corpora
information
needs
personal
search
paper
study
problem
leverage
sparse
click
data
personal
search
introduce
novel
selection
bias
problem
address
learning-to-rank
framework
paper
proposes
bias
estimation
methods
including
novel
query-dependent
one
captures
queries
similar
results
can
successfully
deal
sparse
data
empirically
demonstrate
learning-to-rank
accounts
query-dependent
selection
bias
yields
significant
improvements
search
effectiveness
online
experiments
one
world
largest
personal
search
engines
keywords
personal
search
selection
bias
learning-to-rank
introduction
past
several
years
click-through
data
become
indispensable
resource
online
information
retrieval
services
provides
natural
abundant
continuously
renewable
source
user
feedback
however
despite
tremendous
value
click-through
data
inherently
biased
noisy
previous
research
shows
order
reliably
leverage
click-through
data
one
account
multiple
sources
bias
including
position
bias
22
presentation
bias
33
trust
bias
28
therefore
directly
using
click-through
data
may
result
noisy
biased
training
permission
make
digital
hard
copies
part
work
personal
classroom
use
granted
without
fee
provided
copies
made
distributed
profit
commercial
advantage
copies
bear
notice
full
citation
first
page
copyrights
third-party
components
work
must
honored
uses
contact
owner
author
sigir
16
july
17
21
2016
pisa
italy
2016
copyright
held
owner
author
acm
isbn
978
4503
4069
16
07
doi
http://dx.doi.org/10.1145/2911451.2911537
115
selection
bias
problem
queries
under-sampled
different
extents
thus
biased
click
data
collected
learn
ranking
functions
propose
several
methods
estimate
bias
begin
global
bias
model
refine
segmented
bias
model
show
segmented
bias
model
gives
rise
general
framework
defines
query-dependent
bias
every
query
associated
result
set
can
associated
potentially
different
bias
model
general
query-dependent
framework
especially
powerful
personal
search
scenario
allows
accurate
bias
estimation
without
explicit
access
large
number
clicks
given
query-document
pair
best
knowledge
first
study
proposes
theoretical
framework
eliminating
selection
bias
personal
search
provides
extensive
empirical
evaluation
using
large-scale
live
experiments
primary
contributions
paper
can
summarized
follows
click
models
prior
work
selection
bias
estimation
presented
paper
focus
deriving
useful
implicit
feedback
click-through
data
several
important
differences
first
majority
past
click
modeling
work
focuses
estimating
degree
relevance
query
document
main
goal
paper
study
selection
bias
problem
click
data
used
learning-to-rank
may
drift
away
true
underlying
distribution
second
existing
click
models
assume
sufficient
amount
click
data
available
query-document
pair
position
reliably
train
model
parameters
assumption
holds
web
search
setting
feasible
domains
like
desktop
search
email
search
enterprise
search
user
might
access
different
set
documents
impossible
leverage
wisdom
crowds
aggregate
clicks
across
users
third
click
models
generally
evaluated
using
perplexity
estimated
observed
clicks
15
contrast
directly
evaluate
ranking
effectiveness
methods
offline
evaluation
online
live
experiments
click
data
extensively
used
sponsored
search
main
goal
predict
click-through
rate
ads
30
36
noticeably
selection
bias
estimation
methods
related
position
bias
model
30
main
difference
use
estimated
bias
address
selection
bias
problem
bias
used
expected
ads
impressions
computing
click-through
rate
ads
using
data
multiple
positions
furthermore
also
propose
advanced
query-dependent
bias
models
tractable
even
scenarios
training
data
can
scarce
due
small
sample
sizes
low
search
volume
personal
document
collections
bias
estimation
models
rely
randomized
experimental
data
order
randomization
removes
position
biases
inherent
click
data
therefore
one
can
view
proposed
models
propensity
score
31
estimates
furthermore
randomized
data
basis
proposed
unbiased
offline
evaluator
similar
evaluation
methodologies
proposed
li
et
al
prior
work
24
25
difference
also
use
randomized
data
selection
bias
estimation
improve
ranking
functions
case
past
work
order
randomization
also
eliminates
certain
degree
selection
bias
inherent
many
information
retrieval
applications
employ
pooling
top
retrieved
results
27
previous
work
proposed
methods
avoid
selection
bias
trec-style
evaluation
settings
however
approaches
easily
extend
online
evaluation
case
addressed
work
methods
proposed
paper
can
also
viewed
novel
extension
sample
selection
bias
correction
methods
well-studied
context
regression
classification
20
34
32
online
learning-to-rank
setting
contrast
previously
proposed
learning-to-rank
models
make
explicit
assumptions
user
behavior
19
use
heuristic-based
method
document
selection
learn
selection
bias
directly
experimental
data
propose
problem
selection
bias
address
applying
learning-to-rank
click
data
propose
several
novel
bias
prediction
methods
including
query-dependent
model
need
large
quantity
click
data
given
querydocument
pair
propose
novel
unbiased
theoretically
sound
offline
evaluation
methodology
problem
verify
effectiveness
proposed
methods
context
personal
search
rigorous
offline
experiments
large-scale
online
experiments
remainder
paper
organized
follows
section
review
previous
related
work
problem
formally
defined
section
different
methods
quantifying
selection
bias
described
section
present
extensive
experimental
study
evaluation
methodology
section
finally
conclude
discuss
future
work
section
related
work
abundance
prior
work
interpreting
clicks
implicit
feedback
users
one
seminal
papers
field
joachims
et
al
22
evaluates
reliability
click-through
signals
via
user
study
overall
conclusion
study
clicks
indeed
useful
implicit
feedback
interpretation
long
certain
biases
accounted
including
trust
bias
commonly
referred
position
bias
later
work
leads
clicks
higher-ranked
results
quality
bias
click
behavior
influenced
overall
quality
ranked
list
joachims
et
al
22
also
proposed
five
simple
strategies
eliminate
biases
including
click
skip
strategy
gave
rise
well-known
cascade
model
11
later
work
also
introduced
variety
click
modeling
techniques
including
among
many
others
dynamic
bayesian
network
click
model
click
chain
model
17
session
utility
model
14
whole
page
click
model
multiple
browsing
model
15
general
click
model
36
recent
survey
chuklin
et
al
10
provides
good
overview
latest
advances
field
problem
formulation
section
introduce
selection
bias
problem
learning-to-rank
personal
search
scenarios
begin
briefly
reviewing
general
setting
learning-to-rank
116
3.1
learning-to-rank
let
x1
xn
denote
query
string
set
result
documents
write
indicate
result
set
let
denote
probability
observing
query
based
underlying
distribution
queries
universe
possible
queries
users
can
issue
together
possible
result
combinations
goal
learning-to-rank
find
scoring
function
can
minimize
loss
function
defined
dp
q1
q2
figure
illustration
selection
bias
click
data
shaded
documents
relevant
ones
check
mark
means
document
clicked
incurred
loss
scoring
function
applied
query
let
xi
xj
denote
pairs
xi
xj
result
documents
xi
relevant
xj
example
pair-wise
loss
function
used
35
defined
max
xj
xi
example
consider
eq
clicks
query
set
xi
xj
empty
since
way
derive
preferences
pairs
documents
observation
can
generalized
list-wise
loss
functions
well
following
focus
collection
queries
clicks
use
denote
collection
xi
xj
intuition
behind
loss
function
penalize
out-of-order
pairs
ranked
practice
distribution
queries
unknown
empirical
loss
defined
uniformly
random
sample
used
objective
function
learning
lu
observation
collection
queries
biased
formally
let
denote
probability
mass
query
learning-to-rank
algorithms
differ
loss
function
defined
26
generally
state-of-the-art
loss
functions
pair-wise
list-wise
practically
pairwise
loss
functions
tend
efficient
training
widely
adopted
large
search
engines
35
thus
rest
paper
make
assumption
pair-wise
loss
function
eq
used
however
important
point
methods
described
paper
general
enough
applied
list-wise
loss
functions
well
use
example
better
explain
observation
figure
two
queries
q1
q2
equal
probability
issued
users
q1
q2
equal
probability
relevant
document
q1
position
clicked
every
time
query
issued
contrary
relevant
document
q2
position
clicked
half
time
query
issued
thus
q2
12
q1
helps
illustrate
selection
bias
may
arise
click
data
problem
illustrated
example
rooted
commonly
known
position
bias
confirmed
eye
tracking
studies
22
30
well
found
users
less
likely
see
hence
click
lower-ranked
documents
3.2
3.3
selection
bias
problem
inverse
propensity
weighting
selection
bias
widely
known
problem
many
scientific
communities
health
care
field
problem
arises
clinical
trial
studies
31
many
methods
propensity
matching
inverse
propensity
weighting
doubly
robust
estimation
applied
online
settings
goal
comparing
effect
treatment
vs
control
showing
vs
showing
ad
methods
including
propensity
matching
specifically
designed
comparison
given
individual
treatment
group
match
another
individual
control
group
sense
propensity
scores
equal
effect
obtained
difference
average
treatment
group
matched
individuals
control
group
immediately
clear
adapt
methods
use-case
hand
inverse
propensity
weighting
approach
can
easily
adopted
help
overcome
selection
bias
learning-to-rank
inverse
propensity
weighting
known
propensity
score
let
wq
ratio
probability
appearing
probability
actually
appears
data
set
eq
training
data
used
learn
scoring
function
two
commonly
used
approaches
obtain
relevance
estimates
one
way
sample
set
queries
ask
human
raters
explicitly
judge
relevance
retrieved
documents
way
collect
implicit
relevance
judgments
click-through
data
click-through
data
approach
attracted
attention
research
community
21
much
cheaper
obtain
human-judged
data
especially
major
search
engines
however
mentioned
click
data
biased
noisy
example
position
bias
simple
click
counts
can
used
directly
estimate
relevance
great
deal
previous
work
see
section
focuses
overcoming
bias
infer
actual
unbiased
relevance
focus
general
selection
bias
problem
arises
using
click-through
data
train
learning-to-rank
models
observation
using
click-through
data
learningto-rank
queries
without
clicks
provide
useful
information
optimizing
pair-wise
loss
functions
117
empirical
loss
function
becomes
0.45
ls
wq
0.35
propensity
scores
best
knowledge
work
first
generally
study
selection
bias
improve
effectiveness
learning-to-rank
models
problem
selection
bias
especially
important
scenario
personal
search
personalized
nature
information
needs
strongly
biases
available
training
data
apply
selection
bias
practice
primary
challenge
becomes
estimating
inverse
propensity
weights
wq
open
question
also
whether
weighting
approach
will
significant
impact
effectiveness
learning-torank
models
address
challenge
answer
question
following
sections
0.30
0.25
0.20
0.15
0.10
0.05
0.00
positions
figure
position
bias
propensity
scores
user
emails
cloud
storage
files
proposed
methods
describe
different
methods
inverse
propensity
weighting
estimation
briefly
describe
application
scenario
data
set
used
quantify
bias
application
scenario
application
search
engine
one
world
largest
commercial
email
cloud
file
storage
services
given
query
search
engine
provides
instant
results
results
refresh
user
types
instant
results
provide
efficient
way
user
examine
results
instant
results
pagination
results
retrieved
personal
corpus
emails
cloud
storage
files
therefore
generally
unique
user
user
detects
relevant
result
clicks
clicked
document
immediately
opened
browser
click
data
obtained
exclusively
instant
results
therefore
issued
query
will
either
click
exactly
one
click
rest
paper
study
selection
bias
problem
setting
methods
presented
can
easily
extended
web
search
setting
beyond
scope
current
study
result
randomization
order
quantify
position
bias
will
used
inverse
propensity
weighting
estimation
employ
result
randomization
collect
user
click
data
randomized
result
sets
specifically
given
ranked
result
list
documents
returned
query
instead
showing
original
list
permute
results
uniformly
random
present
shuffled
list
small
fraction
end
users
denote
collected
randomized
data
special
case
randomization
reduces
previously
proposed
fairpairs
algorithm
11
33
rest
section
present
different
methods
quantify
selection
bias
using
collected
randomized
data
4.1
email
file
0.40
position
follows
cq
xi
rx
bi
goal
estimate
bi
given
query
denotes
probability
showing
result
position
randomized
data
probability
showing
given
result
positions
i1
i2
i1
i2
thus
given
i1
i2
rxq
dp
i1
rxq
dp
i2
hence
bi
rx
cxi
dp
dp
xz
cxi
dp
total
number
clicks
position
randomized
data
application
every
query
single
click
let
clicked
position
bi
proportional
ratio
probability
appearing
probability
uniformly
sampled
collection
thus
wq
now
show
empirical
data
global
bias
model
ran
randomization
experiments
two
document
corpora
user
emails
cloud
storage
files
collected
data
set
normalize
bi
bi
plot
normalized
bi
values
figure
results
show
clearly
position
biases
email
corpus
cloud
file
storage
corpus
example
b1
email
corpus
0.40
b4
0.15
confirming
strong
bias
clicking
top
positions
interestingly
figure
also
shows
emails
cloud
storage
files
different
bias
values
bias
files
much
flat
emails
global
bias
model
global
bias
model
can
viewed
standard
position
bias
model
11
30
assumes
bias
function
position
within
ranked
list
formally
let
cq
xi
denote
probability
receiving
click
document
shown
position
query
rxq
probability
relevance
bi
bias
position
meaning
likely
user
examine
document
4.2
segmented
bias
model
global
bias
model
quantifies
position
bias
solely
based
clicked
position
rather
coarse
esti
118
mate
past
work
go
beyond
however
motivated
bias
difference
shown
figure
possible
even
within
single
document
corpus
email
different
segments
queries
different
position
biases
thus
propose
fine-grained
model
called
segmented
bias
model
basic
idea
segmented
bias
model
partition
queries
segments
apply
global
position
bias
model
separately
within
segment
thus
specific
bias
model
segment
may
multiple
application-specific
ways
segmenting
queries
using
query
classifier
application
focus
email
corpus
now
rely
categories
labels
assigned
email
several
labels
available
corpus
promotional
social
email
can
associated
multiple
labels1
multiple
emails
result
list
query
goal
select
single
label
treat
segment
query
inspired
inverse
document
frequency
idf
metric
compute
inverse
query
frequency
iqf
label
label
iqf
needed
means
queries
will
filtered
data
sparseness
second
private
search
scenario
discussed
documents
unique
individual
user
even
query
string
retrieved
documents
will
differ
across
users
order
collect
sufficient
data
query
need
show
different
randomized
results
user
time
time
will
annoy
users
also
miss
purpose
data
randomization
since
data
independence
assumption
will
violated
thus
challenge
tackle
following
prediction
problem
definition
position
bias
prediction
given
query
x1
xn
problem
position
bias
prediction
estimate
click
probability
position
show
set
documents
uniformly
random
order
specifically
recall
set
queries
randomized
data
solve
problem
propose
learningbased
approach
using
multi-class
logistic
regression
positions
thus
classes
regression
query
seek
estimate
probability
query
belonging
class
thus
construct
training
data
randomized
data
describe
approach
follows
means
label
attached
email
retrieved
query
total
number
queries
label
attached
randomized
data
label
query
labels
query
instance
randomized
data
clicked
position
label
instance
class
use
binary
logistic
regression
algorithm
query
becomes
positive
training
example
class
negative
example
classes
arg
max
iqf
by-product
creates
segments
balanced
size
given
queries
labelled
can
estimate
position
bias
bti
segment
thus
query
inverse
propensity
weighting
becomes
wq
features
query
can
construct
feature
vector
setting
feature
can
query-dependent
user-dependent
feature
depends
documents
feature
depend
set
retrieved
documents
without
dependency
actual
order
randomized
data
example
segmented
bias
model
feature
depends
set
emails
bi
clicked
position
though
simple
find
method
quite
effective
experiments
4.3
training
train
logistic
regression
models
single
position
based
feature
vectors
positive
negative
training
examples
defined
logistic
regression
model
position
parameterized
vector
βi
feature
vector
generalized
bias
model
segmented
model
goes
step
model
bias
fine-grained
manner
natural
question
generalize
even
example
possible
query-dependent
bias
model
words
query
can
potentially
different
position
biases
due
large
number
unique
queries
formulation
seems
intractable
however
show
section
result
randomization
makes
possible
formulate
generalized
query-dependent
bias
model
specifically
estimate
position
bias
query
suppose
can
present
randomly
shuffled
list
documents
every
time
query
issued
user
clicks
independent
similar
approach
global
bias
model
can
applied
randomized
data
specifically
query
however
practically
feasible
following
reasons
first
able
accurately
estimate
position
bias
query
hundreds
thousands
data
points
bq
exp
βi
parameter
βi
can
obtained
maximizing
likelihood
training
data
prediction
given
query
features
can
apply
models
obtain
prediction
values
based
eq
value
corresponding
position
click
probability
results
shown
uniformly
randomly
thus
position
bias
call
generalized
bias
model
indeed
global
segmented
models
special
cases
generalized
bias
model
proposition
feature
vector
query
single
constant
element
generalized
bias
model
reduced
global
bias
model
labeling
algorithm
scope
paper
reader
may
refer
grbovic
et
al
16
bekkerman
prior
work
subject
119
feature
type
query
length
proof
sketch
let
cp
denote
total
number
clicks
position
ci
feature
vector
query
single
constant
element
bq
depends
log
likelihood
bi
eq
segment
ci
log
bi
ci
log
bi
maximized
bi
ci
table
generalized
bias
model
features
proposition
segmented
bias
model
becomes
special
case
generalized
bias
model
construct
feature
vectors
follows
create
binary
feature
segment
query
value
feature
corresponding
segment
elsewhere
name
nocorrection
proof
sketch
feature
vector
defined
segmented
model
can
partition
queries
way
segmented
model
log
likelihood
whole
data
set
can
separated
individual
components
corresponding
segment
component
will
maximized
similarly
proposition
thus
yielded
biases
segmented
bias
model
noted
generalized
bias
model
flexible
take
types
features
query-specific
userspecific
features
experiments
use
simple
yet
effective
set
query
length
query
segment
features
described
table
section
will
report
empirical
results
effectiveness
generalized
bias
model
based
features
segmented
global
generalized
5.1
learning-to-rank
algorithm
learning-to-rank
algorithm
adaptive
one
build
new
model
top
existing
score
different
standard
approach
learns
scoring
function
using
entire
set
features
instead
adaptive
approach
aim
train
adjustment
base
score
final
scoring
function
becomes
section
conduct
experiments
compare
different
position
bias
prediction
methods
methods
compare
summarized
table
table
nocorrection
means
learning
scoring
function
without
taking
selection
bias
account
serves
baseline
compare
methods
following
first
describe
experimental
design
present
experimental
results
5.1
method
description
bias
correction
applied
serves
baseline
bias
estimated
position
globally
bias
estimated
position
per
segment
bias
estimated
position
per
query
using
logistic
regression
table
list
position
bias
prediction
methods
experiments
5.1
description
binary
indicator
based
bucketized
number
query
characters
10
10
20
20
30
30
binary
indicator
based
category
segment
described
section
4.2
use
following
ranking
features
learn
adjustment
email
categories
set
categories
used
segmented
bias
model
email
can
belong
multiple
categories
category
binary
feature
indicating
email
belongs
category
otherwise
user
interactions
set
user
interaction
features
logged
email
example
interaction
feature
can
whether
user
opened
email
past
experimental
design
data
sets
use
two
data
sets
paper
regular
click
data
randomized
click
data
yields
tens
ranking
features
although
may
seem
like
small
number
features
base
score
already
highly
optimized
includes
hundreds
different
features
email
category
user
interaction
features
considered
add
additional
information
somewhat
orthogonal
used
compute
base
score
nocorrection
baseline
see
table
train
without
applying
bias
correction
wq
apply
respective
selection
bias
weights
training
global
segmented
generalized
models
additive
nature
adaptive
model
naturally
fits
multiple
additive
regression
trees
mart
learning
algorithm
18
every
iteration
mart
trains
new
tree
added
existing
list
trees
setting
start
base
score
train
additive
trees
regular
data
data
collected
click
logs
used
learning
scoring
function
data
made
random
sample
email
search
logs
2015
12
01
2015
12
07
resulting
million
queries
clicks
training
test
sets
used
offline
evaluations
comprised
50
50
split
data
randomized
data
randomized
data
set
used
estimate
bias
proposed
methods
obtain
randomized
data
randomly
permuted
top
search
results
returned
small
fraction
email
search
queries
2015
11
18
2015
1123
resulting
total
208k
queries
total
estimate
position
bias
retain
queries
exactly
results
yields
data
set
148k
queries
5.2
experimental
results
section
evaluate
position
bias
prediction
models
couple
different
settings
among
120
mean
95
ci
uniform
4.0
global
3.7360
0.0202
segmented
3.7337
0.0201
generalized
3.7336
0.0197
nocorrection
global
segmented
generalized
improvement
table
perplexity
randomized
data
95
confidence
interval
rankq
rank
first
clicked
document
query
context
selection
bias
need
incorporate
bias
correction
wq
metric
well
thus
get
weighted
mrr
wq
mrr
rank
perplexity
randomized
data
position
bias
prediction
problem
can
treated
standard
prediction
problem
thus
can
evaluated
using
techniques
like
cross-validation
split
randomized
data
10
folds
use
leave-one-out
strategy
evaluate
different
prediction
methods
query
prediction
method
gives
distribution
positions
can
thus
use
perplexity
evaluation
metric
defined
perplexity
pn
log2
po
different
position
bias
prediction
models
give
rise
different
wq
values
eq
thus
mrr
metric
comparable
across
different
wq
weights
means
even
data
set
scoring
function
may
get
different
mrr
values
illustrate
table
reports
weighted
mrr
test
data
set
regular
data
normalize
raw
mrr
values
smallest
value
table
row
denoted
nocorrection
corresponds
evaluation
results
data
set
scoring
function
clearly
table
shows
different
values
different
wq
used
mrr
metric
though
can
compute
relative
improvement
different
position
bias
correction
models
nocorrection
model
using
mrr
corresponding
wq
improvement
numbers
still
grounded
reliable
comparison
metric
changing
weight
wq
one
can
artificially
manipulate
improvement
example
high
weight
wq
can
given
queries
new
model
higher
mrr
total
number
observations
test
data
po
probability
observation
predicted
model
evaluated
perplexity
measures
well
distribution
predicts
samples
often
used
evaluate
compare
language
models
also
used
recent
work
compare
click
models
15
case
sample
corresponds
click
position
test
data
po
predicted
bias
probability
sample
lower
perplexity
score
means
model
better
predicting
observations
table
show
perplexity
score
95
confidence
intervals
based
cross-validation
table
uniform
non-informative
baseline
corresponds
uniform
prediction
gives
25
positions
table
can
see
position
bias
methods
outperform
uniform
significantly
example
95
confidence
interval
global
3.7158
3.7562
uniform
perplexity
4.0
outside
range
thus
difference
significant
comparing
across
bias
prediction
methods
can
see
generalized
achieves
best
score
segmented
close
generalized
however
difference
among
methods
showed
table
significant
perplexity
intrinsic
measure
given
method
prediction
accuracy
however
practical
perspective
much
interested
extrinsic
evaluations
models
trained
using
approach
perform
terms
retrieval
effectiveness
following
examine
different
options
directly
evaluating
search
quality
5.2
5.2
obvious
way
evaluating
ranking
quality
offline
apply
learnt
scoring
function
held-out
test
data
set
however
will
show
method
well-grounded
let
us
consider
evaluation
metric
mean
reciprocal
rank
mrr
defined
follows
rankq
unbiased
offline
evaluator
section
address
challenge
offline
evaluation
proposing
novel
unbiased
offline
evaluator
based
randomized
data
method
directly
evaluates
ranking
quality
thus
practically
useful
perplexity
furthermore
theoretically
sound
unbiased
can
overcome
comparability
issues
arise
using
regular
data
offline
evaluations
unbiased
offline
evaluators
studied
extensively
setting
contextual-bandit
problems
12
25
adapt
strategy
problem
setting
proposed
algorithm
detailed
algorithm
algorithm
goes
every
query
randomized
data
set
selects
matched
subset
rs
based
provided
scoring
function
matching
condition
ranking
recorded
log
data
ranked
top
documents
metric
value
computed
selected
subset
rs
offline
evaluation
regular
data
mrr
weighted
mrr
eq
segmented
wq
generalized
wq
1.0055
1.0000
1.0156
1.0101
0.9968
1.0099
table
offline
evaluation
regular
data
based
weighted
mrr
number
table
normalized
smallest
mrr
online
experiments
serve
ultimate
ground
truth
expensive
need
run
live
traffic
thus
explore
cheap
offline
evaluation
methodologies
discuss
strengths
weaknesses
5.2
global
wq
1.0055
1.0154
0.9822
theorem
given
uniformly
randomized
data
algorithm
gives
unbiased
estimate
metric
scoring
function
proof
simplified
version
theorem
25
static
scoring
function
prove
121
algorithm
offline
evaluator
input
scoring
function
randomized
data
evaluation
metric
top
mk
output
evaluation
value
set
matched
data
collection
rs
hx1
xn
let
hxj1
xjn
hx1
xn
re-ranked
hj1
jk
h1
ki
rs
rs
end
end
return
mk
rs
global
0.94
1.08
1.58
1.37
segmented
1.01
1.28
1.67
1.44
generalized
0.97
1.20
1.68
1.41
table
comparison
different
position
bias
prediction
methods
using
unbiased
offline
evaluator
report
relative
improvement
nocorrection
baseline
baseline
nocorrection
global
case
goal
show
rs
unbiased
sample
events
use
scoring
function
since
rankings
rs
ranked
need
prove
marginal
probability
query
string
rs
unbiased
baseline
nocorrection
global
mrr
global
segmented
0.67
0.88
0.21
ctr
global
segmented
0.46
0.71
0.25
generalized
0.79
0.12
generalized
0.62
0.15
table
comparison
different
bias
prediction
methods
using
online
experiments
report
relative
improvement
nocorrection
baseline
notation
means
difference
significant
level
0.1
0.05
0.01
respectively
rs
means
query
string
case
unbiased
since
collect
data
probability
entering
condition
queries
algorithm
one
caveat
algorithm
use
queries
exactly
results
queries
rej
sults
can
weigh
estimating
metric
full
procedure
evaluating
position
bias
prediction
method
follows
control
nocorrection
model
described
section
5.1
half
used
treatment
one
bias
prediction
methods
create
online
experiment
methods
listed
table
ran
four
online
experiments
period
one
week
collecting
millions
clicks
per
experiment
based
click
data
compare
treatment
control
computing
relative
improvement
terms
evaluation
metric
mrr
mean
reciprocal
rank
click
eq
also
compute
relative
improvement
two
treatment
methods
using
nocorrection
calibrator
table
summarizes
results
online
experiments
table
report
relative
improvement
mrr
compared
nocorrection
global
baselines
nocorrection
baseline
can
see
position
bias
methods
yield
statistically
significant
improvements
0.01
level
confirms
selection
bias
click
data
significant
overcoming
can
lead
significant
quality
improvement
global
baseline
can
see
fine-grained
position
bias
models
capable
improving
metric
example
segmented
outperforms
global
significantly
0.1
level
online
experiments
can
also
report
clickthrough
rate
ctr
ctr
metric
reflects
attractive
result
section
whole
also
report
relative
improvement
terms
ctr
table
observations
ctr
metric
parallel
mrr
metric
global
baseline
significantly
increases
ctr
segmented
generalized
methods
provide
improvements
furthermore
segmented
model
achieves
significant
improvement
global
0.05
level
results
online
experiments
indicate
significant
difference
segmented
generalized
models
even
though
outperform
global
baseline
showed
segmented
special
case
general
split
randomized
data
training
test
via
50
50
split
train
bias
prediction
model
using
randomized
training
data
apply
learnt
position
bias
model
regular
training
data
obtain
scoring
function
evaluate
scoring
function
randomized
test
data
based
algorithm
using
nocorrection
baseline
report
relative
improvement
different
bias
prediction
models
table
table
show
mrr
eq
evaluated
top
results
along
size
rs
results
show
position
bias
prediction
methods
outperform
nocorrection
baseline
position
bias
prediction
methods
segmented
generalized
better
global
segmented
slightly
better
generalized
demonstrating
potential
utility
advanced
position
bias
models
however
differences
statistically
significant
due
high
variance
incurred
small
evaluation
data
set
expected
size
rs
size
larger
data
set
used
increase
statistical
power
proposed
methods
5.2
rs
19
8k
7k
3k
3k
online
experiments
live
traffic
online
experiments
ultimate
litmus
tests
evaluate
different
scoring
functions
online
experiments
form
testing
allocate
fraction
live
traffic
experiment
one
half
fraction
used
122
position
position
position
position
relative
contribution
click
odds
1.10
infeasibility
using
existing
click
models
personal
search
proposed
novel
approach
overcome
inherent
selection
bias
application
proposed
methods
estimate
selection
bias
addressed
using
inverse
propensity
weighting
addition
study
offline
online
evaluation
methodologies
also
propose
novel
unbiased
offline
evaluator
extensive
offline
online
experiments
show
proposed
methods
modeling
selection
bias
can
significantly
improve
quality
learning-to-rank
models
use
click
data
training
interesting
lines
future
work
evaluate
methods
context
personal
search
interesting
see
applicable
web
search
experiments
use
queries
single
click
interesting
extend
framework
search
scenarios
allow
multiple
clicks
per
query
given
different
application
cloud
storage
files
effective
features
bias
estimation
inspire
lots
interesting
feature
engineering
work
research
community
expensive
part
method
dependency
randomized
data
collect
randomized
data
cheaper
less-intrusive
manner
also
worth
studying
furthermore
adapt
offline
evaluator
improve
data
utilization
also
interesting
research
problem
1.05
1.00
0.95
bucketed
query
length
figure
importance
query
length
varied
positions
ized
segmented
features
used
suggesting
usefulness
category
segment
feature
importance
feature
engineering
improvement
generalized
bias
model
5.2
regression
models
analysis
generalized
bias
model
provides
flexible
way
position
bias
prediction
also
enables
us
understand
impact
different
features
terms
usefulness
modeling
position
bias
section
analyze
features
distill
additional
insights
two
types
features
regression
models
segment
features
query
length
features
question
group
features
predictive
answer
question
compare
generalized
models
different
sets
features
observe
following
changes
perplexity
defined
eq
segment
query
length
features
3.7336
segment
features
3.7337
query
length
features
3.7358
clearly
can
see
segment
features
useful
query
length
features
reducing
perplexity
furthermore
can
observe
impact
different
lengths
query
features
query
lengths
bucketed
table
larger
bucket
ids
corresponding
longer
queries
position
query
length
bucket
βij
corresponding
coefficient
logistic
regression
model
eβij
represents
contribution
query
length
odd
click
position
bi
bi
plot
relative
contribution
eβij
eβi1
figure
position
eβij
eβi1
becomes
smaller
becomes
larger
words
odds
click
position
decrease
query
longer
contrast
position
eβij
eβi1
becomes
larger
becomes
larger
means
odds
click
position
increase
query
becomes
longer
makes
sense
intuitively
since
queries
longer
users
refined
needs
position
bias
becomes
flatter
means
users
willing
examine
lower-ranked
documents
references
aslam
kanoulas
pavlu
savev
yilmaz
document
selection
methodologies
efficient
effective
learning-to-rank
32nd
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
468
475
2009
bekkerman
automatic
categorization
email
folders
benchmark
experiments
enron
sri
corpora
technical
report
university
massachusetts
amherst
2004
brown
pietra
mercer
pietra
lai
estimate
upper
bound
entropy
english
computational
linguistics
18
31
40
1992
burges
ranknet
lambdarank
lambdamart
overview
technical
report
msr-tr-2010-82
microsoft
research
2010
bu
ttcher
clarke
yeung
soboroff
reliable
information
retrieval
evaluation
incomplete
biased
judgements
30th
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
63
70
2007
carmel
halawi
lewin-eytan
maarek
raviv
rank
time
relevance
revisiting
email
search
24th
acm
international
conference
information
knowledge
management
cikm
pages
283
292
2015
chan
ge
gershony
hesterberg
lambert
evaluating
online
ad
campaigns
pipeline
causal
models
scale
16th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
pages
16
2010
chapelle
zhang
dynamic
bayesian
network
click
model
web
search
ranking
18th
conclusions
paper
studied
problem
learning-to-rank
selection
bias
personal
search
discussed
123
10
11
12
13
14
15
16
17
18
19
20
21
22
international
conference
world
wide
web
www
pages
10
2009
chen
ji
shen
yang
whole
page
click
model
better
interpret
search
engine
click
data
25th
aaai
conference
artificial
intelligence
aaai
2011
chuklin
markov
de
rijke
click
models
web
search
morgan
claypool
2015
craswell
zoeter
taylor
ramsey
experimental
comparison
click
position-bias
models
1st
international
conference
web
search
data
mining
wsdm
pages
87
94
2008
dudı
langford
li
doubly
robust
policy
evaluation
learning
28th
international
conference
machine
learning
icml
pages
1097
1104
2011
dumais
cutrell
cadiz
jancke
sarin
robbins
stuff
ve
seen
system
personal
information
retrieval
re-use
26th
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
72
79
2003
dupret
liao
model
estimate
intrinsic
document
relevance
clickthrough
logs
web
search
engine
3rd
acm
international
conference
web
search
data
mining
wsdm
pages
181
190
2010
dupret
piwowarski
user
browsing
model
predict
search
engine
click
data
past
observations
31st
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
331
338
2008
grbovic
halawi
karnin
maarek
many
folders
really
need
classifying
email
handful
categories
23rd
acm
international
conference
conference
information
knowledge
management
cikm
pages
869
878
2014
guo
liu
kannan
minka
taylor
wang
faloutsos
click
chain
model
web
search
18th
international
conference
world
wide
web
www
pages
11
20
2009
hastie
tibshirani
friedman
elements
statistical
learning
springer
2001
hofmann
whiteson
de
rijke
balancing
exploration
exploitation
learning
rank
online
advances
information
retrieval
33rd
european
conference
ir
research
ecir
pages
251
263
2011
huang
gretton
borgwardt
scho
lkopf
smola
correcting
sample
selection
bias
unlabeled
data
20th
annual
conference
neural
information
processing
systems
nips
pages
601
608
2006
joachims
optimizing
search
engines
using
clickthrough
data
8th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
pages
133
142
2002
joachims
granka
pan
hembrooke
gay
accurately
interpreting
clickthrough
data
implicit
feedback
28th
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
154
161
2005
23
kamvar
kellar
patel
xu
computers
iphones
mobile
phones
oh
logs-based
comparison
search
users
different
devices
18th
international
conference
world
wide
web
www
pages
801
810
2009
24
li
chen
kleban
gupta
counterfactual
estimation
optimization
click
metrics
search
engines
case
study
24th
international
conference
world
wide
web
www
companion
pages
929
934
2015
25
li
chu
langford
wang
unbiased
offline
evaluation
contextual-bandit-based
news
article
recommendation
algorithms
4th
international
conference
web
search
web
data
mining
wsdm
pages
297
306
2011
26
liu
learning
rank
information
retrieval
foundations
trends
information
retrieval
225
331
2009
27
minka
robertson
selection
bias
letor
datasets
sigir
workshop
learning
rank
information
retrieval
lr4ir
pages
48
51
2008
28
brien
keane
modeling
result
list
searching
world
wide
web
role
relevance
topologies
trust
bias
28th
annual
conference
cognitive
science
society
cogsci
pages
881
2006
29
qin
liu
xu
li
letor
benchmark
collection
research
learning
rank
information
retrieval
inf
retr
13
346
374
2010
30
richardson
dominowska
ragno
predicting
clicks
estimating
click-through
rate
new
ads
16th
international
conference
world
wide
web
www
pages
521
530
2007
31
rosenbaum
rubin
central
role
propensity
score
observational
studies
causal
effects
biometrika
70
41
55
1983
32
swaminathan
joachims
counterfactual
risk
minimization
learning
logged
bandit
feedback
32nd
international
conference
machine
learning
icml
pages
814
823
2015
33
yue
patel
roehrig
beyond
position
bias
examining
result
attractiveness
source
presentation
bias
clickthrough
data
19th
international
conference
world
wide
web
www
pages
1011
1018
2010
34
zadrozny
learning
evaluating
classifiers
sample
selection
bias
21st
international
conference
machine
learning
icml
page
114
2004
35
zheng
chen
sun
zha
regression
framework
learning
ranking
functions
using
relative
relevance
judgments
30th
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
pages
287
294
2007
36
zhu
chen
minka
zhu
chen
novel
click
model
applications
online
advertising
3rd
international
conference
web
search
data
mining
wsdm
pages
321
330
2010
124