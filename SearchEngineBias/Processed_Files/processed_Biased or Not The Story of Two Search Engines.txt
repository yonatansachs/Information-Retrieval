biased
story
two
search
engines
gizem
gezici
sabanci
university
istanbul
turkey
huawei
turkey
center
istanbul
turkey
arxiv
2112
12802v1
cs
ir
23
dec
2021
abstract
search
engines
can
considered
gate
world
web
also
decide
see
given
search
query
since
many
people
exposed
information
search
engines
fair
expect
search
engines
neutral
returned
results
must
cover
elements
aspects
search
topic
impartial
results
returned
based
relevance
however
search
engine
results
based
many
features
sophisticated
algorithms
search
neutrality
necessarily
focal
point
work
performed
empirical
study
two
popular
search
engines
analysed
search
engine
result
pages
controversial
topics
abortion
medical
marijuana
gay
marriage
analysis
based
sentiment
search
results
identify
viewpoint
conservative
liberal
also
propose
three
sentiment-based
metrics
show
existence
bias
well
compare
viewpoints
two
search
engines
extensive
experiments
performed
controversial
topics
show
search
engines
biased
moreover
kind
bias
towards
given
controversial
topic
keywords
web
mining
information
retrieval
sentiment
analysis
search
bias
introduction
search
engines
major
source
information
many
people
especially
new
millennium
early
search
engines
alta
vista
ranking
mechanisms
fairly
simple
transparent
later
developments
google
page
rank
algorithm
ranking
methods
based
click
streams
completely
changed
situation
current
search
engines
perform
retrieval
ranking
based
many
features
introducing
complexity
way
beyond
simple
model
relevance
used
early
search
engines
underlying
ranking
mechanisms
search
engines
indirectly
determine
information
people
exposed
regarding
various
topics
including
controversial
ones
hand
due
fact
search
engines
driven
algorithms
general
public
may
expect
neutral
sense
returned
results
expected
cover
elements
aspects
search
topic
impartial
results
returned
based
relevance
however
search
neutrality
necessarily
focal
point
modern
search
engines1
given
search
engine
users
exposed
news
coming
different
sources
search
engine
result
pages
serps
gathered
ranked
search
engine
may
bias
towards
view
point
result
set
assume
user
searches
2016
presidential
election
top-k
ranked
results
displayed
user
resulting
set
may
biased
towards
specific
political
https://plato.stanford.edu/entries/ethics-search/#searengibiasprobopac
view
conservative
liberal
defining
bias
set
documents
challenging
issue
becomes
even
complicated
ranking
taken
account
since
interested
distribution
positive
negative
opinion
result
set
also
ranking
distribution
current
research
ranking
employs
learning
rank
methods
return
relevant
results
web
documents
requested
query
user
bias
may
come
design
ranking
algorithm
training
set
learning
rank
algorithms
click-through
logs
intrinsically
contain
user
bias
machine
learning
features
proposed
algorithm
designers
inherit
human-like
tendencies
lead
human
bias
train
system
personalisation
involved
together
learning
rank
algorithms
bias
can
justified
fact
resulting
set
documents
determined
based
user
preferences
instance
user
inclined
towards
liberal
news
implied
previous
clicks
returned
results
expected
system
learns
user
prefers
positive
news
documents
regarding
liberals
presents
user
top-k
liberally
biased
documents
otherwise
presented
results
call
unjustified
bias
towards
view
point
people
seriously
rely
results
search
engines
influenced
returning
documents
given
topic
therefore
unjustified
bias
becoming
dangerous
users
query
strongly
polarized
controversial
topics
hence
unjustified
bias
retrieved
results
especially
controversial
queries
main
focus
analyse
serps
two
major
search
engines
google
bing
order
detect
significant
difference
viewpoints
reflected
serps
utilize
opinion
mining
understand
search
result
page
positive
negative
bias
towards
specific
view
point
collected
analysed
serps
two
popular
search
engines
ensure
bias
specific
one
particular
search
engine
work
try
answer
following
research
questions
rq1
search
engines
return
non-biased
result
sets
response
queries
related
controversial
issues
rq2
conservative-liberal
scale
viewpoints
search
engines
significantly
different
towards
controversial
topics
rq3
retrieved
results
search
engines
include
bias
can
determine
source
bias
input
bias
algoritmic
bias
given
two
research
questions
scope
rq1
detecting
unjustified
bias
challenging
problem
especially
ranking
considered
bias
measured
distribution
sentiment
result
set
bias
measure
incorporate
rank
returned
results
well
acm
fat
2019
atlanta
usa
second
question
compare
two
search
engines
taking
account
sentiment
well
ranking
information
see
show
similar
different
viewpoints
given
controversial
topic
specifically
propose
sentiment-based
comparison
procedure
understand
retrieved
documents
search
engines
contain
bias
also
make
pairwise
comparison
perspectives
controversial
issues
experimental
results
demonstrate
search
engines
return
biased
results
sentiment-based
metrics
however
bias
may
originate
corpus
known
input
bias
corpus
moreover
show
bing
google
similar
perspectives
corresponding
controversial
topics
paper
organised
follows
first
provide
related
work
search
bias
second
present
framework
explaining
sentiment-based
comparison
metrics
detail
provide
details
regarding
serp
dataset
crawling
evaluation
framework
report
experimental
results
discuss
findings
finally
conclude
paper
related
work
term
search
neutrality
explained
nytimes
article
principle
search
engines
editorial
policies
results
comprehensive
impartial
based
solely
relevance
issues
regarding
search
engine
bias
summarized
search-engine
technology
neutral
instead
embedded
features
design
favor
values
others
major
search
engines
systematically
favor
sites
kind
sites
others
lists
results
return
response
user
search
queries
search
algorithms
use
objective
criteria
generating
lists
results
search
queries
study
done
robert
epstein
ronald
robertson
demonstrated
effect
search
engines
elections
search
engine
manipulation
effect
seme
possible
impact
outcomes
elections
excerpt
abstract
present
evidence
five
experiments
two
countries
suggesting
power
robustness
search
engine
manipulation
effect
seme
specifically
show
biased
search
rankings
can
shift
voting
preferences
undecided
voters
20
percent
ii
shift
can
much
higher
demographic
groups
iii
rankings
can
masked
people
show
awareness
manipulation
work
based
user
studied
different
countries
techniques
automatically
detect
specific
bias
ranked
results
recent
work
researchers
presented
existence
bias
election-related
search
results
can
strong
undetectable
influence
preferences
undecided
voters
work
also
include
method
automatically
identifying
search
bias
rather
empirical
study
gizem
gezici
blog
reports
study
google
bias
search
results
40
percent
lean
left
liberal
relied
human
labelers
different
political
spectrum
another
popular
article
guardian
states
google
search
algorithm
appears
systematically
promoting
information
either
false
slanted
extreme
rightwing
bias
subjects
varied
climate
change
homosexuality
main
focus
work
identifying
measuring
degree
bias
search
engine
result
pages
serps
emphasis
news
sources
nytimes
bbc
news
example
based
analysis
performed
popular
search
engine
yom-tov
et
al
10
show
people
fact
likely
read
opinions
matching
people
likely
read
news
opposing
sites
given
language
model
particular
news
item
close
language
model
political
view
yom-tov
et
al
also
show
people
presented
diverse
results
continued
reading
diverse
results
overall
became
interested
news
related
research
areas
bias
search
engines
topic
discovery
text
sentiment
analysis
echo
chambers
filter
bubbles
topic
discovery
sentiment
analysis
essential
automatically
identify
results
query
provide
information
aspects
opinions
aspects
thereby
avoiding
bias
search
results
specific
topic
fang
et
al
proposed
novel
opinion
mining
research
problem
call
contrastive
opinion
modeling
com
given
query
topic
database
text
collections
representing
multiple
perspectives
fang
et
al
define
task
com
presenting
opinions
individual
perspectives
query
topic
authors
also
quantify
difference
perspectives
jensen-shannon
divergence
among
individual
topic-opinion
distributions
framework
observed
topic
discovery
may
work
comparing
search
engines
however
sufficient
fairness
analysis
search
engines
reason
decided
use
sentiment
instead
topic
information
compare
perspectives
given
search
engines
measure
fairness
retrieved
documents
aktolga
allan
consider
sentiment
towards
topics
propose
different
query
diversification
methods
based
topic
sentiment
also
measure
stability
sentiment
classification
various
accuracy
levels
parallel
work
naveed
et
al
related
feature
sentiment
diversification
user
generated
reviews
freud
approach
similar
work
fulfilled
kulshrestha
et
al
measured
different
types
bias
search
engines
input
source
bias
ranking
system
bias
output
bias
cumulative
two
proposed
different
metrics
compute
differentiate
distinct
types
bias
politicalrelated
tweets
work
different
work
mainly
four
ways
firstly
used
news
documents
contain
structural
textual
content
tweets
detailed
analysis
secondly
propose
new
metrics
rather
utilized
commonly
used
http://www.canirank.com/blog/analysis-of-political-bias-in-internet-search-
engine-results
https://www.nytimes.com/2009/12/28/opinion/28raff.html
https://www.theguardian.com/technology/2016/dec/16/google-autocomplete-
rightwing-bias-algorithm-political-propaganda
biased
story
two
search
engines
traditional
ir
metrics
ndcg
average
precision
modifying
relevance
grades
sentiment
polarity
values
detect
bias
thirdly
evaluated
retrieved
documents
two
commercial
search
engines
instead
twitter
compared
perspectives
well
addition
solely
detecting
bias
lastly
measure
political
bias
investigated
degree
bias
using
fairness
metric
search
engines
many
diverse
controversial
topics-related
queries
addition
search
engine
ranking
system
bias
bias
may
also
resulted
query
query
polarity
supports
specific
viewpoint
given
topic
things
tend
phrased
way
confirmatory
user
existing
beliefs
obama
born
kenya
phasing
can
also
take
side
via
terminology
used
different
sides
gun
control
vs
gun
rights
will
also
consider
query
bias
fulfill
experiments
accordingly
sentiment-wise
comparison
two
search
engines
3.1
document-level
analysis
document-level
sentiment
analysis
obtain
sentiment
value
document
textblob
without
pre-processing
step
documents
since
textblob
handles
punctuation
part
analysis
coarse-grained
compared
sentencelevel
analysis
3.2
sentence-level
analysis
differently
document-level
sentence-level
sentiment
analysis
view
document
pile
sentences
split
given
document
sentences
obtain
polarity
value
sentences
textblob
order
obtain
sentiment
score
given
document
compute
average
sentiment
score
summing
polarity
values
sentences
dividing
number
sentences
document
provides
us
fine-grained
analysis
given
documents
say
textblob
point
view
sentence-level
analysis
sentence
seen
document
processed
accordingly
clarification
one
polarity
value
document
compare
retrieved
document
sets
two
search
engines
although
compute
document
scores
two
distinct
ways
document
sentence
level
3.3
sentiment
transformation
procedure
apart
sentiment
analysis
levels
make
consistent
comparison
among
controversial
topics
one
needs
think
connection
semantic
orientation
conveying
sentiment
positive
negative
neutral
perspective
document
scope
controversial
topic
instance
document
positive
sentiment
score
whose
controversial
topic
abortion
document
perspective
tends
liberal
whereas
document
shows
positive
attitude
towards
brexit
perspective
close
conservative
therefore
interpret
sentiment
score
document
conservative
liberal
scale
use
comparison
necessary
consider
sentiment
given
document
along
perspective
information
associated
controversial
topic
acm
fat
2019
atlanta
usa
based
example
interpreting
semantic
orientation
context
controversial
topic
obtaining
document
polarity
values
two
different
levels
via
textblob
applied
transformation
document
polarity
values
controversial
topics
transformation
procedure
employed
order
put
controversial
topics
scale
make
consistent
comparison
illustrate
exist
two
documents
one
supports
liberal-perspective
towards
abortion
one
advocates
brexit
will
possess
different
sentiment
orientations
liberal
means
positive
attitude
context
abortion
whereas
conveys
negative
sentiment
towards
brexit
thus
need
apply
transformation
one
documents
consistency
specific
example
transform
brexit
document
multiply
polarity
value
evaluation
scheme
prefer
favor
liberal
documents
relevant
document
means
liberal
document
controversial
query
thus
transform
polarity
negative
document
supports
liberal-perspective
towards
controversial
topic
since
transformation
applied
document
polarities
computed
difference
document-level
sentence-level
transformation
procedure
note
transformed
topics
specified
table
table
two
datasets
3.4
comparison
metrics
use
three
different
sentiment-based
metrics
compare
returning
documents
search
engines
first
metric
directly
use
average
sentiment
polarities
obtained
textblob
secondly
propose
ndcg-senti
metric
blends
sentiment
ranking
information
lastly
third
metric
use
average
precision
metric
favors
ranked
lists
containing
frequent
relevant
documents
liberal
documents
scheme
given
controversial
topic
3.4
average
sentiment
polarity
first
metric
compares
two
search
engines
purely
using
sentiment
scores
returned
textblob
range
polarity
values
obtain
sentiment
scores
documents
sentences
directly
textblob
compute
average
polarity
value
mentioned
previous
section
document
exploiting
document
sentence
sentiment
values
separately
means
care
document
sentiment
scores
comparison
purposes
yet
computing
document
scores
two
different
levels
sentiment
analysis
transforming
polarity
values
needed
3.4
ndcg-senti
addition
average
sentiment
polarity
metric
also
need
informative
metric
will
convenient
compare
particularly
retrieved
document
sets
two
search
engines
reason
introduce
new
metric
ndcg-senti
blends
ranking
sentiment
information
variant
commonly
used
ndcg
metric
utilize
traditional
formula
ndcg
constitute
ranking-sentiment
metric
ndcg-senti
replacing
relevance
scores
ùëüùëíùëôùëñ
sentiment
scores
ùëùùëúùëôùëñ
formula
slight
modification
traditional
formula
ndcg
come
modified
ndcg
scoring
function
also
takes
document
rank
consideration
comparing
retrieved
document
sets
search
engines
however
still
issue
acm
fat
2019
atlanta
usa
gizem
gezici
computation
transformed
ndcg
function
needs
considered
relevance
grades
traditional
ndcg
metric
always
positive
whereas
real
sentiment
values
well
transformed
versions
scores
become
negative
since
polarity
values
obtained
textblob
lie
therefore
polarity
values
documents
firstly
transformed
normalized
used
computation
secondly
proposed
metric
short
compute
ndcg-senti
scores
transformed
subsequently
normalized
sentiment
scores
textblob
normalization
procedure
described
map
polarities
onto
range
query
compute
minimum
maximum
value
computed
polarity
values
document
set
query
normalize
polarity
values
using
minimum
maximum
values
obtain
non-negative
scores
given
set
documents
values
used
compute
ndcg-senti
average
precision
given
set
documents
3.4
average
precision
third
comparison
metric
average
precision
computes
frequency
relevant
documents
document
list
taking
account
ranking
information
presented
scheme
precision
computed
using
frequency
liberal
documents
seen
relevant
documents
framework
since
favor
liberal
documents
encourage
appear
lower
ranks
computation
used
definition
expected
value
average
precision
formula
proposed
collected
two
datasets
using
news
api
corresponding
search
engines
bing
google
purpose
implemented
scripts
python
automate
crawling
process
api
keys
search
engines
crawled
document
contents
returned
search
engine
response
controversial
topics
selected
website
https://www.procon.org/
contains
up-to-date
controversial
issues
deliberately
selected
queries
controversial
evaluation
scheme
measure
bias
exists
observe
differences
retrieval
results
two
popular
search
engines
clearly
note
data
collection
process
fulfilled
controlled
environment
queries
sent
search
engines
almost
moment
since
documents
retrieved
may
vary
time
3.4
fairness
evaluation
scheme
compare
search
engines
mainly
two
ways
first
investigate
return
fair
bias
results
subsequently
return
sentiment-wise
similar
set
documents
controversial
query
however
evaluate
fairness
search
results
retrieved
search
engine
challenging
need
make
definition
fair
mathematically
baseline
fairness
metric
make
comparison
introduce
fairness
metric
need
define
fair
version
three
proposed
sentiment-based
comparison
metrics
can
make
comparison
fair
metric
score
score
computed
given
set
documents
way
can
measure
returning
results
search
engine
significantly
deviate
fair
set
documents
using
corresponding
comparison
metric
average
sentiment
polarity
ndcg-senti
average
precision
fair
average
sentiment
polarity
fair
set
documents
metric
get
value
since
sentiment
scores
obtained
textblob
lie
range
fair
ndcg-senti
compute
fair
version
ndcgsenti
randomly
generate
50
lists
document
scores
compute
ndcg
score
lists
obtain
average
score
ndcg
scores
note
generated
list
contains
10
documents
randomly
choose
document
score
list
ndcg-senti
scores
computed
normalized
sentiment
scores
values
lie
procedure
create
fair
ndcg-senti
score
can
see
given
document
set
fair
using
metric
fair
average
precision
generate
fair
version
average
precision
metric
use
50
lists
documents
compute
average
precision
score
lists
calculate
average
score
document
sets
3.4
normalization
procedure
order
get
rid
negative
sentiment
scores
utilize
min-max
normalization
technique
experimental
results
4.1
dataset
crawling
4.1
preliminary
dataset
preliminary
dataset
chose
15
highly
controversial
topic
https://www.procon.org/
25th
october
2017
see
proposed
method
useful
measure
bias
compare
returning
document
lists
search
engines
way
rather
small
dataset
expect
distinguished
results
two
search
engines
since
selected
controversial
topics
website
since
small
number
topics
retrieve
documents
related
topic
extended
query
set
using
google
trends
filtered
expanded
query
set
expanded
query
contain
controversial
topic
words
ii
comprises
sentiment-baring
words
filtering
first
type
extended
queries
enables
us
retrieve
relevant
documents
given
controversial
topic
without
losing
context
information
hand
second
type
filtering
helps
us
eliminating
queries
query
set
possibly
introduce
bias
search
results
keywords
may
convey
positive
negative
opinion
controversial
topic
thus
able
focus
contents
retrieved
documents
fair
comparison
analysis
way
aim
send
extensive
objective
queries
search
engine
obtain
related
documents
aspects
controversial
topic
query
sent
search
engine
crawled
first
10
documents
returned
search
engine
dataset
composed
2030
documents
total
one
search
engine
topic
distribution
crawled
dataset
shown
table
table
one
can
see
160
documents
crawled
controversial
topic
abortion
means
16
expanded
queries
abortion
since
use
first
10
documents
retrieved
corresponding
search
engine
moreover
topics
whose
sentiment
scores
will
transformed
computing
metrics
bold-faced
table
topics
transformed
biased
story
two
search
engines
acm
fat
2019
atlanta
usa
determined
majority
voting
human
annotators
topics
whose
sides
clearly
certain
conservative-liberal
scheme
selected
transformation
expanded
query
set
applied
filtering
process
extended
query
sets
controversial
topics
dataset
almost
query
set
sizes
decreased
one
may
argue
query
sets
really
small
number
shown
table
however
note
small
query
set
sizes
outcome
filtering
process
since
process
exclude
high
number
queries
query
sets
small
query
set
sizes
may
caused
fact
initial
version
extensive
query
sets
beginning
also
small
comparing
rest
hand
one
reason
small
sized
initial
query
sets
less
number
various
aspects
subtopics
corresponding
controversial
topic
get
attention
discussed
internet
topic
abortion
animal
testing
assisted
suicide
brexit
climate
change
gay
marriage
gun
control
medical
marijuana
minimum
wage
obamacare
prostitution
syrian
refugees
transgender
military
travel
ban
trump
including
uppercased
characters
without
removing
punctuations
note
since
employ
query
expansion
second
dataset
controversial
topics
queries
time
can
use
controversial
topic
controversial
query
interchangeably
second
dataset
controversial
queries
stable
dataset
shown
topics
whose
sentiment
scores
will
transformed
bold-faced
table
topics
determined
majority
voting
human
annotators
ones
whose
sides
clearly
certain
conservative-liberal
scheme
selected
transformation
since
query
expansion
phase
one
list
documents
controversial
query
topic
hence
single
ndcg-senti
average
precision
score
computed
topic
statistical
significance
test
can
applied
computed
scores
directly
average
sentiment
polarity
hand
one
sentiment
score
document
given
set
thus
need
compute
average
scores
obtain
single
score
topic
compared
however
case
first
dataset
exists
one
ndcg-senti
average
precision
score
query
since
controversial
topic
consists
many
queries
need
compute
average
scores
generate
one
single
score
controversial
topic
additionally
average
sentiment
polarity
initially
computed
average
document
polarities
given
document
set
one
expanded
query
subsequently
needed
compute
mean
averaged
sentiment
polarities
obtain
one
mean
average
sentiment
polarity
comparison
documents
160
60
70
150
300
180
200
130
220
60
40
80
60
90
230
lastly
also
made
query
analysis
obtained
sentiment
polarities
controversial
queries
textblob
polar
queries
displayed
table
polarity
information
repeated
analyses
without
non-neutral
queries
remove
possible
source
bias
rooted
queries
table
preliminary
dataset
15
controversial
topic
distributions
transformation
applied
bold-faced
topics
4.1
stable
dataset
second
dataset
comprehensive
analysis
got
62
controversial
topics
website
https://www.procon.org/
31th
july
2018
stable
dataset
differs
preliminary
dataset
essentially
following
elements
select
controversial
topic
set
since
need
bigger
dataset
also
affect
results
injecting
bias
input
controversial
issues
given
website
mostly
us
behaved
location
somewhere
us
crawling
differently
preliminary
dataset
crawled
ooth
first
10
100
returned
documents
controversial
query
also
expand
query
set
dataset
issues
encountered
establishing
first
one
seems
prevent
us
creating
controlled
environment
possibly
introducing
bias
input
thus
used
62
controversial
issues
presented
website
exactly
four
factors
aimed
crawl
stable
dataset
controlled
environment
without
introducing
bias
input
analyse
contents
documents
returned
search
engines
terms
sentiment
4.2
evaluation
framework
evaluating
search
engines
main
intent
compare
two
search
engines
conservative
liberal
perspective
towards
given
controversial
topic
comparison
utilized
textblob
sentiment
scores
two
different
levels
document
sentence
evaluation
used
three
sentiment-based
metrics
computed
two
different
levels
document
sentence
returning
documents
bing
google
using
metrics
aimed
answer
two
following
research
questions
raised
beginning
paper
rq1
search
engines
return
non-biased
result
sets
response
queries
related
controversial
issues
acm
fat
2019
atlanta
usa
gizem
gezici
topic
names
three
topics
line
2016
presidential
election
abortion
aclu
good
america
alternative
energy
vs
fossil
fuels
animal
testing
banned
books
bill
clinton
born
gay
origins
sexual
orientation
bottled
water
ban
cell
phone
radiation
churches
taxes
climate
change
college
education
worth
concealed
handguns
corporal
punishment
corporate
tax
rate
jobs
cuba
embargo
daylight
saving
time
death
penalty
drinking
age
lower
drone
strikes
overseas
drug
use
sports
electoral
college
euthanasia
assisted
suicide
felon
voting
fighting
hockey
gay
marriage
gold
standard
golf
sport
gun
control
illegal
immigration
israeli-palestinian
conflict
medical
marijuana
milk
healthy
minimum
wage
national
anthem
protests
net
neutrality
obamacare
obesity
disease
olympics
penny
keep
police
body
cameras
prescription
drug
ads
prostitution
legalize
recreational
marijuana
right
health
care
ronald
reagan
sanctuary
cities
school
uniforms
school
vouchers
social
media
social
security
privatization
standardized
tests
student
loan
debt
tablets
vs
textbooks
teacher
tenure
god
pledge
universal
basic
income
vaccines
kids
vegetarianism
video
games
violence
voting
machines
table
stable
dataset
62
controversial
topics
transformation
applied
bold-faced
topics
topic
aclu
good
america
born
gay
origins
sexual
orientation
college
education
worth
gay
marriage
illegal
immigration
milk
healthy
right
health
care
social
media
social
security
privatization
polarity
0.70
0.46
0.30
0.42
0.50
0.50
0.29
0.03
0.03
values
separately
comparison
metric
see
significant
difference
bing
google
terms
corresponding
metric
4.2
comparison
analysis
answer
research
questions
used
two
datasets
first
dataset
used
preliminary
results
name
implies
comprehensive
analysis
done
stable
dataset
prepared
controlled
environment
contains
first
10
well
100
retrieved
documents
also
incorporates
query
analysis
avoid
input
bias
stable
dataset
results
raised
research
questions
will
answered
second
dataset
comprehensive
manner
controversial
queries
can
see
table
three
comparison
metrics
gave
consistent
results
computed
scores
demonstrate
bing
google
similar
seem
fair
moreover
average
sentiment
polarity
scores
computed
first
100
documents
also
support
claim
therefore
may
mean
ranking
algorithms
search
engines
rather
corpus
controversial
issues
biased
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.00192
0.00095
0.41807
0.00134
0.00097
0.74972
significant
yes
yes
yes
yes
table
stable
dataset
two-tail
t-test
0.01
average
sentiment
polarity
10
table
polar
controversial
queries
stable
dataset
find
answer
question
need
compare
retrieved
results
search
engine
fair
list
documents
purpose
computed
scores
three
proposed
comparison
metric
scores
retrieved
results
search
engine
scores
fair
version
metrics
applied
twotail
statistical
significance
test
scores
metric
average
sentiment
polarity
ndcg-senti
average
precision
see
significant
difference
retrieved
results
search
engine
fair
list
documents
way
can
decide
search
engine
returns
fair
results
terms
corresponding
metric
rq2
conservative-liberal
scale
viewpoints
search
engines
significantly
different
towards
controversial
topics
subsequently
investigate
answer
second
question
need
compare
overall
perspectives
bing
google
controversial
topics
reason
computed
three
comparison
metric
scores
bing
google
returning
document
lists
controversial
queries
query
set
applied
two-tail
statistical
significance
test
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
4e-09
0e-07
0.9259
8e-09
7e-08
0.6696
significant
yes
yes
yes
yes
table
stable
dataset
two-tail
t-test
0.01
ndcg
10
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.00107
0.00480
0.92069
0.00196
0.00353
0.89756
significant
yes
yes
yes
yes
table
stable
dataset
two-tail
t-test
0.01
ap
10
biased
story
two
search
engines
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.00051
0.00221
0.75505
0.00086
0.00248
0.77645
acm
fat
2019
atlanta
usa
significant
yes
yes
yes
yes
table
stable
dataset
two-tail
t-test
0.01
average
sentiment
polarity
100
moreover
plotted
controversial
topics
visualize
distribution
average
sentiment
polarity
ndcg-senti
average
precision
scores
analysis
first
10
documents
average
sentiment
polarity
values
first
100
documents
figure
plots
denoted
controversial
query
point
blue
document-level
green
sentencelevel
score
put
line
scores
bing
google
equal
line
comparison
point
line
google
score
positive
bing
vice-versa
can
also
find
number
points
percentages
axis
label
corresponding
plot
figure
stable
dataset
controversial
topic
distribution
ncdg
10
scores
figure
stable
dataset
controversial
topic
distribution
ap
10
scores
figure
stable
dataset
controversial
topic
distribution
average
sentiment
polarity
10
scores
non-polar
controversial
queries
part
removed
polar
queries
displayed
table
way
aimed
eliminate
input
bias
may
caused
polar
queries
obtain
accurate
results
one
can
defend
polar
queries
shown
table
may
look
like
polarized
queries
however
want
pick
polar
ones
manually
affect
results
therefore
procedure
accepted
given
query
polar
textblob
assigns
sentiment
polarity
different
since
also
obtained
sentiment
scores
textblob
used
framework
whole
comparison
procedure
consistent
although
queries
seem
strongly
polarized
experiments
fulfilled
non-polar
queries
result
can
drawn
average
sentiment
scores
first
10
100
documents
probably
corpus
biased
input
bias
rather
ranking
algorithms
search
engines
seen
table
11
yet
differently
previous
dataset
average
sentiment
scores
first
10
100
documents
bing
google
seem
fair
confidence
99
set
0.05
scores
also
support
claim
scores
previous
dataset
thus
one
can
advocate
eliminate
polar
queries
query
set
terms
average
sentiment
polarity
search
engines
seem
return
fair
results
encourages
fact
eliminated
injected
input
bias
eliminated
polar
queries
acm
fat
2019
atlanta
usa
gizem
gezici
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.00684
0.02263
0.77357
0.00986
0.02540
0.84905
significant
yes
yes
table
11
non-polar
stable
dataset
two-tail
t-test
0.01
average
sentiment
polarity
100
figure
stable
dataset
controversial
topic
distribution
average
sentiment
polarity
100
scores
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.01704
0.00631
0.27039
0.01178
0.00721
0.46517
significant
yes
yes
table
non-polar
stable
dataset
two-tail
t-test
0.01
average
sentiment
polarity
10
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
7e-08
9e-07
0.78749
7e-06
6e-10
0.44474
significant
yes
yes
yes
yes
table
non-polar
stable
dataset
two-tail
t-test
0.01
ndcg
10
comparison
bing-fair
google-fair
bing-google
bing-fair
google-fair
bing-google
analysis
level
doc
doc
doc
sent
sent
sent
p-value
0.00586
0.00642
0.85482
0.01378
0.00241
0.69829
significant
yes
yes
yes
yes
table
10
non-polar
stable
dataset
two-tail
t-test
0.01
ap
10
conclusion
future
work
sum
experiments
support
commercial
search
engines
seem
return
biased
results
controversial
queries
since
also
measured
bias
corpus
containing
first
100
documents
bias
may
also
caused
documents
corpus
given
controversial
query
moreover
overall
perspective
bing
google
seem
similar
points
view
controversial
topic
future
work
plan
apply
aspect-level
sentiment
analysis
retrieved
documents
detailed
framework
way
can
analyse
search
results
fine-grained
manner
necessary
especially
controversial
queries
including
two
sides
textual
content
alternative
energy
vs
fossil
fuels
israeli-palestinian
conflict
etc
proper
comparison
furthermore
documents
can
retrieved
search
engines
different
countries
location-based
bias
analysis
can
incorporated
existing
framework
biased
story
two
search
engines
references
elif
aktolga
james
allan
2013
sentiment
diversification
different
biases
proceedings
36th
international
acm
sigir
conference
research
development
information
retrieval
acm
593
602
javed
aslam
emine
yilmaz
virgiliu
pavlu
2005
maximum
entropy
method
analyzing
retrieval
measures
proceedings
28th
annual
international
acm
sigir
conference
research
development
information
retrieval
acm
27
34
robert
epstein
ronald
robertson
2015
search
engine
manipulation
effect
seme
possible
impact
outcomes
elections
proceedings
national
academy
sciences
112
33
2015
e4512
e4521
robert
epstein
ronald
robertson
david
lazer
christo
wilson
2017
suppressing
search
engine
manipulation
effect
seme
proceedings
acm
human-computer
interaction
2017
42
yi
fang
luo
si
naveen
somasundaram
zhengtao
yu
2012
mining
contrastive
opinions
political
texts
using
cross-perspective
topic
model
proceedings
fifth
acm
international
conference
web
search
data
mining
acm
63
72
danai
koutra
paul
bennett
eric
horvitz
2015
events
controversies
influences
shocking
news
event
information
seeking
proceedings
24th
international
conference
world
wide
web
international
world
wide
web
conferences
steering
committee
614
624
juhi
kulshrestha
motahhare
eslami
johnnatan
messias
muhammad
bilal
zafar
saptarshi
ghosh
krishna
gummadi
karrie
karahalios
2017
quantifying
search
bias
investigating
sources
bias
political
searches
social
media
proceedings
2017
acm
conference
computer
supported
cooperative
work
social
computing
acm
417
432
nasir
naveed
thomas
gottron
steffen
staab
2013
feature
sentiment
diversification
user
generated
reviews
freud
approach
seventh
international
aaai
conference
weblogs
social
media
herman
tavani
2012
search
engines
ethics
2012
10
elad
yom-tov
susan
dumais
qi
guo
2014
promoting
civil
discourse
search
engine
diversity
social
science
computer
review
32
2014
145
154
acm
fat
2019
atlanta
usa