Fairness Vs. Personalization: Towards Equity in Epistemic Utility

JENNIFER CHIEN, UC San Diego, USA
DAVID DANKS, UC San Diego, USA
The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search engine results, and more. These systems o?er a more e?cient way to navigate the vast array of items available. However, alongside this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional im- plementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path towards achieving fairness in personalized systems.

Additional Key Words and Phrases: fairness, personalization, recommender systems, equity, epistemic utility, epistemic harms, policy

ACM Reference Format:
Jennifer Chien and David Danks. 2023. Fairness Vs. Personalization: Towards Equity in Epistemic Utility. 1, 1 (September 2023), 11 pages.


1 INTRODUCTION
Personalized algorithmic systems are being increasingly deployed in a broad range of sectors. These algorithms hold the potential to provide more appropriate outputs on an individual basis by personalizing based on preferences, values, needs, or environmental conditions. Most prominently, recommender systems are now widely used to provide the most useful recommendations to each individual in the given domain[1].
  At the same time, we have seen a signi?cant increase in the recognition that algorithms can exhibit biases and produce unfair or unjust outcomes. There are many di?erent potential sources of bias in algorithms and models, and hence many di?erent responses may be appropriate or required. Algorithm development and deployment e?orts now typically recognize the possibility of algorithmic bias, and the need to (often) do something to mitigate it[2].
  We contend that these two desiderata for algorithms - personalization and fairness - stand in signi?cant tension. At a high level, personalization is fundamentally about treating each individual in distinct ways; the goal is to not give the same output for each individual, but rather to tailor the outputs to their speci?c situation. In contrast, fairness is fundamentally about treating individuals similarly; the algorithm ought to be "the same", in some sense, for everyone. Of course, this high-level gloss on the tension is far too quick: for example, fairness allows for di?erential treatment or outcomes, as long as it is based on morally or legally defensible grounds. Nonetheless, these high-level observations point towards a tension that, we contend, continues to hold when we look deeper. More speci?cally, we analyze fairness in the context of personalized systems using the notion of epistemic utility - essentially, the bene?t that an agent receives by an epistemic improvement (e.g., reduction in uncertainty) - and provide both practical and policy guidance about how to achieve fair, personalized systems.

Authors' addresses: Jennifer Chien, jjchien@ucsd.edu, UC San Diego, USA; David Danks, ddanks@ucsd.edu, UC San Diego, USA.

2023. Manuscript submitted to ACM

2 PRELIMINARIES
  Recommender System. We consider a recommender system as that which uses an algorithmic approach to provide an ordering or prioritization for items based on relevance to a user. This form of personalization is often based on their preferences, past behavior, or similarities with other users with the overarching goal of improving user experience and engagement.

  Group Fairness. Algorithmic fairness is typically framed as metric parity1 across two or more relevantly compara- ble entities such that the di?erence between them is deemed socially, legally, and/or culturally irrelevant. For instance, group fairness may be de?ned as demographic parity, meaning equality in the likelihood of a given outcome condi- tioned on the group (e.g. gender, age, disability)[4]. Formally:
                           P (yˆ | g = gi ) = P (yˆ | g = gj )?gi, gj ? {1, ..., G }	(1) where G is the total number of groups and yˆ is the likelihood of a given outcome.
  Epistemic Utility. We de?ne epistemic utility as the value or usefulness of information in terms of improving knowl- edge, understanding, or beliefs. It encompasses the epistemic bene?ts gained from acquiring accurate, reliable, and relevant information. These bene?ts could, of course, enhance decision-making, prediction, problem-solving, or over- all intellectual development, but epistemic utility refers only to the bene?ts in terms of the agent's epistemic state (even if those improvements do not immediately lead to improved outcomes). It thereby captures the intrinsic value of acquiring knowledge and understanding. It underscores the role of information as a valuable resource for intellectual growth, social mobility and production, and captures the motivational value of information for people's behavior.

3 RELATED WORK
This work is most closely related to e?orts to understand fairness in recommender systems, many of which de?ne fairness as parity in some performance measure across some sensitive attribute [5, 6, 7, 8, 9, 10]. Methods for individual notions of fairness similarly require a domain-speci?c similarity function and focus on trade-o?s between individual and group fairness[11, 12]. In contrast, we start from the position that fairness requires equity across all individuals, rather than being de?ned solely through comparisons. This approach enables us to account for cases of unfairness where everyone is worse o? (including situations with non-comparative injustices), not only those in which one group is.
  More speci?cally, prior work has aimed to ensure proportional representation and exposure, often in e-commerce settings. Measures such as disparate exposure and visibility still require access to sensitive or group attributes [8, 9, 10]. Calibration, in which the expected proportions of predicted classes match those observed in available data, implements group fairness without explicit knowledge of sensitive attribute status[13, 14, 15, 16, 17]. However, its application in domains such as news can contribute to hegemonic regimes of representation, making it limited to applications where less diversity is valued by users[17]. Additionally, fairness de?nitions that target disparities in utility compare average sensitive attribute group performance rather than providing assurances for individuals or the entire population as a whole [11, 18, 19, 20].
  Concerns about unfairness in personalized algorithms is connected to research on epistemic injustice[21]. The core concern in this latter work is that epistemic limitations may impair an agent's ability to recognize or respond to

1within some tolerance as is legally permissible, e.g., the four-?fths rule[3]

injustice. For example, if an individual does not have the concept of 'racial discrimination', then they might fail to detect its occurrence despite su?ering harms from it. That is, the injustice is not only about what happens to the individual, but also about their (epistemic) inability to recognize, understand, and respond to it. Personalized systems can readily lead to di?erent individuals having di?erent information - in fact, that is one goal of such systems - and so we expect that individuals would develop di?erent concepts as a result. Hence, personalized systems can signi?cantly raise the risks of epistemic injustice.
  Research on over-personalization and information access is similarly closely related. While the in?uence of personal- ization in ?lter bubbles and echo chambers has been denied for purely technical systems [22, 23, 24], when considering human interactions, the in?uence becomes somewhat complex. This is, in part, due to signi?cant variability in how users engage with a site, accept or reject information, and perceive the impartiality of the content they receive [25, 26, 27, 28, 29]. Our research adopts a sociotechnical approach to de?ne and scope the problem, taking into account the concept of epistemic utility and making reasonable assessments of human e?ort.

4 CORE TENSION: PERSONALIZATION AND STANDARD APPROACHES TO FAIRNESS
Fairness is often de?ned as metric parity across similar groups or individuals, where determining similarity across units remains an open question. In search settings, one might start by grouping together individuals with similar values and interests, as, on fairness grounds, they may be expected to have a similar disambiguation or content recommendation experience. However, determining values can be di?cult to operationalize because explicit and implicit preferences may be con?icting or di?cult to collect [30]. In addition, the level of granularity by which to measure such values remains unknown. In this section, we construct arguments for why user similarity of values is di?cult, impractical, or perhaps even impossible to determine, thereby making similarity-based fairness de?nitions di?cult to deploy or conceptually impossible.

  Sensitive attributes are problematic and coarse proxies for values. The simplest approach to constructing "similar" groups for fairness considerations could use sensitive attributes (i.e. race, ethnicity, gender, age, religious a?liation, disability, etc). However, this approach assumes that individuals with the same sensitive attributes not only face similar struggles, lived experiences, discrimination, and prejudice, but also have relevantly similar values. In practice, this approach would likely result in the treatment of minority groups as monoliths - perhaps even as a homogeneous 'Other'. This is problematic not only in its instrumental failures of insu?cient speci?city and complexity, but also in its intrinsic treatment of a single sensitive attribute as representative of all values. Additionally, conditional groupings only ensure parity guarantees for the average member of a group, not for each individual nor necessarily an individual that exists, therefore enabling unfairness for anyone that doesn't satisfy such assumptions.
  Taking an intersectional approach, i.e. considering subgroups of sensitive attributes may better approximate sim- ilarity of values. Consider, for example, race and gender, meaning groups such as "Black Women" and "White Men". This still considers an entire group of people homogeneously, thereby rendering the plight of, for example, all Black Women into a singular experience. In the extreme, considering all possible sub-groups across all sensitive attributes faces practical challenges, as each subgroup becomes impractically small (for data availability purposes), thereby reify- ing sensitive attributes in a constant struggle for relative popularity and public consciousness [31].

  User search history is noisy, underdetermines values, and can vary temporally. An alternative approach would be to use the individual's behavior to determine the relevant group for fairness considerations. For instance, in the context

of a search engine, we could condition on exact search history. Unfortunately, this approach faces three open chal- lenges which we highlight in the context of search, though the problems naturally generalize to other systems that use observed behavior to personalize on the basis of unobserved (inferred) features.
  First, not all search behaviors are equally informative. For example, a search query may be made due to di?erent situational demands (e.g., homework, third party question) or may arise from very di?erent motivations (e.g., devil's advocate vs. searching for validative or supporting arguments). Hence, a speci?c history of search queries may or may not be informative about an individual's values; the history alone is not su?cient.
  Second, even if search behaviors are "genuine," they are noisy indicators for the totality of an individual's interests or values. For example, the individual agent might not search for a topic they already have su?cient information on, but it may still be a core value. More generally, consider two users with identical search histories at any given point in time. Conditioning on search history would imply that they should receive similar experiences. However, it is highly unlikely that these individuals have the exact same sets of values and interests: that is, multiple sets of values and interests could be consistent with the observed behavior (i.e., the space of values are underdetermined by the evidence). Therefore, since queries subject to the limitations of observational data and missing contextual information, conditioning on search history alone has the potential to greatly under-specify an individual's values.
  Finally, search history provides no guarantees of stability over time. Consider, again, two individuals with the same user history. Adding a di?erent element to each user's search history could lead them to be placed in fundamentally di?erent groups. More generally, at any point in time, there are in?nitely many ways that a search history could progress, thereby making it impossible to guarantee fairness over time for these two individuals, let alone all. Fairness groupings not sensitive to the time and search history, therefore, may not ever converge to fair groupings in the limit. This echoes a similar problem in fairness research, where fairness constraints at each round of deployment do not ensure long-term aggregate parity in the same metric [32]. While this has not rendered particular fairness metrics obsolete, this instability points to ambiguity in the extent to which fairness must provide guarantees temporally.

5 A DIFFERENT APPROACH
5.1 Personalization
Delving further into the notion of a system tailored to each person's needs, let us de?ne personalization. Given a search engine, companies may strive to deliver a service that maximizes epistemic utility and convenience. In order to avoid wasting time, resources, or eroding consumer patience, search engine providers may optimize to disambiguate queries as e?ciently as possible. Increases in e?ciency can be made by leveraging all accessible information about a user. This may include search history, publicly available data, social media pro?les, and inferred or disclosed demographic information to accurately disambiguate a user's query. This motivates the following de?nition:

  De?nition 5.1. In?nite Personalization. Consider a platform that recommends relevant content to a user uj tai- lored to their previous history hj,t at time t . Since the space of all possible information is very large, for each piece of content ci ? C, the system calculates probabilities P (ci | hj,t ) estimating the likelihood of relevance. We de?ne in?nite personalization as the phenomena in which a system is allowed to produce extremely ?ne-grained personalization. Formally, there exists some ci such that P (ci | hj,t ) < 0 and P (ci | hj,t +E ) < 0 , meaning that there exists some content such that it is arbitrarily improbable < 0 to recommend a particular piece of content ci to a user, even if a user were to add f queries to their search history.

  For instance, user uj with a search history exclusively composed of cats inputs "bengal toys" into a search engine and receives recommendations about Bengal cat toys. We consider the system to be exhibiting in?nite personalization if the user can take f queries about dogs (or anything else) and still not receive a recommendation for Bengal dog toys when searching "bengal toys".

  Having de?ned personalization and it's e?ects in the extreme case, we now de?ne fairness as equity and provide motivation for the outcome we target.

5.2 Equity
In ensuring parity across similar groups, fairness guarantees are only made across those deemed su?ciently similar. An equity-based approach, however, centers around every individual achieving their desired outcome. Such approaches promote social cohesion, collective liberation, and address long-standing inequities, such as poverty, public health, social mobility, and overall well-being. Moreover, they tackle concerns regarding temporal stability and robustness by establishing a consistent criterion for each deployment phase. Equity-based approaches lend themselves to audits, legislation, and transparency, as they de?ne a singular outcome requirement for all users. An open challenge, however, is what metrics to de?ne equity with respect to, as each metric targets a di?erent kind of unfairness.
  Equity across quality of information targets unfairness when one person gets more utility from their query com- pared to a di?erent query of another person. For example, this type of equity requires that we equalize the number of recommendations and quality of information provided for any given query. However, this is infeasible as we can- not control the quantity of information known for a given topic. Thus, given that there are in?nitely many distinct queries any user can make, ensuring that each query has the same number of relevant articles would require perfect knowledge.
  Equity in speed of access to information targets unfairness when one person receives the relevant information faster than another. This equalizes the utility for every user by either: slowing down or acting intentionally uncooperatively for queries that are more easily disambiguated, or speeding up the service for "slower" queries. Both are infeasible. The former fails to maximize overall utility and sacri?ces function in the name of fairness. This in turn renders adoption by stakeholders subject to capitalistic pressures even more unlikely. The latter is also infeasible, as we cannot directly control disambiguation rate and therefore speed up some queries over others. More generally, information is an in- ?nitely shareable resource, so providing information to one person does not deprive someone else the bene?t of that same information. Therefore speed of access to information is a poor target for an equity-based intervention.
  In contrast, equity in epistemic utility targets unfairness in personalized access to the information resources. This is motivated by the intuition that although we cannot directly control the availability of information or disambiguation rate, everyone should have some baseline access to all information. Thus, we propose an upper bound on the number of queries to access to any piece of information. This means that if the information does exist, all users should be able to access it by exerting a "reasonable" amount of e?ort. This does not require that all queries have the same accessibility or disambiguation, nor that all users be provided the exact same service. It does provide a soft guarantee that everyone is able to achieve some base-level epistemic utility without requiring the grouping or comparison of individuals on the basis of their values, needs, or histories. Epistemic utility, particularly through the internet, is recognized as a universal human right by Article 19 of the Universal Declaration of Human Rights [33], stating that, "Everyone has the right to freedom...to seek, receive and impart information and ideas through any media and regardless of frontiers."

De?nition 5.2. f-Equity Fairness. Given a user uj with history hj,t , we de?ne f-Equity fairness as the upper bound
(f) of the number of additions to the search history such that for any given content ci ? C, adding f queries to the history will result in P (ci | hj,t +E ) = 0 , enabling the user to obtain the relevant content. Here, we consider the case in which the relevant content does not exist as out of scope and focus purely on access.

  Embracing equity as a fundamental principle of fairness presents a range of complex challenges. These challenges encompass de?ning a baseline level of equity for all users, determining the pertinent factors that contribute to establish- ing such a baseline, and deliberating whether these considerations should be domain-speci?c or universally applicable. Rather than proposing a singular standard, we propose adopting a heuristic operationalization that prioritizes those who are most disadvantaged in terms of knowledge and understanding. Our intention is not to argue for a strictly pri- oritarian perspective, but that the incremental advancements can achieve the ultimate goal of equity in the long-term. Thus, we advocate for equity as a guiding concept to achieve equality by inductively reaching fairness in the limit.
  Having established de?nitions in?nite personalization and f-equity fairness, next we ?esh out the tension between the two.

6 INHERENT CONFLICT AND TRADE-OFFS
In?nite personalization, we posit, comes at a cost to f-equity fairness. More explicitly, in in?nite personalization, the relevance probability of some information ci to a user can be less than 0 . This is due to the e?ects of personalization: indexing and re-ranking for e?cient navigation across the vast space of information. However, this violates f-equity
fairness, as some information will be e?ectively rendered unreachable within a ?xed window of queries. We provide some examples of trade-o?s below:
• Search engine personalization prioritizes results relative to a user's input query and prior history. For any given topic orthogonal to a user's past history, personalization may rank desired items far beyond the average, ex- pected, or typical number of results any given user explores, rendering it practically infeasible to reach.
• Social media serves many functions: dissemination of news and information, entertainment, and social connec-
tion. Personalization in the extreme may contribute to propagation of mis- and disinformation, echo chambers, ?lter bubbles, radicalization, and social disconnection.
• Github Copilot (developed by OpenAI) focuses on code completion, providing suggestions for code lines or
entire functions directly integrated into interactive development environments. Here, personalization has the potential to provide codebase-speci?c suggestions and completion, but may also infringe on a coder's ability construct novel functionality.
• Google Bard is a language model for synthesizing insights for settings with a wide range of opinions, perspec-
tives, or no right answer. As an augmentation to search-engine information retrieval, this may facilitate more e?cient ordering and disambiguation of search results, but may also infringe on access to seemingly irrelevant information.
• ChatGPT is a general-purpose large language model (LLM) designed to engage in human-like conversations and
answer a wide range of questions. Personalization in an extreme case, may lead to conversational loops - or conversations that remain within the scope of a single topic, rather than having the ?exibility to move between topics despite a user's prompt or request. 2
2Here we list several examples that use LLMs due to their relevance to personalization, information synthesis and curation, and appeals to human- language usability. We acknowledge that there are many open relevant problems related to personalization, such as contribution or exacerbation of representational harms. For the scope of this work, we focus on how such systems can impact access to information.

The key takeaway of this list is not that personalization necessitates unfairness or inequity, nor that fairness is not achievable in personalized systems. Rather, to emphasize that personalization without consideration of fairness can readily lead to systems that deny some individuals a base-level of expected epistemic utility.

7 POLICY GOALS AND EXAMPLES OF IMPLEMENTATIONS
While this conceptual analysis provides guidance about how best to think about fairness in the context of personalized algorithms, it does not provide guidance about how to achieve fairer, less biased systems. As we turn to more practical recommendations, we start by considering key stakeholders and their associated roles (Table 1). This creates a lexicon and clari?es the expected function of each stakeholder.
  Each of these stakeholders has a wide range of actions available that can lead to fairer personalized systems (see Table 2). These actions include a wide range of governance mechanisms, ranging from hard (e.g., regulation) to soft (e.g., social norms). We emphasize that there is no single response to "?x" unfairness in personalized systems: inevitably, many interdependent actions will likely need to be taken in order to make progress on this problem. Moreover, these are by no means intended to be exhaustive, but rather provide a grounded starting point that emphasizes the necessity of cross-stakeholder collaborations.



Stakeholder	Role
Government	Develop guidelines, principles, procedures, regulations, and standards for safe, sustainable deployment. May hold the power to create structural sup- porting systems directly within the government or ?nancially support exter- nal systems of enforcement of such regulations. May also enact other means of operationalizing policy, such as hosting interdisciplinary research summits or sending representatives to build collaborations with other stakeholders to better inform outputs. This stakeholder may also set roles and norms for other stakeholder responsibilities and consequences.

Civil Society	Conduct evaluations and investigations of algorithms and software devel- oped by others. This stakeholder may serve as an external, third-party entity for conducting unbiased evaluations of performance and compliance with policy. May be responsible for developing novel methods of measurement.
Industry		Implement technical methods, tools, and technological innovations, usually manifesting as a consumer-facing service or product. May be subject to prac- tical implementation constraints (i.e. ?nancial, competition, or legislative). While a particular product may be theoretically feasible, it may not be prac- tically viable or sustainable for a company to produce. Responsibilities may also include foreseeing and adequately mitigating harms of deployed prod- ucts on those directly and indirectly a?ected.

Academia (broad)		Develop and implement technical, theoretical, and conceptual knowledge without direct consideration of pro?ts. Results can include work that builds upon understanding, implications, or evaluation methods (i.e. downstream impacts, sociotechnical approaches). This stakeholder has the potential to act as an unbiased actor in developing best-practices for sustainable, long-term practices, potentially at the direct cost of pro?ts (i.e. considering environmen- tal, social, and cultural norms and in?uences).
General Public		Provide input, feedback, criticism, and opinions on direction and alignment with values for research and innovation via implicit and explicit signals on the order of communities, groups, and/or individuals. Public support (or lack thereof) can be collected across a multitude of avenues and granularities, in- cluding ?nancial support, interest, engagement, collective action, protests, or- ganizations, and projects.

Table 1. Stakeholder Roles. We define the expected roles of stakeholders for clarity of responsibilities enumerated in Table 2. We note that these responsibilities are not restrictive. There are many ways in which one stakeholder may overlap or even take over the responsibilities of another. For instance, if a company develops a policy for user privacy while it's competitors do not, self-imposed constraints on data collection may impact their product performance. However, as this design value receives public support, this can become a standard competitive service without the intervention of government or civil society.



Goal	Example Implementation/Operationalization
Mitigation	Industry, Academia: Develop algorithms that facilitate connections between dis- joint parts of the data manifold. This might be analogous to connecting di?erent parts of the internet together through links, but done by creating arbitrary injec- tions that fabricate similarities, covariances, or links between disjoint parts of the training data.
Government: Construct structural incentives (?nancial, prestige/recognition, standards) for achieving baseline levels of interconnectivity to ensure equitable access.
Audit	Industry, Academia, Civil Society: Develop measures that quantify epistemic utility for users using a range of behaviors and data (e.g. tool use, experienced frus- tration, time to complete a task, etc.). Produce formal, domain-speci?c representa- tions of distributions of utility, such as non-uniform distributions being indicators of threats to fairness. Measure utility and study barriers to utility for non-users and those who have left the platform.
Civil Society: Report the utility distributions for non-users to compare against those that are already using the service.
Academia: De?ning and measuring downstream impact may be essential to thor- ough quanti?cation of disparities and unfairness. Downstream impacts such as so- cial in?uences on platform usage can be in?uenced by an individual's experience. Government: Regulation for timing and structure of fairness audits for person- alized systems, including guidance on composition of internal and external stake- holders.
Transparency	Industry, Civil Society: Disclosure of aforementioned distributions of utility across various groups of users. Reports should include comments about whether these distributions are problematic, what are the proposed solutions, and on what timeline.
Government: Clear consequences and/or reparations for users a?ected through- out improvement and for continued negligence.
General Public: Active engagement and feedback on reports to inform appropriate consequences and informed public (dis)favor.
Individual Control	Industry, Academia: Design control mechanisms over the degree of exploration to a query when serving recommendations. This may include prompting users for additional input before producing a prediction. Alternatively, allowing users to re- set their history or set manual preferences over personalization ?lters.
Government: Regulation and enforcement of satisfaction and compliance. General Public: Participation and feedback on norm setting of what features should be available, how accessible services should be, determine embedded val- ues such as degree of "reasonableness" that are feasible and realistic for all users.
Education & Awareness Industry, Academia, Government, General Public: Workshops and training sessions can facilitate comprehension of the dangers, limitations, and better cali- brate users to appropriate expectations of functionality. These workshops can in- clude education on rights and how to advocate for them. For instance, if companies fail to comply, where can people ?le their grievances so they can be aggregated and collectively analyzed?
Industry, Academia, Civil Society: Development of interactive tools that enable visualization and comparison of an individual's utility compared to the general population or those relevantly comparable.
Civil society, Government: Regulation and continued meMasaunursecmripetnsutbtmoitteend stouAreCM such tools ?ag anomalous data and ensure improvement within some time-frame. Create and publicly release of reports of corrections, adjustments, and investiga- tions made to remedy grievances.
Table 2. Example Policy Interventions and Goals by Stakeholder. Although we separate actions by stakeholders, we emphasize that these goals require collaborations and contributions across multiple stakeholders to achieve such solutions.

REFERENCES
[1] Zeynep Tufekci. How recommendationalgorithms run the world. Apr. 2019. URL: https://www.wired.com/story/how-recommendation-
[2] Gurkan Solmaz, Jesmin Jahan Tithi, and Juan Miguel de Joya. Why algorithmic fairness? Oct. 2020. URL: https://selects.acm.org/selectio
[3] Elizabeth Anne Watkins, Michael McKenna, and Jiahao Chen. "The four-?fths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness". In: arXiv preprint arXiv:2202.09519 (2022).
[4] Cynthia Dwork et al. "Fairness through awareness". In: Proceedings of the 3rd innovations in theoretical computer science conference. 2012, pp. 214-226.
[5] Rishabh Mehrotra et al. "Towards a fair marketplace: Counterfactual evaluation of the trade-o? between rele- vance, fairness & satisfaction in recommendation systems". In: Proceedings of the 27th acm international confer- ence on information and knowledge management. 2018, pp. 2243-2251.
[6] Nasim Sonboli et al. "Opportunistic multi-aspect fairness through personalized re-ranking". In: Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization. 2020, pp. 239-247.
[7] Weiwen Liu et al. "Personalized fairness-aware re-ranking for microlending". In: Proceedings of the 13th ACM Conference on Recommender Systems. 2019, pp. 467-471.
[8] Ludovico Boratto, Gianni Fenu, and Mirko Marras. "Interplay between upsampling and regularization for provider fairness in recommender systems". In: User Modeling and User-Adapted Interaction 31.3 (2021), pp. 421-455.
[9] Elizabeth Gómez et al. "The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems". In: Proceedings of the 44th International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval. 2021, pp. 1808-1812.
[10] Abhisek Dash et al. "When the umpire is also a player: Bias in private label product recommendations on e- commerce marketplaces". In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans- parency. 2021, pp. 873-884.
[11] Ziwei Zhu, Xia Hu, and James Caverlee. "Fairness-aware tensor-based recommendation". In: Proceedings of the 27th ACM international conference on information and knowledge management. 2018, pp. 1153-1162.
[12] Bora Edizel et al. "FaiRecSys: mitigating algorithmic bias in recommender systems". In: International Journal of Data Science and Analytics 9 (2020), pp. 197-213.
[13] Himan Abdollahpouri et al. "Calibrated recommendations as a minimum-cost ?ow problem". In: Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 2023, pp. 571-579.
[14] Himan Abdollahpouri et al. "User-centered evaluation of popularity bias in recommender systems". In: Proceed- ings of the 29th ACM Conference on User Modeling, Adaptation and Personalization. 2021, pp. 119-129.
[15] Bruna Wundervald. "Cluster-based quotas for fairness improvements in music recommendation systems". In:
International Journal of Multimedia Information Retrieval 10.1 (2021), pp. 25-32.
[16] Diego Corrêa da Silva, Marcelo Garcia Manzato, and Frederico Araújo Durão. "Exploiting personalized calibra- tion and metrics for fairness recommendation". In: Expert Systems with Applications 181 (2021), p. 115112.
[17] Yashar Deldjoo et al. "Fairness in recommender systems: research landscape and future directions". In: User Modeling and User-Adapted Interaction (2023), pp. 1-50.
[18] Yashar Deldjoo et al. "Recommender systems fairness evaluation via generalized cross entropy". In: arXiv preprint arXiv:1908.06708 (2019).

[19] Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. "Adversarial machine learning in recommender systems (aml-recsys)". In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020,
pp. 869-872.
[20] Yunqi Li et al. "User-oriented fairness in recommendation". In: Proceedings of the Web Conference 2021. 2021,
pp. 624-632.
[21] Miranda Fricker. Epistemic injustice: Power and the ethics of knowing. Oxford University Press, 2007.
[22] Cédric Courtois, Laura Slechten, and Lennert Coenen. "Challenging Google Search ?lter bubbles in social and political information: Disconforming evidence from a digital methods case study". In: Telematics and Informatics
35.7 (2018), pp. 2006-2015.
[23] William H Dutton et al. "Searching through ?lter bubbles, echo chambers". In: Society and the internet: How networks of information and communication are changing our lives (2019), p. 228.
[24] Efrat Nechushtai and Seth C Lewis. "What kind of news gatekeepers do we want machines to be? Filter bub- bles, fragmentation, and the normative dimensions of algorithmic recommendations". In: Computers in human behavior 90 (2019), pp. 298-307.
[25] Tawanna R Dillahunt, Christopher A Brooks, and Samarth Gulati. "Detecting and visualizing ?lter bubbles in Google and Bing". In: Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems. 2015, pp. 1851-1856.
[26] Axel G Ekström, Diederick C Niehorster, and Erik J Olsson. "Self-imposed ?lter bubbles: Selective attention and exposure in online search". In: Computers in Human Behavior Reports 7 (2022), p. 100226.
[27] Mykola Makhortykh and Mariëlle Wijermars. "Can ?lter bubbles protect information freedom? Discussions of algorithmic news recommenders in Eastern Europe". In: Digital Journalism (2021), pp. 1-25.
[28] Jaime Teevan. "How people recall, recognize, and reuse search results". In: ACM Transactions on Information Systems (TOIS) 26.4 (2008), pp. 1-27.
[29] Kelly Shelton. Council post: The value of search results rankings. Nov. 2017. URL: https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/t
[30] Tessa ES Charlesworth and Mahzarin R Banaji. "Patterns of implicit and explicit attitudes: IV. change and sta- bility from 2007 to 2020". In: Psychological Science 33.9 (2022), pp. 1347-1371.
[31] Youjin Kong. "Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Anal- ysis". In: 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 485-494.
[32] Lily Hu and Yiling Chen. "A short-term intervention for long-term fairness in the labor market". In: Proceedings of the 2018 World Wide Web Conference. 2018, pp. 1389-1398.
[33] URL: https://www.unesco.org/en/right-information.











Manuscript submitted to ACM






























This figure "acm-jdslogo.png" is available in "png"	format from: http://arxiv.org/ps/2309.11503v1






























