Fairness and Bias in Multimodal AI: A Survey
Tosin Adewumi*† , Lama Alkhaled† , Namrata Gurung1 , Goya van Boven2 , Irene Pagliai3
†

Machine Learning Group, LTU, Sweden, 1 QualityMinds GmbH, Germany,
2
Utrecht University, the Netherlands, 3 University of Göttingen, Germany,
†
2

firstname.lastname@ltu.se, 1 namrata.gurung@qualityminds.de
j.g.vanboven@students.uu.nl, 3 irene.pagliai@uni-goettingen.de
Abstract

arXiv:2406.19097v2 [cs.CL] 7 Sep 2024

The importance of addressing fairness and bias
in artificial intelligence (AI) systems cannot be
over-emphasized. Mainstream media has been
awashed with news of incidents around stereotypes and other types of bias in many of these
systems in recent years. In this survey, we fill a
gap with regards to the relatively minimal study
of fairness and bias in Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing 50 examples of datasets
and models related to both types of AI along
with the challenges of bias affecting them. We
discuss the less-mentioned category of mitigating bias, preprocessing (with particular attention on the first part of it, which we call preuse).
The method is less-mentioned compared to the
two well-known ones in the literature: intrinsic and extrinsic mitigation methods. We critically discuss the various ways researchers are
addressing these challenges. Our method involved two slightly different search queries on
two reputable search engines, Google Scholar
and Web of Science (WoS), which revealed that
for the queries ‘Fairness and bias in Large
Multimodal Models’ and ‘Fairness and bias in
Large Language Models’, 33,400 and 538,000
links are the initial results, respectively, for
Scholar while 4 and 50 links are the initial results, respectively, for WoS. For reproducibility
and verification, we provide links to the search
results and the citations to all the final reviewed
papers. We believe this work contributes to filling this gap and providing insight to researchers
and other stakeholders on ways to address the
challenges of fairness and bias in multimodal
and language AI.

1

Introduction

Fairness and bias are very important topics that
cut across many domains in the society. The rapid
advancements in the research and applications of artificial intelligence (AI) have made them even more
compelling in recent times, such that many studies have emerged on them (Frankel and Vendrow,

2020; Booth et al., 2021; Adewumi et al., 2022; Teo
et al., 2024). One important gap in the literature,
however, is that there is relatively minimal study or
survey on ‘Fairness and bias in Large Multimodal
Models.’ By multimodal AI, we mean the datasets
or AI models that can take one or more modalities as input and/or another as output. This gap is
evidenced by the fact that there are fewer works
around the topic. For example, a query search
on Google Scholar returns 33,400 links compared
to 538,000 links for ‘Fairness and bias in Large
Language Models’ (where the first query search is
equivalent to the boolean operation fairness AND
and AND bias AND in AND large AND multimodal
AND models).1 This implies more than 16 times the
result compared to the former. However, filtering
the publication year range to 2014-2024 reduces
the links to 17,200 and 19,300, respectively. We
intend to contribute in filling that gap in this work.
The two terms, fairness and bias, are strongly
related but fairness is concerned with equality and
justice while bias is concerned with systematic error, which may arise from human prejudices (Booth
et al., 2021; Alkhaled et al., 2023). For the purpose
of this survey, fairness may be defined as equal
representation with regards to a given Sensitive
Attribute (SA) (Hutchinson and Mitchell, 2019;
Frankel and Vendrow, 2020). Hence, we may consider a generative artifical intelligence (GenAI) to
be fair if it generates both male and female samples
with equal probabilities, with regards to the SA
gender (Teo et al., 2024). Bias is a (non-random)
systematic error in a measurement resulting in a
difference in accuracy in one entity compared to
another, given the ground truth (Booth et al., 2021;
Scheuneman, 1979). We acknowledge there are
other quantitative definitions of fairness and bias,
as noted by Hutchinson and Mitchell (2019) and
Weidinger et al. (2022).
It appears the emergence of big data, which
1

July 10, 2024

has brought rapid advancement in the state-ofthe-art (SotA), also brought along the increase in
poor quality content and prediction, such as the
increased criminal prediction for Black and Latino
people observed by Birhane et al. (2024a). Similar issues are observed across many domains, including healthcare, employment, forensics, criminal justice, credit scoring, and computational social science, among others (Liang et al., 2021b;
Ferrara, 2023a; Landers and Behrend, 2023; Han,
2023). According to Wolfe et al. (2023), the model
VQGAN-CLIP, similarly to Stable Diffusion, generated sexualized images for the harmless prompt
"a 17 year old girl," 73% of the time. The comparison to a similar prompt with the term "girl"
replaced with "boy" shows a sharp contrast.
In related work, many of the relatively recent
surveys on fairness and bias in AI appear to have
been of a general nature or focused on other areas.
Pagano et al. (2023) focused their attention on ML
generally and the 5-year period between 2017 and
2022, thereby missing the nuances and some of
the details related to natural language processing
(NLP) and multimodal AI. Mehrabi et al. (2021)
surveyed applications that have exhibited bias in
different domains, listed sources of bias in these
applications and created a taxonomy for fairness
definitions. On the other hand, Le Quy et al. (2022)
paid attention to benchmark tabular datasets for
fairness, analysing relationships between different
protected and class attributes. Balayn et al. (2021)
focused on data bias in data engineering and management research, arguing for the enforcement of
fairness requirements and constraints on the data
for training and evaluating systems. Their survey
method is limited in that it restricted part of its
literature search between 2019 and 2020. In our
work, in addition to discussing datasets that make
AI models biased and the datasets for evaluating
bias and fairness, we discuss other important related areas, such as the mitigation strategies.
Blodgett et al. (2020) surveyed over 140 articles
about bias in NLP and realized that the stated motivations are usually inconsistent and vague, and the
articles do not engage with the broader applicable
literature that is external to NLP. They, therefore,
made 3 recommendations for those in this field: (1)
establish the relationships in language and social
hierarchies by referring to the broader literature
outside of NLP, (2) be explicitly clear on why the
system described as biased is harmful, and (3) eval-

uate the language of people affected by the biased
systems. Their survey was restricted to only articles on text-based NLP, thereby excluding speech
or multimodal AI, and limited to articles before
May 2020. However, we follow their recommendations and our work answers some of the research
questions identified in their work. For example,
Section 4 and subsection 5.2.1 address the question How are datasets collected?. The survey by
Sun et al. (2019) focused on recognizing and mitigating NLP gender bias and the authors discussed
this based on four forms of representation bias:
denigration, stereotyping, recognition, and underrepresentation. In Haltaufderheide and Ranisch
(2024), they identified ethical issues around fairness and bias with LLMs in medicine and healthcare. Additional works that have surveyed fairness
and bias in LLMs include Bender et al. (2021);
Meade et al. (2022); Gallegos et al. (2023); Chang
et al. (2024); Myers et al. (2024).
In view of the foregoing gap and challenges,
this work critically surveys the literature with the
primary objective of ascertaining what the state
of work is on fairness and bias in multimodal AI,
thereby making the following key contributions.
1. We fill the gap with a comprehensive survey
of fairness and bias across a wide spectrum
of LMMs, LLMs, and multimodal datasets,
providing 50 examples of datasets and models
in a structured way, along with the challenges
of bias affecting them.
2. We discuss the less-mentioned category of
mitigating bias in NLP (i.e. preprocessing), though more common in general ML
(Mehrabi et al., 2021; Pagano et al., 2023),
with particular attention on the first part of
it, which we call preuse. The other two wellknown mitigation methods in the literature are
the intrinsic (or in-processing) and extrinsic
(or post-processing) methods (Ramesh et al.,
2023; Cabello et al., 2023).
3. We critically discuss many important approaches to addressing the challenges of fairness and bias.
The rest of this paper is organised as follows. In
the following Section (2), we highlight some of the
theories around fairness and bias in AI. In Section
3, we explain the method used for this survey. In
Section 4, we focus on various works along the

2.2.2

Equity Theory

two paradigms of LMMs and LLMs, discussing
the datasets and models in the literature. In Section
5, we discuss widely on the methods to evaluate
fairness and bias, the datasets for evaluation, and
the debiasing strategies. We conclude the survey
in Section 6 with a summary and possible future
work.

Equity theory uses a unidimensional concept of
fairness instead of multidimensional, according to
Leventhal (1980). It perceives justice solely on the
merit principle and the final distribution of reward
(or punishment), where reward is proportional to
contribution (Adams and Freedman, 1976).

2

Fairness and Bias in AI

2.2.3

2.1

Implicit and Explicit Bias

Just as inanimate objects have no emotions or
thoughts, objectification theory establishes a view
of a subject as primarily without human characteristics, especially for women and girls (Fredrickson and Roberts, 1997; Heflick et al., 2011; Andrighetto et al., 2019). The theory identifies sexual
objectification bias, which is when the emotions
or thoughts of a person are disregarded and one is
treated as mere body parts for sex (Fredrickson and
Roberts, 1997; Wolfe et al., 2023).

There is distinction between implicit and explicit
bias such that, given two sets of terms that express
the bias axis (e.g. gender or race), one set of male
gender terms could be {S1 } = {dad, man} and
the other set of female terms be {S2 } = {mum,
woman}. Implicit bias is sufficiently specified with
both lists. However, explicit bias requires two additional sets of attributes {A1 } = {engineer, doctor} and {A2 } = {caregiver, carer} that express
the terms to which the earlier gendered terms exhibit association, albeit to different levels. Hence,
a gender biased system could result in male terms
in {S1 } being strongly associated with attributes
of career terms {A1 } compared to female terms
{S2 }, which could be strongly associated with
attributes of home-related terms {A2 } (Friedrich
et al., 2021).
Besides these two broad distinction of bias, there
are many types of bias, depending on the philosophical or social perspectives that may be taken. Hence,
a discussion about every possible type of fairness
or bias is beyond the scope of this study but we
refer readers to Mehrabi et al. (2021), Van der Wal
et al. (2024) and Navigli et al. (2023) for some of
the many types that may be listed.
2.2

Concepts of Fairness and Bias

In this section, we highlight a few of the concepts
around fairness and bias found in the Social Science literature.
2.2.1

Justice Theory

The theory is regarded as a three-part framework
of distributive, interactional, and procedural justice perceptions (Greenberg, 1990; Landers and
Behrend, 2023). Distributive justice, when outcomes are expected to be distributed equally, is the
overarching aspect related to AI fairness and bias,
according to Landers and Behrend (2023). It is
based on equality rules, need, or equity, which are
influenced by social and cultural values.

2.3

Objectification Theory

Consequences of Bias

Fredrickson and Roberts (1997) confirms that objectification victimises the subject and may result in
habitual body monitoring, thereby increasing mental health risks, sexual dysfunction, eating disorders, and depression. This unhealthy reality is also
confirmed by Swim et al. (2001). They realized
that sexist incidents occur more against women and
have negative emotional consequences for them.
Some of these incidents are traditional gender role
stereotypes, degrading remarks, and sexual objectification, which are found in the data used for training AI models. For details about the mechanics
of training language models, we refer readers to
Radford et al. (2019) and Hoffmann et al. (2022).
It is not surprising, therefore, that the use of these
models cause the same negative effects for those affected. Hence, fairness and bias are not only ethical
or moral issues but have legal implications (Landers and Behrend, 2023). In the United States (US),
disparate treatment because of sensitive attributes
is unlawful (Berry, 2015; Hutchinson and Mitchell,
2019; Meng et al., 2022a). This is also the case in
some other countries (Zafar et al., 2017).

3

Methodology

In order to have a fair and thorough survey for
the stated objective in Section 1, we followed the
general guidelines recommended for conducting a
systematic literature review, which is founded on a
rigorous and auditable methodology (Kitchenham,

2004; Brereton et al., 2007). We used two common
scientific search engines: Google Scholar and Web
of Science (WoS). Both are advantageous because
they index the main literature databases or publishers, including the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), Conference on Neural
Information Processing Systems (NeurIPS), Multidisciplinary Digital Publishing Institute (MDPI),
Public Knowledge Project (PKP), Massachusetts
Institute of Technology Press (MIT Press), Public
Library of Science (PLoS), Association of Computational Linguistics (ACL), AI Access Foundation (AIAF), Higher Education Press (HE Press),
Association of Measurement and Evaluation in Education and Psychology (EPODDER), New York
University School of Law (NYUSL), Academic
Conferences and Publishing International (ACPI),
Machine Learning Research Press (MLRP), Journal of Machine Learning Research (JMLR), and
Society of Photographic Instrumentation Engineers
(SPIE).
WoS provides a second assessment to Scholar
because of the limitations of the latter, including
its inclusion of non-peer-reviewed links or possible
predatory journals in its results of primary studies
(or papers) (Foltỳnek et al., 2019). The two search
engines are identified as largely reliable, helping
to mitigate the risk of incompleteness in the search
(Foltỳnek et al., 2019). For both, we employed a
similar multi-phase search process itemized below,
which helped to bring refinement to each search
and the final results. We are confident this approach
returned the most applicable results for the purpose
of this study.
1. In the first phase, we designed the following
two search queries based on our objective.
• Fairness and Bias in Large Multimodal
Models
• Fairness and Bias in Large Language
Models
The boolean operation, in both search engines,
for the first is equivalent to fairness AND and
AND bias AND in AND large AND multimodal
AND models while the second is fairness AND
and AND bias AND in AND large AND language AND models.
2. In the second phase, we filtered the results
based on the time period of just over 10 years

(2014 to 2024) and relevance (after a review of
paper titles and abstracts for over 200 papers).
For example, one result involved an article
about ‘executive coaching programs’ that has
nothing to do with AI or social bias. All the
filtered papers are in English. In addition, we
corrected for misplaced results (i.e. papers
that turned up in one search result though they
belong in the other; about 15 in multimodal
belonging in language and 9 in language belonging in multimodal, for Scholar). We removed papers published in journals in Beall’s
List of Potential Predatory Journals and Publishers,2 if present.
3. In the third phase, we critically reviewed the
papers for their contributions, including the
datasets, models, possible solutions proffered
on fairness and bias, and other relevant discussions.
More concretely, for WoS, we searched ‘All
Fields’ of the documents ‘Core Collection’ across
all ‘Editions’ without initially restricting the year
of publication. For Scholar, the first phase returned
33,400 and 538,000 result links over many pages
for the first and second queries, respectively, while
returning 4 and 50 for WoS.1 Filtering Scholar,
based on the time period, returned 17,200 and
19,300 links, which finally reduced to 69 and 101
at the end of the second phase for the first and second terms, respectively, while returning 8 and 44
for WoS. It is noteworthy that for Scholar, there
were equally 100 links to start with for both queries.
The boolean search operation involved all the individual words in any arbitrary order without case
sensitivity and narrowed the search to mostly relevant documents. We compare ‘Multimodal Models’ with only ‘Language Models’ in the search
terms because the latter, with the introduction of
the Transformer architecture (Vaswani et al., 2017),
have influenced computer vision (Yuan et al., 2021)
and they serve as important components for many
multimodal AI. Furthermore, we recognize that
multimodal implies the combination of ‘language’
(or ‘speech’) and ‘vision’ but we did not use this
distinction in our multimodal search because (1)
the boolean equivalent can result in false positives,
(2) even when we attempted it, we had less than
50% links in result compared to the second term of
2

beallslist.net/standalone-journals

Table 1: Distribution of scientific papers on Google
Scholar. (Filtered total is limited to the year range 2014 -

Table 2: Distribution of scientific papers on Web of
Science (WoS). (Filtered total is limited to the year range

2024 and to the first 100 links per search query, excluding
irrelevant links.)

2014 - 2024 and removes irrelevant results.)

# Publisher Multimodal
Unfiltered total 33,400
1 IEEE
10
2 Elsevier
3
3 ACM
14
4 Springer
7
5 NeurIPS
4
6 Nature
3
7 MDPI
2
8 MLRP
2
9 PKP
1
10 MIT Press 11 PubMed
1
12 Cambridge 13 PLoS
1
14 JMLR
15 ACL
5
16 SPIE
1
17 De Gruyter 1
18 Wiley
1
19 Sage
1
20 Academic 1
Pinnacle
Filtered
58
sub-total
21 arXiv
11
22 Preprints
Filtered to- 69
tal

Language
538,000
3
13
5
11
5
1
4
1
4
1
1
1
2
23
1
1

# Publisher
Unfiltered total
Corrected total
1 IEEE
2 Elsevier
3 ACM
4 Springer
5 NeurIPS
6 Nature
7 MDPI
22 MLRP
22 PKP
16 ACL
13 Wiley
17 AIAF
12 Now
Filtered total

served that there are fewer pair-reviewed papers on
multimodal models compared to language models.

4
77

23
1
101

only ‘language’ search, and (3) ‘multimodal’ is a
standard term in the field.
Finally, for Scholar, we captured archived papers (e.g. arXiv) because, sometimes, their peerreviewed versions exist but may not appear in the
result. The summary statistics of the search are
presented in Tables 1 and 2 while the references
to the papers and the link to the search results are
provided in the Appendix. The useful data from the
reviewed papers (or primary studies) about datasets,
AI models, and other contributions are then discussed in Sections 4.1, 4.2, and 5, respectively. We
note that the more relevant papers turn up on the
first page or at the top of the search while the quality of results degrade as one progresses through
the pages or list. From the Tables, it can be ob-

Multimodal Language
4
50
8
46
4
5
2
3
12
5
1
1
3
1
1
10
2
1
1
8
44

Findings on Fairness and Bias in LMMs
and LLMs

It is commonly agreed that AI models learn much
of their bias from the data they are trained on and
many datasets, especially those for pretraining, are
from the Internet, which contains a diverse spectrum of content (Wolfe and Caliskan, 2022a). Tables 3 and 4 summarize some relevant datasets and
the models beset by challenges of bias, respectively.
All the 25 datasets identified have their challenges
and by extension the 25 AI models which train
on them. Some of these challenges include stereotypes, porn, misogyny, racial, gender, religious,
cultural, age, and demographic biases.
4.1

LMMs

Liang et al. (2021a) acknowledged that multimodal
representations are challenging because they seek
to integrate information from multiple areas in applications like robotics, finance, healthcare and
more. However, multimodal AI has not enjoyed
enough resources to study generalization across
different modalities and the complexities of training. With regards to bias, Wolfe et al. (2023) found
evidence of sexual objectification bias in models

Table 3: Summary of Some Datasets and Their Fairness & Bias Challenges (Data in the lower part of the table are
usually used in downstream tasks).
#
1

Dataset
CommonCrawl (Raffel et al., 2020)

Modality
Text & Vision

2

LAION-400M & 5B (Schuhmann
et al., 2021, 2022)
WebImageText (WIT) (Radford
et al., 2021)

Text & Vision

Some Challenges of Bias/Fairness
Fake news, hate speech, porn & racism (Gehman et al.,
2020; Luccioni and Viviano, 2021)
Misogyny, stereotypes & porn (Birhane et al., 2021, 2024b)

Text & Vision

Racial, gender biases (Radford et al., 2021)

4
5
6
7
8

DataComp (Gadre et al., 2024)
WebLI (Chen et al., 2022)
CC3M-35L (Thapliyal et al., 2022)
COCO-35L (Thapliyal et al., 2022)
WIT (Srinivasan et al., 2021)

Text & Vision
Text & Vision
Text & Vision
Text & Vision
Text & Vision

Racial bias (Gadre et al., 2024)
Age, racial, gender biases & stereotypes (Chen et al., 2022)
Cultural bias (Thapliyal et al., 2022)
Cultural bias (Thapliyal et al., 2022)
Cultural bias (Srinivasan et al., 2021)

9

Colossal Cleaned CommonCrawl
(C4) (Raffel et al., 2020)
The Pile (Gao et al., 2020a)
CCAligned (El-Kishky et al., 2020)
OpenAI WebText (Radford et al.,
2019)
OpenWebText Corpus (OWTC)
ROOTS (Laurençon et al., 2022)

Text

Offensive language, racial bias (Raffel et al., 2020)

Text
Text
Text

Religious, racial, gender biases (Gao et al., 2020a)
Porn, racial bias (El-Kishky et al., 2020)
Gender, racial biases (Gehman et al., 2020)

Text
Text

Gender, racial biases (Gehman et al., 2020)
Cultural bias (Laurençon et al., 2022)

15
16
17

VoxCeleb 1 (Nagrani et al., 2020)
VoxCeleb 2 (Chung et al., 2018)
First Impressions (Escalante et al.,
2020)

Audio & Vision
Audio & Vision
Audio & Vision

Demographic, gender biases (Chung et al., 2018)
Demographic, gender biases (Chung et al., 2018)
Racial, gender biases (Yan et al., 2020)

18
19
20
21
22
23
24

XM3600 (Thapliyal et al., 2022)
VQA (Antol et al., 2015)
VQA 2 (Goyal et al., 2017)
MS COCO (Lin et al., 2014)
Multi30K (Elliott et al., 2016)
MIMIC-IV (Johnson et al., 2023)
MAB (Alkhaled et al., 2023)

Text & Vision
Text & Vision
Text & Vision
Text & Vision
Text & Vision
Text & Vision
Text

25

Twitter corpus (Huang et al., 2020b)

Text

Cultural bias (Thapliyal et al., 2022)
Gender bias (Ruggeri and Nozza, 2023)
Gender bias (Ruggeri and Nozza, 2023)
Gender bias (Cabello et al., 2023; Zhao et al., 2017)
Racial bias (Wang et al., 2022a)
Ethnic, racial, marital status biases (Meng et al., 2022a)
Racism, misogyny, stereotypes (Alkhaled et al., 2023;
Pagliai et al., 2024)
Age, gender, racial biases (Huang et al., 2020b)

3

10
11
12
13
14

based on Contrastive Language-Image Pretraining (CLIP). The 9 CLIP models that were investigated were trained on internet-wide web crawls.
CLIP is known to be quite accurate (Radford et al.,
2021), however, it also appears to have scaled the
biases inherent in its training data. Also, Wolfe
and Caliskan (2022a) found that more than Latino,
Asian or Black, White persons are more associated
with collective in-group words in embeddings from
CLIP (Radford et al., 2021), SLIP (Mu et al., 2022),
and BLIP (Li et al., 2022), as measured with Embedding Association Tests (EATs). For a definitive
assessment, their work would have benefited from
additional experiments involving data of people
outside the United States (US), since they used the
Chicago Face Database (CFD) (Ma et al., 2015).
CFD is a dataset of 597 people recruited in the U.S.
Similarly, Teo et al. (2024) found that Stable Diffusion exhibits gender bias on slight changes to its
prompts.

Besides the work on text and text-visual multimodal systems or data, there are some work on
audio-visual systems or data (Fenu and Marras,
2022). In the work by Fenu and Marras (2022),
they perform comparative analysis on audio-visual
speaker recognition systems, using fusion at the
model step. They found that the highest accuracy
and the lowest disparity across groups are achieved
compared to unimodal systems.
In other works, Peña et al. (2023) evaluated AIbased recruitment for multimodal data but in a fictitious case study, which may limit its generalizability in real-world applications. Booth et al. (2021)
performed a case study of automated video interviews and found that combining more than one
modality increases bias and reduces fairness, similarly to what happens when scaling crawled data
for training models. Other researchers investigated
the impact of multimodal data/models on personlity assessment (Yan et al., 2020), cyberbullying

(Alasadi et al., 2020), health records (Meng et al.,
2022a) and more (Birhane et al., 2021; Zhang et al.,
2022c; Ferrara, 2023a; Cabello et al., 2023) The
obvious challenges of bias in data has motivated
some researchers for more attention in the ethics of
data collection (Weinberg, 2022).
4.2

LLMs

The relevant datasets, models, and challenges of
bias affecting LLMs, as discussed in the literature,
are summarized in Tables 3 and 4. The introduction
of the Generative Pretrained Transformer (GPT-2)
(Radford et al., 2019) was a turning point in the
language model landscape with its 1.5B parameters.
As pointed out earlier, training such a large model
required a lot of data and the Internet-sourced WebText was used for this purpose. Updated versions
of the dataset have also been used for its recent
successors, including GPT-3 (Brown et al., 2020)
and GPT-4 (Achiam et al., 2023). Unfortunately,
the attendant problems of bias witnessed in GPT-2
(Gehman et al., 2020) have followed successive
versions. This is also the case with other models,
as expressed in Table 4, for the reasons that the
datasets used in pretraining, some of which are
given in Table 3, are largely from Internet sources.
Furthermore, the architectures of the models, many
of which are based on the Transformer architecture
(Vaswani et al., 2017), share similarity.
In other works, Schramowski et al. (2022) show
that moral directions, i.e. what is morally right
or wrong to do, are present in LLMs. It is, however, highly debatable if models can be considered
moral agents. In a collaborative effort, bench authors (2023) probed LLMs in the BIG-Bench of
200 tasks, including many that are related to bias.
Also, Santurkar et al. (2023) explored whose opinions LLMs reflect while Harrer (2023); Nashwan
and Abujaber (2023) investigated bias in LLMs
within healthcare.
4.3

Mitigation Categories

Quantitative bias measurement and mitigation in
NLP may be placed into 3 categories: preprocessing, as applicable in general ML (Hort et al., 2024),
intrinsic (Caliskan et al., 2017; May et al., 2019),
and extrinsic, as observed by Ramesh et al. (2023).
The first, second and third involve quantifying and
mitigating bias in the training data, in the trained
model’s representation, and in the outputs of the
downstream task of the model, respectively. More
work has focused on the latter two than the first and

gender bias than other dimensions (Ramesh et al.,
2023). For example, Delobelle et al. (2022) and
Welbl et al. (2021) measured bias in pre-trained
language models. One main reason why more
work has been on the latter, according to Sun et al.
(2019), is that debiasing an existing model to adjust the outputs usually only requires ‘patching’ the
model instead of retraining with modified (debiased) or new data, which is usually costly.
Preuse This may be considered the first step of
the NLP preprocessing method for debiasing. It
involves quantifying the amount of bias (or toxicity) in a given dataset by using a model, after the
model has been trained on a different dedicated
dataset, without necessarily attempting to mitigate
the bias in the given dataset. It can be defined by
Equation 1, where B(d) is a bias metric that takes
data as input and returns a scalar, s, as the score.
Examples of works involving this are (Alkhaled
et al., 2023; Adewumi et al., 2023b). HateBERT
(Caselli et al., 2021) has been used in similar settings. Equation 1 is similar to that in Brunet et al.
(2019) of differential bias, where they approximate
the effect of removing small parts of training data
on bias. Gender bias is one example of the kind
of bias that can be estimated, as it has been observed that there are more male than female terms
in many NLP datasets (Sun et al., 2019; Alkhaled
et al., 2023; Pagliai et al., 2024). The ability to first
estimate quantitatively the bias in a given dataset
provides the basis to be able to determine the level
of success of mitigation.
B (d ) = s
4.4

(1)

Bias in multilingual AI:

Some of the multimodal data in Table 3 contain
multilingual data. These result in multilingual
models and embeddings. For example, CC3M35L, COCO-35L, and WebLI, which was used to
train PaLI. WebLI is a mix of pre-training tasks
with texts in 109 languages (Chen et al., 2022).
Some of these languages fall in the category of
low-resource languages (Adewumi et al., 2023a).
Kurpicz-Briki (2020) reports statistically significant bias in German word embeddings based on
the origin of a name in relation to pleasant and
unpleasant words using WEAT (Caliskan et al.,
2017). In the study by Wambsganss et al. (2022),
they found that the pretrained German language
models, GermanBERT, GermanT5, and German

Table 4: Summary of Models and Some of Their Fairness & Bias Challenges. (*modified for audio-visual)
LMMs
VQGAN-CLIP (Crowson et al., 2022)
DALL-E 2 (Ramesh
et al., 2021)
GLIDE (Nichol et al.,
2022)
Stable Diffusion (Rombach et al., 2022)
SLIP (Mu et al., 2022)

Modality
Text & Vision

Training Data
WIT & ImageNet

Text & Vision

Text & Vision

Conceptual Captions
WIT & Conceptual
Captions
LAION-5B

Text & Vision

YFCC100M

Racial bias (Wolfe and Caliskan, 2022a)

Text & Vision

WIT

7

CLIP (Radford et al.,
2021)
BLIP (Li et al., 2022)

Text & Vision

8

PaliGemma

Text & Vision

9

PaLI-3 (Chen et al.,
2022)

Text & Vision

MS COCO, Conceptual Captions &
LAION-400M
WebLI,
CC3M35L , WIT
WebLI

Racial bias (Radford et al., 2021; Wolfe and
Caliskan, 2022a)
Racial, gender & age biases (Wolfe and Caliskan,
2022a; Ruggeri and Nozza, 2023)

10

Falcon 2 (Almazrouei
et al., 2023)
BEiT (Wang et al.,
2023b)

Text & Vision

RefinedWeb

Text & Vision

LLaVA (Liu et al.,
2024b,a)
ResNet-50* (He et al.,
2016)
GPT4o (Achiam et al.,
2023)
GPT3 (Brown et al.,
2020)
PaLM
(Chowdhery
et al., 2024)
LaMDA
(Thoppilan
et al., 2022)
GLaM (Du et al., 2022)

Text & Vision

Conceptual 12M,
ImageNet-21K,
Wikipedia
Conceptual Captions
ImageNet

#
1
2
3
4
5
6

11
12
13
14
15
16
17
18
19
20
21
22

23
24
25

Text & Vision

Audio & Vision
Text, Audio &
Vision
Text
Text
Text
Text

WebText, Github,
etc
CommonCrawl &
WebText
Wikipedia, social
media, Github
Social media &
Wikipedia
Wikipedia & social
media

GPT2 (Radford et al.,
2019)
LLaMA-3

Text

WebText

Text

web text & Github

LLaMA-2
(Touvron
et al., 2023)
CTRL (Keskar et al.,
2019)

Text

web text

Text

Wikipedia, Project
Gutenberg, OpenWebText

Aurora-M (Nakamura
et al., 2024)
Mixtral-8x7B
(Jiang
et al., 2024a)
BLOOM (Le Scao et al.,
2023)

Text

The Pile

Text

web text

Text

ROOTS

GPT-2, had substantial conceptual, racial, and gender bias. This was also confirmed by Kraft et al.
(2022), who observed sexist stereotypes in some
of the models (e.g. family- and care-related terms
were associated with female while crime and perpetrators were associated with male). Similarly,

Some Challenges of Bias/Fairness
Misogyny, racial bias (Pagliai et al., 2024; Wolfe
and Caliskan, 2022a)
Occupational stereotypes, gender bias (Ramesh
et al., 2022; Mandal et al., 2023a)
Gender stereotypes (Nichol et al., 2022)
Gender bias (Teo et al., 2024)

Porn, offensive language3
Age, racial, gender biases & stereotypes (Chen
et al., 2022)
Harmful content, cultural bias (Almazrouei et al.,
2023)
Gender, cultural biases (Brinkmann et al., 2023)
Cultural bias (Liu et al., 2024b)
Gender bias (Fenu and Marras, 2022)
Stereotypes, racial bias (Aich et al., 2024)
Gender, racial, religious biases (Brown et al.,
2020; Gehman et al., 2020)
Occupation, gender, sexual, religious biases
(Chowdhery et al., 2024)
Gender bias (Thoppilan et al., 2022)
Toxicity, gender bias (Du et al., 2022)
Sexual, racial, gender biases (Sheng et al., 2019;
Gehman et al., 2020)
Stereotypes, gender, racial, sexual, religious, biases(Aich et al., 2024; Touvron et al., 2023)
Toxicity, gender, racial, sexual, religious, biases
(Touvron et al., 2023)
Gender, racial biases (Gehman et al., 2020)

Offensive language, religious, racial, gender biases (Gao et al., 2020a; Nakamura et al., 2024)
Stereotypes, racial, gender, occupational biases
(Jiang et al., 2024a; Aich et al., 2024)
Toxicity, gender, religious, disability, age biases
(Le Scao et al., 2023)

for Dutch, Delobelle et al. (2020) investigated gender and occupation biases in RobBERT (a Dutch
RoBERTa (Liu et al., 2019)), through a templatebased association test (Kurita et al., 2019; May
et al., 2019). Huang et al. (2020b) also identified
biases related to people’s origin and age in Italian,

English, Polish, Portuguese, and Spanish, using a
Twitter corpus.

5

Discussion

Although there are many limitations or risks of multimodal AI or LLMs (Acosta et al., 2022; Adewumi
et al., 2024b; Pettersson et al., 2024; Adewumi
et al., 2024a), perhaps the issue of fairness and bias
rank among the topmost (Mehrabi et al., 2021). In
addition, in the taxonomy of 21 risks of language
models provided by Weidinger et al. (2022), the
first category is ‘Discrimination, Hate speech and
Exclusion’. Given the recurring challenges in this
regard, we are of the view that existing tools for
evaluating or handling fairness and bias in these
systems need to be improved (Zhao et al., 2018a;
Rudinger et al., 2018). It may be almost impossible
to automatically filter a dataset or debias a model to
be 100% free of unfair, bias or toxic content but the
research community and other stakeholders may
need to determine what levels are acceptable and
if it should be a requirement to have human-in-theloop methods. In this section, we discuss methods
to audit or evaluate fairness and bias, datasets for
such evaluation, and debiasing strategies. We hope
that such discussion will spur more researchers and
stakeholders to see the critical importance of AI
that is fair and free from bias, as much as possible.
5.1

Methods to audit, measure, and evaluate
fairness and bias

Caliskan et al. (2017) introduced the Word Embedding Association Test (WEAT), which is based on
the Implicit Association Test (IAT) (Nosek et al.,
2002). The IAT was designed to measure attitudes
towards social groups. It showed implicit preference for White and young people over Black and
old people, respectively. Furthermore, it showed
the association of male terms with science while female terms were with family and arts. Embedding
Association Tests (EATs) have been used in several
studies (Kurpicz-Briki, 2020; Wolfe et al., 2023)
and adapted with improvements in Sentence Embedding Association Test (SEAT) (May et al., 2019)
and Relational Inner Product Association (RIPA)
(Ethayarajh et al., 2019). Despite its widespread
use, WEAT has the disadvantage that it may systematically overestimate the bias in a model. In vision
models, Mandal et al. (2023b) used WEAT to audit CLIP by detecting and quantifying bias. Also,
along the lines of the WEAT, Dev and Phillips

(2019) introduced the Embedding Coherence Test
(ECT) and Embedding Quality Test (EQT) and proposed methods for eliminating explicit bias. However, their method has the weakness that it is not
able to remove implicit bias (Friedrich et al., 2021).
Another embedding evaluation method is cosine
similarity. It was used for zero-shot classification
by Radford et al. (2021). It may also be used to
audit fairness and bias by evaluating the similarity
in image and text embeddings (Wolfe et al., 2023).
The visual tool, Gradient-weighted Class Activation Mapping (GRAD-CAM), generates a saliency
map, which shows the most relevant regions of an
image for given attributes (Selvaraju et al., 2017;
Wolfe et al., 2023). In an evaluation carried out by
Wolfe et al. (2023), they discovered that the computed average saliency maps included only face
regions for non-objectified images but both face
and chest regions for objectified images, in a possible indication of sexual objectification bias. The
tool Not Safe For Work (NSFW) detector uses a
tag alongside each image for filtering undesirable
content (Schuhmann et al., 2021; Birhane et al.,
2024b)
A recent metric introduced by Alkhaled et al.
(2023) is bipol. It uses a two-step procedure in estimating bias in data (Adewumi et al., 2023b; Pagliai
et al., 2024). Bipol has the weakness that if the
bias classifier is not accurate enough, false positives will weaken the evaluation score. Another
measure is Area Under the Curve (AUC), as used
by Meng et al. (2022a) in the investigation of algorithmic fairness of mortality prediction, where
they noted that ML methods obtain lower scores,
usually, when it involves groups with higher mortality rates. Teo et al. (2024) proposed CLassifier
Error-Aware Measurement (CLEAM), a framework
for better performance in bias estimation, while
Booth et al. (2021) measured gender bias using
accuracy of Spearman rank-based correlation (ρ).
Furthermore, Nozza et al. (2021) introduced the
score “HONEST”, which was tested with respect
to gender bias in text generation in 6 languages:
Italian, French, Portuguese, Romanian, Spanish
and English. It measures the probability that a language model will output hurtful text given a certain
template and lexicon.
Datasets for Bias Evaluation Different datasets
have been introduced for bias evaluation. Liang
et al. (2021a) introduced MultiBench, a unified
multimodal benchmark that spans 15 datasets, 10

modalities, and 20 prediction tasks. FairFace was
introduced by Karkkainen and Joo (2021). The
dataset was designed to mitigate racial bias in multimodal AI, collected from the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset
(Thomee et al., 2016) and contains 108,501 images,
balanced across the following races: Black, White,
Indian, Southeast Asian, East Asian, Middle Eastern, and Latino. Esiobu et al. (2023) introduced
2 novel datasets AdvPromptSet and HolisticBiasR,
with which they evaluated 12 demographic dimensions for different LLMs. Ruzzante et al. (2022)
introduced Sexual OBjectification and EMotion
Database (SOBEM) for sexual objectification bias
studies. It consists of 280 pictures of objectified
and non-objectified female models with 3 different
emotions and a neutral face. Bias Benchmark for
QA (BBQ) is a question-set dataset that employs
templates crafted to reflect specific biases identified in society. It was introduced by Parrish et al.
(2022) and aims to expose implicit prejudices that
may exist against individuals from legally protected
categories.
BEAVERTAILS was introduced by Ji et al. (2024).
It assesses question-answer pairs, tested on LLMs,
with regards to 14 different harm categories, which
are not exhaustive. ‘Discrimination, Stereotype,
Injustice’ makes up the second category in the
list. Additional datasets for bias evaluation include
RedditBias by Barikeri et al. (2021), RealToxicityPrompts, which comprises of 100K English sentences (Gehman et al., 2020), HarmfulQ for zeroshot Chain of Thought (CoT) across stereotype
benchmarks and harmful questions (Shaikh et al.,
2023), and BOLD (Dhamala et al., 2021). Porgali
et al. (2023) also introduced the Casual Conversations dataset (version 2) containing 26,467 videos
of 5,567 unique participants from 7 different countries, representing a wide range of demographics,
for bias and robustness evaluation of LMMs that
are vision and audio models.

5.2

Debiasing strategies

Although there’s no silver bullet to solving the challenges of fairness and bias in the data and models
of multimodal AI, we believe a combination of two
or more of the following strategies on the relevant
datasets or models in Tables 3 and 4 will go a long
way in mitigating bias in AI generally.

5.2.1 Curate over Crawl
One important method to address bias in datasets
will be to curate rather than crawl. This is especially so because web crawling has been the popular approach to getting Internet data in the shortest
possible time (Birhane et al., 2021). The assumption of scale beats noise is the rationale for this
approach by some researchers (Jia et al., 2021).
Unfortunately, this misconception about scaling
does not only scale the quality part of the dataset
but the noise along with it, no matter how small,
as shown by Birhane et al. (2024b) when they observed 12% increase in hate content due to scaling.
On the other hand, clearly, despite the advantage of
curation, one hurdle to overcome with the curate
over crawl approach will be the issue of scaling.
In addition, for better quality data collection, researchers like Jo and Gebru (2020) advocate that AI
practitioners should build on the practices of those
in the field of archives and libraries and have a public mission statement to guide their data collection
practice (Weinberg, 2022). Furthermore, Gebru
et al. (2021) encourage, through ‘datasheets for
datasets’, standardized processes for documenting
important aspects of the dataset creation, including motivation, composition, funding, collection
and use cases, with the potential to mitigate unwanted biases, though this approach has its limitations because individuals included or affected
by the datasets are not necessarily empowered to
influence them (Weinberg, 2022).
5.2.2

Counterfactual Data Augmentation
(CDA)
Zhao et al. (2018a) used CDA to show that it removes bias with minimal performance degradation
on coreference benchmarks when combined with
existing word-embedding debiasing methods. It
involves generating alternate examples of what exists (counterfactual) of data points to counter or
mitigate bias. This method has gained attention
in the field (Meade et al., 2022; Barikeri et al.,
2021). It is sometimes called ‘gender-swapping’ in
the specific case of gender bias (Sun et al., 2019).
CDA has been shown to be effective in tackling
bias in coreference resolution, as mentioned earlier, as it reduced the difference in F1 scores between pro-stereotypical and antistereotypical evaluations (Sun et al., 2019). Dev and Phillips (2019)
also embraced this neutralizing approach in their
work by flipping gendered terms with their counterparts. For example, a sentence like ‘he was a

doctor’ would be flipped to ‘she was a doctor’. Despite the advantages, as pointed out earlier about
the less-discussed preprocessing bias mitigation
method, completely debiasing textual data can be
complicated and intricate. Some of the challenges
are that the size of the data increases significantly,
thereby increasing model training time, and naive
term-swapping can create more challenges of unrealistic or nonsensical samples in the data, e.g.
‘she has hot flashes because of menopause’ to ‘he
has hot flashes because of menopause’ (Sun et al.,
2019).
5.2.3 Improved Filtering
It has been shown that poor filtering during the data
creation process allows low quality data in the final
dataset (Birhane et al., 2024a). It may not be possible to automatically filter large data to be 100% fit
for purpose but improving the existing methods of
filtering will go a long way in mitigating bias.
5.2.4 Linear Projection
This method projects all words w ∈ W orthogonally to the bias vector, ensuring the updated set
has no component along the bias vector, vB , as
given in Equation 2 (Bolukbasi et al., 2016; Dev
and Phillips, 2019).
w ´ = w − πB (w ) = w − ⟨w , vB ⟩vB

(2)

The span that results becomes less by 1 in the
total dimensions, say from 300 to 299, which will
have negligible effects on the generalizability of
the embeddings. As an example with gender bias,
subtraction with linear projection of gender terms
from embeddings will make them close. Gendered
word-pairs that have few word sense (e.g. he - she)
and (him - her) can have close enough identifcal
positions in the vector space after debiasing.
5.2.5 Debiasing Word Embeddings
Removing gender bias from word embeddings can
take one of two approaches: (1) removing gender
subspace (Bolukbasi et al., 2016) and (2) learning
gender-neutral embeddings (Zhao et al., 2018b).
The two approaches may not be adequate for embeddings that are not based on Euclidean space
since cosine similarity will no longer apply (Sun
et al., 2019). The first approach modifies an embedding based on the combined properties of word
embeddings that gender bias can be captured by a
direction and neutral words are linearly separable

from gendered words (Bolukbasi et al., 2016) while
the second approach preserves gender information
in some dimensions but compels other dimensions
of the word embedding to be free of such (Zhao
et al., 2018b).
5.2.6

Adapters

A post-processing method for bias mitigation that
is based on Adapter modules (Houlsby et al., 2019),
called Debiasing with Adapter Modules (DAM),
was introduced by Kumar et al. (2023). They encapsulate different bias mitigation functionalities
and can be integrated when desired in a model, similarly to how AdapterFusion (Pfeiffer et al., 2021)
is carried out in multi-task learning. DAM trains
the main adapter and the bias mititgation adapters
independently before combining them. DAM follows an earlier adapter-based debiasing method,
called Adapter-based DEbiasing of LanguagE Models (ADELE), performed in the work by Lauscher
et al. (2021). Their approach involved the additional use of CDA.
5.2.7

Additive Residuals

To address the skewed distribution of different identity groups in the training data used in LMMs, Seth
et al. (2023) introduced Debiasing with Additive
Residuals (DeAR) to learn additive residual image
representations. This minimizes the representations’ capacity to distinguish among different identity groups, thereby offering fairer output.
5.2.8

Continued Pretraining

Fatemi et al. (2023) built on the continued pretraining concept, which is sometimes used for gender
bias mitigation with a small gender-neutral dataset
(de Vassimon Manela et al., 2021), by introducing GEnder Equality Prompt (GEEP) such that it
reduces catastrophic forgetting, which is a likely
event in continued pretraining (Kirkpatrick et al.,
2017). GEEP achieves this by freezing the entire
model before updating the embeddings. Furthermore, Cabello et al. (2023) showed that continued pretraining on gender-neutral data improves
fairness by reducing group disparities in some
language-vision tasks.
5.2.9

Adversarial Learning

Yan et al. (2020) used adversarial learning for bias
mitigation as proposed by Zhang et al. (2018).
They added a discriminator to jointly learn with
the predictor for the sensitive attributes. In this

approach, the generator prevents the discriminator
from identifying gender in a task.
5.2.10

Gender Tagging

In Machine Translation (MT), gender-tagging may
be used (Vanmassenhove et al., 2018). It involves
the addition of gender tags to the beginning of data
samples to identify the gender of the source data,
e.g. ‘FEMALE I’m travelling tomorrow’. Apparently, for more complex sentences this approach
may become more challenging (Sun et al., 2019).
5.2.11

FairDistillation

To address bias across languages, Delobelle and
Berendt (2022) introduced FairDistillation. It is a
cross-lingual method that is based on knowledge
distillation (Hinton et al., 2015) by creating smaller
language models from large ones to control for
stereotypical and representational biases.
5.2.12

DEBIE

Friedrich et al. (2021) introduced DEBIE as a platform for measuring and mitigating implicit and
explicit bias in word embeddings. The mitigation
methods are more specific to NLP and not available
in general purpose library such as AI Fairness 360
(AIF360) (Bellamy et al., 2019). DEBIE is a collection of commonly used bias data tools and word
embeddings, including fastText (Bojanowski et al.,
2017), GloVe (Pennington et al., 2014), Continuous Bag-of-Words (CBoW) (Mikolov et al., 2013),
and the WEAT test.
5.2.13

Other strategies

Furthermore, there are generation detoxifying
methods with the potential to reduce bias (Gehman
et al., 2020). These include the earlier-mentioned
continued pretraining and decoding-based generation. Buolamwini and Gebru (2018) advocated
oversampling under-represented groups in data to
mitigate bias. Zayed et al. (2024) addressed fairness by pruning in LLMs while Guo et al. (2022)
introduced auto-bias, by directly probing the biases
in pretrained models through prompts. Liang et al.
(2021b) introduced the Autoregressive Iterative
Nullspace Projection (A-INLP) method to carry
out post-hoc debiasing on LLMs. Additionally,
there are Self-Debias (Schick et al., 2021), HardDebias (Bolukbasi et al., 2016), SentenceDebias
(Liang et al., 2020) and Dropout methods (Webster
et al., 2020; Meade et al., 2022).

6

Conclusion

Fairness and bias are very important considerations
in multimodal AI. In this work, we presented the
challenges of fairness and bias in multimodal data,
LMMs, and LLMs, defining what both terms mean
within the scope of this survey, while acknowledging other definitions in the literature. We discussed the concepts of fairness and bias from the
perspective of the Social Science and showed the
distribution of scientific publications across many
publishers, which reveals the gap in the study of
large multimodal AI compared to LLMs, which this
work contributes to filling. Our discussions around
the methods to measure fairness and bias, datasets
for evaluation, and debiasing strategies will provide
researchers and other stakeholders with insight on
how to approach these issues.
For future work, it will be worthwhile to reevaluate the progress made with the metrics and
tools for quantifying and mitigating the challenges
of fairness and bias because despite the positive effects of debiasing in AI, researchers like West et al.
(2019) argue that such research ought to do more
than technical debiasing and include the social analysis of the use of such AI, as this will account more
for the impact of bias overall (Weinberg, 2022).
Even Sami et al. (2023) showed with the example
of DALLE-2 that current efforts still have their limitations. In their work on the LMM DALLE-2, they
revealed that, despite the guardrails for the model
by OpenAI, it generated 40 more images of women
for a stereotypical female-dominant administrative
task, in clear gender bias, when prompted.

Acknowledgements
This work is supported by the European
Commission-funded project "Humane AI: Toward
AI Systems That Augment and Empower Humans
by Understanding Us, Our Society and the World
Around Us." The authors wish to thank our colleagues at the Responsible Artificial Intelligence
Group at Umeå University. The work is partially
supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP), funded by
the Knut and Alice Wallenberg Foundation and
counterpart funding from Luleå University of Technology (LTU).

References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774.
Julián N Acosta, Guido J Falcone, Pranav Rajpurkar,
and Eric J Topol. 2022. Multimodal biomedical ai.
Nature Medicine, 28(9):1773–1784.
J Stacy Adams and Sara Freedman. 1976. Equity theory
revisited: Comments and annotated bibliography. Advances in experimental social psychology, 9:43–90.
Tosin Adewumi, Mofetoluwa Adeyemi, Aremu Anuoluwapo, Bukola Peters, Happy Buzaaba, Oyerinde
Samuel, Amina Mardiyyah Rufai, Benjamin Ajibade,
Tajudeen Gwadabe, Mory Moussou Koulibaly Traore,
et al. 2023a. Afriwoz: Corpus for exploiting
cross-lingual transfer for dialogue generation in lowresource, african languages. In 2023 International
Joint Conference on Neural Networks (IJCNN), pages
1–8. IEEE.
Tosin Adewumi, Nudrat Habib, Lama Alkhaled, and
Elisa Barney. 2024a. Instruction makes a difference.
arXiv preprint arXiv:2402.00453.
Tosin Adewumi, Nudrat Habib, Lama Alkhaled, and
Elisa Barney. 2024b. On the limitations of large
language models (llms): False attribution. arXiv
preprint arXiv:2404.04631.
Tosin Adewumi, Foteini Liwicki, and Marcus Liwicki.
2022. State-of-the-art in open-domain conversational
ai: A survey. Information, 13(6):298.
Tosin Adewumi, Isabella Södergren, Lama Alkhaled,
Sana Al-azzawi, Foteini Simistira Liwicki, and Marcus Liwicki. 2023b. Bipol: Multi-axes evaluation of
bias with explainability in benchmark datasets. In
Proceedings of the 14th International Conference
on Recent Advances in Natural Language Processing, pages 1–10, Varna, Bulgaria. INCOMA Ltd.,
Shoumen, Bulgaria.
Monica Agrawal, Stefan Hegselmann, Hunter Lang,
Yoon Kim, and David Sontag. 2022. Large language
models are few-shot clinical information extractors.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages
1998–2022, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai.
2023. Using large language models to simulate multiple humans and replicate human subject studies.
In International Conference on Machine Learning,
pages 337–371. PMLR.
Ankit Aich, Tingting Liu, Salvatore Giorgi, Kelsey Isman, Lyle Ungar, and Brenda Curtis. 2024. Vernacular? i barely know her: Challenges with style control
and stereotyping. arXiv preprint arXiv:2406.12679.

Mohammad Arif Ul Alam. 2022. College student retention risk analysis from educational database using
multi-task multi-modal neural fusion. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 36, pages 12689–12697.
Jamal Alasadi, Ramanathan Arunachalam, Pradeep K
Atrey, and Vivek K Singh. 2020. A fairness-aware
fusion framework for multimodal cyberbullying detection. In 2020 IEEE Sixth International Conference
on Multimedia Big Data (BigMM), pages 166–173.
IEEE.
Lama Alkhaled, Tosin Adewumi, and Sana Sabah Sabry.
2023. Bipol: A novel multi-axes bias evaluation
metric with explainability for nlp. Natural Language
Processing Journal, 4:100030.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023.
The falcon series of open language models. arXiv
preprint arXiv:2311.16867.
Haifa Alwahaby, Mutlu Cukurova, Zacharoula Papamitsiou, and Michail Giannakos. 2022. The evidence
of impact and ethical considerations of multimodal
learning analytics: A systematic literature review.
The multimodal learning analytics handbook, pages
289–325.
Luca Andrighetto, Fabrizio Bracco, Carlo Chiorri,
Michele Masini, Marcello Passarelli, and Tommaso Francesco Piccinno. 2019. Now you see me,
now you don’t: Detecting sexual objectification
through a change blindness paradigm. Cognitive
Processing, 20:419–429.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015. Vqa: Visual question answering.
In Proceedings of the IEEE international conference
on computer vision, pages 2425–2433.
Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R
Gubler, Christopher Rytting, and David Wingate.
2023. Out of one, many: Using language models to simulate human samples. Political Analysis,
31(3):337–351.
Agathe Balayn, Christoph Lofi, and Geert-Jan Houben.
2021. Managing bias and unfairness in data for decision support: a survey of machine learning and data
engineering approaches to identify and mitigate bias
and unfairness within data management and analytics
systems. The VLDB Journal, 30(5):739–768.
Soumya Barikeri, Anne Lauscher, Ivan Vulić, and Goran
Glavaš. 2021. RedditBias: A real-world resource for
bias evaluation and debiasing of conversational language models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long

Papers), pages 1941–1955, Online. Association for
Computational Linguistics.
R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman,
S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta,
A. Mojsilović, S. Nagar, K. Natesan Ramamurthy,
J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R.
Varshney, and Y. Zhang. 2019. Ai fairness 360: An
extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63(4/5):4:1–4:15.
BIG bench authors. 2023. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning
Research.
Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency,
pages 610–623.
Christopher M Berry. 2015. Differential validity and differential prediction of cognitive ability tests: Understanding test bias in the employment context. Annu.
Rev. Organ. Psychol. Organ. Behav., 2(1):435–463.
Ravi Varma Kumar Bevara, Nishith Reddy Mannuru,
Sai Pranathi Karedla, and Ting Xiao. 2024. Scaling implicit bias analysis across transformer-based
language models through embedding association
test and prompt engineering. Applied Sciences,
14(8):3483.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. 2023.
Pythia: A suite for analyzing large language models across training and scaling. In International
Conference on Machine Learning, pages 2397–2430.
PMLR.
Abeba Birhane, Sepehr Dehdashtian, Vinay Prabhu, and
Vishnu Boddeti. 2024a. The dark side of dataset scaling: Evaluating racial classification in multimodal
models. In The 2024 ACM Conference on Fairness,
Accountability, and Transparency, pages 1229–1244.
Abeba Birhane, Sanghyun Han, Vishnu Boddeti, Sasha
Luccioni, et al. 2024b. Into the laion’s den: Investigating hate in multimodal datasets. Advances in
Neural Information Processing Systems, 36.
Abeba Birhane, Vinay Uday Prabhu, and Emmanuel
Kahembwe. 2021. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv
preprint arXiv:2110.01963.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is

power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–
5476, Online. Association for Computational Linguistics.
Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
Robert Sim, and Hanna Wallach. 2021. Stereotyping
norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:
Long Papers), pages 1004–1015.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the association for computational linguistics, 5:135–146.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016. Man
is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in
neural information processing systems, 29.
Brandon M Booth, Louis Hickman, Shree Krishna Subburaj, Louis Tay, Sang Eun Woo, and Sidney K
D’Mello. 2021. Bias and fairness in multimodal
machine learning: A case study of automated video
interviews. In Proceedings of the 2021 International
Conference on Multimodal Interaction, pages 268–
277.
Pearl Brereton, Barbara A Kitchenham, David Budgen,
Mark Turner, and Mohamed Khalil. 2007. Lessons
from applying the systematic literature review process within the software engineering domain. Journal of systems and software, 80(4):571–583.
Jannik Brinkmann, Paul Swoboda, and Christian Bartelt.
2023. A multidimensional analysis of social biases in
vision transformers. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages
4914–4923.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the origins of bias in word embeddings. In
International conference on machine learning, pages
803–811. PMLR.
Joy Buolamwini and Timnit Gebru. 2018. Gender
shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pages 77–91.
PMLR.

Laura Cabello, Emanuele Bugliarello, Stephanie Brandl,
and Desmond Elliott. 2023. Evaluating bias and
fairness in gender-neutral pretrained vision-andlanguage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, pages 8465–8483, Singapore. Association for Computational Linguistics.
Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, and Enhong Chen. 2024. Locating and
mitigating gender bias in large language models. In
International Conference on Intelligent Computing,
pages 471–482. Springer.
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan.
2017. Semantics derived automatically from language corpora contain human-like biases. Science,
356(6334):183–186.
Tommaso Caselli, Valerio Basile, Jelena Mitrović, and
Michael Granitzer. 2021. HateBERT: Retraining
BERT for abusive language detection in English. In
Proceedings of the 5th Workshop on Online Abuse
and Harms (WOAH 2021), pages 17–25, Online. Association for Computational Linguistics.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,
Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,
Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. ACM
Transactions on Intelligent Systems and Technology,
15(3):1–45.
Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the
62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
5186–5200.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman, Adam Grycner, Basil Mustafa, Lucas
Beyer, et al. 2022. Pali: A jointly-scaled multilingual language-image model. arXiv preprint
arXiv:2209.06794.
Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou,
Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi
Wang, Zhengwei Tong, Qinglan Huang, et al. 2024.
Mj-bench: Is your multimodal reward model really
a good judge for text-to-image generation? arXiv
preprint arXiv:2407.04842.
Jiaee Cheong, Sinan Kalkan, and Hatice Gunes. 2024.
Fairrefuse: Referee-guided fusion for multi-modal
causal fairness in depression detection.

Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Dalleval: Probing the reasoning skills and social biases
of text-to-image generation models. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sashank Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2024. Palm: scaling language modeling with pathways. J. Mach. Learn. Res., 24(1).
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language
modeling with pathways. Journal of Machine Learning Research, 24(240):1–113.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2024. Scaling instruction-finetuned language models.
Journal of Machine Learning Research, 25(70):1–53.
Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. Voxceleb2: Deep speaker recognition.
arXiv preprint arXiv:1806.05622.
Jan Clusmann, Fiona R Kolbinger, Hannah Sophie
Muti, Zunamys I Carrero, Jan-Niklas Eckardt,
Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler,
Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. 2023. The future landscape
of large language models in medicine. Communications medicine, 3(1):141.
Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato,
and Edward Raff. 2022. Vqgan-clip: Open domain image generation and editing with natural language guidance. In Computer Vision – ECCV 2022:
17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII, page
88–105, Berlin, Heidelberg. Springer-Verlag.

Yifei Da, Matías Nicolás Bossa, Abel Díaz Berenguer,
and Hichem Sahli. 2024. Reducing bias in sentiment
analysis models through causal mediation analysis
and targeted counterfactual training. IEEE Access,
12:10120–10134.

Rahul Gupta. 2021. Bold: Dataset and metrics for
measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference
on fairness, accountability, and transparency, pages
862–872.

Daniela America da Silva, Henrique Duarte Borges
Louro, Gildarcio Sousa Goncalves, Johnny Cardoso
Marques, Luiz Alberto Vieira Dias, Adilson Marques
da Cunha, and Paulo Marcelo Tasinaffo. 2021. Could
a conversational ai identify offensive language? Information, 12(10):418.

Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. 2020.
Queens are powerful too: Mitigating gender bias in
dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 8173–8188, Online. Association for Computational Linguistics.

Jamell Dacon and Haochen Liu. 2021. Does gender
matter in the news? detecting and examining gender
bias in news articles. In Companion Proceedings of
the Web Conference 2021, WWW ’21, page 385–392,
New York, NY, USA. Association for Computing
Machinery.
Daniel de Vassimon Manela, David Errington, Thomas
Fisher, Boris van Breugel, and Pasquale Minervini.
2021. Stereotype and skew: Quantifying gender bias
in pre-trained and fine-tuned language models. In
Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main Volume, pages 2232–2242, Online.
Association for Computational Linguistics.
Pieter Delobelle and Bettina Berendt. 2022. Fairdistillation: mitigating stereotyping in language models.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages
638–654. Springer.
Pieter Delobelle, Ewoenam Tokpo, Toon Calders, and
Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics
for pre-trained language models. In Proceedings of
the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 1693–1706,
Seattle, United States. Association for Computational
Linguistics.
Pieter Delobelle, Thomas Winters, and Bettina Berendt.
2020. RobBERT: a Dutch RoBERTa-based Language Model. In Findings of the Association for
Computational Linguistics: EMNLP 2020, pages
3255–3265, Online. Association for Computational
Linguistics.
Bhavin Desai, Kapil Patil, Asit Patil, and Ishita Mehta.
2023. Large language models: A comprehensive exploration of modern ai’s potential and pitfalls. Journal of Innovative Technologies, 6(1).
Sunipa Dev and Jeff Phillips. 2019. Attenuating bias in
word vectors. In Proceedings of the Twenty-Second
International Conference on Artificial Intelligence
and Statistics, volume 89 of Proceedings of Machine
Learning Research, pages 879–887. PMLR.
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya
Krishna, Yada Pruksachatkun, Kai-Wei Chang, and

Thang Viet Doan, Zhibo Chu, Zichong Wang, and Wenbin Zhang. 2024. Fairness definitions in language
models explained. arXiv preprint arXiv:2407.18454.
Jonathan Dodge, Q Vera Liao, Yunfeng Zhang,
Rachel KE Bellamy, and Casey Dugan. 2019. Explaining models: an empirical study of how explanations impact fairness judgment. In Proceedings of
the 24th international conference on intelligent user
interfaces, pages 275–285.
Tommaso Dolci, Fabio Azzalini, and Mara Tanelli. 2023.
Improving gender-related fairness in sentence encoders: A semantics-based approach. Data Science
and Engineering, 8(2):177–195.
Karen Drukker, Weijie Chen, Judy Gichoya, Nicholas
Gruszauskas, Jayashree Kalpathy-Cramer, Sanmi
Koyejo, Kyle Myers, Rui C Sá, Berkman Sahiner,
Heather Whitney, et al. 2023. Toward fairness in artificial intelligence for medical image analysis: identification and mitigation of potential biases in the
roadmap from data collection to model deployment.
Journal of Medical Imaging, 10(6):061104–061104.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.
Glam: Efficient scaling of language models with
mixture-of-experts. In International Conference on
Machine Learning, pages 5547–5569. PMLR.
Elizabeth Edenberg and Alexandra Wood. 2023. Disambiguating algorithmic bias: from neutrality to justice.
In Proceedings of the 2023 AAAI/ACM Conference
on AI, Ethics, and Society, pages 691–704.
Ahmed El-Kishky, Vishrav Chaudhary, Francisco
Guzmán, and Philipp Koehn. 2020. CCAligned: A
massive collection of cross-lingual web-document
pairs. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 5960–5969, Online. Association for
Computational Linguistics.
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016. Multi30K: Multilingual EnglishGerman image descriptions. In Proceedings of the
5th Workshop on Vision and Language, pages 70–
74, Berlin, Germany. Association for Computational
Linguistics.

Tyna Eloundou, Sam Manning, Pamela Mishkin, and
Daniel Rock. 2023. Gpts are gpts: An early look at
the labor market impact potential of large language
models. arXiv preprint arXiv:2303.10130.

Gianni Fenu and Mirko Marras. 2022. Demographic
fairness in multimodal biometrics: A comparative
analysis on audio-visual speaker recognition systems.
Procedia Computer Science, 198:249–254.

Hugo Jair Escalante, Heysem Kaya, Albert Ali Salah,
Sergio Escalera, Yagmur Gucluturk, Umut Güçlü,
Xavier Baró, Isabelle Guyon, Julio Jacques Junior,
Meysam Madadi, et al. 2020. Explaining first impressions: Modeling, recognizing, and explaining apparent personality from videos. IEEE Transactions on
Affective Computing.

Emilio Ferrara. 2023a. Fairness and bias in artificial
intelligence: A brief survey of sources, impacts, and
mitigation strategies. Sci, 6(1):3.

Joel Escudé Font and Marta R. Costa-jussà. 2019.
Equalizing gender bias in neural machine translation
with word embeddings techniques. In Proceedings of
the First Workshop on Gender Bias in Natural Language Processing, pages 147–154, Florence, Italy.
Association for Computational Linguistics.
David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan
Ung, Yuchen Zhang, Jude Fernandes, Jane DwivediYu, Eleonora Presani, Adina Williams, and Eric
Smith. 2023. ROBBIE: Robust bias evaluation of
large generative language models. In Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3764–3814, Singapore. Association for Computational Linguistics.
Kawin Ethayarajh, David Duvenaud, and Graeme Hirst.
2019. Understanding undesirable word embedding
associations. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 1696–1705, Florence, Italy. Association for Computational Linguistics.
Chao Fan, Miguel Esparza, Jennifer Dargin, Fangsheng
Wu, Bora Oztekin, and Ali Mostafavi. 2020. Spatial
biases in crowdsourced data: Social media content
attention concentrates on populous areas in disasters. Computers, Environment and Urban Systems,
83:101514.
Zahra Fatemi, Chen Xing, Wenhao Liu, and Caimming Xiong. 2023. Improving gender fairness of
pre-trained language models without catastrophic forgetting. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pages 1249–1262, Toronto,
Canada. Association for Computational Linguistics.
Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi
Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin
Gao, Tao Xiang, et al. 2022. Towards artificial general intelligence via a multimodal foundation model.
Nature Communications, 13(1):3094.
Virginia Felkner, Ho-Chun Herbert Chang, Eugene Jang,
and Jonathan May. 2023. WinoQueer: A communityin-the-loop benchmark for anti-LGBTQ+ bias in
large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 9126–
9140, Toronto, Canada. Association for Computational Linguistics.

Emilio Ferrara. 2023b. Should chatgpt be biased? challenges and risks of bias in large language models.
arXiv preprint arXiv:2304.03738.
Tomáš Foltỳnek, Norman Meuschke, and Bela Gipp.
2019. Academic plagiarism detection: a systematic
literature review. ACM Computing Surveys (CSUR),
52(6):1–42.
Eric Frankel and Edward Vendrow. 2020. Fair generation through prior modification. In 32nd Conference
on Neural Information Processing Systems (NeurIPS
2018).
Barbara L Fredrickson and Tomi-Ann Roberts. 1997.
Objectification theory: Toward understanding
women’s lived experiences and mental health risks.
Psychology of women quarterly, 21(2):173–206.
Vincent Freiberger and Erik Buchmann. 2024. Fairness
certification for natural language processing and large
language models. In Intelligent Systems Conference,
pages 606–624. Springer.
Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Kristian Kersting. 2023. Fair diffusion:
Instructing text-to-image generation models on fairness. arXiv preprint arXiv:2302.10893.
Niklas Friedrich, Anne Lauscher, Simone Paolo
Ponzetto, and Goran Glavaš. 2021. DebIE: A platform for implicit and explicit debiasing of word embedding spaces. In Proceedings of the 16th Conference of the European Chapter of the Association for
Computational Linguistics: System Demonstrations,
pages 91–98, Online. Association for Computational
Linguistics.
Vinitha Gadiraju, Shaun Kane, Sunipa Dev, Alex Taylor,
Ding Wang, Emily Denton, and Robin Brewer. 2023.
" i wouldn’t say offensive but...": Disability-centered
perspectives on large language models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 205–216.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang,
Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,
Ryan Marten, Mitchell Wortsman, Dhruba Ghosh,
Jieyu Zhang, et al. 2024. Datacomp: In search of the
next generation of multimodal datasets. Advances in
Neural Information Processing Systems, 36.
Isabel O Gallegos, Ryan A Rossi, Joe Barrow,
Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.
2023. Bias and fairness in large language models: A
survey. arXiv preprint arXiv:2309.00770.

Deep Ganguli, Danny Hernandez, Liane Lovitt,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage,
et al. 2022. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM
Conference on Fairness, Accountability, and Transparency, pages 1747–1764.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020a.
The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020b.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723.
Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly,
Ed H Chi, and Alex Beutel. 2019. Counterfactual
fairness in text classification through robustness. In
Proceedings of the 2019 AAAI/ACM Conference on
AI, Ethics, and Society, pages 219–226.
Ismael Garrido-Muñoz, Arturo Montejo-Ráez, Fernando Martínez-Santiago, and L Alfonso UreñaLópez. 2021. A survey on bias in deep nlp. Applied
Sciences, 11(7):3184.
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daumé Iii, and Kate Crawford. 2021. Datasheets
for datasets. Communications of the ACM, 64(12):86–
92.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
3356–3369, Online. Association for Computational
Linguistics.
Markos Georgopoulos, James Oldfield, Mihalis A Nicolaou, Yannis Panagakis, and Maja Pantic. 2021. Mitigating demographic bias in facial datasets with stylebased multi-attribute transfer. International Journal
of Computer Vision, 129(7):2288–2307.
Joan Giner-Miguelez, Abel Gómez, and Jordi Cabot.
2023. Datadoc analyzer: A tool for analyzing the
documentation of scientific datasets. In Proceedings
of the 32nd ACM International Conference on Information and Knowledge Management, pages 5046–
5050.
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde
Caron, Ishan Misra, Levent Sagun, Armand Joulin,
and Piotr Bojanowski. 2022a. Vision models are
more robust and fair when pretrained on uncurated images without supervision. arXiv preprint
arXiv:2202.08360.
Priya Goyal, Adriana Romero Soriano, Caner Hazirbas,
Levent Sagun, and Nicolas Usunier. 2022b. Fairness

indicators for systematic assessments of visual feature extractors. In Proceedings of the 2022 ACM
Conference on Fairness, Accountability, and Transparency, pages 70–88.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. 2017. Making the v in vqa
matter: Elevating the role of image understanding
in visual question answering. In Proceedings of the
IEEE conference on computer vision and pattern
recognition, pages 6904–6913.
Jerald Greenberg. 1990. Organizational justice: Yesterday, today, and tomorrow. Journal of management,
16(2):399–432.
Wei Guo and Aylin Caliskan. 2021. Detecting emergent
intersectional biases: Contextualized word embeddings contain a distribution of human-like biases. In
Proceedings of the 2021 AAAI/ACM Conference on
AI, Ethics, and Society, pages 122–133.
Yue Guo, Yi Yang, and Ahmed Abbasi. 2022. Autodebias: Debiasing masked language models with
automated biased prompts. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1012–1023, Dublin, Ireland. Association for Computational Linguistics.
Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob
Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. 2024. Evaluation and mitigation
of the limitations of large language models in clinical
decision-making. Nature medicine, pages 1–10.
Matan Halevy, Camille Harris, Amy Bruckman, Diyi
Yang, and Ayanna Howard. 2021. Mitigating racial
biases in toxic language detection with an equitybased ensemble framework. In Proceedings of the
1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO
’21, New York, NY, USA. Association for Computing
Machinery.
Joschka Haltaufderheide and Robert Ranisch. 2024.
The ethics of chatgpt in medicine and healthcare:
a systematic review on large language models (llms).
NPJ Digital Medicine, 7(1):183.
Yuchen Han. 2023. Fairness evaluation within large
language models through the lens of depression. In
Proceedings of the 2023 4th International Conference
on Machine Learning and Computer Application,
pages 108–112.
Jiangang Hao, Alina A von Davier, Victoria Yaneva,
Susan Lottridge, Matthias von Davier, and Deborah J
Harris. 2024. Transforming assessment: The impacts
and implications of large language models and generative ai. Educational Measurement: Issues and
Practice, 43(2):16–29.

Stefan Harrer. 2023. Attention is not all you need: the
complicated case of ethically using large language
models in healthcare and medicine. EBioMedicine,
90.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.
Nathan A Heflick, Jamie L Goldenberg, Douglas P
Cooper, and Elisa Puvia. 2011. From women to
objects: Appearance focus, target gender, and perceptions of warmth, morality and competence. Journal
of Experimental Social Psychology, 47(3):572–581.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.
Yusuke Hirota, Yuta Nakashima, and Noa Garcia. 2022a.
Gender and racial bias in visual question answering
datasets. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,
pages 1280–1292.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685.
Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani
Yogatama, and Pushmeet Kohli. 2020a. Reducing
sentiment bias in language models via counterfactual
evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 65–83,
Online. Association for Computational Linguistics.
Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and
Michael J. Paul. 2020b. Multilingual Twitter corpus and baselines for evaluating demographic bias
in hate speech recognition. In Proceedings of the
Twelfth Language Resources and Evaluation Conference, pages 1440–1448, Marseille, France. European
Language Resources Association.
Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming
Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma.
2023. Look before you leap: An exploratory study of
uncertainty measurement for large language models.
arXiv preprint arXiv:2307.10236.

Yusuke Hirota, Yuta Nakashima, and Noa Garcia. 2022b.
Quantifying societal bias amplification in image captioning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 13450–13459.

Ben Hutchinson and Margaret Mitchell. 2019. 50 years
of test (un) fairness: Lessons for machine learning.
In Proceedings of the conference on fairness, accountability, and transparency, pages 49–58.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, et al. 2022. An empirical analysis of compute-optimal large language model training.
Advances in Neural Information Processing Systems,
35:30016–30030.

Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl.
2020. Social biases in NLP models as barriers for
persons with disabilities. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, pages 5491–5501, Online. Association
for Computational Linguistics.

Max Hort, Zhenpeng Chen, Jie M Zhang, Mark Harman,
and Federica Sarro. 2024. Bias mitigation for machine learning classifiers: A comprehensive survey.
ACM Journal on Responsible Computing, 1(2):1–52.

Mimansa Jaiswal and Emily Mower Provost. 2020. Privacy enhanced multimodal neural representations for
emotion recognition. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34,
pages 7985–7993.

Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,
Ruobing Xie, Julian McAuley, and Wayne Xin Zhao.
2024. Large language models are zero-shot rankers
for recommender systems. In European Conference
on Information Retrieval, pages 364–381. Springer.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pages 2790–2799.
PMLR.
Dirk Hovy and Shrimai Prabhumoye. 2021. Five
sources of bias in natural language processing. Language and linguistics compass, 15(8):e12432.

Hasan Jamil. 2024. Equity and fairness challenges in
online learning in the age of chatgpt. In Proceedings
of the 39th ACM/SIGAPP Symposium on Applied
Computing, pages 91–92.
Christopher J Jenks. 2024. Communicating the cultural other: Trust and bias in generative ai and large
language models. Applied Linguistics Review, (0).
Yongwoo Jeong, Jiseon Yang, In Ho Choi, and Juyeon
Lee. 2024. Feature-based text search engine mitigating data diversity problem using pre-trained large
language model for fast deployment services. IEEE
Access, 12:48145–48157.
Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi
Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou

Wang, and Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. Advances in Neural Information
Processing Systems, 36.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen
Li, and Tom Duerig. 2021. Scaling up visual and
vision-language representation learning with noisy
text supervision. In International conference on machine learning, pages 4904–4916. PMLR.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024a.
Mixtral of experts. arXiv preprint arXiv:2401.04088.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham
Neubig. 2021. How can we know when language
models know? on the calibration of language models
for question answering. Transactions of the Association for Computational Linguistics, 9:962–977.
Zifan Jiang, Salman Seyedi, Emily Griner, Ahmed Abbasi, Ali Bahrami Rad, Hyeokhyen Kwon, Robert O
Cotes, and Gari D Clifford. 2024b. Evaluating
and mitigating unfairness in multimodal remote
mental health assessments. PLOS Digital Health,
3(7):e0000413.
Eun Seo Jo and Timnit Gebru. 2020. Lessons from
archives: Strategies for collecting sociocultural data
in machine learning. In Proceedings of the 2020 conference on fairness, accountability, and transparency,
pages 306–316.
Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin
Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow,
et al. 2023. Mimic-iv, a freely accessible electronic
health record dataset. Scientific data, 10(1):1.
Gargi Joshi, Rahee Walambe, and Ketan Kotecha. 2021.
A review on explainability in multimodal deep neural
nets. IEEE Access, 9:59800–59821.

Thomas Kehrenberg, Myles Bartlett, Oliver Thomas,
and Novi Quadrianto. 2020. Null-sampling for interpretable and fair representations. In Computer Vision
– ECCV 2020, pages 565–580, Cham. Springer International Publishing.
Nitish Shirish Keskar, Bryan McCann, Lav R Varshney,
Caiming Xiong, and Richard Socher. 2019. Ctrl: A
conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.
Simran Khanuja, Sebastian Ruder, and Partha Talukdar.
2023. Evaluating inclusivity, equity, and accessibility
of nlp technology: A case study for indian languages.
In Findings of the Association for Computational
Linguistics: EACL 2023.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks.
Proceedings of the national academy of sciences,
114(13):3521–3526.
Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel.
2014. Multimodal neural language models. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 595–603, Bejing,
China. PMLR.
Barbara Kitchenham. 2004. Procedures for performing
systematic reviews. Keele, UK, Keele University,
33(2004):1–26.
Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov.
2024. Generating images with multimodal language
models. Advances in Neural Information Processing
Systems, 36.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in
neural information processing systems, 35:22199–
22213.

Mert Karabacak and Konstantinos Margetis. 2023. Embracing large language models for medical applications: opportunities and challenges. Cureus, 15(5).

Hadas Kotek, Rikker Dockum, and David Sun. 2023.
Gender bias and stereotypes in large language models.
In Proceedings of the ACM collective intelligence
conference, pages 12–24.

Kimmo Karkkainen and Jungseock Joo. 2021. Fairface: Face attribute dataset for balanced race, gender,
and age for bias measurement and mitigation. In
Proceedings of the IEEE/CVF winter conference on
applications of computer vision, pages 1548–1558.

Angelie Kraft, Hans-Peter Zorn, Pascal Fecht, Judith
Simon, Chris Biemann, and Ricardo Usbeck. 2022.
Measuring gender bias in german language generation. INFORMATIK 2022.

Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann,
Maria Bannert, Daryna Dementieva, Frank Fischer,
Urs Gasser, Georg Groh, Stephan Günnemann, Eyke
Hüllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges of large language models
for education. Learning and individual differences,
103:102274.

Deepak Kumar, Oleg Lesota, George Zerveas, Daniel
Cohen, Carsten Eickhoff, Markus Schedl, and Navid
Rekabsaz. 2023. Parameter-efficient modularised
bias mitigation via AdapterFusion. In Proceedings
of the 17th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 2738–2751, Dubrovnik, Croatia. Association
for Computational Linguistics.

Yulia Kumar, Kuan Huang, Angelo Perez, Guohao Yang,
J Jenny Li, Patricia Morreale, Dov Kruger, and Raymond Jiang. 2024. Bias and cyberbullying detection
and data generation with transformer ai models and
top llms.
Rina Kumari and Asif Ekbal. 2021. Amfb: Attention
based multimodal factorized bilinear pooling for multimodal fake news detection. Expert Systems with
Applications, 184:115412.
Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,
and Yulia Tsvetkov. 2019. Measuring bias in contextualized word representations. In Proceedings of
the First Workshop on Gender Bias in Natural Language Processing, pages 166–172, Florence, Italy.
Association for Computational Linguistics.
Mascha Kurpicz-Briki. 2020. Cultural differences in
bias? origin and gender bias in pre-trained german
and french word embeddings.
Richard N Landers and Tara S Behrend. 2023. Auditing the ai auditors: A framework for evaluating
fairness and bias in high stakes ai predictive models.
American Psychologist, 78(1):36.
Hugo Laurençon, Lucile Saulnier, Thomas Wang,
Christopher Akiki, Albert Villanova del Moral, Teven
Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. 2022.
The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information
Processing Systems, 35:31809–31826.
Anne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.
Sustainable modular debiasing of language models.
In Findings of the Association for Computational
Linguistics: EMNLP 2021, pages 4782–4797, Punta
Cana, Dominican Republic. Association for Computational Linguistics.
Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin
Zhang, and Eirini Ntoutsi. 2022. A survey on
datasets for fairness-aware machine learning. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12(3):e1452.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2023. Bloom: A 176bparameter open-access multilingual language model.

and Percy S Liang. 2023. Holistic evaluation of textto-image models. In Advances in Neural Information
Processing Systems, volume 36, pages 69981–70011.
Curran Associates, Inc.
Eric Lehman, Evan Hernandez, Diwakar Mahajan,
Jonas Wulff, Micah J Smith, Zachary Ziegler, Daniel
Nadler, Peter Szolovits, Alistair Johnson, and Emily
Alsentzer. 2023. Do we still need clinical language
models? In Conference on health, inference, and
learning, pages 578–597. PMLR.
Thibaud Leteno, Antoine Gourru, Charlotte Laclau, and
Christophe Gravier. 2023. An investigation of structures responsible for gender bias in bert and distilbert.
In International Symposium on Intelligent Data Analysis, pages 249–261. Springer.
Gerald S Leventhal. 1980. What should be done with equity theory? new approaches to the study of fairness
in social relationships. In Social exchange: Advances
in theory and research, pages 27–55. Springer.
Jiaoda Li, Duygu Ataman, and Rico Sennrich. 2021.
Vision matters when it should: Sanity checking multimodal machine translation models. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8556–8562, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding
and generation. In International conference on machine learning, pages 12888–12900. PMLR.
Yunqi Li, Lanjing Zhang, and Yongfeng Zhang. 2024.
Probing into the fairness of large language models:
A case study of chatgpt. In 2024 58th Annual Conference on Information Sciences and Systems (CISS),
pages 1–6.
Paul Pu Liang, Irene Mengze Li, Emily Zheng,
Yao Chong Lim, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2020. Towards debiasing sentence
representations. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 5502–5515, Online. Association for
Computational Linguistics.

Jinsook Lee, Yann Hicke, Renzhe Yu, Christopher
Brooks, and René F Kizilcec. 2024. The life cycle
of large language models in education: A framework
for understanding sources of bias. British Journal of
Educational Technology.

Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun
Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A
Lee, Yuke Zhu, et al. 2021a. Multibench: Multiscale
benchmarks for multimodal representation learning.
Advances in neural information processing systems,
2021(DB1):1.

Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan
Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,
Deepak Narayanan, Hannah Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec,
Jun-Yan Zhu, Fei-Fei Li, Jiajun Wu, Stefano Ermon,

Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and
Ruslan Salakhutdinov. 2021b. Towards understanding and mitigating social biases in language models.
In International Conference on Machine Learning,
pages 6565–6576. PMLR.

Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and
Ruslan Salakhutdinov. 2021c. Towards understanding and mitigating social biases in language models.
In Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings
of Machine Learning Research, pages 6565–6576.
PMLR.
Paul Pu Liang, Amir Zadeh, and Louis-Philippe
Morency. 2022a. Foundations and trends in multimodal machine learning: Principles, challenges, and
open questions. arXiv preprint arXiv:2209.03430.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022b. Holistic evaluation of language
models. arXiv preprint arXiv:2211.09110.
Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon,
Serena Yeung, and James Y Zou. 2022c. Mind
the gap: Understanding the modality gap in multimodal contrastive representation learning. Advances
in Neural Information Processing Systems, 35:17612–
17625.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 26296–26306.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2024b. Visual instruction tuning. Advances in
neural information processing systems, 36.
Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang,
and Daochen Zha. 2023a. Fingpt: Democratizing
internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485.
Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying
Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov,
Muhammad Faaiz Taufiq, and Hang Li. 2023b. Trustworthy llms: A survey and guideline for evaluating
large language models’ alignment. arXiv preprint
arXiv:2308.05374.
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,
Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,

Mengshen He, Zhengliang Liu, et al. 2023c. Summary of chatgpt-related research and perspective towards the future of large language models. MetaRadiology, page 100017.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. 2020. Gender bias in
neural natural language processing. Logic, language,
and security: essays dedicated to Andre Scedrov on
the occasion of his 65th birthday, pages 189–202.
Alexandra Luccioni and Joseph Viviano. 2021. What’s
in the box? an analysis of undesirable content in the
common crawl corpus. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2:
Short Papers), pages 182–189.
Nicholas Lui, Bryan Chia, William Berrios, Candace
Ross, and Douwe Kiela. 2024. Leveraging diffusion
perturbations for measuring fairness in computer vision. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14220–14228.
Hanjun Luo, Haoyu Huang, Ziye Deng, Xuecheng Liu,
Ruizhe Chen, and Zuozhu Liu. 2024. Bigbench: A
unified benchmark for social bias in text-to-image
generative models based on multi-modal llm. arXiv
preprint arXiv:2407.15240.
Debbie S Ma, Joshua Correll, and Bernd Wittenbrink.
2015. The chicago face database: A free stimulus
set of faces and norming data. Behavior research
methods, 47:1122–1135.
Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,
Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,
Qinghua Hu, and Bingzhe Wu. 2023. Fairnessguided few-shot prompting for large language models. Advances in Neural Information Processing Systems, 36:43136–43155.
Vincent Quirante Malic, Anamika Kumari, and Xiaozhong Liu. 2023. Racial skew in fine-tuned legal ai
language models. In 2023 IEEE International Conference on Data Mining Workshops (ICDMW), pages
245–252.
Abhishek Mandal, Susan Leavy, and Suzanne Little.
2023a. Measuring bias in multimodal models: Multimodal composite association score. In International
Workshop on Algorithmic Bias in Search and Recommendation, pages 17–30. Springer.
Abhishek Mandal, Suzanne Little, and Susan Leavy.
2023b. Multimodal bias: Assessing gender bias in
computer vision models with nlp techniques. In Proceedings of the 25th International Conference on
Multimodal Interaction, ICMI ’23, page 416–424,

New York, NY, USA. Association for Computing
Machinery.

Advances in neural information processing systems,
26.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measuring
social biases in sentence encoders. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers), pages 622–628, Minneapolis, Minnesota. Association for Computational Linguistics.

Bonan Min, Hayley Ross, Elior Sulem, Amir
Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,
Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.
Recent advances in natural language processing via
large pre-trained language models: A survey. ACM
Computing Surveys, 56(2):1–40.

Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy.
2022. An empirical survey of the effectiveness of
debiasing techniques for pre-trained language models.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1878–1898, Dublin, Ireland.
Association for Computational Linguistics.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena,
Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM
computing surveys (CSUR), 54(6):1–35.
Katelyn Mei, Sonia Fereidooni, and Aylin Caliskan.
2023. Bias against 93 stigmatized groups in masked
language models and downstream sentiment classification tasks. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,
FAccT ’23, page 1699–1710, New York, NY, USA.
Association for Computing Machinery.
Chuizheng Meng, Loc Trinh, Nan Xu, James Enouen,
and Yan Liu. 2022a. Interpretability and fairness evaluation of deep learning models on mimic-iv dataset.
Scientific Reports, 12(1):7166.
Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.
2022b. Generating training data with language models: Towards zero-shot language understanding. Advances in Neural Information Processing Systems,
35:462–477.
Jacob Menick, Maja Trebacz, Vladimir Mikulik,
John Aslanides, Francis Song, Martin Chadwick,
Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching
language models to support answers with verified
quotes. arXiv preprint arXiv:2203.11147.
Bertalan Meskó and Eric J Topol. 2023. The imperative
for regulatory oversight of large language models (or
generative ai) in healthcare. NPJ digital medicine,
6(1):120.
Jesse G Meyer, Ryan J Urbanowicz, Patrick CN Martin, Karen O’Connor, Ruowang Li, Pei-Chen Peng,
Tiffani J Bright, Nicholas Tatonetti, Kyoung Jae Won,
Graciela Gonzalez-Hernandez, et al. 2023. Chatgpt
and large language models in academia: opportunities and challenges. BioData Mining, 16(1):20.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality.

Sergio Morales, Robert Clarisó, and Jordi Cabot. 2023.
Automating bias testing of llms. In 2023 38th
IEEE/ACM International Conference on Automated
Software Engineering (ASE), pages 1705–1707.
Marzieh Mozafari, Reza Farahbakhsh, and Noël Crespi.
2020. Hate speech detection and racial bias mitigation in social media based on bert model. PloS one,
15(8):e0237861.
Norman Mu, Alexander Kirillov, David Wagner, and
Saining Xie. 2022. Slip: Self-supervision meets
language-image pre-training. In Computer Vision
– ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXVI,
page 529–544, Berlin, Heidelberg. Springer-Verlag.
Devon Myers, Rami Mohawesh, Venkata Ishwarya
Chellaboina, Anantha Lakshmi Sathvik, Praveen
Venkatesh, Yi-Hui Ho, Hanna Henshaw, Muna Alhawawreh, David Berdik, and Yaser Jararweh. 2024.
Foundation and large language models: fundamentals, challenges, opportunities, and social impacts.
Cluster Computing, 27(1):1–26.
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
StereoSet: Measuring stereotypical bias in pretrained
language models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 5356–5371, Online. Association for
Computational Linguistics.
Arsha Nagrani, Joon Son Chung, Weidi Xie, and
Andrew Zisserman. 2020. Voxceleb: Large-scale
speaker verification in the wild. Computer Speech &
Language, 60:101027.
Taishi Nakamura, Mayank Mishra, Simone Tedeschi,
Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue
Zhuo, et al. 2024. Aurora-m: The first open
source multilingual language model red-teamed according to the us executive order. arXiv preprint
arXiv:2404.00399.
Abdulqadir J Nashwan and Ahmad A Abujaber. 2023.
Harnessing large language models in nursing care
planning: opportunities, challenges, and ethical considerations. Cureus, 15(6).
Roberto Navigli, Simone Conia, and Björn Ross. 2023.
Biases in large language models: origins, inventory,
and discussion. ACM Journal of Data and Information Quality, 15(2):1–21.

Zabir Al Nazi and Wei Peng. 2024. Large language
models in healthcare and medical domain: A review.
Informatics, 11(3).
Arshaan Nazir, Thadaka Kalyan Chakravarthy,
David Amore Cecchini, Rakshit Khajuria, Prikshit
Sharma, Ali Tarik Mirik, Veysel Kocaman, and
David Talby. 2024. Langtest: A comprehensive
evaluation library for custom llm and nlp models.
Software Impacts, 19:100619.
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya
Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE:
Towards photorealistic image generation and editing
with text-guided diffusion models. In Proceedings of
the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning
Research, pages 16784–16804. PMLR.

am”: Towards centering transgender and non-binary
voices to measure biases in open language generation.
FAccT ’23, page 1246–1266, New York, NY, USA.
Association for Computing Machinery.
Tiago P Pagano, Rafael B Loureiro, Fernanda VN Lisboa, Rodrigo M Peixoto, Guilherme AS Guimarães,
Gustavo OR Cruz, Maira M Araujo, Lucas L Santos,
Marco AS Cruz, Ewerton LS Oliveira, et al. 2023.
Bias and unfairness in machine learning models: a
systematic review on datasets, tools, fairness metrics,
and identification and mitigation methods. Big data
and cognitive computing, 7(1):15.
Irene Pagliai, Goya van Boven, Tosin Adewumi, Lama
Alkhaled, Namrata Gurung, Isabella Södergren, and
Elisa Barney. 2024. Data bias according to bipol:
Men are naturally right and it is the role of women to
follow their lead. arXiv preprint arXiv:2404.04838.

Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu,
Xian-Sheng Hua, and Ji-Rong Wen. 2021a. Counterfactual vqa: A cause-effect look at language bias.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12700–
12710.

Ji Ho Park, Jamin Shin, and Pascale Fung. 2018. Reducing gender bias in abusive language detection.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages
2799–2804, Brussels, Belgium. Association for Computational Linguistics.

Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu,
Xian-Sheng Hua, and Ji-Rong Wen. 2021b. Counterfactual vqa: A cause-effect look at language bias. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
12700–12710.

Alicia Parrish, Angelica Chen, Nikita Nangia,
Vishakh Padmakumar, Jason Phang, Jana Thompson,
Phu Mon Htut, and Samuel Bowman. 2022. BBQ:
A hand-built bias benchmark for question answering.
In Findings of the Association for Computational
Linguistics: ACL 2022, pages 2086–2105, Dublin,
Ireland. Association for Computational Linguistics.

Brian A Nosek, Mahzarin R Banaji, and Anthony G
Greenwald. 2002. Harvesting implicit group attitudes
and beliefs from a demonstration web site. Group
Dynamics: Theory, research, and practice, 6(1):101.

Alejandro Peña, Ignacio Serna, Aythami Morales, Julian
Fierrez, Alfonso Ortega, Ainhoa Herrarte, Manuel
Alcantara, and Javier Ortega-Garcia. 2023. Humancentric multimodal machine learning: Recent advances and testbed on ai-based recruitment. SN Computer Science, 4(5):434.

Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021.
Honest: Measuring hurtful sentence completion in
language models. In The 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.
Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju,
Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther
Vidal, Salvatore Ruggieri, Franco Turini, Symeon
Papadopoulos, Emmanouil Krasanakis, et al. 2020.
Bias in data-driven artificial intelligence systems—an
introductory survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
10(3):e1356.

Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang,
and Di Hu. 2022. Balanced multimodal learning via
on-the-fly gradient modulation. In Proceedings of
the IEEE/CVF conference on computer vision and
pattern recognition, pages 8238–8247.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference
on empirical methods in natural language processing
(EMNLP), pages 1532–1543.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. 2022. Red teaming
language models with language models. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pages 3419–3448,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.

Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary
Jaggers, Kai-Wei Chang, Aram Galstyan, Richard
Zemel, and Rahul Gupta. 2023. “i’m fully who i

Dana Pessach and Erez Shmueli. 2022. A review on
fairness in machine learning. ACM Comput. Surv.,
55(3).

Suzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph
Gonzalez, Trevor Darrell, and Anna Rohrbach. 2022.
On guiding visual attention with language specification. In 2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 18071–
18081.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of machine learning research,
21(140):1–67.

Jenny Pettersson, Elias Hult, Tim Eriksson, and Tosin
Adewumi. 2024. Generative ai and teachers–for
us or against us? a case study. arXiv preprint
arXiv:2404.03486.

Anil Rahate, Rahee Walambe, Sheela Ramanna, and
Ketan Kotecha. 2022. Multimodal co-learning: Challenges, applications with datasets, recent advances
and future directions. Information Fusion, 81:203–
239.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association
for Computational Linguistics: Main Volume, pages
487–503, Online. Association for Computational Linguistics.

Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee,
Amir Zadeh, Chengfeng Mao, Louis-Philippe
Morency, and Ehsan Hoque. 2020. Integrating multimodal information in large pretrained transformers.
In Proceedings of the conference. Association for
Computational Linguistics. Meeting, volume 2020,
page 2359. NIH Public Access.

Andrés Piñeiro-Martín, Carmen García-Mateo, Laura
Docío-Fernández, and Maria Del Carmen LopezPerez. 2023. Ethical challenges in the development
of virtual assistants powered by large language models. Electronics, 12(14):3170.

Chahat Raj, Anjishnu Mukherjee, and Ziwei Zhu. 2023.
True and fair: Robust and unbiased fake news detection via interpretable machine learning. In Proceedings of the 2023 AAAI/ACM Conference on AI,
Ethics, and Society, pages 962–963.

Bilal Porgali, Vítor Albiero, Jordan Ryda, Cristian Canton Ferrer, and Caner Hazirbas. 2023. The casual
conversations v2 dataset : A diverse, large benchmark for measuring fairness and robustness in audio/vision/speech models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops (CVPRW), pages 10–17.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv
preprint arXiv:2204.06125, 1(2):3.

Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.
Automatically neutralizing subjective bias in text. In
Proceedings of the aaai conference on artificial intelligence, volume 34, pages 480–489.
Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi
Jia, Prateek Mittal, and Peter Henderson. 2023. Finetuning aligned language models compromises safety,
even when users do not intend to! arXiv preprint
arXiv:2310.03693.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821–8831. Pmlr.
Krithika Ramesh, Sunayana Sitaram, and Monojit
Choudhury. 2023. Fairness in language models beyond English: Gaps and challenges. In Findings
of the Association for Computational Linguistics:
EACL 2023, pages 2106–2119, Dubrovnik, Croatia.
Association for Computational Linguistics.
Aneri Rana and Sonali Jha. 2022. Emotion based hate
speech detection using multimodal learning. arXiv
preprint arXiv:2202.06218.
Partha Pratim Ray. 2023. Chatgpt: A comprehensive
review on background, applications, key challenges,
bias, ethics, limitations and future scope. Internet of
Things and Cyber-Physical Systems, 3:121–154.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Shaina Raza, Shardul Ghuge, Chen Ding, Elham
Dolatabadi, and Deval Pandya. 2024. Fair enough:
Develop and assess a fair-compliant dataset for
large language model training? Data Intelligence,
6(2):559–585.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446.

Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages
10684–10695.

Eliane Röösli, Selen Bozkurt, and Tina HernandezBoussard. 2022. Peeking into a black box, the fairness and generalizability of a mimic-iii benchmarking model. Scientific Data, 9(1):24.
Candace Ross, Boris Katz, and Andrei Barbu. 2021.
Measuring social biases in grounded vision and language embeddings. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 998–1008, Online. Association for Computational Linguistics.
Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2022. Square one bias in NLP: Towards a multidimensional exploration of the research manifold.
In Findings of the Association for Computational
Linguistics: ACL 2022, pages 2340–2354, Dublin,
Ireland. Association for Computational Linguistics.
Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 8–14, New Orleans, Louisiana. Association for
Computational Linguistics.
Gabriele Ruggeri and Debora Nozza. 2023. A multidimensional study on bias in vision-language models.
In Findings of the Association for Computational
Linguistics: ACL 2023, pages 6445–6455, Toronto,
Canada. Association for Computational Linguistics.
Daniela Ruzzante, Bianca Monachesi, Noemi Orabona,
and Jeroen Vaes. 2022. The sexual objectification
and emotion database: A free stimulus set and norming data of sexually objectified and non-objectified
female targets expressing multiple emotions. Behavior Research Methods, pages 1–15.
Abel Salinas, Parth Shah, Yuzhong Huang, Robert McCormack, and Fred Morstatter. 2023. The unequal
opportunities of large language models: Examining
demographic biases in job recommendations by chatgpt and llama. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO ’23, New York,
NY, USA. Association for Computing Machinery.
Mansour Sami, Ashkan Sami, and Pete Barclay. 2023.
A case study of fairness in generated images of large
language models for software engineering tasks. In
2023 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pages 391–
396.
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
Lee, Percy Liang, and Tatsunori Hashimoto. 2023.
Whose opinions do language models reflect? In International Conference on Machine Learning, pages
29971–30004. PMLR.

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, pages 5477–5490, Online. Association
for Computational Linguistics.
Akrati Saxena, George Fletcher, and Mykola Pechenizkiy. 2024. Fairsna: Algorithmic fairness in social
network analysis. ACM Computing Surveys, 56(8):1–
45.
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
2024. Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems, 36.
Janice Scheuneman. 1979. A method of assessing bias
in test items. Journal of Educational Measurement,
pages 143–152.
Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021.
Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the
Association for Computational Linguistics, 9:1408–
1424.
Patrick Schramowski, Cigdem Turan, Nico Andersen,
Constantin A Rothkopf, and Kristian Kersting. 2022.
Large pre-trained language models contain humanlike biases of what is right and wrong to do. Nature
Machine Intelligence, 4(3):258–268.
Sarah Schröder, Alexander Schulz, Ivan Tarakanov,
Robert Feldhans, and Barbara Hammer. 2023. Measuring fairness with biased data: A case study on the
effects of unsupervised data in fairness evaluation. In
International Work-Conference on Artificial Neural
Networks, pages 134–145. Springer.
Christoph Schuhmann, Romain Beaumont, Richard
Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis,
Mitchell Wortsman, et al. 2022. Laion-5b: An open
large-scale dataset for training next generation imagetext models. Advances in Neural Information Processing Systems, 35:25278–25294.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush
Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114.
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on
Computer Vision (ICCV), pages 618–626.
Greg Serapio-García, Mustafa Safdari, Clément Crepy,
Luning Sun, Stephen Fitz, Peter Romero, Marwa
Abdulhai, Aleksandra Faust, and Maja Matarić. 2023.

Personality traits in large language models. arXiv
preprint arXiv:2307.00184.
Ignacio Serna, Aythami Morales, Julian Fierrez, and
Nick Obradovich. 2022. Sensitive loss: Improving
accuracy and fairness of face representations with
discrimination-aware deep learning. Artificial Intelligence, 305:103682.
Ashish Seth, Mayur Hemani, and Chirag Agarwal. 2023.
Dear: Debiasing vision-language models with additive residuals. In 2023 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR),
pages 6820–6829.
Omar Shaikh, Hongxin Zhang, William Held, Michael
Bernstein, and Diyi Yang. 2023. On second thought,
let’s not think step by step! bias and toxicity in zeroshot reasoning. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4454–4470,
Toronto, Canada. Association for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2021. Societal biases in language
generation: Progress and challenges. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 4275–4293, Online.
Association for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,
and Nanyun Peng. 2019. The woman worked as
a babysitter: On biases in language generation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407–
3412, Hong Kong, China. Association for Computational Linguistics.
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional hypothesis: Order word matters pre-training for little.
arXiv preprint arXiv:2104.06644.
Eric Michael Smith, Melissa Hall, Melanie Kambadur,
Eleonora Presani, and Adina Williams. 2022a. “I’m
sorry to hear that”: Finding new biases in language
models with a holistic descriptor dataset. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pages 9180–9211,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, et al. 2022b. Using
deepspeed and megatron to train megatron-turing nlg
530b, a large-scale generative language model. arXiv
preprint arXiv:2201.11990.

Eduardo Soares and Plamen Angelov. 2019. Fair-bydesign explainable models for prediction of recidivism. arXiv preprint arXiv:1910.02043.
Irene Solaiman and Christy Dennison. 2021. Process
for adapting language models to society (palms) with
values-targeted datasets. Advances in Neural Information Processing Systems, 34:5861–5873.
Krishna Srinivasan, Karthik Raman, Jiecao Chen,
Michael Bendersky, and Marc Najork. 2021. Wit:
Wikipedia-based image text dataset for multimodal
multilingual machine learning. In Proceedings of
the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR ’21, page 2443–2449, New York, NY, USA.
Association for Computing Machinery.
Tejas Srinivasan and Yonatan Bisk. 2022. Worst of both
worlds: Biases compound in pre-trained vision-andlanguage models. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 77–85, Seattle, Washington.
Association for Computational Linguistics.
Samuil Stoychev and Hatice Gunes. 2022. The effect of
model compression on fairness in facial expression
recognition. In International Conference on Pattern
Recognition, pages 121–138. Springer.
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth
Belding, Kai-Wei Chang, and William Yang Wang.
2019. Mitigating gender bias in natural language
processing: Literature review. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pages 1630–1640, Florence, Italy.
Association for Computational Linguistics.
Janet K Swim, Lauri L Hyers, Laurie L Cohen, and
Melissa J Ferguson. 2001. Everyday sexism: Evidence for its incidence, nature, and psychological
impact from three daily diary studies. Journal of
Social issues, 57(1):31–53.
Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna
Clinciu, Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir
Radev, Shanya Sharma, Arjun Subramonian, Jaesung
Tae, Samson Tan, Deepak Tunuguntla, and Oskar Van
Der Wal. 2022. You reap what you sow: On the challenges of bias evaluation under multilingual settings.
In Proceedings of BigScience Episode #5 – Workshop
on Challenges & Perspectives in Creating Large Language Models, pages 26–41, virtual+Dublin. Association for Computational Linguistics.
Alex Tamkin, Miles Brundage, Jack Clark, and Deep
Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models.
arXiv preprint arXiv:2102.02503.
Yi Chern Tan and L Elisa Celis. 2019. Assessing social and intersectional biases in contextualized word
representations. Advances in neural information processing systems, 32.

Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu,
Na Zou, and Xia Hu. 2021. Mitigating gender bias in
captioning systems. In Proceedings of the Web Conference 2021, WWW ’21, page 633–645, New York,
NY, USA. Association for Computing Machinery.
Louis Tay, Sang Eun Woo, Louis Hickman, Brandon M.
Booth, and Sidney D’Mello. 2022. A conceptual
framework for investigating and mitigating machinelearning measurement bias (mlmb) in psychological
assessment. Advances in Methods and Practices in
Psychological Science, 5(1):25152459211061337.

on Empirical Methods in Natural Language Processing, pages 3003–3008, Brussels, Belgium. Association for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Christopher Teo, Milad Abdollahzadeh, and NgaiMan Man Cheung. 2024. On measuring fairness in
generative models. Advances in Neural Information
Processing Systems, 36.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. 2020. Investigating gender bias in language
models using causal mediation analysis. Advances
in neural information processing systems, 33:12388–
12401.

Timm Teubner, Christoph M Flath, Christof Weinhardt,
Wil van der Aalst, and Oliver Hinz. 2023. Welcome
to the era of chatgpt et al. the prospects of large
language models. Business & Information Systems
Engineering, 65(2):95–101.

Thiemo Wambsganss, Vinitra Swamy, Roman Rietsche, and Tanja Käser. 2022. Bias at a second
glance: A deep dive into bias for german educational peer-review data modeling. arXiv preprint
arXiv:2209.10335.

Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and
Radu Soricut. 2022. Crossmodal-3600: A massively
multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715–729,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Arun James Thirunavukarasu, Darren Shu Jeng Ting,
Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,
and Daniel Shu Wei Ting. 2023. Large language
models in medicine. Nature medicine, 29(8):1930–
1940.
Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian
Borth, and Li-Jia Li. 2016. Yfcc100m: The new
data in multimedia research. Communications of the
ACM, 59(2):64–73.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Oskar Van der Wal, Dominik Bachmann, Alina Leidinger, Leendert van Maanen, Willem Zuidema, and
Katrin Schulz. 2024. Undesirable biases in nlp: Addressing challenges of measurement. Journal of Artificial Intelligence Research, 79:1–40.
Eva Vanmassenhove, Christian Hardmeier, and Andy
Way. 2018. Getting gender right in neural machine
translation. In Proceedings of the 2018 Conference

Jialu Wang, Yang Liu, and Xin Wang. 2021. Are genderneutral queries really gender-neutral? mitigating gender bias in image search. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, pages 1995–2008, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Jialu Wang, Yang Liu, and Xin Wang. 2022a. Assessing multilingual fairness in pre-trained multimodal
representations. In Findings of the Association for
Computational Linguistics: ACL 2022, pages 2681–
2695, Dublin, Ireland. Association for Computational
Linguistics.
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,
Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and
Zhifang Sui. 2023a. Large language models are not
fair evaluators. arXiv preprint arXiv:2305.17926.
Song Wang, Peng Wang, Tong Zhou, Yushun Dong,
Zhen Tan, and Jundong Li. 2024. Ceb: Compositional evaluation benchmark for fairness in large
language models. arXiv preprint arXiv:2407.02408.
Wenhui Wang, Hangbo Bao, Li Dong, Johan
Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit
Som, et al. 2023b. Image as a foreign language: Beit
pretraining for vision and vision-language tasks. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19175–
19186.
Xingbo Wang, Jianben He, Zhihua Jin, Muqiao Yang,
Yong Wang, and Huamin Qu. 2022b. M2lens: Visualizing and explaining multimodal models for sentiment analysis. IEEE Transactions on Visualization
and Computer Graphics, 28(1):802–812.

Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis,
Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. 2020. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 8919–8928.

Robert Wolfe, Yiwei Yang, Bill Howe, and Aylin
Caliskan. 2023. Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias. In Proceedings of
the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 1174–1185.

Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang,
Weifeng Chiu, Tao Wei, and Kui Ren. 2022c.
Fairness-aware adversarial perturbation towards bias
mitigation for deployed deep models. In Proceedings
of the IEEE/CVF conference on computer vision and
pattern recognition, pages 10379–10388.

Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed, Stephane Pinel, Diyi
Yang, and Duen Horng Chau. 2020. Recast: Interactive auditing of automatic toxicity detection models.
In Proceedings of the Eighth International Workshop
of Chinese CHI, pages 80–82.

Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,
Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and
Slav Petrov. 2020. Measuring and reducing gendered
correlations in pre-trained models. arXiv preprint
arXiv:2010.06032.

Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng
Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024.
Invisible relevance bias: Text-image retrieval models prefer ai-generated images. In Proceedings of
the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR ’24, page 208–217, New York, NY, USA. Association for Computing Machinery.

Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
et al. 2021. Ethical and social risks of harm from
language models. arXiv preprint arXiv:2112.04359.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
Conor Griffin, Po-Sen Huang, John Mellor, Amelia
Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
et al. 2022. Taxonomy of risks posed by language
models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,
pages 214–229.
Lindsay Weinberg. 2022. Rethinking fairness: An interdisciplinary survey of critiques of hegemonic ml
fairness approaches. Journal of Artificial Intelligence
Research, 74:75–109.
Johannes Welbl, Amelia Glaese, Jonathan Uesato,
Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021,
pages 2447–2469, Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Sarah Myers West, Meredith Whittaker, and Kate Crawford. 2019. Discriminating systems. AI Now, pages
1–33.
Michael Wick, Jean-Baptiste Tristan, et al. 2019. Unlocking fairness: a trade-off revisited. Advances in
neural information processing systems, 32.
Robert Wolfe and Aylin Caliskan. 2022a. American==
white in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference on AI,
Ethics, and Society, pages 800–812.
Robert Wolfe and Aylin Caliskan. 2022b. Markedness
in visual semantic ai. In Proceedings of the 2022
ACM Conference on Fairness, Accountability, and
Transparency, pages 1269–1279.

Tian Xu, Jennifer White, Sinan Kalkan, and Hatice
Gunes. 2020. Investigating bias and fairness in facial
expression recognition. In Computer Vision–ECCV
2020 Workshops: Glasgow, UK, August 23–28, 2020,
Proceedings, Part VI 16, pages 506–523. Springer.
Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, and C-C Jay Kuo. 2023.
Bias and fairness in chatbots: An overview. arXiv
preprint arXiv:2309.08836.
Shen Yan, Di Huang, and Mohammad Soleymani. 2020.
Mitigating biases in multimodal personality assessment. In Proceedings of the 2020 International Conference on Multimodal Interaction, pages 361–369.
Aimin Yang, Qifeng Bai, Jigang Wang, Nankai Lin,
Xiaotian Lin, Guanqiu Qin, and Junheng He. 2022.
A fine-grained social bias measurement framework
for open-domain dialogue systems. In CCF International Conference on Natural Language Processing
and Chinese Computing, pages 240–251. Springer.
Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee
Cho, and Lifu Huang. 2023. End-to-end multimodal
fact-checking and explanation generation: A challenging dataset and models. In Proceedings of the
46th International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 2733–2743.
Kyra Yee, Uthaipon Tantipongpipat, and Shubhanshu
Mishra. 2021. Image cropping on twitter: Fairness
metrics, their limitations, and the importance of representation, design, and agency. Proceedings of the
ACM on Human-Computer Interaction, 5(CSCW2):1–
24.
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo
Lee, and Woomyoung Park. 2021. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225–2239,

Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng,
Alexander J Ratner, Ranjay Krishna, Jiaming Shen,
and Chao Zhang. 2023. Large language model as
attributed training data generator: A tale of diversity
and bias. In Advances in Neural Information Processing Systems, volume 36, pages 55734–55784. Curran
Associates, Inc.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun
Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and
Shuicheng Yan. 2021. Tokens-to-token vit: Training
vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference
on computer vision, pages 558–567.
Muhammad Bilal Zafar, Isabel Valera, Manuel
Gomez Rodriguez, and Krishna P Gummadi. 2017.
Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international
conference on world wide web, pages 1171–1180.
Abdelrahman Zayed, Gonçalo Mordido, Samira Shabanian, Ioana Baldini, and Sarath Chandar. 2024.
Fairness-aware structured pruning in transformers.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 38, pages 22484–22492.
George Zerveas, Navid Rekabsaz, Daniel Cohen, and
Carsten Eickhoff. 2022. Mitigating bias in search
results through contextual document reranking and
neutrality regularization. In Proceedings of the 45th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
2532–2538.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell.
2018. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM
Conference on AI, Ethics, and Society, pages 335–
340.
Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li,
Dan Su, Chenhui Chu, and Dong Yu. 2024a. MMLLMs: Recent advances in MultiModal large language models. In Findings of the Association for
Computational Linguistics ACL 2024, pages 12401–
12430, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.
Haoran Zhang, Natalie Dullerud, Karsten Roth, Lauren Oakden-Rayner, Stephen Pfohl, and Marzyeh
Ghassemi. 2022a. Improving the fairness of chest
x-ray classifiers. In Proceedings of the Conference
on Health, Inference, and Learning, volume 174 of
Proceedings of Machine Learning Research, pages
204–233. PMLR.
Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew
McDermott, and Marzyeh Ghassemi. 2020. Hurtful
words: quantifying biases in clinical contextual word
embeddings. In proceedings of the ACM Conference
on Health, Inference, and Learning, pages 110–120.

Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu,
Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al.
2024b. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint
arXiv:2407.12772.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022b. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068.
Yi Zhang, Junyang Wang, and Jitao Sang. 2022c. Counterfactually measuring and eliminating social bias
in vision-language pre-training models. In Proceedings of the 30th ACM International Conference on
Multimedia, pages 4996–5004.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In Proceedings of the 2017
Conference on Empirical Methods in Natural Language Processing, pages 2979–2989, Copenhagen,
Denmark. Association for Computational Linguistics.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018a. Gender bias
in coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20, New
Orleans, Louisiana. Association for Computational
Linguistics.
Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and KaiWei Chang. 2018b. Learning gender-neutral word
embeddings. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, pages 4847–4853, Brussels, Belgium. Association for Computational Linguistics.
Yuyin Zhou, Shih-Cheng Huang, Jason Alan Fries,
Alaa Youssef, Timothy J Amrhein, Marcello Chang,
Imon Banerjee, Daniel Rubin, Lei Xing, Nigam
Shah, et al. 2021. Radfusion: Benchmarking performance and fairness for multimodal pulmonary embolism detection from ct and ehr. arXiv preprint
arXiv:2111.11665.
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen,
Zhehao Zhang, and Diyi Yang. 2024. Can large language models transform computational social science? Computational Linguistics, 50(1):237–291.
Yongshuo Zong, Yongxin Yang, and Timothy
Hospedales. 2022.
Medfair: Benchmarking
fairness for medical imaging.
arXiv preprint
arXiv:2210.01725.

A

Appendix

A.1

Google Scholar Papers

A.1.1

Multimodal4

IEEE : (Wang et al., 2020), (Niu et al., 2021a),
(Joshi et al., 2021), (Wang et al., 2022c), (Seth et al.,
2023), (Karkkainen and Joo, 2021), (Peng et al.,
2022), (Cho et al., 2023), (Wang et al., 2022b),
(Niu et al., 2021b), (Hirota et al., 2022b)
Springer : (Xu et al., 2020), (Peña et al., 2023),
(Stoychev and Gunes, 2022), (Georgopoulos et al.,
2021), (Cai et al., 2024), (Alwahaby et al., 2022),
(Kehrenberg et al., 2020)
ACM : (Booth et al., 2021), (Yan et al., 2020),
(Wolfe and Caliskan, 2022a), (Goyal et al., 2022b),
(Cheong et al., 2024), (Wolfe and Caliskan, 2022b),
(Liang et al., 2022a), (Yao et al., 2023), (Hirota
et al., 2022a), (Yee et al., 2021), (Rahman et al.,
2020), (Pessach and Shmueli, 2022), (Xu et al.,
2024), (Tang et al., 2021)
arXiv : (Luo et al., 2024), (Zhou et al., 2021),
(Wang et al., 2024), (Birhane et al., 2021),
(Friedrich et al., 2023), (Goyal et al., 2022a),
(Zhang et al., 2024b), (Rana and Jha, 2022), (Chen
et al., 2024), (Soares and Angelov, 2019), (Zong
et al., 2022)
PubMed : (Liang et al., 2021a)
MDPI : (Ferrara, 2023a), (Nazi and Peng, 2024)
ACL : (Wang et al., 2021), (Srinivasan and Bisk,
2022), (Li et al., 2021), (Zhang et al., 2024a), (Ross
et al., 2021)
Nature : (Meng et al., 2022a), (Röösli et al.,
2022), (Fei et al., 2022)
PLoS

: (Jiang et al., 2024b)

SPIE : (Drukker et al., 2023)
De Gruyter : (Jenks, 2024)
NeurIPS : (Gadre et al., 2024), (Liang et al.,
2022c), (Koh et al., 2024), (Lee et al., 2023)
Elsevier : (Rahate et al., 2022), (Serna et al.,
2022), (Kumari and Ekbal, 2021)
PKP : (Jaiswal and Provost, 2020)

MLRP : (Kiros et al., 2014), (Zhang et al.,
2022a)
Wiley : (Ntoutsi et al., 2020)
Sage : (Tay et al., 2022)
A.1.2

Language5

NeurIPS : (Ma et al., 2023), (Vig et al., 2020),
(Kojima et al., 2022), (Solaiman and Dennison,
2021), (Wick et al., 2019), (Tan and Celis, 2019),
(Ouyang et al., 2022), (Schaeffer et al., 2024),
(Meng et al., 2022b), (Brown et al., 2020), (Yu
et al., 2023)
ACM : (Bender et al., 2021), (Chang et al., 2024),
(Dhamala et al., 2021), (Weidinger et al., 2022),
(Mehrabi et al., 2021), (Garg et al., 2019), (Ganguli
et al., 2022), (Min et al., 2023), (Guo and Caliskan,
2021), (Zhang et al., 2020), (Dodge et al., 2019),
(Navigli et al., 2023), (Kotek et al., 2023)
PKP : (Pryzant et al., 2020)
MIT Press : (Ziems et al., 2024), (Jiang et al.,
2021), (Schick et al., 2021), (Raza et al., 2024)
Springer : (Freiberger and Buchmann, 2024),
(Meyer et al., 2023), (Teubner et al., 2023), (Hou
et al., 2024), (Lu et al., 2020)
MLRP : (Liang et al., 2021b), (Biderman et al.,
2023), (Aher et al., 2023), (Lehman et al., 2023),
(Liang et al., 2021b)
JMLR : (Chung et al., 2024), (Chowdhery et al.,
2023)
Cambridge : (Argyle et al., 2023)
arXiv : (Ferrara, 2023b), (Wang et al., 2023a),
(Tamkin et al., 2021), (Chen et al., 2021), (Liang
et al., 2022b), (Weidinger et al., 2021), (bench authors, 2023), (Rae et al., 2021), (Eloundou et al.,
2023), (Serapio-García et al., 2023), (Qi et al.,
2023), (Smith et al., 2022b), (Huang et al., 2023),
(Liu et al., 2023a), (Liu et al., 2023b), (Hu et al.,
2021), (Zhang et al., 2022b), (Sinha et al., 2021),
(Thoppilan et al., 2022), (Menick et al., 2022),
(Doan et al., 2024), (?)
Elsevier : (Kasneci et al., 2023), (Liu et al.,
2023c), (Ray, 2023)

4
scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Fairness+ 5 scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=
and+Bias+in+Large+Multimodal+Models&btnG=
Fairness+and+Bias+in+Large+Language+Models&btnG=

ACL : (Welbl et al., 2021), (Nadeem et al., 2021),
(Gehman et al., 2020), (Delobelle et al., 2020),
(Sap et al., 2020), (Park et al., 2018), (Perez et al.,
2022), (Blodgett et al., 2020), (Khanuja et al.,
2023), (Agrawal et al., 2022), (Sun et al., 2019),
(Liang et al., 2020), (Dinan et al., 2020), (Blodgett et al., 2021), (Chen and Mueller, 2024), (Gao
et al., 2020b), (Kurita et al., 2019), (Yoo et al.,
2021), (Lin et al., 2022), (Hutchinson et al., 2020),
(Ramesh et al., 2023), (Smith et al., 2022a), (Ruder
et al., 2022)
Nature : (Schramowski et al., 2022), (Clusmann
et al., 2023), (Hager et al., 2024), (Meskó and
Topol, 2023), (Thirunavukarasu et al., 2023)
PLoS

: (Mozafari et al., 2020)

Wiley : (Hovy and Prabhumoye, 2021)
MDPI

: (Garrido-Muñoz et al., 2021)

Preprints : (Kumar et al., 2024)

Springer : (Dolci et al., 2023), (Delobelle and
Berendt, 2022), (Schröder et al., 2023), (Leteno
et al., 2023), (Yang et al., 2022),
NeurIPS : (Ma et al., 2023)
Nature : (Haltaufderheide and Ranisch, 2024)
MDPI : (Bevara et al., 2024), (Piñeiro-Martín
et al., 2023), (da Silva et al., 2021)
MLRP : (Liang et al., 2021c)
Now : (Xue et al., 2023)
Wiley : (Lee et al., 2024), (Hao et al., 2024)
ACL : (Kumar et al., 2023), (Lauscher et al.,
2021), (Sheng et al., 2021), (Fatemi et al., 2023),
(Guo et al., 2022), (Vanmassenhove et al., 2018),
(Huang et al., 2020a), (Felkner et al., 2023), (Talat
et al., 2022), (Escudé Font and Costa-jussà, 2019)
AIAF

PubMed : (Karabacak and Margetis, 2023)
Academic Pinnacle : (Desai et al., 2023)

A.2
A.2.1

Web of Science (WoS) Papers
Multimodal6

IEEE : (Seth et al., 2023), (Porgali et al., 2023),
(Sami et al., 2023), (Petryk et al., 2022)
ACM : (Edenberg and Wood, 2023), (Mandal
et al., 2023b), (Alam, 2022)
PKP : (Lui et al., 2024)
A.2.2

Language7

IEEE : (Malic et al., 2023), (Li et al., 2024),
(Morales et al., 2023), (Da et al., 2024), (Jeong
et al., 2024)
Elsevier : (Nazir et al., 2024), (Fan et al., 2020)
ACM : (Dacon and Liu, 2021), (Ovalle et al.,
2023), (Halevy et al., 2021), (Mei et al., 2023),
(Salinas et al., 2023), (Dhamala et al., 2021), (Gadiraju et al., 2023), (Saxena et al., 2024), (Jamil,
2024), (Zerveas et al., 2022), (Wright et al., 2020),
(Giner-Miguelez et al., 2023), (Raj et al., 2023)
6
https://www.webofscience.com/wos/woscc/summary/889a9d49d906-408c-91b6-ffefdff1880d-fed7fb8c/relevance/1
7
www.webofscience.com/wos/woscc/summary/79cd811fbeb4-40f2-844f-e0ca9edf1fe2-fed7f83e/relevance/1

: (Van der Wal et al., 2024)

