the
vldb
journal
2022
31
431
458
https://doi.org/10.1007/s00778-021-00697-y
regular
paper
fairness
in
rankings
and
recommendations
an
overview
evaggelia
pitoura1
kostas
stefanidis2
georgia
koutrika3
received
18
december
2020
revised
july
2021
accepted
28
august
2021
published
online
october
2021
the
author
2021
abstract
we
increasingly
depend
on
variety
of
data-driven
algorithmic
systems
to
assist
us
in
many
aspects
of
life
search
engines
and
recommender
systems
among
others
are
used
as
sources
of
information
and
to
help
us
in
making
all
sort
of
decisions
from
selecting
restaurants
and
books
to
choosing
friends
and
careers
this
has
given
rise
to
important
concerns
regarding
the
fairness
of
such
systems
in
this
work
we
aim
at
presenting
toolkit
of
definitions
models
and
methods
used
for
ensuring
fairness
in
rankings
and
recommendations
our
objectives
are
threefold
to
provide
solid
framework
on
novel
quickly
evolving
and
impactful
domain
to
present
related
methods
and
put
them
into
perspective
and
to
highlight
open
challenges
and
research
paths
for
future
work
keywords
fairness
rankings
recommendations
introduction
algorithmic
systems
driven
by
large
amounts
of
data
are
increasingly
being
used
in
all
aspects
of
society
to
assist
people
in
forming
opinions
and
taking
decisions
such
algorithmic
systems
offer
enormous
opportunities
since
they
accelerate
scientific
discovery
in
various
domains
including
personalized
medicine
smart
weather
forecasting
and
many
other
fields
they
can
also
automate
tasks
regarding
simple
personal
decisions
and
help
in
improving
our
daily
life
through
personal
assistants
and
recommendations
like
where
to
eat
and
what
are
the
news
moving
forward
they
have
the
potential
of
transforming
society
through
open
government
and
many
more
benefits
often
such
systems
are
used
to
assist
or
even
replace
human
decision
making
in
diverse
domains
examples
include
software
systems
used
in
school
admissions
housing
pricing
of
goods
and
services
credit
score
estimation
kostas
stefanidis
konstantinos.stefanidis@tuni.fi
evaggelia
pitoura
pitoura@cs.uoi.gr
georgia
koutrika
georgia@athenarc.gr
university
of
ioannina
ioannina
greece
tampere
university
tampere
finland
athena
research
center
marousi
greece
job
applicant
selection
and
sentencing
decisions
in
courts
and
surveillance
this
automation
raises
concerns
about
how
much
we
can
trust
such
systems
steady
stream
of
studies
has
shown
that
decision
support
systems
can
unintentionally
both
encode
existing
human
biases
and
introduce
new
ones
20
for
example
in
image
search
when
the
query
is
about
doctors
or
nurses
what
is
the
percentage
of
images
portraying
women
that
we
get
in
the
result
evidence
shows
stereotype
exaggeration
and
systematic
underrepresentation
of
women
when
compared
with
the
actual
percentage
as
estimated
by
the
us
bureau
of
labor
and
statistics
50
two
interesting
conclusions
from
the
study
were
that
people
prefer
and
rate
search
results
higher
when
these
results
are
consistent
with
stereotypes
another
interesting
result
is
that
if
you
shift
the
representation
of
gender
in
image
search
results
then
the
people
perception
about
real-world
distribution
tends
to
shift
too
another
well-known
example
is
the
compas
system
which
is
commercial
tool
that
uses
risk
assessment
algorithm
to
predict
some
categories
of
future
crime
specifically
this
tool
is
used
in
courts
in
the
usa
to
assist
bail
and
sentencing
decisions
and
it
was
found
that
the
false
positive
rate
that
is
the
people
who
were
labeled
by
the
tool
as
high
risk
but
did
not
re-offend
was
nearly
twice
as
high
for
africanamerican
as
for
white
defendants
this
means
that
many
times
the
ubiquitous
use
of
decision
support
systems
may
create
possible
threats
of
economic
loss
social
stigmatization
or
even
loss
of
liberty
there
are
many
more
case
studies
123
432
like
the
above
ones
for
example
names
that
are
used
by
men
and
women
of
color
are
much
more
likely
to
generate
ads
related
to
arrest
records
81
also
using
tool
called
adfisher
it
was
found
that
if
you
set
the
gender
to
female
this
will
result
in
getting
ads
for
less
high
paid
jobs1
or
in
the
case
of
word
embeddings
the
vector
that
represents
computer
programming
is
closer
to
men
than
to
women
data-driven
systems
are
also
being
employed
by
search
and
recommendation
engines
in
movie
and
music
platforms
advertisements
social
media
and
news
outlets
among
others
recent
studies
report
that
social
media
has
become
the
main
source
of
online
news
with
more
than
2.4
billion
internet
users
of
which
nearly
64.5
receive
breaking
news
from
social
media
instead
of
traditional
sources
63
thus
to
great
extent
search
and
recommendation
engines
in
such
systems
play
central
role
in
shaping
our
experiences
and
influencing
our
perception
of
the
world
for
example
people
come
to
their
musical
tastes
in
all
kinds
of
ways
but
how
most
of
us
listen
to
music
now
offers
specific
problems
of
embedded
bias
when
streaming
service
offers
music
recommendations
it
does
so
by
studying
what
music
has
been
listened
to
before
that
creates
suggestions
loop
amplifying
existing
bias
and
reducing
diversity
recent
study
analyzed
the
publicly
available
listening
records
of
330
000
users
of
one
service
and
showed
that
female
artists
only
represented
25
of
the
music
listened
to
by
users
the
study
identified
that
gender
fairness
is
one
of
the
artists
main
concerns
as
female
artists
are
not
given
equal
exposure
in
music
recommendations
29
another
study
led
by
university
of
southern
california
on
facebook
ad
recommendations
revealed
that
the
recommendation
system
disproportionately
showed
certain
types
of
job
ads
to
men
and
women
43
the
system
was
more
likely
to
present
job
ads
to
users
if
their
gender
identity
reflected
the
concentration
of
that
gender
in
particular
position
or
industry
hence
recommendations
amplified
existing
bias
and
created
fewer
opportunities
for
people
based
on
their
gender
however
ads
may
be
targeted
based
on
qualifications
but
not
on
protected
categories
based
on
us
law
in
this
article
we
pay
special
attention
to
the
concept
of
fairness
in
rankings
and
recommendations
by
fairness
we
typically
mean
lack
of
discrimination
bias
bias
may
come
from
the
algorithm
reflecting
for
example
commercial
or
other
preferences
of
its
designers
or
even
from
the
actual
data
for
example
if
survey
contains
biased
questions
or
if
some
specific
population
is
misrepresented
in
the
input
data
as
fairness
is
an
elusive
concept
an
abundance
of
definitions
and
models
of
fairness
have
been
proposed
as
well
as
several
algorithmic
approaches
for
fair
rankings
and
recommendations
making
the
landscape
very
convoluted
in
order
https://fairlyaccountable.org/adfisher/.
123
pitoura
et
al
to
make
real
progress
in
building
fair-aware
systems
we
need
to
de-mystify
what
has
been
done
understand
how
and
when
each
model
and
approach
can
be
used
and
finally
distinguish
the
research
challenges
ahead
of
us
therefore
we
follow
systematic
and
structured
approach
to
explain
the
various
sides
of
and
approaches
to
fairness
in
this
survey
we
present
fairness
models
for
rankings
and
recommendations
separately
from
the
computational
methods
used
to
enforce
them
since
many
of
the
computational
methods
originally
introduced
for
specific
model
are
applicable
to
other
models
as
well
by
providing
an
overview
of
the
spectrum
of
different
models
and
computational
methods
new
ways
to
combine
them
may
evolve
we
start
by
presenting
fairness
models
first
we
provide
birds
eye
view
of
how
notions
of
fairness
in
rankings
and
recommendations
have
been
formalized
we
also
present
taxonomy
specifically
we
distinguish
between
individual
and
group
fairness
consumer
and
producer
fairness
and
fairness
for
single
and
multiple
outputs
then
we
present
concrete
models
and
definitions
for
rankings
and
recommendations
we
highlight
their
differences
and
commonalities
and
present
how
these
models
fit
into
our
taxonomy
we
describe
solutions
for
fair
rankings
and
recommendations
we
organize
them
into
pre-processing
approaches
that
aim
at
transforming
the
data
to
remove
any
underlying
bias
or
discrimination
in-processing
approaches
that
aim
at
modifying
existing
or
introducing
new
algorithms
that
result
in
fair
rankings
and
recommendations
and
post-processing
approaches
that
modify
the
output
of
the
algorithm
within
each
category
we
further
classify
approaches
along
several
dimensions
we
discuss
other
cases
where
system
needs
to
make
decisions
and
where
fairness
is
also
important
and
present
open
research
challenges
pertaining
to
fairness
in
the
broader
context
of
data
management
to
the
best
of
our
knowledge
this
is
the
first
survey
that
provides
toolkit
of
definitions
models
and
methods
used
for
ensuring
fairness
in
rankings
and
recommendations
recent
survey
focuses
on
fairness
in
ranking
91
the
two
surveys
are
complementary
to
each
other
both
in
terms
of
perspective
and
in
terms
of
coverage
fairness
is
an
evasive
concept
and
integrating
it
in
algorithms
and
systems
is
an
emerging
fastchanging
field
we
provide
more
technical
classification
of
recent
work
whereas
the
view
in
91
is
socio-technical
one
that
aims
at
placing
the
various
approaches
to
fairness
within
value
framework
content
is
different
as
well
since
we
also
cover
recommendations
and
rank
aggregation
recent
tutorials
with
stricter
focus
than
ours
focusing
on
concepts
and
metrics
of
fairness
and
the
challenges
in
applying
these
to
recommendations
and
information
retrieval
as
well
as
to
scoring
methods
are
presented
respectively
in
25
and
66
on
the
other
hand
this
article
has
much
wider
coverage
and
depth
presenting
structured
survey
and
comparison
of
fairness
in
rankings
and
recommendations
an
overview
methods
and
models
for
ensuring
fairness
in
rankings
and
recommendations
the
remaining
of
this
survey
is
organized
as
follows
section
presents
the
core
definitions
of
fairness
and
sect
reviews
definitions
of
fairness
that
are
applicable
specifically
to
rankings
recommenders
and
rank
aggregation
section
discusses
distinction
of
the
methods
for
achieving
fairness
while
sects
and
organize
and
present
in
detail
the
pre
in
and
post-processing
methods
section
offers
comparison
between
the
in
and
post-processing
methods
section
studies
how
we
can
verify
whether
program
is
fair
finally
sect
10
elaborates
on
critical
open
issues
and
challenges
for
future
work
and
sect
11
summarizes
the
status
of
the
current
research
on
fairness
in
ranking
and
recommender
systems
the
fairness
problem
in
this
section
we
start
with
an
overview
of
approaches
to
modeling
fairness
and
then
provide
taxonomy
of
the
different
types
of
fairness
models
in
ranking
and
recommendations
433
is
not
unjustifiably
influenced
by
the
values
of
their
protected
attributes
recommendation
systems
retrieve
interesting
items
for
users
based
on
their
profiles
and
their
history
depending
on
the
application
and
the
recommendation
system
history
may
include
explicit
user
ratings
of
items
or
selection
of
items
views
clicks
in
general
recommenders
estimate
score
or
rating
for
user
and
an
item
that
reflects
the
preference
of
user
for
item
or
in
other
words
the
relevance
of
item
for
user
then
recommendation
list
is
formed
for
user
that
includes
the
items
having
the
highest
estimated
score
for
these
scores
can
be
seen
as
the
utility
scores
in
the
case
of
recommenders
in
abstract
terms
recommendation
is
fair
if
the
values
of
the
protected
attributes
of
the
users
or
the
items
do
not
affect
the
outcome
of
the
recommendation
on
high
level
we
can
distinguish
between
two
approaches
to
formalizing
fairness
24
individual
fairness
definitions
are
based
on
the
premise
that
similar
entities
should
be
treated
similarly
group
fairness
definitions
group
entities
based
on
the
value
of
one
or
more
protected
attributes
and
ask
that
all
groups
are
treated
similarly
2.1
general
view
on
fairness
most
approaches
to
algorithmic
fairness
interpret
fairness
as
lack
of
discrimination
31
32
asking
that
an
algorithm
should
not
discriminate
against
its
input
entities
based
on
attributes
that
are
not
relevant
to
the
task
at
hand
such
attributes
are
called
protected
or
sensitive
and
often
include
among
others
gender
religion
age
sexual
orientation
and
race
so
far
most
work
on
defining
detecting
and
removing
unfairness
has
focused
on
classification
algorithms
used
in
decision
making
in
classification
algorithms
each
input
entity
is
assigned
to
one
from
set
of
predefined
classes
in
this
case
characteristics
of
the
input
entities
that
are
not
relevant
to
the
task
at
hand
should
not
influence
the
output
of
the
classifier
for
example
the
values
of
protected
attributes
should
not
hinder
the
assignment
of
an
entity
to
the
positive
class
where
the
positive
class
may
for
example
correspond
to
getting
job
or
being
admitted
at
school
in
this
paper
we
focus
on
ranking
and
recommendation
algorithms
given
set
of
entities
ranking
algorithm
produces
ranking
of
the
entities
where
is
an
assignment
mapping
of
entities
to
ranking
positions
ranking
is
based
on
some
measure
of
the
relative
quality
of
the
entities
for
the
task
at
hand
for
example
the
entities
in
the
output
of
search
query
are
ranked
mainly
based
on
their
relevance
to
the
query
in
the
following
we
will
also
refer
to
the
measure
of
quality
as
utility
abstractly
fair
ranking
is
one
where
the
assignment
of
entities
to
positions
to
operationalize
both
approaches
to
fairness
we
need
to
define
similarity
for
the
input
and
the
output
of
an
algorithm
for
input
similarity
we
need
means
of
quantifying
similarity
of
entities
in
the
case
of
individual
fairness
and
way
of
partitioning
entities
into
groups
in
the
case
of
group
fairness
for
output
similarity
for
both
individual
and
group
fairness
we
need
formal
definition
of
what
similar
treatment
means
input
similarity
for
individual
fairness
common
approach
to
defining
input
similarity
is
distance-based
one
24
let
be
the
set
of
entities
we
assume
there
is
distance
metric
between
each
pair
of
entities
such
that
the
more
dissimilar
the
entities
the
larger
their
distance
this
metric
should
be
task
specific
that
is
two
entities
may
be
similar
for
one
task
and
dissimilar
for
another
for
example
two
individuals
may
be
considered
similar
to
each
other
have
similar
qualifications
when
it
comes
to
being
admitted
to
college
but
dissimilar
when
it
comes
to
receiving
loan
the
metric
may
be
externally
imposed
by
regulatory
body
or
externally
proposed
by
civil
rights
organization
ideally
the
metric
should
express
the
ground
truth
or
the
best
available
approximation
of
it
finally
this
metric
should
be
made
public
and
open
to
discussion
and
refinement
for
group
fairness
the
challenge
lies
in
determining
how
to
partition
entities
into
groups
output
similarity
specifying
what
it
means
for
entities
or
groups
of
entities
to
be
treated
similarly
is
an
intricate
problem
from
both
social
and
technical
perspective
123
434
from
social
perspective
fundamental
distinction
is
made
between
equity
and
equality
simply
put
equality
refers
to
treating
entities
equally
while
equity
refers
to
treating
entities
according
to
their
needs
so
that
they
all
finally
receive
the
same
output
even
when
some
individuals
are
disadvantaged
note
that
often
blindness
hiding
the
values
of
the
protected
attributes
does
not
suffice
to
produce
fair
outputs
since
there
may
be
other
proxy
attributes
correlated
with
the
protected
ones
case
also
known
as
redundant
encoding
take
for
example
zip
code
attribute
zip
codes
may
reveal
sensitive
information
when
the
majority
of
the
residents
of
neighborhood
belong
to
specific
ethnic
group
45
in
fact
such
considerations
have
led
to
making
redlining
the
practice
of
arbitrary
denying
or
limiting
financial
services
to
specific
neighborhoods
illegal
in
the
usa
another
social-based
differentiation
can
be
made
between
disparate
treatment
and
disparate
impact
disparate
treatment
is
the
often
illegal
practice
of
treating
an
entity
differently
based
on
its
protected
attributes
disparate
impact
refers
to
cases
where
the
output
depends
on
the
protected
attributes
even
if
all
entities
are
treated
the
same
way
the
disparate
impact
doctrine
was
solidified
in
the
usa
after
griggs
duke
power
co
1971
where
high
school
diploma
was
required
for
unskilled
work
excluding
applicants
of
color
from
technical
perspective
how
output
similarity
is
translated
into
quantifiable
measures
depends
clearly
on
the
specific
type
of
algorithm
in
this
paper
we
focus
on
ranking
and
recommendation
algorithms
overall
in
the
case
of
ranking
central
issue
is
the
manifestation
of
position
bias
the
fact
that
people
tend
to
consider
only
the
items
that
appear
in
the
top
few
positions
of
ranking
even
more
the
attention
that
items
receive
that
is
the
visibility
of
the
items
is
highly
skewed
with
regard
to
their
position
in
the
list
at
high
level
output
similarity
in
the
case
of
ranking
refers
to
offering
similar
visibility
to
similar
items
or
group
of
items
that
is
placing
them
at
similar
positions
in
the
ranking
especially
when
it
comes
to
top
positions
for
recommendations
one
approach
to
defining
output
fairness
is
to
consider
the
recommendation
problem
as
classification
problem
where
the
positive
class
is
the
recommendation
list
another
approach
is
to
consider
the
recommendation
list
as
ranked
list
in
which
case
the
position
of
each
recommended
item
in
the
list
should
also
be
taken
into
account
we
refine
individual
and
group
fairness
based
on
the
type
of
output
similarity
in
the
next
section
2.2
taxonomy
of
fairness
definitions
in
the
previous
section
we
distinguished
between
individual
and
group
fairness
formulations
we
will
refer
to
this
123
pitoura
et
al
distinction
of
fairness
models
as
the
level
of
fairness
when
it
comes
to
ranking
and
recommendation
systems
besides
different
levels
we
also
have
more
than
one
side
at
which
fairness
criteria
are
applicable
finally
we
differentiate
fairness
models
based
on
whether
the
fairness
requirements
are
applied
at
single
or
at
multiple
outputs
of
an
algorithm
the
different
dimensions
of
fairness
are
summarized
in
fig
note
that
the
various
fairness
definitions
in
this
and
the
following
sections
can
be
used
both
as
conditions
that
system
must
satisfy
for
being
fair
and
as
measures
of
fairness
for
instance
we
can
measure
how
much
fairness
condition
is
violated
or
define
condition
by
setting
threshold
on
fairness
measure
2.2
levels
of
fairness
we
now
refine
individual
and
group
fairness
based
on
how
output
similarity
is
specified
seminal
research
in
formalizing
algorithmic
fairness
has
focused
on
classification
algorithms
used
in
decision
making
such
research
has
been
influential
in
the
study
of
fairness
in
other
types
of
algorithms
so
we
refer
to
such
research
briefly
here
and
relate
it
to
ranking
and
recommendations
types
of
individual
fairness
one
way
of
formulating
individual
fairness
is
distance-based
one
intuitively
given
distance
measure
between
two
entities
and
distance
measure
between
the
outputs
of
an
algorithm
we
would
like
the
distance
between
the
output
of
the
algorithm
for
two
entities
to
be
small
when
the
entities
are
similar
let
us
see
concrete
example
from
the
area
of
probabilistic
classifiers
24
let
be
classifier
that
maps
entities
to
outcomes
in
the
case
of
probabilistic
classifiers
these
are
randomized
mappings
from
entities
to
probability
distributions
over
outcomes
specifically
to
classify
an
entity
we
choose
an
outcome
according
to
the
distribution
we
say
that
classifier
is
individually
fair
if
the
mapping
satisfies
the
lipschitz
property
that
is
where
is
distance
measure
between
probability
distributions
and
distance
metric
between
entities
in
words
the
distance
between
probability
distributions
assigned
by
the
classifier
should
be
no
greater
than
the
actual
distance
between
the
entities
another
form
of
individual
fairness
is
counterfactual
fairness
57
the
intuition
in
this
case
is
that
an
output
is
fair
toward
an
entity
if
it
is
the
same
in
both
the
actual
world
and
counterfactual
world
where
the
entity
belonged
to
different
group
causal
inference
is
used
to
formalize
this
notion
of
fairness
types
of
group
fairness
for
simplicity
let
us
assume
two
groups
namely
the
protected
group
and
the
non
fairness
in
rankings
and
recommendations
an
overview
435
fig
fairness
definitions
taxonomy
protected
or
privileged
group
we
will
start
by
presenting
statistical
approaches
commonly
used
in
classification
assume
that
is
the
actual
and
the
predicted
output
of
binary
classifier
that
is
is
the
ground
truth
and
the
output
of
the
algorithm
let
be
the
positive
class
that
leads
to
favorable
decision
someone
getting
loan
or
being
admitted
at
competitive
school
and
be
the
predicted
probability
for
certain
classification
statistical
approaches
to
group
fairness
can
be
distinguished
as
33
84
base
rates
approaches
that
use
only
the
output
of
the
algorithm
accuracy
approaches
that
use
both
the
output
of
the
algorithm
and
the
ground
truth
and
calibration
approaches
that
use
the
predicted
probability
and
the
ground
truth
in
classification
base
rate
fairness
compares
the
probability
that
an
entity
receives
the
favorable
outcome
when
belongs
to
the
protected
group
with
the
corresponding
probability
that
receives
the
favorable
outcome
when
belongs
to
the
non-protected
group
to
compare
the
two
we
may
take
their
ratio
92
28
or
their
difference
15
now
in
abstract
terms
base
rate
fairness
definition
for
ranking
may
compare
the
probabilities
of
items
from
each
group
to
appear
in
similarly
good
ranking
positions
while
for
recommendations
the
probabilities
of
them
being
recommended
when
the
probabilities
of
favorable
outcome
are
equal
for
the
two
groups
we
have
special
type
of
fairness
termed
demographic
or
statistical
parity
statistical
parity
preserves
the
input
ratio
that
is
the
demographics
of
the
individuals
receiving
favorable
outcome
are
the
same
as
the
demographics
of
the
underlying
population
statistical
parity
is
natural
way
to
model
equity
members
of
each
group
have
the
same
chance
of
receiving
the
favorable
output
base
rate
fairness
ignores
the
actual
output
the
output
may
be
fair
but
it
may
not
reflect
the
ground
truth
for
example
assume
that
the
classification
task
is
getting
or
not
job
and
the
protected
attribute
is
gender
statistical
parity
asks
for
specific
ratio
of
women
in
the
positive
class
even
when
there
are
not
that
many
women
in
the
input
who
are
well
qualified
for
the
job
accuracy
and
calibration
look
at
traditional
evaluation
measures
and
require
that
the
algorithm
works
equally
well
in
terms
of
prediction
errors
for
both
groups
in
classification
accuracy-based
fairness
warrants
that
various
types
of
classification
errors
true
positives
false
positives
are
equal
across
groups
depending
on
the
type
of
classification
errors
considered
the
achieved
type
of
fairness
takes
different
names
40
for
example
the
case
in
which
we
ask
that
the
case
of
equal
true
positive
rate
for
the
two
groups
is
called
equal
opportunity
comparing
equal
opportunity
with
statistical
parity
again
the
members
of
the
two
groups
have
the
same
chance
of
getting
the
favorable
outcome
but
only
when
these
members
qualify
thus
equal
opportunity
is
more
close
to
an
equality
interpretation
of
fairness
in
analogy
to
accuracy
in
the
case
of
ranking
the
ground
truth
is
reflected
in
the
utility
of
the
items
thus
approaches
that
take
into
account
utility
when
defining
fairness
in
ranking
can
be
seen
as
accuracy-based
ones
in
recommendations
accuracy-based
definitions
look
at
the
differences
between
the
actual
and
the
predicted
ratings
of
the
items
for
the
two
groups
calibration-based
fairness
considers
probabilistic
classifiers
that
predict
probability
for
each
class
22
53
in
general
classification
algorithm
is
considered
to
be
wellcalibrated
if
when
the
algorithm
predicts
set
of
individuals
as
having
probability
of
belonging
to
the
positive
class
then
approximately
fraction
of
this
set
is
actual
members
of
the
positive
class
in
terms
of
fairness
intuitively
we
would
like
the
classifier
to
be
equally
well
calibrated
for
both
groups
an
example
calibration-based
fairness
is
asking
that
for
any
predicted
probability
score
in
the
probability
of
actually
getting
favorable
outcome
is
equal
for
both
groups
123
436
group-based
measures
in
general
tend
to
ignore
the
merits
of
each
individual
in
the
group
some
individuals
in
group
may
be
better
for
given
task
than
other
individuals
in
the
group
which
is
not
captured
by
some
group-based
fairness
definitions
this
issue
may
lead
to
two
problematic
behaviors
namely
the
self-fulfilling
prophecy
where
by
deliberately
choosing
the
less
qualified
members
of
the
protected
group
we
aim
at
building
bad
track
record
for
the
group
and
reverse
tokenism
where
by
not
choosing
well
qualified
member
of
the
non-protected
group
we
aim
at
creating
convincing
refutations
for
the
members
of
the
protected
group
that
are
also
not
selected
2.2
multi-sided
fairness
in
ranking
and
recommendations
there
are
at
least
two
sides
involved
the
items
that
are
being
ranked
or
recommended
and
the
users
that
receive
the
rankings
or
the
recommendations
we
distinguish
between
producer
or
item-side
fairness
and
consumer
or
user-side
fairness
note
that
the
items
that
are
being
ranked
or
recommended
may
be
also
people
for
example
in
case
of
ranking
job
applicants
but
we
call
them
items
for
simplicity
producer
or
item-side
fairness
focuses
on
the
items
that
are
being
ranked
or
recommended
in
this
case
we
would
like
similar
items
or
groups
of
items
to
be
ranked
or
be
recommended
in
similar
way
to
appear
in
similar
positions
in
ranking
this
is
the
main
type
of
fairness
we
have
discussed
so
far
for
instance
if
we
consider
political
orientation
as
the
protected
attribute
of
an
article
we
may
ask
that
the
value
of
this
attribute
does
not
affect
the
ranking
of
articles
in
search
result
or
news
feed
consumer
or
user-side
fairness
focuses
on
the
users
who
receive
or
consume
the
data
items
in
ranking
search
result
or
recommendation
in
abstract
terms
we
would
like
similar
users
or
groups
of
users
to
receive
similar
rankings
or
recommendations
for
instance
if
gender
is
the
protected
attribute
of
user
receiving
job
recommendations
we
may
ask
that
the
gender
of
the
user
does
not
influence
the
job
recommendations
that
the
user
receives
there
are
cases
in
which
system
may
require
fairness
for
both
consumers
and
providers
when
for
instance
both
the
users
and
the
items
belong
to
protected
groups
for
example
assume
rental
property
business
that
wishes
to
treat
minority
applicants
as
protected
class
and
ensure
that
they
have
access
to
properties
similar
to
other
renters
while
at
the
same
time
wishes
to
treat
minority
landlords
as
protected
class
and
ensure
that
highly
qualified
tenants
are
referred
to
them
at
the
same
rate
as
to
other
landlords
different
types
of
recommendation
systems
may
call
for
specializations
of
consumer
and
producer
fairness
such
case
is
group
recommendation
systems
group
recommendation
systems
recommend
items
to
groups
of
users
as
opposed
123
pitoura
et
al
to
single
user
for
example
movie
to
group
of
friends
an
event
to
an
online
community
or
excursion
to
group
of
tourists
46
in
this
case
we
have
different
types
of
consumer
fairness
since
now
the
consumer
is
not
just
single
user
another
case
is
bundle
and
package
recommendation
systems
that
recommend
complex
items
or
sets
of
items
instead
of
just
single
item
for
example
set
of
places
to
visit
or
courses
to
attend
86
in
this
case
we
may
have
different
types
of
producer
fairness
since
now
the
recommended
items
are
composite
we
discuss
these
special
types
of
fairness
in
sect
3.3
we
can
expand
the
sides
of
fairness
further
by
considering
the
additional
stakeholders
that
may
be
involved
in
recommendation
system
besides
the
consumers
and
the
producers
for
example
in
recommendation
system
the
items
being
recommended
may
belong
to
different
providers
for
instance
in
the
case
of
movie
recommendations
the
movies
may
be
produced
by
different
studios
in
this
case
we
may
ask
for
producer
fairness
with
respect
to
the
providers
of
the
items
instead
of
the
single
items
for
example
in
an
online
craft
marketplace
we
may
want
to
ensure
market
diversity
and
avoid
monopoly
domination
where
the
system
wishes
to
ensure
that
new
entrants
to
the
market
get
reasonable
share
of
recommendations
even
though
they
have
fewer
shoppers
than
established
vendors
note
that
one
way
to
model
provider
fairness
is
by
treating
the
provider
as
protected
attribute
of
the
items
other
sides
of
fairness
include
fairness
for
the
owners
of
the
recommendation
system
especially
when
the
owners
are
different
than
the
producers
and
fairness
for
system
regulators
and
auditors
for
example
data
scientists
machine
learning
researchers
policymakers
and
governmental
auditors
that
are
using
the
system
for
decision
making
2.2
output
multiplicity
there
may
be
cases
in
which
it
is
not
feasible
to
achieve
fairness
by
considering
just
single
ranking
or
single
recommendation
output
thus
recently
there
exist
approaches
that
propose
achieving
fairness
via
series
of
outputs
consider
for
example
the
case
where
the
same
items
appear
in
the
results
of
multiple
search
queries
or
the
same
users
receive
more
than
one
recommendation
in
such
cases
we
may
ask
that
an
item
is
not
necessarily
being
treated
fairly
in
each
and
every
search
result
but
the
item
is
treated
fairly
overall
in
set
of
search
results
similarly
user
may
be
treated
unfairly
in
single
recommendation
but
fairly
in
sequence
of
recommendations
concretely
we
distinguish
between
single
output
and
multiple
output
fairness
in
multiple
output
fairness
we
ask
for
eventual
or
amortized
consumer
or
producer
fairness
we
ask
that
the
consumers
or
producers
are
treated
fairly
in
series
of
rankings
or
recommendations
as
whole
although
fairness
in
rankings
and
recommendations
an
overview
they
may
be
treated
unfairly
in
one
or
more
single
ranking
or
recommendation
in
the
series
another
case
is
sequential
recommenders
that
suggest
items
of
interest
by
modeling
the
sequential
dependencies
over
the
user
item
interactions
in
sequence
this
means
that
the
recommender
treats
the
user
item
interactions
as
dynamic
sequence
and
takes
the
sequential
dependencies
into
account
to
capture
the
current
and
previous
user
preferences
for
increasing
the
quality
of
recommendations
80
the
system
recommends
different
items
at
each
interaction
while
retaining
knowledge
from
past
interactions
interestingly
due
to
the
multiple
user
item
interactions
in
sequential
recommender
systems
fairness
correction
can
be
performed
while
moving
from
one
interaction
to
the
next
437
the
fairness
constraint
ed
that
requires
that
there
are
at
least
two
items
with
property
red
in
the
top-4
positions
is
satisfied
by
rankings
rm
and
rr
but
not
by
ranking
rl
discounted
cumulative
fairness
another
approach
looks
at
the
proportional
representation
of
the
items
of
the
protected
group
at
top
prefixes
of
the
ranking
for
various
values
of
87
the
proposed
model
builds
on
the
discounted
cumulative
gain
dcg
measure
dcg
is
standard
way
of
measuring
the
ranking
quality
of
the
top-k
items
dcg
accumulates
the
utility
util
of
each
item
at
position
from
the
top
position
up
to
position
logarithmically
discounted
by
the
position
of
the
item
thus
favoring
higher
utility
scores
at
first
positions
dcg
models
of
fairness
in
the
previous
section
we
presented
an
overview
of
fairness
and
its
various
dimensions
in
ranking
and
recommendations
in
this
section
we
present
number
of
concrete
models
and
definitions
of
fairness
proposed
for
ranking
recommendations
and
rank
aggregation
3.1
fairness
in
rankings
most
approaches
to
fairness
in
ranking
handle
producer
fairness
that
is
their
goal
is
to
ensure
that
the
items
being
ranked
are
treated
fairly
in
general
ranking
fairness
asks
that
similar
items
or
group
of
items
receive
similar
visibility
that
is
they
appear
at
similar
positions
in
the
ranking
main
issue
is
accounting
for
position
bias
since
in
western
cultures
we
read
from
top
to
bottom
and
from
left
to
right
the
visibility
of
lower-ranked
items
drops
rapidly
when
compared
to
the
visibility
of
higher-ranked
ones
we
will
use
the
example
rankings
in
fig
the
protected
attribute
is
color
red
is
the
protected
group
and
score
is
the
utility
of
the
item
the
ranking
on
the
left
rl
corresponds
to
ranking
based
solely
on
utility
the
ranking
in
the
middle
rm
achieves
the
highest
possible
representation
of
the
protected
group
in
the
top
positions
while
the
ranking
in
the
right
rr
is
an
intermediate
one
fairness
constraints
number
of
group-based
fairness
models
for
ranking
focus
on
the
representation
number
of
items
of
the
protected
group
in
the
top-k
position
in
the
ranking
one
such
type
of
group
fairness
is
achieved
by
constraining
the
number
of
items
from
the
different
groups
that
can
appear
in
the
top-k
positions
specifically
in
the
fairness
constraints
approach
18
given
number
of
protected
attributes
or
properties
fairness
requirements
are
expressed
by
specifying
an
upper
bound
ul
and
lower
bound
on
the
number
of
items
with
property
that
are
allowed
to
appear
in
the
top-k
positions
of
the
ranking
for
example
in
fig
util
log2
the
dcg
value
is
then
normalized
by
the
dcg
of
the
perfect
ranking
for
obtaining
ndcg
for
example
the
dcg
values
of
the
three
rankings
in
fig
are
dcg
10
rl
1.81
dcg
10
rm
1.7
and
dcg
10
rr
1.77
clearly
rl
that
ranks
items
solely
by
utility
has
the
largest
dcg
value
discounted
cumulative
fairness
accumulates
the
number
of
items
belonging
to
the
protected
group
at
discrete
positions
in
the
ranking
at
positions
10
and
discounts
these
numbers
accordingly
so
as
to
favor
the
representation
of
the
protected
group
at
prefixes
at
higher
positions
three
different
definitions
based
on
this
general
idea
have
been
provided
the
first
one
the
normalized
discounted
difference
of
ranking
measures
the
difference
in
the
proportion
of
the
items
of
the
protected
group
in
the
top
prefixes
for
various
values
of
and
in
the
overall
population
opt_r
10
log2
where
is
the
total
number
of
items
is
the
number
of
items
of
the
protected
group
in
the
top
positions
and
opt_r
is
the
optimal
value
for
example
the
optimal
value
in
fig
is
the
one
of
ranking
rm
that
is
of
the
ranking
with
the
maximum
possible
representation
for
the
protected
group
and
it
is
equal
0.93
the
rm
ranking
to
opt_r
log1
45
10
which
is
based
on
utility
has
the
smallest
rm
log1
25
10
log
10
10
10
while
for
0.93
rr
we
have
rr
0.93
log1
35
10
log
10
10
10
0.5
variation
termed
normalized
discounted
ratio
measures
the
difference
between
the
proportion
of
the
items
of
123
438
pitoura
et
al
fig
example
rankings
rl
is
based
solely
on
utility
rm
is
the
optimal
ranking
in
terms
of
the
representation
of
the
protected
group
rr
is
an
intermediate
ranking
between
the
two
red
is
the
protected
value
the
protected
group
in
the
top
positions
and
the
items
of
the
non-protected
group
in
the
top
positions
for
various
values
of
this
is
achieved
by
modifying
equation
so
that
instead
of
dividing
with
the
total
number
of
items
up
to
position
we
divide
with
the
number
of
items
of
the
protected
group
in
the
top
positions
and
instead
of
dividing
with
we
divide
by
finally
the
normalized
kl-divergence
definition
of
fairness
uses
kl-divergence
to
compute
the
expectation
of
the
difference
between
the
membership
probability
distribution
of
the
protected
group
at
the
top
positions
for
10
and
in
the
overall
population
fairness
of
exposure
problem
with
the
discounted
cumulative
approach
is
the
fact
that
it
does
not
account
for
skew
in
visibility
counting
items
at
discrete
positions
does
not
fully
capture
the
fact
that
minimal
differences
in
relevance
scores
may
translate
into
large
differences
in
visibility
for
different
groups
because
of
position
bias
that
results
in
large
skew
in
the
distribution
of
exposure
for
example
in
fig
the
average
utility
of
the
items
in
rl
that
belong
to
the
non-protected
group
is
0.35
whereas
the
average
utility
of
the
items
that
belong
to
the
protected
group
is
0.33
this
gives
us
difference
of
just
0.02
however
if
we
compute
their
exposure
using
dcg
that
is
if
we
discount
utility
logarithmically
the
exposure
for
the
items
in
the
protected
group
is
25
smaller
than
the
exposure
for
the
items
in
the
non-protected
group
the
fairness
of
exposure
approach
77
generalizes
the
logarithmic
discount
by
assigning
to
each
position
in
the
ranking
specific
value
that
represents
the
importance
of
the
position
the
fraction
of
users
that
examine
an
item
at
position
this
is
captured
using
position
discount
vector
where
represents
the
importance
of
position
note
that
we
can
get
logarithmic
discount
if
we
set
log
1j
rankings
are
seen
as
probabilistic
in
particular
ranking
of
items
in
positions
is
modeled
as
doubly
stochastic
matrix
where
pi
is
the
probability
that
item
is
ranked
at
position
123
given
the
position
discount
vector
the
exposure
of
item
in
ranking
is
defined
as
exposure
pi
the
exposure
of
group
is
defined
as
the
average
exposure
of
the
items
in
the
group
exposure
exposure
in
analogy
to
base
rate
statistical
parity
in
classification
we
get
demographic
parity
definition
of
ranking
fairness
by
asking
that
the
two
groups
get
the
same
exposure
exposure
exposure
as
with
classification
we
can
also
get
additional
statistical
fairness
definitions
by
taking
into
account
the
actual
output
in
this
case
the
utility
of
the
items
their
relevance
to
search
query
this
is
called
disparate
treatment
constraint
in
77
and
it
is
expressed
by
asking
that
the
exposure
that
the
two
groups
receive
is
proportional
to
their
average
utility
exposure
exposure
utility
utility
yet
another
definition
termed
disparate
impact
considers
instead
of
just
the
exposure
the
impact
of
ranking
impact
is
measured
using
the
click-through
rate
ctr
where
ctr
is
estimated
as
function
of
both
exposure
and
relevance
this
definition
asks
that
the
impact
of
the
ranking
of
the
two
groups
is
proportional
to
their
average
utility
ctr
ctr
utility
utility
fairness
in
rankings
and
recommendations
an
overview
439
fairness
of
exposure
approach
has
also
be
taken
to
define
individual
fairness
in
rankings
specifically
equity
of
attention
11
asks
that
each
item
receives
attention
ai
views
clicks
that
is
proportional
to
its
utility
utili
relevance
to
given
query
a2
a1
i1
i2
util1
util2
in
general
it
is
unlikely
that
equity
of
attention
can
be
satisfied
in
any
single
ranking
for
example
multiple
items
may
be
similarly
relevant
for
given
query
yet
they
obviously
cannot
all
occupy
the
same
ranking
position
this
is
the
case
for
example
with
items
with
id
x329
and
x23
in
fig
to
address
this
amortized
fairness
was
proposed
sequence
of
rankings
offers
amortized
equity
of
attention
11
if
each
item
receives
cumulative
attention
proportional
to
its
cumulative
relevance
a1
ml
2l
util1
util2
in
this
case
unfairness
is
defined
as
the
distance
between
the
attention
and
utility
distributions
unfairness
ai
utili
next
we
present
number
of
approaches
proposed
specifically
for
recommenders
and
discuss
their
relationship
with
approaches
presented
for
ranking
unfairness
in
predictions
recommendation
systems
have
been
widely
applied
in
several
domains
to
suggest
data
items
like
movies
jobs
and
courses
however
since
predictions
are
based
on
observed
data
they
can
inherit
bias
that
may
already
exist
to
handle
this
issue
measures
of
consumer-side
unfairness
are
introduced
in
88
that
look
into
the
discrepancy
between
the
prediction
behavior
for
protected
and
non-protected
users
specifically
the
proposed
accuracy-based
fairness
metrics
count
the
difference
between
the
predicted
and
actual
scores
the
errors
in
prediction
of
the
data
items
recommended
to
users
in
the
protected
group
and
the
items
recommended
to
users
in
the
non-protected
group
let
be
the
size
of
the
recommendation
list
and
be
the
average
predicted
score
that
an
item
receives
for
the
protected
users
and
non-protected
users
respectively
and
and
be
the
corresponding
average
actual
score
of
item
alternatives
for
defining
unfairness
can
be
summarized
as
follows
value
unfairness
uval
counts
inconsistencies
in
estimation
errors
across
groups
when
one
group
is
given
higher
or
lower
predictions
than
their
true
preferences
that
is
10
normalized
version
of
this
unfairness
definition
that
considers
the
number
of
items
to
be
ranked
and
the
number
of
rankings
in
the
sequence
is
proposed
in
13
formally
unfairness
nm
11
3.2
fairness
in
recommenders
in
general
recommendation
systems
estimate
score
or
rating
for
user
and
an
item
that
reflects
the
relevance
of
for
then
recommendation
list
is
formed
for
user
that
includes
the
items
having
the
highest
estimated
score
for
simple
approach
to
defining
producer-side
that
is
item-side
fairness
for
recommendations
is
to
consider
the
recommendation
problem
as
classification
problem
where
the
positive
class
is
the
recommendation
list
then
any
of
the
fairness
definitions
in
sect
2.2
are
readily
applicable
to
defining
producer-side
fairness
yet
another
approach
to
defining
producer-side
fairness
is
to
consider
the
recommendation
list
as
ranked
list
and
apply
the
various
definitions
described
in
sect
3.1
uval
12
value
unfairness
occurs
when
one
group
of
users
is
consistently
given
higher
or
lower
predictions
than
their
actual
preferences
for
example
when
considering
course
recommendations
value
unfairness
may
suggest
to
male
students
engineering
courses
even
when
they
are
not
interested
in
engineering
topics
while
female
students
not
being
recommended
engineering
courses
even
if
they
are
interested
in
such
topics
absolute
unfairness
uabs
counts
inconsistencies
in
absolute
estimation
errors
across
user
groups
that
is
uval
13
absolute
unfairness
is
unsigned
so
it
captures
single
statistic
representing
the
quality
of
prediction
for
each
group
underestimation
unfairness
uunder
counts
inconsistencies
in
how
much
the
predictions
underestimate
the
true
ratings
that
is
123
440
pitoura
et
al
max
uunder
max
14
underestimation
unfairness
is
important
when
missing
recommendations
are
more
critical
than
extra
recommendations
for
instance
underestimation
may
lead
to
top
students
not
being
recommended
to
explore
topics
they
would
excel
in
overestimation
unfairness
uover
counts
inconsistencies
in
how
much
the
predictions
overestimate
the
true
ratings
and
is
important
when
users
may
be
overwhelmed
by
recommendations
that
is
max
max
uover
15
finally
non-parity
unfairness
par
counts
the
absolute
difference
between
the
overall
average
ratings
of
protected
users
and
non-protected
users
that
is
upar
16
calibrated
recommendations
calibration-based
approach
to
producer-side
fairness
is
proposed
in
78
classification
algorithm
is
considered
to
be
well
calibrated
if
the
predicted
proportions
of
the
groups
in
the
various
classes
agree
with
their
actual
proportions
in
the
input
data
in
analogy
the
goal
of
calibrated
recommendation
algorithm
is
to
reflect
the
interests
of
user
in
the
recommendations
and
with
their
appropriate
proportions
intuitively
the
proportion
of
the
different
groups
of
items
in
recommendation
list
should
be
similar
with
their
corresponding
proportions
in
the
history
of
the
user
as
an
example
consider
movies
as
the
items
to
be
recommended
and
genre
as
the
protected
attribute
for
quantifying
the
degree
of
calibration
of
list
of
recommended
movies
with
respect
to
the
user
history
of
played
movies
this
approach
considers
two
distributions
of
the
genre
for
each
movie
specifically
is
the
distribution
over
genres
of
the
set
of
movies
in
the
history
of
the
user
wu
wu
17
where
is
the
set
of
movies
played
by
user
in
the
past
and
wu
is
the
weight
of
movie
reflecting
how
recently
it
was
played
by
in
turn
is
the
distribution
over
genres
of
the
list
of
movies
recommended
to
123
wr
wr
18
where
is
the
set
of
recommended
movies
and
wr
is
the
weight
of
movie
due
to
its
rank
in
the
recommendation
list
to
compare
these
distributions
several
methods
can
be
used
like
for
example
the
kullback
leibler
kl
divergence
that
is
employed
as
calibration
metric
log
19
where
is
the
target
distribution
and
with
small
is
used
to
handle
the
fact
that
kl-divergence
diverge
for
and
kl-divergence
ensures
that
the
genres
that
the
user
rarely
played
will
also
be
reflected
in
the
recommended
list
with
their
corresponding
proportions
namely
it
is
sensitive
to
small
discrepancies
between
distributions
it
favors
more
uniform
and
less
extreme
distributions
and
in
the
case
of
perfect
calibration
its
value
is
pairwise
fairness
instead
of
looking
at
the
scores
that
the
items
receive
pairwise
fairness
looks
at
the
relative
position
of
pairs
of
items
in
recommendation
list
the
pairwise
approach
proposed
in
10
is
an
accuracy-based
one
where
the
positive
class
includes
the
items
that
receive
positive
feedback
from
the
user
such
as
clicks
high
ratings
or
increased
user
engagement
dwell
time
for
simplicity
in
the
following
we
will
assume
only
click-based
feedback
let
be
if
user
clicked
on
item
and
otherwise
assume
that
is
the
predicted
probability
that
clicks
on
and
monotonic
ranking
function
on
let
be
the
set
of
items
and
and
the
group
of
protected
and
non-protected
items
respectively
pairwise
accuracy
is
based
on
the
probability
that
clicked
item
is
ranked
above
another
unclicked
item
for
the
same
user
20
for
succinctness
let
cu
the
main
idea
is
to
ask
that
the
two
groups
and
have
similar
pairwise
accuracy
specifically
we
achieve
pairwise
fairness
if
cu
cu
21
this
work
also
considers
actual
engagement
by
conditioning
that
the
items
have
been
engaged
with
the
same
amount
we
can
also
distinguish
between
intra
and
inter-group
fairness
intra-group
pairwise
fairness
is
achieved
if
the
like
fairness
in
rankings
and
recommendations
an
overview
lihood
of
clicked
item
being
ranked
above
another
relevant
unclicked
item
from
the
same
group
is
the
same
independent
of
the
group
when
both
and
in
eq
21
belong
to
the
same
group
inter-group
pairwise
fairness
is
achieved
if
the
likelihood
of
clicked
item
being
ranked
above
another
relevant
unclicked
item
from
the
opposite
group
is
the
same
independent
of
the
group
and
in
eq
21
belong
to
opposite
groups
3.3
fairness
in
rank
aggregation
in
addition
to
the
efforts
that
focus
on
fairness
in
single
rankings
and
recommendations
the
problem
of
fairness
in
rank
aggregation
has
also
emerged
this
problem
arises
when
number
of
ranked
outputs
are
produced
and
we
need
to
aggregate
these
outputs
in
order
to
construct
new
ranked
consensus
output
typically
the
problem
of
fair
rank
aggregation
is
largely
unexplored
only
recently
some
works
study
how
to
mitigate
any
bias
introduced
during
the
aggregation
phase
this
is
done
mainly
under
the
umbrella
of
group
recommendations
where
instead
of
an
individual
user
requesting
recommendations
from
the
system
the
request
is
made
by
group
of
users
as
an
example
consider
group
of
friends
that
wants
to
watch
movie
and
each
member
in
the
group
has
his
or
her
own
likes
and
dislikes
the
system
needs
to
properly
balance
all
users
preferences
and
offer
to
the
group
list
of
movies
that
has
degree
of
relevance
to
each
member
the
typical
way
for
doing
so
is
to
apply
ranking
or
recommendation
method
to
each
member
individually
and
then
aggregate
the
separate
lists
into
one
for
the
group
64
72
for
the
aggregation
phase
intuitively
for
each
movie
we
can
calculate
the
average
score
across
all
users
in
the
group
preference
scores
for
the
movie
average
approach
as
an
alternative
we
can
use
the
minimum
function
rather
than
the
average
one
least
misery
approach
or
even
we
can
focus
on
how
to
ensure
fairness
by
attempting
to
minimize
the
feeling
of
dissatisfaction
within
group
members
next
we
present
fairness
model
for
the
general
rank
aggregation
problem
and
additional
models
defined
for
group
recommendations
top-k
parity
55
formalizes
the
fair
rank
aggregation
problem
as
constrained
optimization
problem
specifically
given
set
of
rankings
the
fair
rank
aggregation
problem
returns
the
closest
ranking
to
the
given
set
of
rankings
that
satisfies
particular
fairness
criterion
given
that
each
data
item
has
protected
attribute
that
partitions
the
dataset
in
disjoint
groups
this
work
uses
as
fairness
criterion
general
formulation
of
statistical
parity
for
rankings
that
considers
the
top-k
prefix
of
the
rankings
namely
the
top-k
parity
formally
given
ranking
of
data
items
belonging
to
mutually
exclusive
groups
and
the
ranking
441
satisfies
top-k
parity
if
the
following
condition
is
met
for
all
22
where
denotes
the
position
of
the
data
item
in
the
ranking
dissatisfaction
fairness
for
counting
fairness
measure
of
quantifying
the
satisfaction
or
utility
of
each
user
in
group
given
list
of
recommendations
for
this
group
can
be
used
namely
by
checking
how
relevant
the
recommended
items
are
to
each
user
59
formally
given
user
in
group
and
set
of
items
recommended
to
the
individual
utility
util
of
the
items
for
can
be
defined
as
the
average
items
utility
normalized
in
with
respect
to
their
relevance
for
el
util
el
elmax
23
where
elmax
denotes
the
maximum
value
el
can
take
in
turn
the
overall
satisfaction
of
users
about
the
group
recommendation
quality
or
group
utility
is
estimated
via
aggregating
all
the
individual
utilities
this
is
called
social
welfare
sw
and
is
defined
as
sw
util
24
then
for
estimating
fairness
we
need
to
compare
the
utilities
of
the
users
in
the
group
intuitively
for
example
list
that
minimizes
the
dissatisfaction
of
any
user
in
the
group
can
be
considered
as
the
most
fair
in
this
sense
fairness
enforces
the
least
misery
principle
among
users
utilities
emphasizing
the
gap
between
the
least
and
highest
utilities
of
the
group
members
following
this
concept
fairness
can
be
defined
as
min
util
25
similarly
fairness
can
encourage
the
group
members
to
achieve
close
utilities
between
each
other
using
variance
variance
util
26
pareto
optimal
fairness
instead
of
computing
users
individual
utility
for
list
of
recommendations
by
summing
up
the
relevance
scores
of
all
items
in
the
list
59
the
item
positions
in
the
recommendation
list
can
be
considered
73
specifically
the
solution
for
making
fair
group
recommendations
is
based
on
the
notion
of
pareto
optimality
which
means
that
an
item
is
pareto
optimal
for
group
if
there
exists
no
other
item
that
ranks
higher
according
to
all
users
in
the
group
there
is
no
item
that
dominates
item
123
442
pitoura
et
al
n-level
pareto
optimal
in
turn
is
direct
extension
that
contains
items
dominated
by
at
most
other
items
and
is
used
for
identifying
the
best
items
to
recommend
such
set
of
items
is
fair
by
definition
since
it
contains
the
top
choices
for
each
user
in
the
group
fairness
in
package-to-group
recommendations
given
group
an
approach
to
fair
package-to-group
recommendations
is
to
recommend
to
package
of
items
by
requiring
that
for
each
user
in
at
least
one
item
high
in
preferences
is
included
in
76
even
if
such
resulting
package
is
not
the
best
overall
it
is
fair
since
there
exists
at
least
one
item
in
that
satisfies
each
user
in
specifically
two
different
aspects
of
fairness
are
examined
76
fairness
proportionality
ensuring
that
each
user
finds
sufficient
number
of
items
in
the
package
that
he
she
likes
compared
to
items
not
in
the
package
and
fairness
envyfreeness
ensuring
that
for
each
user
there
are
sufficient
number
of
items
in
the
package
that
he
she
likes
more
than
other
users
do
formally
m-proportionality
for
group
of
users
and
package
the
m-proportionality
of
for
is
defined
as
fprop
27
where
is
the
set
of
users
in
for
which
is
mproportional
in
turn
is
m-proportional
to
user
if
there
exist
at
least
items
in
such
that
each
one
is
ranked
in
the
top-δ
of
the
preferences
of
over
all
items
in
for
an
input
parameter
m-envy-freeness
for
group
of
users
and
package
the
m-envy-freeness
of
for
is
defined
as
fe
28
where
is
the
set
of
users
in
for
which
is
m-envy-free
in
turn
is
m-envy-free
for
user
if
is
envy-free
for
at
least
items
in
each
item
el
is
in
the
top-δ
of
the
preferences
in
the
set
el
sequential
hybrid
aggregation
an
approach
for
fair
sequential
group
recommenders
targets
two
independent
objectives
80
the
first
one
considers
the
group
as
an
entity
and
aims
at
offering
the
best
possible
results
by
maximizing
the
overall
group
satisfaction
over
sequence
of
recommendations
the
satisfaction
of
each
user
in
group
for
the
group
recommendation
gr
received
at
the
jth
round
of
recommendations
is
computed
by
comparing
the
quality
of
recommendations
that
receives
as
member
of
the
group
over
the
quality
of
recommendations
would
have
received
as
an
individual
given
the
list
au
with
the
top-n
items
for
the
user
satisfaction
is
calculated
based
on
the
group
recommendation
list
for
every
item
in
gr
we
sum
the
utility
scores
as
they
appear
in
each
user
au
over
the
123
ideal
case
for
the
user
by
sum
the
utility
scores
of
the
top-n
items
in
au
formally
sat
gr
dz
gr
util
dz
dz
au
util
dz
29
the
second
objective
considers
the
group
members
independently
and
aims
to
behave
as
fairly
as
possible
toward
all
members
by
minimizing
the
variance
between
the
user
satisfaction
scores
intuitively
this
variance
represents
the
potential
disagreement
between
the
users
in
the
group
formally
the
disagreement
is
defined
as
groupdis
gr
max
sat
gr
min
sat
gr
30
where
sat
gr
is
the
overall
satisfaction
of
for
sequence
gr
of
recommendations
defined
as
the
average
of
the
satisfaction
scores
after
each
round
that
is
group
disagreement
is
the
difference
in
the
overall
satisfaction
scores
between
the
most
satisfied
and
the
least
satisfied
user
in
the
group
when
this
measure
takes
low
values
the
group
members
are
all
satisfied
to
the
same
degree
3.4
summary
in
short
we
can
categorize
the
various
definitions
of
fairness
used
in
rankings
recommendations
and
rank
aggregation
methods
based
on
the
level
and
side
of
fairness
and
their
output
multiplicity
specifically
regarding
level
fairness
can
be
distinguished
between
individual
and
group
fairness
the
side
dimension
considers
consumer
and
producer
fairness
while
the
output
multiplicity
dimension
considers
single
and
multiple
outputs
table
presents
summary
of
the
various
definitions
of
fairness
we
make
several
observations
specifically
we
observe
that
these
fairness
definitions
are
based
on
one
of
the
following
criteria
position
of
item
in
the
ranking
or
recommendation
list
item
utility
prediction
error
rating
and
number
of
items
user
satisfaction
is
defined
through
item
utility
in
rankings
fairness
is
typically
defined
for
the
items
to
be
ranked
matching
all
existing
definitions
to
producer
fairness
except
equity
of
attention
the
definitions
are
group
based
and
to
position
bias
they
use
constraints
on
the
proportion
of
items
in
the
top
positions
or
are
based
on
the
idea
of
providing
exposure
proportional
to
utility
in
recommenders
all
definitions
are
group
based
in
this
case
the
distinction
between
consumer
fairness
and
producer
fairness
makes
more
sense
given
that
they
focus
on
either
the
individuals
that
receive
recommendation
or
the
individuals
that
are
recommended
nevertheless
most
existing
works
target
consumer
fairness
fairness
in
rankings
and
recommendations
an
overview
443
table
fairness
definitions
taxonomy
in
rankings
recommenders
and
rank
aggregation
individual
group
consumer
producer
single
multiple
criterion
rankings
fairness
constraints
18
position
discounted
cumulative
fairness
87
position
fairness
of
exposure
77
position
utility
equity
of
attention
11
position
utility
recommenders
calibrated
recommendations
78
number
of
items
value
absolute
unfairness
88
error
on
predictions
under
overestimation
unfairness
88
error
in
ratings
non-parity
unfairness
88
ratings
rank
aggregation
top-k
parity
55
position
dissatisfaction
fairness
59
user
satisfaction
pareto
optimal
fairness
73
position
proportionality
fairness
76
number
of
items
envy-freeness
fairness
76
number
of
items
sequential
hybrid
aggregation
80
user
satisfaction
in
rank
aggregation
only
recently
and
mainly
for
group
recommenders
fairness
is
considered
when
we
need
to
aggregate
number
of
ranked
outputs
in
order
to
produce
new
ranked
consensus
output
specifically
for
group
recommenders
the
goal
is
to
evaluate
if
the
system
takes
into
consideration
the
individual
preferences
of
each
single
user
in
the
group
making
all
approaches
in
the
research
literature
to
target
at
individual
and
consumer
fairness
only
recently
there
are
few
approaches
that
focus
on
another
form
of
fairness
that
is
applicable
when
we
consider
sequence
of
rankings
or
recommendations
instead
of
just
single
one
in
what
follows
we
will
study
how
these
models
and
definitions
of
fairness
are
applied
to
algorithms
methods
for
achieving
fairness
taking
cross-type
view
we
present
in
this
section
taxonomy
to
organize
and
place
related
works
into
perspective
specifically
while
in
fig
3a
we
show
the
traditional
way
that
results
in
ranked
outputs
in
fig
3b
we
present
the
various
options
and
the
general
distinction
of
the
methods
for
generating
fair
ranked
outputs
and
recommendations
namely
these
methods
are
distinguished
into
the
following
categories
pre-processing
methods
aim
at
transforming
the
data
to
remove
any
underlying
bias
or
discrimination
typically
such
methods
are
application
agnostic
and
consider
bias
in
the
training
data
which
they
try
to
mitigate
bias
in
the
data
may
be
produced
due
to
the
data
collection
process
for
example
based
on
decisions
about
the
pieces
of
data
we
collect
or
not
or
what
assumptions
we
make
for
the
missing
values
or
even
when
using
data
in
different
way
than
intended
during
collection
in-processing
methods
aim
at
modifying
existing
or
introducing
new
algorithms
that
result
in
fair
rankings
and
recommendations
by
removing
bias
and
discrimination
during
the
model
training
process
typically
such
methods
targets
at
learning
model
with
no
bias
while
considering
fairness
during
the
training
of
model
for
example
by
incorporating
changes
into
the
objective
function
of
an
algorithm
by
fairness
term
or
imposing
fairness
constraints
without
offering
any
guarantees
about
fairness
on
the
ranked
outputs
post-processing
methods
modify
the
output
of
the
algorithm
typically
such
methods
can
only
treat
the
ranking
or
recommendation
algorithm
as
black
box
without
any
ability
to
modify
it
and
to
improve
fairness
they
re-rank
the
data
items
of
the
output
naturally
in
post-processing
methods
fairness
comes
at
the
cost
of
accuracy
since
by
definition
the
methods
transform
the
optimal
output
on
the
other
hand
clear
advantage
of
the
post-processing
methods
is
that
they
offer
ranked
outputs
that
are
easy
to
understand
when
comparing
their
outputs
with
the
outputs
before
any
application
of
post-processing
fairness
method
123
444
pitoura
et
al
fig
the
general
distinction
of
the
methods
for
ensuring
fair
ranked
outputs
next
we
will
use
this
taxonomy
to
organize
and
present
the
related
works
that
we
describe
in
the
following
sections
pre-processing
methods
bias
in
the
underlying
data
on
which
systems
are
trained
can
take
two
forms
bias
in
the
rows
of
the
data
exists
when
there
are
not
enough
representative
individuals
from
minority
groups
for
example
according
to
reuters
article
23
amazon
experimental
automated
system
to
review
job
applicants
resumes
showed
significant
gender
bias
toward
male
candidates
over
females
that
was
due
to
historical
discrimination
in
the
training
data
bias
in
the
columns
is
when
features
are
biased
correlated
with
sensitive
attributes
for
example
zip
code
tends
to
predict
race
due
to
history
of
segregation
44
direct
discrimination
occurs
when
protected
attributes
are
used
explicitly
in
making
decisions
disparate
treatment
more
pervasive
nowadays
is
indirect
discrimination
in
which
protected
attributes
are
not
used
but
reliance
on
variables
correlated
with
them
leads
to
significantly
different
outcomes
for
different
groups
also
known
as
disparate
impact
to
address
bias
and
avoid
discrimination
several
methods
have
been
proposed
for
pre-processing
data
many
of
these
methods
are
studied
in
the
context
of
classification
while
few
have
been
proposed
in
the
context
of
recommender
systems
5.1
suppression
to
tackle
bias
in
the
data
naïve
approach
used
in
practice
is
to
simply
omit
the
protected
attribute
say
race
or
gender
when
training
the
classifier
48
simply
excluding
protected
variable
is
insufficient
to
avoid
discriminatory
predictions
as
any
included
variables
that
are
correlated
with
the
protected
variables
still
contain
information
about
the
protected
characteristic
and
the
classifier
still
learns
the
discrimination
reflected
in
the
training
data
for
example
answers
to
personality
tests
identify
people
with
disabilities
85
word
embeddings
trained
on
google
news
articles
exhibit
female
male
gender
stereotypes
123
fig
job
application
example
48
12
to
tackle
such
dependencies
one
can
further
find
the
attributes
that
correlate
most
with
the
sensitive
attribute
and
remove
these
as
well
5.2
class
relabeling
this
approach
also
known
as
massaging
48
changes
the
labels
of
some
objects
in
the
dataset
in
order
to
remove
the
discrimination
from
the
input
data
good
selection
of
which
labels
to
change
is
essential
the
idea
is
to
consider
subset
of
data
from
the
minority
group
as
promotion
candidates
and
change
their
class
label
similarly
subset
of
the
majority
group
is
chosen
as
demotion
candidates
to
select
the
best
candidates
for
relabeling
ranker
is
used
that
ranks
the
objects
based
on
their
probability
of
having
positive
labels
for
example
naïve
bayesian
classifier
can
be
used
for
both
ranking
and
learning
47
48
then
the
top-k
minority
for
promotion
objects
and
the
bottom-k
majority
for
demotion
objects
are
chosen
the
number
of
pairs
needed
to
be
modified
to
make
dataset
discrimination-free
can
be
calculated
as
follows
let
us
assume
as
before
two
groups
namely
the
protected
group
and
the
non-protected
or
privileged
group
if
we
modify
objects
from
each
group
the
resulting
dis
fairness
in
rankings
and
recommendations
an
overview
445
crimination
will
be
disc
disc
31
to
reach
zero
discrimination
the
number
of
modifications
needed
is
disc
32
where
are
the
number
of
positive
objects
that
belong
to
the
minority
group
majority
group
discrimination
disc
in
is
the
probability
of
being
in
the
positive
class
between
the
objects
in
the
minority
group
versus
those
in
the
majority
group
for
example
consider
the
dataset
in
fig
this
dataset
contains
the
sex
ethnicity
and
highest
degree
for
ten
job
applicants
the
job
type
they
applied
for
and
the
outcome
of
the
selection
procedure
class
we
want
to
learn
classifier
to
predict
the
class
of
objects
for
which
the
predictions
are
non-discriminatory
toward
females
we
can
rank
the
objects
by
their
positive
class
probability
given
by
naïve
bayes
classification
model
figure
shows
an
extra
column
that
gives
the
probability
that
each
applicant
belongs
to
the
positive
class
in
the
second
step
we
arrange
the
data
separately
for
female
applicants
with
class
in
descending
order
and
for
male
applicants
with
class
in
ascending
order
with
respect
to
their
positive
class
probability
the
ordered
promotion
and
demotion
candidates
are
given
in
fig
to
reach
zero
discrimination
the
number
of
modifications
needed
is
0.4
disc
10
33
we
relabel
the
highest
scoring
female
with
negative
label
and
the
lowest
scoring
male
with
positive
label
then
the
discrimination
becomes
zero
the
resulting
dataset
will
be
used
for
training
classifier
the
problem
of
classification
without
discrimination
sensitive
attribute
is
multi-objective
optimization
problem
lowering
the
discrimination
will
result
in
lowering
the
accuracy
and
vice
versa
5.3
reweighing
the
previous
approach
is
rather
intrusive
as
it
changes
the
labels
of
the
objects
instead
of
that
weights
can
be
assigned
to
the
objects
to
compensate
for
the
bias
48
the
idea
is
to
assign
lower
weights
to
objects
that
have
been
deprived
fig
job
applications
with
positive
class
probability
fig
promotion
and
demotion
candidates
or
favored
then
the
weights
can
be
used
directly
in
any
method
based
on
frequency
counts
frequently
used
family
of
analytical
methods
are
grouped
under
propensity
score
matching
41
such
methods
model
the
probability
of
each
object
or
group
receiving
the
treatment
and
use
these
predicted
probabilities
or
propensities
to
make
up
for
the
confounding
of
the
treatment
with
the
other
variables
of
interest
and
balance
the
data
simple
probability-based
reweighing
method
is
the
following
48
let
us
consider
the
sensitive
attribute
then
every
object
will
be
assigned
weight
pex
class
class
pobs
class
class
34
the
weight
of
an
object
will
be
the
expected
probability
to
see
an
instance
with
its
sensitive
attribute
value
and
class
given
independence
divided
by
its
observed
probability
for
example
consider
the
dataset
in
fig
if
the
dataset
is
unbiased
then
the
sensitive
attribute
sex
in
our
example
and
the
class
are
statistically
independent
then
the
expected
probability
for
females
to
be
promoted
would
be
pexp
sex
class
0.5
0.6
0.3
in
reality
however
the
observed
probability
based
on
the
dataset
is
sex
class
0.2
hence
one
can
use
re-weighting
factor
0.3
0.2
1.5
to
balance
the
bias
in
the
dataset
entropy
balancing
aims
at
covariate
balance
in
data
for
binary
classification
39
it
relies
on
maximum
entropy
reweighting
scheme
that
calibrates
individual
weights
so
that
the
reweighed
groups
satisfy
set
of
balance
constraints
that
123
446
pitoura
et
al
are
imposed
on
the
sample
moments
of
the
covariate
distributions
the
balance
constraints
ensure
that
the
reweighed
groups
match
exactly
on
the
specified
moments
adjusting
in
this
way
inequalities
in
representation
the
generated
weights
can
be
passed
to
any
standard
classifier
adaptive
sensitive
reweighing
uses
convex
model
to
estimate
distributions
of
underlying
labels
with
which
to
adapt
weights
54
it
assumes
that
there
exists
an
unobservable
underlying
set
of
class
labels
corresponding
to
training
samples
that
if
predicted
would
yield
unbiased
classification
with
respect
to
fairness
objective
it
searches
for
sample
weights
that
make
weighted
training
on
the
original
dataset
also
train
toward
those
labels
without
explicitly
observing
them
more
specifically
consider
binary
probabilistic
classifier
which
produces
probability
estimates
yi
yi
for
training
samples
with
features
xi
and
class
labels
yi
there
exist
underlying
unobservable
class
labels
that
yield
estimated
labels
which
conform
to
designated
fairness
and
accuracy
trade-offs
the
training
goal
is
to
minimize
both
weighted
error
on
observed
labels
and
the
distance
between
weighted
observed
labels
and
unweighed
underlying
labels
min
wi
yi
35
min
wi
yi
36
to
simultaneously
adjust
training
weights
alongside
classifier
training
classifier-agnostic
iterative
approach
is
proposed
first
classifier
is
fully
trained
based
on
uniform
weights
and
then
the
method
appropriately
readjusts
those
weights
this
process
is
repeated
until
convergence
5.4
data
transformation
common
theme
is
the
importance
of
balancing
discrimination
control
against
utility
of
the
processed
data
this
can
be
formulated
as
an
optimization
problem
for
producing
preprocessing
transformations
that
trade-off
discrimination
control
data
utility
and
individual
distortion
67
assuming
is
the
one
or
more
protected
sensitive
variables
denotes
other
non-protected
variables
and
is
an
outcome
random
variable
the
goal
is
to
determine
randomized
mapping
px
that
transforms
both
the
training
data
and
the
test
data
the
mapping
should
satisfy
three
properties
discrimination
control
the
first
objective
is
to
limit
the
dependence
of
the
transformed
outcome
on
the
protected
variables
which
requires
the
conditional
distribution
py
to
be
close
to
target
distribution
pyt
for
all
values
of
123
distortion
control
the
mapping
px
should
satisfy
distortion
constraints
to
reduce
or
avoid
certain
large
changes
very
low
credit
score
being
mapped
to
very
high
credit
score
utility
preservation
the
distribution
of
should
be
statistically
close
to
the
distribution
of
this
is
to
ensure
that
model
learned
from
the
transformed
data
when
averaged
over
the
protected
variables
is
not
too
different
from
one
learned
from
the
original
data
for
example
bank
existing
policy
for
approving
loans
does
not
change
much
when
learnt
over
the
transformed
data
5.5
database
repair
handling
bias
in
the
data
can
be
considered
database
repair
problem
one
approach
is
to
remove
information
about
the
protected
variables
from
the
set
of
covariates
to
be
used
in
predictive
models
28
61
test
for
disparate
impact
based
on
how
well
the
protected
class
can
be
predicted
from
the
other
attributes
and
data
repair
algorithm
for
numerical
attributes
have
been
proposed
28
the
algorithm
strongly
preserves
rank
which
means
it
changes
the
data
in
such
way
that
predicting
the
class
is
still
possible
chain
of
conditional
models
can
be
used
for
both
protecting
and
adjusting
variables
of
arbitrary
type
61
this
framework
allows
for
an
arbitrary
number
of
variables
to
be
adjusted
and
for
each
of
these
variables
and
the
protected
variables
to
be
continuous
or
discrete
another
data
repair
approach
is
based
on
measuring
the
discriminatory
causal
influence
of
the
protected
attribute
on
the
outcome
of
an
algorithm
this
approach
removes
discrimination
by
repairing
the
training
data
in
order
to
remove
the
effect
of
any
discriminatory
causal
relationship
between
the
protected
attribute
and
classifier
predictions
74
this
work
introduced
the
notion
of
interventional
fairness
which
ensures
that
the
protected
attribute
does
not
affect
the
output
of
the
algorithm
in
any
configuration
of
the
system
obtained
by
fixing
other
variables
at
some
arbitrary
values
the
system
repairs
the
input
data
by
inserting
or
removing
tuples
changing
the
empirical
probability
distribution
to
remove
the
influence
of
the
protected
attribute
on
the
outcome
through
any
causal
pathway
that
includes
inadmissible
attributes
attributes
that
should
not
influence
the
protected
attribute
5.6
data
augmentation
different
approach
is
augment
the
training
data
with
additional
data
71
this
framework
starts
from
an
existing
matrix
factorization
recommender
system
that
has
already
been
trained
with
some
input
ratings
data
and
adds
new
users
who
provide
ratings
of
existing
items
the
new
users
ratings
called
antidote
data
are
chosen
so
as
to
improve
fairness
in
rankings
and
recommendations
an
overview
socially
relevant
property
of
the
recommendations
that
are
provided
to
the
original
users
the
proposed
framework
includes
measures
of
both
individual
and
group
unfairness
5.7
summary
of
pre-processing
methods
table
summarizes
pre-processing
approaches
to
fairness
based
on
whether
they
focus
on
bias
in
rows
or
columns
the
level
of
fairness
individual
or
group
and
the
algorithm
that
will
use
the
pre-processed
data
many
of
the
pre-processing
methods
are
studied
in
the
context
of
classification
ranking
suppression
is
simple
brute-force
approach
that
does
not
depend
on
the
algorithm
on
the
downside
the
algorithm
may
still
learn
the
discrimination
from
correlated
attributes
trying
to
remove
these
attributes
as
well
can
seriously
hurt
the
value
of
the
dataset
the
class
relabeling
approach
works
with
different
rankers
naive
bayes
classifier
or
nearest-neighbor
classifier
however
its
objective
is
to
lower
the
discrimination
which
will
result
in
lowering
the
accuracy
and
vice
versa
finding
the
right
balance
is
challenging
moreover
it
is
intrusive
as
it
changes
the
dataset
the
reweighing
methods
are
parameter-free
as
they
do
not
rely
on
ranker
hence
they
can
work
with
any
ranking
algorithm
as
long
as
they
leverage
frequencies
adaptive
sensitive
reweighing
has
an
additional
classification
overhead
since
it
simultaneously
adjusts
training
weights
alongside
classifier
training
through
an
iterative
approach
which
is
repeated
until
convergence
data
transformation
methods
work
with
different
classifiers
but
they
can
be
applied
only
on
numerical
datasets
and
they
modify
the
data
the
aforementioned
approaches
modify
the
training
data
explicitly
by
suppressing
attributes
or
changing
class
labels
or
implicitly
by
adding
weights
data
augmentation
leaves
the
training
data
as
is
and
just
augment
it
with
additional
data
one
data
augmentation
method
has
been
proposed
in
the
context
of
recommender
system
and
in
particular
for
matrix
factorization
this
approach
has
studied
both
individual
and
group
fairness
71
in
general
group
fairness
is
easier
to
track
and
handle
there
is
an
abundance
of
machine
learning
algorithms
used
in
practice
for
search
and
recommendations
and
in
general
dictating
clear
need
for
future
systematic
study
of
the
relationship
between
dataset
features
algorithms
and
pre-processing
performance
in-processing
methods
in-processing
methods
for
achieving
fairness
in
rankings
and
recommendations
focus
on
modifying
existing
or
introducing
new
models
or
algorithms
in
this
section
we
survey
in
447
unified
way
these
methods
by
distinguishing
between
learning
approaches
and
approaches
using
preference
functions
6.1
learning
approaches
for
both
rankings
and
recommenders
learning
approaches
typically
use
machine
learning
to
construct
ranking
models
most
often
using
set
of
labeled
training
data
as
input
in
general
the
ranking
model
ranks
unseen
lists
in
way
similar
to
the
ranking
of
the
training
data
the
overall
goal
is
to
learn
model
that
minimizes
loss
function
that
captures
the
distance
between
the
learned
and
the
input
ranking
various
approaches
exist
varying
on
the
form
of
training
data
and
the
type
of
loss
function
60
in
the
point-wise
approach
21
the
training
data
are
item
relevancescore
pairs
for
each
query
in
this
case
learning
can
be
seen
as
regression
problem
where
given
an
item
and
query
the
goal
is
to
predict
the
score
of
the
item
in
the
pair-wise
approach
the
training
data
are
pairs
of
items
where
the
first
item
is
more
relevant
than
the
second
item
for
given
query
14
30
in
this
case
learning
can
be
seen
as
binary
classification
problem
where
given
two
items
the
classifier
decides
whether
the
first
item
is
better
than
the
second
one
finally
in
the
list-wise
approach
16
the
input
consists
of
query
and
list
of
items
ordered
by
their
relevance
to
the
query
note
that
the
loss
function
takes
many
different
forms
depending
on
the
approach
for
example
in
the
pair-wise
approach
loss
may
be
computed
as
the
average
number
of
inversions
in
ranked
output
in
the
following
we
present
number
of
approaches
toward
making
the
ranking
models
fair
note
that
the
proposed
approaches
can
be
adopted
to
work
for
different
types
of
input
data
and
loss
functions
6.1
adding
regularization
terms
general
in-processing
approach
to
achieving
fairness
is
by
adding
regularization
terms
to
the
loss
function
of
the
learning
model
these
regularization
terms
express
measures
of
unfairness
that
the
model
must
minimize
in
addition
to
the
minimization
of
the
original
loss
function
depending
on
the
form
of
the
training
data
the
loss
function
and
the
measure
of
fairness
different
instantiations
of
this
general
approach
are
possible
the
deltr
approach
90
extends
the
listnet
16
learning
to
rank
framework
listnet
is
list-wise
framework
where
the
training
set
consists
of
query
and
list
of
items
ordered
by
their
relevance
to
listnet
learns
ranking
function
that
minimizes
loss
function
that
measures
the
extent
to
which
the
ordering
of
items
induced
by
for
query
differs
from
the
ordering
that
the
items
appear
in
the
training
set
for
this
query
the
loss
function
deltr
of
123
448
pitoura
et
al
table
pre-processing
methods
bias
in
rows
bias
in
columns
fairness
algorithm
suppression
48
group
any
class
relabeling
47
48
group
ranker
reweighing
39
48
54
group
individual
ranker
data
transformation
67
group
individual
ranker
data
repair
28
61
74
group
ranker
data
augmentation
71
group
individual
matrix
factorization
deltr
is
deltr
unfairness
37
deltr
extends
the
original
loss
function
of
listnet
with
term
that
imposes
fairness
constraint
parameter
controls
the
trade-off
between
ranking
utility
distance
from
input
ranking
captured
by
the
original
loss
function
and
fairness
exposure
is
used
as
measure
of
unfairness
in
the
produced
output
specifically
unfairness
max
exposure
exposure
38
using
the
squared
hinge
loss
makes
the
loss
function
differentiable
also
the
model
prefers
rankings
in
which
the
exposure
of
the
protected
group
is
not
less
than
the
exposure
of
the
non-protected
group
but
not
vice
versa
regularization
approach
is
also
taken
for
recommender
systems
in
49
let
and
denote
random
variables
for
the
users
and
items
respectively
and
denote
random
variable
for
the
recommendation
output
let
also
be
the
sensitive
attribute
that
is
information
to
be
ignored
in
the
recommendation
process
like
the
gender
of
user
or
the
popularity
of
an
item
the
goal
in
this
case
is
to
achieve
recommendation
or
statistical
independence
this
means
to
include
no
information
about
the
sensitive
feature
that
influences
the
outcome
as
well
as
recommendations
should
satisfy
recommendation
independence
constraint
the
core
of
this
regularization
approach
is
included
in
equation
39
that
adopts
regularizer
imposing
constraint
of
independence
while
training
the
recommendation
model
loss
ri
xi
yi
si
ind
reg
39
where
is
the
independence
parameter
that
controls
the
balance
between
independence
and
accuracy
and
ind
is
the
123
independence
term
the
regularizer
to
constrain
independence
the
larger
value
indicates
that
recommendations
and
sensitive
values
are
more
independent
loss
is
the
empirical
loss
while
is
the
regularization
parameter
and
is
the
l2
regularizer
several
alternatives
can
be
used
for
the
independence
term
like
for
example
the
mutual
information
with
histogram
models
or
normal
distributions
or
by
exploiting
distance
measures
as
in
the
case
of
distribution
matching
using
the
bhattacharyya
distance
6.1
learning
via
variational
autoencoders
variational
autoencoders
vae
are
proposed
as
the
state-ofthe-art
for
the
collaborative
filtering
task
in
recommenders
with
multinomial
likelihood
generative
model
and
controlled
regularization
parameter
it
is
possible
to
estimate
normal
distribution
parameters
in
the
middle
layer
of
an
mlp
that
enriches
the
rating
data
representation
and
outperforms
previous
neural
network-based
approaches
58
the
situation
requires
drawing
samples
from
the
inferred
distributions
in
order
to
propagate
values
to
the
decoder
but
it
is
not
trivial
task
to
take
gradients
when
having
sampling
step
the
re-parameterization
trick
52
is
to
re-parameterize
the
sampled
values
by
incorporating
normal
distributed
noise
so
the
gradient
can
back-propagate
through
the
sampled
variable
during
the
training
instead
of
using
only
the
re-parameterization
trick
during
the
training
phase
the
noise
variable
can
be
incorporated
in
the
test
phase
of
vae
as
well
13
in
order
to
enhance
fairness
in
the
ranking
order
of
recommendations
using
as
definition
of
fairness
eq
10
the
motivation
here
is
that
different
noise
distributions
directly
affect
the
rankings
depending
on
how
frequently
the
latent
values
vary
around
the
mean
inside
the
interval
defined
by
the
variance
specifically
it
is
experimentally
shown
that
the
noisy
effect
of
the
gaussian
and
uniform
distributions
varies
the
output
scores
when
having
the
same
data
as
input
while
unfairness
is
reduced
despite
of
small
decrease
in
the
quality
of
the
ranking
the
higher
the
variance
of
the
new
component
the
greater
the
effect
in
the
predicted
scores
and
consequently
in
the
ranking
order
fairness
in
rankings
and
recommendations
an
overview
449
6.1
learning
fair
representations
the
main
idea
in
this
approach
is
to
learn
fair
representation
of
the
input
data
and
use
it
for
the
task
at
hand
previous
work
in
fair
classification
used
this
idea
to
achieve
fairness
by
introducing
an
intermediate
level
between
the
input
space
that
represents
individuals
and
the
output
space
that
represents
classification
outcomes
92
should
be
fair
representation
of
that
best
encodes
and
obfuscates
any
information
about
membership
in
the
protected
group
specifically
is
modeled
as
multinomial
random
variable
of
size
where
each
of
the
values
represents
prototype
cluster
in
the
space
of
the
goal
is
to
learn
such
that
to
minimize
loss
function
λz
40
where
the
first
term
refers
to
the
quality
of
the
encoding
expresses
the
requirement
that
the
distance
from
points
in
to
their
representation
in
should
be
small
the
second
term
refers
to
fairness
and
the
last
term
refers
to
accuracy
the
prediction
based
on
the
representation
should
be
accurate
parameters
λx
and
λz
are
hyper-parameters
that
control
the
trade-off
among
these
three
objectives
one
can
enforce
different
forms
of
fairness
by
appropriately
defining
the
objective
statistical
parity
is
used
in
92
captured
by
the
following
objective
pr
pr
41
that
is
the
probability
that
random
element
that
belongs
to
the
protected
group
of
maps
to
particular
prototype
of
is
equal
to
the
probability
that
random
element
that
belongs
to
the
non-protected
group
of
maps
to
the
same
prototype
the
fair
representation
approach
has
also
been
used
for
fair
ranking
instead
of
classification
this
was
achieved
by
modifying
the
last
objective
in
equation
40
to
represent
accuracy
in
the
case
of
ranking
as
opposed
to
accuracy
in
classification
87
the
modified
objective
asks
that
the
distance
between
the
ground-truth
ranking
and
the
estimated
ranking
is
small
6.2
linear
preference
functions
in
some
applications
items
are
ranked
based
on
score
that
is
weighted
linear
combination
of
the
values
of
their
attributes
specifically
let
be
an
item
with
scoring
attributes
linear
ranking
function
uses
weight
vector
w1
wd
to
compute
utility
goodness
score
for
each
item
dj
in
this
case
fairness
is
formulated
as
the
following
problem
given
function
with
weight
vector
w1
wd
find
function
with
weight
vector
w1
wd
such
that
produces
fair
ranking
and
its
weights
are
as
close
to
the
weights
of
the
original
as
possible
cosine
is
minimized
6.3
constraint
optimization
for
rank
aggregation
for
fairness-preserving
rank
aggregation
55
presents
solution
that
balances
aggregation
accuracy
with
fairness
using
pairwise
rank
representation
in
this
case
ranking
is
represented
as
set
of
pairwise
comparisons
between
data
items
then
for
two
rankings
the
number
of
pairs
with
items
that
do
not
have
the
same
order
in
the
two
rankings
expresses
their
kendall
tau
distance
51
overall
given
set
of
rankings
the
ranking
with
the
minimum
average
kendall
tau
distance
to
the
rankings
in
the
set
is
known
as
the
kemeny
optimal
rank
aggregation
consider
the
case
where
we
have
data
items
from
two
different
groups
and
the
pairs
in
their
cartesian
product
can
be
divided
into
three
subsets
pairs
containing
items
from
pairs
containing
items
from
and
pairs
containing
one
item
from
and
one
item
from
then
rpar
computes
the
probability
for
ranking
that
an
item
from
group
is
ranked
above
an
item
from
group
rpar
xi
xi
42
following
from
this
the
pairwise
formulation
of
statistical
parity
for
two
groups
is
defined
as
given
ranking
consisting
of
data
items
that
belong
to
mutually
exclusive
groups
satisfies
pairwise
statistical
parity
if
the
following
condition
is
met
rpar
rpar
43
linear
integer
programming
solution
with
parity
constraint
is
offered
for
aggregating
many
rankings
while
producing
fair
consensus
for
achieving
better
efficiency
branch-and-bound
fairness-aware
ranking
algorithm
is
designed
that
integrates
rank
parity-preserving
heuristic
in
sect
we
summarize
the
in-processing
methods
along
with
the
post-processing
ones
highlighting
the
advantages
of
each
category
post-processing
methods
post-processing
approaches
are
agnostic
to
the
ranking
or
the
recommendation
algorithm
typically
they
take
as
input
123
450
ranking
and
specification
of
the
required
form
of
fairness
and
produce
new
ranking
that
satisfies
the
fairness
requirements
and
respects
the
initial
ranking
to
the
extent
possible
7.1
fairness
as
generative
process
let
us
consider
simple
case
of
group
parity
where
we
ask
that
specific
proportion
of
the
items
at
the
top
positions
in
the
ranking
belong
to
the
protected
group
given
and
ranking
the
generative
process
introduced
in
87
creates
ranking
by
initializing
to
the
empty
list
and
incrementally
adding
items
to
specifically
for
each
position
in
bernoulli
trial
is
performed
with
if
the
trial
succeeds
we
select
the
best
available
most
highly
ranked
in
item
from
the
protected
group
otherwise
we
select
the
best
available
item
from
the
nonprotected
group
an
example
is
shown
in
fig
the
ranking
rl
in
the
left
is
the
original
ranking
based
on
the
utility
score
of
the
items
the
middle
ranking
rm
corresponds
to
the
extreme
case
of
where
all
members
of
the
protected
group
are
placed
in
the
top
positions
while
the
ranking
tr
on
the
right
corresponds
to
0.5
the
produced
ranking
satisfies
the
in-group
monotonicity
constraints
this
means
that
within
each
group
the
items
are
ordered
with
decreasing
qualifications
it
is
also
shown
that
under
some
assumptions
the
ranking
also
maximizes
the
utility
expressed
as
the
average
score
of
the
items
in
the
top-k
positions
89
statistical
test
for
this
generative
model
is
proposed
in
89
specifically
given
that
at
specific
position
we
have
seen
specific
number
of
items
from
each
group
an
onetailed
binomial
test
is
used
to
compare
the
null
hypotheses
that
the
ranking
was
generated
using
the
model
with
parameter
or
with
which
would
mean
that
the
protected
group
is
represented
less
than
desired
7.2
fair
ranking
as
constraint
optimization
problem
another
post-processing
approach
formulates
the
problem
of
producing
fair
ranking
as
an
optimization
problem
let
be
fairness
measure
for
rankings
and
let
be
measure
of
the
utility
of
ranking
for
particular
task
for
example
let
be
the
relevance
of
ranking
for
given
query
there
are
two
general
ways
of
formulating
an
optimization
problem
involving
fairness
and
utility
namely
maxfcou
maximizing
fairness
subject
to
constraint
in
utility
maxucof
maximizing
utility
subject
to
constraint
in
fairness
123
pitoura
et
al
in
the
maxfcou
formulation
the
underlying
idea
is
to
produce
ranking
that
is
as
fair
as
possible
while
remaining
relevant
to
11
for
example
we
ask
for
the
most
fair
ranking
among
all
rankings
such
that
also
satisfies
utility
constraint
the
loss
in
utility
with
regards
to
the
original
ranking
remains
below
given
threshold
argmaxr
distance
alternatively
in
the
maxucof
formulation
we
look
for
the
ranking
that
has
the
maximum
possible
utility
among
all
rankings
whose
fairness
is
sufficient
11
77
for
example
the
approach
proposed
in
77
produces
ranking
such
that
argmaxr
is
fair
we
characterize
such
maxucof
approaches
as
postprocessing
since
they
assume
that
the
utility
of
each
item
is
known
or
can
be
estimated
thus
implicitly
there
is
an
original
non-fair
ranking
in
which
the
items
are
ordered
solely
by
their
utility
then
given
these
individual
utilities
new
ranking
is
produced
that
also
satisfies
fairness
constraint
in
general
the
complexity
of
both
the
maxfcou
and
maxucof
optimization
problems
depends
on
the
type
of
the
utility
and
fairness
functions
and
the
form
of
the
constraints
in
some
cases
the
optimization
problems
can
be
solved
using
linear
integer
programming
ilp
11
or
in
some
special
cases
using
dynamic
programming
algorithm
18
constraints
can
be
also
used
for
producing
fair
recommendation
packages
of
items
for
groups
of
users
76
the
intuition
of
the
method
is
to
greedily
construct
package
by
adding
in
rounds
to
the
item
that
satisfies
the
largest
number
of
non-satisfied
users
specifically
given
that
satg
denotes
the
users
in
that
are
satisfied
by
at
each
round
the
goal
is
to
maximize
satg
satg
44
when
considering
fairness
proportionality
equation
28
satg
contains
the
users
for
which
item
belongs
in
their
top-δ
most
preferable
items
for
envy-freeness
fairness
eq
27
satg
contains
the
users
that
are
envy-free
for
the
item
that
is
is
fair
for
user
m-proportional
or
m-envy-free
if
there
are
at
least
items
in
such
that
satg
this
method
is
generalized
to
include
constraints
that
restrict
the
set
of
candidate
packages
that
can
be
recommended
to
group
of
users
two
types
of
constraints
are
discussed
namely
category
and
distance
constraints
in
simple
words
with
category
constraints
when
selecting
an
item
from
specific
category
we
remove
the
items
of
this
category
from
the
candidate
set
with
distance
constraints
we
fairness
in
rankings
and
recommendations
an
overview
451
consider
as
candidate
items
only
the
items
that
when
added
to
the
existing
solution
satisfy
the
specific
input
distance
constraints
7.3
fairness
via
calibration
methods
calibration
methods
suggest
to
re-rank
list
of
items
as
post-processing
step
equation
19
in
sect
3.2
quantifies
the
degree
of
calibration
for
recommender
outputs
based
on
specific
metrics
78
to
determine
the
optimal
set
of
recommended
items
movies
in
the
suggested
scenario
the
maximum
marginal
relevance
function
17
is
used
argmax
45
where
determines
the
trade-off
between
the
prediction
scores
of
the
movies
with
and
the
calibration
metric
equation
19
namely
the
tradeoff
between
accuracy
and
calibration
is
controlled
by
greedily
the
method
starts
with
an
empty
set
and
iteratively
adds
one
movie
at
time
namely
the
movie
that
maximizes
eq
45
in
similar
way
for
group
recommendations
the
best
suggestions
for
group
should
maximize
the
social
welfare
sw
eq
24
and
fairness
eqs
25
and
26
using
the
scheme
59
sw
46
greedy
solution
is
to
select
an
item
that
when
added
to
the
current
recommendation
list
achieves
the
highest
fairness
more
time-efficient
alternatives
are
offered
via
integer
programming
techniques
when
considering
the
notion
of
pareto
optimality
for
group
recommendations
simple
heuristic
can
be
used
to
compile
and
approximately
identify
the
list
of
the
top-n
recommendations
for
group
73
specifically
given
where
is
the
largest
number
of
items
the
system
can
recommend
for
an
individual
user
the
method
proceeds
as
follows
it
requests
the
top-n
recommendations
for
each
user
in
the
group
it
takes
their
union
and
it
identifies
the
level
pareto
optimal
items
among
the
items
in
the
union
7.4
fairness
in
multiple
outputs
when
providing
rankings
or
recommendations
users
typically
pay
more
attention
to
the
first
positions
and
attention
wears
off
for
items
in
lower
positions
in
the
ranking
in
situation
where
the
greatest
estimated
probabilities
are
quite
close
or
equal
to
each
other
the
algorithm
needs
to
arrange
them
in
proper
order
and
necessarily
present
high
scores
in
high
positions
this
promotes
an
unfair
result
that
can
be
mitigated
in
the
long
term
by
changing
the
position
of
items
in
sequential
rounds
of
rankings
or
recommendations
in
the
case
of
rankings
one
approach
11
is
to
require
that
ranked
items
receive
attention
that
is
proportional
to
their
utility
in
sequence
of
rankings
eq
this
way
the
unfair
position
one
item
appears
in
single
ranking
can
be
compensated
in
the
next
rankings
when
it
changes
position
and
the
whole
session
contemplates
long-term
fairness
however
the
act
of
reducing
unfairness
implies
reduction
in
the
ranking
quality
due
to
the
perturbation
of
the
utility-based
rankings
the
trade-off
between
ranking
quality
and
fairness
is
formulated
as
constrained
optimization
problem
11
targeting
at
minimizing
unfairness
subject
to
constraints
on
quality
lower-bound
the
minimum
acceptable
quality
specifically
for
sequence
of
rankings
where
the
items
are
ordered
by
the
utility
score
inducing
zero
quality
loss
the
aim
is
at
reordering
them
into
to
minimize
the
distance
between
the
attention
and
utility
distributions
with
constraints
on
the
ndcg-quality
loss
in
each
ranking
formally
min
ai
ui
47
subject
to
ndcg-quality
where
ai
and
ui
denote
the
cumulative
attention
and
utility
scores
that
the
item
gained
across
all
rankings
consider
different
scenario
where
group
of
users
interacts
with
recommender
multiple
times
80
when
following
traditional
methods
for
group
recommendations
like
the
average
aggregation
method
and
the
least
misery
one
the
degree
of
satisfaction
for
each
user
in
the
group
equation
29
cannot
be
good
enough
for
all
users
in
the
group
leading
to
cases
in
which
almost
none
of
the
reported
items
are
of
interest
to
some
users
in
the
group
that
is
the
recommender
system
is
unfair
to
these
users
and
unfairness
continues
throughout
number
of
recommendations
rounds
to
overcome
the
drawbacks
of
the
average
and
the
least
misery
aggregation
methods
and
capitalize
on
their
advantages
an
aggregation
method
called
sequential
hybrid
aggregation
method
offers
weighted
combination
of
them
80
specifically
score
dz
avgscore
dz
leastscore
dz
48
for
group
avgscor
dz
returns
the
score
of
the
item
dz
as
it
is
computed
by
the
average
aggregation
method
during
round
and
least
scor
dz
returns
the
least
satisfied
user
score
of
dz
at
round
to
self-regulate
the
value
of
between
and
so
as
to
more
effectively
describe
the
consensus
of
the
group
is
set
dynamically
in
each
iteration
by
subtracting
the
minimum
satisfaction
score
123
452
pitoura
et
al
table
in
and
post-processing
methods
level
of
fairness
individual
group
side
of
fairness
consumer
producer
output
multiplicity
single
multiple
adding
regularization
terms
49
90
49
90
49
90
learning
fair
representations
92
92
92
learning
with
vaes
13
13
linear
preference
functions
constraint
optimization
for
rank
aggregation
55
in-processing
methods
55
13
55
post-processing
methods
fairness
as
generative
process
87
89
87
89
fairness
as
constraint
optimization
problem
11
76
87
89
18
77
76
11
18
77
18
76
77
fairness
with
calibration
methods
59
73
78
59
73
78
59
73
78
fairness
in
multiple
rounds
11
80
80
11
of
the
group
members
in
the
previous
iteration
from
the
maximum
score
max
sat
gr
min
sat
gr
49
where
sat
gr
defines
the
satisfaction
of
user
for
the
group
recommendations
gr
at
round
the
dynamic
calculations
of
counteract
the
individual
drawbacks
of
the
average
and
least
misery
method
intuitively
if
the
group
members
are
equally
satisfied
at
the
last
round
then
takes
low
values
and
the
aggregation
will
closely
follow
that
of
average
where
everyone
is
treated
as
an
equal
on
the
other
hand
if
one
group
member
is
extremely
unsatisfied
in
specific
round
then
takes
high
value
and
promotes
that
member
preferences
on
the
next
round
next
we
summarize
both
the
in-processing
and
postprocessing
methods
and
provide
their
advantages
summary
of
in
and
post-processing
methods
in
this
section
we
summarize
the
in-processing
and
postprocessing
approaches
for
achieving
fairness
overall
there
exist
methods
that
have
been
proposed
in
the
context
of
rankings
recommender
systems
as
well
as
for
the
rank
aggregation
problem
table
organizes
the
methods
based
on
the
level
of
fairness
namely
individual
or
group
the
side
of
fairness
namely
consumer
or
producer
and
the
output
multiplicity
namely
if
method
focuses
on
single
output
or
multiple
outputs
in
general
all
existing
learning
and
linear
preference
functions
in-processing
approaches
target
group
and
producer
fairness
most
approaches
consider
single
output
with
123
11
11
80
the
recent
exception
of
13
using
vaes
that
considers
multiple
outputs
typically
the
approaches
in
this
category
extend
the
objective
function
that
they
use
by
including
fairness
or
unfairness
term
the
target
is
to
find
the
best
balance
between
the
accuracy
objective
and
fairness
objective
of
the
optimization
problem
that
particular
approach
applies
regarding
the
post-processing
approaches
we
observe
that
there
exist
works
focusing
on
all
different
options
of
fairness
definitions
as
in
the
pre-processing
case
postprocessing
methods
treat
the
algorithms
for
producing
rankings
and
recommendations
as
black
boxes
without
changing
their
inner
workings
this
means
that
any
post-processing
approach
receives
as
input
ranked
output
and
re-ranks
the
data
items
in
this
output
to
improve
fairness
respecting
the
initial
ranked
output
to
the
extent
possible
typically
in-processing
approaches
manage
to
offer
better
trade-offs
between
fairness
and
accuracy
compared
to
the
post-processing
methods
since
they
naturally
identify
this
balance
via
the
objective
function
they
use
however
at
the
same
time
they
cannot
offer
any
guarantees
about
fairness
in
the
output
rankings
and
recommendations
since
fairness
is
considered
only
during
the
training
phase
when
considering
post-processing
approaches
it
is
important
to
note
that
they
can
lead
to
unpredictable
losses
in
accuracy
since
they
treat
the
algorithms
for
producing
rankings
and
recommendations
as
black
boxes
this
is
especially
true
for
pre-processing
methods
as
well
on
the
positive
side
postprocessing
methods
offer
outputs
that
are
easy
to
understand
when
comparing
their
outputs
with
the
outputs
before
any
application
of
post-processing
fairness
method
and
realize
that
they
offer
fairer
output
recently
92
combines
both
pre-processing
and
inprocessing
strategies
by
jointly
learning
fair
representation
of
the
data
and
the
classifier
parameters
this
approach
has
two
main
limitations
it
leads
to
non-convex
optimiza
fairness
in
rankings
and
recommendations
an
overview
tion
problem
and
does
not
guarantee
optimality
and
the
accuracy
of
the
classifier
depends
on
the
dimension
of
the
fair
representation
which
needs
to
be
chosen
rather
arbitrarily
verifying
fairness
in
the
previous
sections
we
studied
methods
for
achieving
fairness
both
pre-processing
and
post-processing
methods
treat
the
recommendation
or
ranking
algorithm
as
black
box
and
try
to
address
fairness
in
the
input
or
the
output
of
the
algorithm
respectively
in
particular
post-processing
methods
assume
that
we
already
know
that
we
have
an
algorithm
that
creates
discrimination
in
its
output
and
try
to
mitigate
that
the
question
that
naturally
arises
is
how
we
can
verify
whether
program
is
fair
in
the
first
place
program
fairness
verification
aims
at
analyzing
given
decision-making
program
and
constructing
proof
of
its
fairness
or
unfairness
just
as
traditional
static
program
verifier
would
prove
correctness
of
program
with
respect
to
for
example
lack
of
divisions
by
zero
however
there
are
several
challenges
what
class
of
decision-making
programs
the
program
model
will
capture
what
the
input
to
the
program
is
how
to
describe
what
it
means
for
the
program
to
be
fair
and
how
to
fully
automate
the
verification
process
one
simple
approach
is
to
take
decision-making
program
and
dataset
as
input
34
using
concrete
dataset
simplifies
the
verification
problem
but
it
also
raises
questions
of
whether
the
dataset
is
representative
for
the
population
for
which
we
are
trying
to
prove
fairness
an
alternative
approach
would
be
to
use
population
model
as
input
which
can
be
probabilistic
model
that
defines
joint
probability
distribution
on
the
inputs
of
then
we
need
to
define
when
and
why
program
is
fair
or
unfair
if
we
want
to
prove
group
fairness
for
example
that
the
algorithm
is
just
as
likely
to
hire
minority
applicant
as
it
is
for
other
nonminority
applicants
we
could
define
post-condition
like
the
following
pr
true
vs
pr
true
vs
the
verifier
then
will
prove
or
disprove
that
is
fair
for
the
given
population
in
general
proving
group
fairness
is
easier
the
verification
process
reduces
to
computing
the
probability
of
number
of
events
with
respect
to
the
program
and
the
population
model
however
proving
individual
fairness
requires
more
complex
reasoning
involving
multiple
runs
of
the
program
notoriously
hard
problem
moreover
in
the
case
of
negative
result
the
verifier
should
provide
the
users
with
proof
of
unfairness
depending
on
the
fairness
definition
producing
453
human
readable
proof
might
be
challenging
as
the
argument
might
involve
multiple
and
potentially
infinite
inputs
for
example
for
group
fairness
it
might
be
challenging
to
explain
why
the
program
outputs
true
on
40
of
the
minority
inputs
and
on
70
of
the
majority
inputs
overall
program
fairness
verification
is
difficult
and
less
investigated
topic
different
approach
is
to
make
fairness
first-class
citizen
in
programming
in
fairness-aware
programming
developers
can
state
fairness
expectations
natively
in
their
code
and
have
run-time
system
monitor
decision-making
and
report
violations
of
fairness
this
approach
is
analogous
to
the
notion
of
assertions
in
modern
programming
languages
for
instance
the
developer
might
assert
that
indicating
that
they
expect
the
value
of
to
be
positive
at
certain
point
in
the
code
the
difficulty
however
is
that
fairness
definitions
are
typically
probabilistic
and
therefore
detecting
their
violation
cannot
be
done
through
single
execution
as
in
traditional
assertions
instead
we
have
to
monitor
the
decisions
made
by
the
procedure
and
then
using
statistical
tools
infer
that
fairness
property
does
not
hold
with
reasonably
high
confidence
for
example
consider
movie
recommendation
system
where
user
data
have
been
used
to
train
recommender
that
given
user
profile
recommends
single
movie
suppose
that
the
recommender
was
constructed
with
the
goal
of
ensuring
that
male
users
are
not
isolated
from
movies
with
strong
female
lead
then
the
developer
may
add
the
following
specification
to
their
recommender
code
spec
pr
emalelead
male
0.2
the
above
specification
ensures
that
for
male
users
the
procedure
recommends
movie
with
female
lead
at
least
20
of
the
time
to
determine
that
procedure
satisfies
fairness
specification
we
need
to
maintain
statistics
over
the
inputs
and
outputs
of
the
procedure
as
it
is
being
applied
specifically
we
compile
the
specification
into
run-time
monitoring
code
that
executes
every
time
is
applied
storing
aggregate
results
of
every
probability
event
appearing
in
in
the
earlier
example
with
movie
recommendation
the
monitoring
code
would
maintain
the
number
of
times
the
procedure
returned
true
for
movie
with
female
lead
again
big
challenge
is
checking
individual
fairness
in
this
case
the
run-time
system
has
to
remember
all
decisions
made
explicitly
so
as
to
compare
new
decisions
with
past
ones
10
open
challenges
in
this
paper
we
have
just
begun
to
realize
the
need
for
fairness
in
particular
fields
namely
in
rankings
and
recommender
systems
next
we
highlight
few
critical
open
123
454
issues
and
challenges
for
future
work
which
aim
to
support
advanced
services
for
making
accountable
complex
rankings
and
recommender
systems
codification
of
definitions
as
described
in
sects
and
there
is
not
universal
definition
for
expressing
fairness
in
rankings
and
recommenders
instead
the
list
of
potential
definitions
is
very
long
specifically
just
for
fairness
alone
there
exist
plethora
of
different
definitions
some
of
them
are
not
even
compatible
in
the
sense
that
there
is
no
method
that
can
satisfy
all
of
them
simultaneously
except
in
highly
constrained
special
cases
53
furthermore
there
is
also
need
to
make
explicit
the
correspondence
between
these
definitions
and
the
interpretation
of
bias
or
diversity
that
each
of
them
materializes
the
limitations
of
each
definition
the
compatibility
among
them
the
incurred
trade-offs
the
domain
of
applicability
assumptions
and
parameters
are
not
well
understood
yet
moreover
it
is
interesting
to
see
how
the
general
public
views
fairness
in
decision
making
by
testing
people
perception
of
different
fairness
definitions
we
can
understand
definitions
of
fairness
that
are
appropriate
for
particular
contexts
37
69
one
such
attempt
investigates
which
definitions
people
perceive
to
be
the
fairest
in
the
context
of
loan
decisions
75
and
whether
fairness
perceptions
change
with
the
addition
of
sensitive
information
race
of
the
loan
applicants
lack
of
data
major
challenge
is
that
the
available
data
are
often
limited
this
way
any
analysis
is
done
with
data
that
has
been
acquired
independently
through
process
on
which
the
data
scientist
has
limited
control
collecting
more
data
for
analysis
is
challenging
and
will
help
to
discover
more
types
of
biases
on
it
in
the
long
run
one
could
envision
benchmarks
for
measuring
the
societal
impact
of
an
algorithm
along
the
lines
of
the
tpc
benchmark2
for
database
performance
unified
approach
for
the
data
pipeline
one
limitation
of
the
current
work
is
that
fairness
has
been
studied
for
specific
tasks
in
isolation
with
most
current
work
in
fairness
focusing
on
the
classification
task
with
the
goal
of
non-discrimination
however
there
is
need
to
consider
fairness
along
the
whole
data
pipeline
79
this
includes
pre-processing
steps
such
as
data
selection
acquisition
cleaning
filtering
and
integration
pre-processing
for
removing
bias
can
be
viewed
as
the
action
of
repairing
by
replacing
modifying
or
deleting
data
that
cause
bias
this
pipeline
also
includes
post-processing
steps
such
as
data
representation
data
visualization
and
user
interfaces
for
example
how
results
are
presented
can
introduce
bias
and
this
is
why
we
need
to
understand
the
implications
for
fairness
pitoura
et
al
lack
of
evaluation
tools
besides
coming
up
with
the
correct
way
of
defining
fairness
in
rankings
and
recommenders
there
is
also
need
for
tools
for
investigating
bias
and
evaluating
the
quality
of
dataset
an
algorithm
or
system
there
are
some
first
attempts
such
as
ibm
ai
fairness
3603
and
tensor
flow
fairness
indicators4
however
both
of
them
focus
mainly
on
statistical
group
measures
of
fairness
in
classification
concepts
like
context
and
provenance
are
important
and
can
be
directly
considered
in
designing
such
tools
in
this
direction
we
also
need
efficient
ways
for
measuring
fairness
and
monitoring
its
evolution
over
time
previous
research
in
stream
processing
and
incrementally
maintaining
statistics
may
be
relevant
here
perhaps
the
most
pending
question
is
how
to
quantify
the
long-term
impact
of
enforcing
methods
that
target
at
ensuring
fairness
would
they
work
in
favor
of
the
social
good
or
would
they
backfire
in
ways
that
we
cannot
predict
lack
of
real
applications
of
fairness
while
lot
of
work
is
done
in
research
setting
we
still
do
not
see
many
actual
applications
and
their
results
there
are
many
challenges
in
making
algorithms
and
systems
fairer
in
the
real
world
company
for
example
needs
to
consider
its
business
metrics
click-through
rate
purchases
and
make
sure
that
these
are
not
affected
for
example
how
to
design
algorithms
that
take
into
account
all
these
different
objectives
is
challenging
an
example
of
real
application
of
fairness
is
found
in
linkedin
35
where
they
developed
fair
framework
and
they
applied
it
to
linkedin
talent
search
online
tests
showed
considerable
improvement
in
the
fairness
metrics
without
significant
impact
on
the
business
metrics
which
paved
the
way
for
deployment
to
linkedin
users
worldwide
multi-level
architecture
of
value
systems
and
algorithms
problem
intrinsic
to
the
definition
of
all
fairness
definitions
in
rankings
and
recommenders
is
the
fact
that
they
attempt
to
quantify
philosophical
legal
often
elusive
and
even
controversial
notions
of
justice
and
social
good
complexity
is
aggregated
when
notions
that
reflect
value
systems
and
beliefs
interact
with
the
mechanisms
for
implementing
them
in
the
very
least
there
should
be
clear
distinction
between
what
constitutes
belief
and
what
is
the
mechanism
or
measure
for
codifying
this
belief
from
technical
point
of
view
we
should
then
be
able
to
focus
on
assessing
whether
proposed
measure
is
an
appropriate
codification
of
given
belief
as
opposed
to
assessing
the
belief
itself
this
calls
for
developing
different
levels
of
abstractions
and
mappings
between
them
this
is
somehow
reminiscent
of
how
data
independence
is
supported
in
database
management
systems
by
the
three-level
architecture
with
the
physical
conceptual
and
external
level
and
the
mappings
between
these
https://aif360.mybluemix.net/.
http://www.tpc.org/information/benchmarks.asp.
123
https://www.tensorflow.org/tfx/guide/fairness_indicators.
fairness
in
rankings
and
recommendations
an
overview
levels
70
at
the
lower
level
we
could
have
beliefs
and
value
systems
and
at
the
higher
level
fairness
definitions
intermediate
levels
could
be
used
to
support
transformations
for
getting
from
the
lower
to
the
higher
level
relating
algorithmic
fairness
with
other
notions
of
fairness
in
systems
the
focus
of
this
survey
is
on
fairness
in
decision-making
processes
and
in
particular
on
rankings
and
recommendation
systems
used
in
this
context
however
there
are
several
other
cases
where
system
needs
to
make
decisions
and
where
fairness
is
also
important
in
particular
there
are
several
problems
more
familiar
to
the
data
management
community
such
as
resource
allocation
and
scheduling
where
fairness
has
been
studied
we
present
here
representative
examples
for
instance
an
approach
is
presented
in
56
for
cache
allocation
where
fairness
is
based
on
pareto
efficiency
and
sharing
incentives
multiple
resource
allocation
approach
is
introduced
in
36
that
generalizes
max
min
fairness
to
multiple
resource
types
where
max
min
fairness
in
this
context
refers
to
maximizing
the
minimum
allocation
received
by
user
in
the
system
another
notion
of
fairness
termed
proportionate
progress
is
proposed
in
for
the
periodic
scheduling
problem
where
weighted
resources
are
allocated
to
tasks
for
specific
time
units
in
each
interval
in
proportionate
progress
fairness
each
task
is
scheduled
resources
according
to
its
importance
its
weight
finally
for
scheduling
in
38
fairness
means
that
the
workload
scheduler
performs
in
way
that
no
query
starves
for
resources
another
notion
of
fairness
for
different
problem
namely
the
chairman
selection
problem
is
proposed
in
82
in
the
chairman
selection
problem
set
of
states
want
to
form
union
and
select
chairman
for
each
year
fairness
in
this
context
refers
to
guaranteeing
small
discrepancy
for
the
number
of
chairmen
representing
each
state
so
that
all
states
are
satisfied
clearly
fairness
concerns
have
emerged
in
various
contexts
through
the
years
providing
unified
view
of
all
these
different
notions
of
fairness
and
the
computational
methods
used
to
enforce
them
is
an
open
problem
and
an
opportunity
how
to
embed
models
and
algorithms
for
fairness
into
any
system
that
involves
any
type
of
decision
making
is
also
open
of
course
new
models
and
algorithms
may
be
needed
in
different
contexts
where
the
principles
of
fairness
may
be
different
what
works
well
for
recommendation
problem
may
not
work
well
for
query
optimizer
or
resource
sharing
in
cloud
computing
however
this
unified
view
will
potentially
provide
new
insights
and
opportunities
for
crossfertilization
fairness
in
other
domains
in
this
paper
we
have
focused
on
ranking
and
recommendation
algorithms
many
more
algorithms
are
being
revisited
under
the
lens
of
fairness
two
such
examples
are
clustering
and
ranking
in
networks
for
455
instance
fair
clustering
adopts
parity
definition
of
fairness
and
asks
that
each
group
must
have
approximately
equal
representation
in
every
cluster
19
for
the
link
analysis
problem
in
networks
fair
algorithms
are
introduced
in
83
that
use
parity-based
definition
of
fairness
and
apply
constraints
on
the
proportion
of
pagerank
allocated
to
the
members
of
each
group
of
nodes
in
the
network
algorithmic
fairness
is
fast
changing
field
it
is
an
open
challenge
to
provide
fairness
measures
and
definitions
for
all
kinds
of
algorithms
and
use
them
in
evaluating
such
algorithms
in
analogy
of
say
response
time
for
measuring
performance
68
recently
the
concept
of
fairness
has
also
been
studied
in
different
domains
and
for
covering
different
needs
we
discuss
two
such
domains
labor
market
and
social
matching
for
instance
previous
research
examines
labor
market
cases
under
fairness
concerns
and
shows
the
relevance
of
such
concerns
on
economic
outcomes
especially
when
considering
employment
contracts
over
time
27
recently
in
the
same
domain
the
work
in
42
pinpoints
the
persistence
of
racial
inequalities
and
designs
solutions
with
respect
to
dynamic
reputational
model
of
the
labor
market
highlighting
the
results
from
groups
divergent
accesses
to
resources
other
recent
work
examines
the
fairness
of
online
job
marketplaces
in
terms
of
ranking
job
applicants
26
instead
of
partitioning
the
individuals
in
predefined
groups
the
authors
seek
to
find
partitioning
of
the
individuals
based
on
their
protected
attributes
that
exhibits
the
highest
unfairness
for
the
problem
of
professional
social
matching
conventional
mechanisms
such
as
optimizing
for
similarity
and
triadic
closure
are
studied
in
65
and
shown
to
involve
risks
of
strengthening
the
homophily
bias
and
echo
chambering
for
the
problem
of
team
assembly
the
work
in
62
designs
fairness-aware
solutions
when
multidisciplinary
teams
need
to
be
formed
and
allocated
to
work
on
different
projects
with
requirements
on
members
skills
the
application
of
models
and
mechanisms
of
algorithmic
fairness
in
variety
of
domains
that
involve
social
and
economic
activities
such
as
in
labor
market
social
matching
and
team
formation
opens
lot
of
opportunities
for
fruitful
interdisciplinary
work
11
conclusions
ranking
and
recommender
systems
have
several
applications
such
as
hiring
lending
and
college
admissions
where
the
notion
of
fairness
is
very
important
since
decision
making
is
involved
we
have
only
begun
to
understand
the
nature
representation
and
variety
of
the
several
definitions
of
fairness
and
the
appropriate
methods
for
ensuring
it
in
this
article
we
follow
systematic
and
structured
approach
to
explain
the
various
sides
of
and
approaches
123
456
to
fairness
first
we
lay
the
ground
by
presenting
general
fairness
definitions
then
we
zoom
in
on
models
and
definitions
for
rankings
recommendations
and
the
problem
of
rank
aggregation
we
organize
them
in
taxonomy
and
highlight
their
differences
and
commonalities
this
analysis
naturally
leads
to
number
of
open
questions
such
as
how
do
fairness
definitions
fare
which
definition
is
suitable
for
which
context
how
do
people
perceive
fairness
in
different
contexts
what
does
it
mean
to
be
fair
after
all
is
there
unified
way
to
be
able
to
judge
whether
an
outcome
or
an
algorithm
is
fair
we
move
on
to
describing
solutions
for
fair
rankings
and
recommendations
we
organize
the
approaches
to
tackle
unfairness
or
ensure
fairer
outcome
into
pre
in
and
postprocessing
approaches
within
each
category
we
further
classify
them
along
several
dimensions
it
is
still
very
early
to
say
which
one
works
best
for
which
context
there
is
no
evaluation
that
puts
them
all
under
the
same
lens
and
there
are
generally
not
conclusive
results
as
to
which
fare
better
it
may
be
the
case
that
combination
of
methods
should
be
applied
combining
pre-processing
and
in-processing
steps
this
is
also
an
open
question
as
different
efforts
have
adapted
single
angle
in
the
problem
while
the
focus
of
this
survey
is
on
fairness
in
rankings
and
recommendation
systems
we
discuss
several
other
cases
where
system
needs
to
make
decisions
and
fairness
is
also
important
and
how
we
can
verify
whether
program
is
fair
finally
we
discuss
open
research
challenges
pertaining
to
fairness
in
the
broader
context
of
data
management
and
on
designing
building
managing
and
evaluating
fair
data
systems
and
applications
open
access
this
article
is
licensed
under
creative
commons
attribution
4.0
international
license
which
permits
use
sharing
adaptation
distribution
and
reproduction
in
any
medium
or
format
as
long
as
you
give
appropriate
credit
to
the
original
author
and
the
source
provide
link
to
the
creative
commons
licence
and
indicate
if
changes
were
made
the
images
or
other
third
party
material
in
this
article
are
included
in
the
article
creative
commons
licence
unless
indicated
otherwise
in
credit
line
to
the
material
if
material
is
not
included
in
the
article
creative
commons
licence
and
your
intended
use
is
not
permitted
by
statutory
regulation
or
exceeds
the
permitted
use
you
will
need
to
obtain
permission
directly
from
the
copyright
holder
to
view
copy
of
this
licence
visit
http://creativecomm
ons
org
licenses
by
4.0
references
angwin
et
al
machine
bias
propublica
2016
https
www.propublica.org/article/machine-bias-risk-assessments-incriminal-sentencing
albarghouthi
antoni
drews
nori
fairness
as
program
property
corr
abs
1610.06067
2016
http://arxiv.
org
abs
1610.06067
123
pitoura
et
al
albarghouthi
vinitsky
fairness-aware
programming
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
fat
2019
pp
211
219
acm
2019
amer-yahia
roy
chawla
das
yu
group
recommendation
semantics
and
efficiency
proc
vldb
endow
754
765
2009
asudeh
jagadish
fairly
evaluating
and
scoring
items
in
data
set
proc
vldb
endow
13
12
3445
3448
2020
asudeh
jagadish
stoyanovich
das
designing
fair
ranking
schemes
in
proceedings
of
the
2019
international
conference
on
management
of
data
sigmod
conference
2019
pp
1259
1276
acm
2019
asudeh
jin
jagadish
assessing
and
remedying
coverage
for
given
dataset
in
35th
ieee
international
conference
on
data
engineering
icde
2019
pp
554
565
ieee
2019
baeza-yates
bias
on
the
web
commun
acm
61
54
61
2018
baruah
cohen
plaxton
varvel
proportionate
progress
notion
of
fairness
in
resource
allocation
algorithmica
15
600
625
1996
10
beutel
chen
doshi
qian
wei
wu
heldt
zhao
hong
chi
goodrow
fairness
in
recommendation
ranking
through
pairwise
comparisons
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
kdd
2019
pp
2212
2220
acm
2019
11
biega
gummadi
weikum
equity
of
attention
amortizing
individual
fairness
in
rankings
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
sigir
2018
pp
405
414
acm
2018
12
bolukbasi
chang
zou
saligrama
kalai
man
is
to
computer
programmer
as
woman
is
to
homemaker
debiasing
word
embeddings
in
advances
in
neural
information
processing
systems
29
annual
conference
on
neural
information
processing
systems
2016
pp
4349
4357
2016
13
borges
stefanidis
enhancing
long
term
fairness
in
recommendations
with
variational
autoencoders
in
11th
international
conference
on
management
of
digital
ecosystems
medes
2019
pp
95
102
acm
2019
14
burges
shaked
renshaw
lazier
deeds
hamilton
hullender
learning
to
rank
using
gradient
descent
in
machine
learning
proceedings
of
the
twenty-second
international
conference
icml
2005
acm
international
conference
proceeding
series
vol
119
pp
89
96
acm
2005
15
calders
verwer
three
naive
bayes
approaches
for
discrimination-free
classification
data
min
knowl
discov
21
277
292
2010
16
cao
qin
liu
tsai
li
learning
to
rank
from
pairwise
approach
to
listwise
approach
in
machine
learning
proceedings
of
the
twenty-fourth
international
conference
icml
2007
acm
international
conference
proceeding
series
vol
227
pp
129
136
acm
2007
17
carbinell
goldstein
the
use
of
mmr
diversity-based
reranking
for
reordering
documents
and
producing
summaries
sigir
forum
51
209
210
2017
18
celis
straszak
vishnoi
ranking
with
fairness
constraints
in
45th
international
colloquium
on
automata
languages
and
programming
icalp
2018
lipics
vol
107
pp
28
28
15
schloss
dagstuhl
leibniz-zentrum
für
informatik
2018
19
chierichetti
kumar
lattanzi
vassilvitskii
fair
clustering
through
fairlets
in
advances
in
neural
information
processing
systems
30
annual
conference
on
neural
information
processing
systems
2017
december
2017
long
beach
ca
usa
pp
5029
5037
2017
fairness
in
rankings
and
recommendations
an
overview
20
chouldechova
roth
snapshot
of
the
frontiers
of
fairness
in
machine
learning
commun
acm
63
82
89
2020
https
doi
org
10.1145
3376898
21
crammer
singer
pranking
with
ranking
in
advances
in
neural
information
processing
systems
14
neural
information
processing
systems
natural
and
synthetic
nips
2001
pp
641
647
mit
press
2001
22
chouldechova
fair
prediction
with
disparate
impact
study
of
bias
in
recidivism
prediction
instruments
big
data
153
163
2017
23
dastin
rpt-insight-amazon
scraps
secret
ai
recruiting
tool
that
showed
bias
against
women
in
reuters
2018
24
dwork
hardt
pitassi
reingold
zemel
fairness
through
awareness
in
innovations
in
theoretical
computer
science
2012
pp
214
226
acm
2012
25
ekstrand
burke
diaz
fairness
and
discrimination
in
retrieval
and
recommendation
in
proceedings
of
the
42nd
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
2019
pp
1403
1404
acm
2019
26
elbassuoni
amer-yahia
ghizzawi
fairness
of
scoring
in
online
job
marketplaces
trans
data
sci
29
29
30
2020
27
fehr
goette
zehnder
behavioral
account
of
the
labor
market
the
role
of
fairness
concerns
ann
rev
econ
355
384
2009
28
feldman
friedler
moeller
scheidegger
venkatasubramanian
certifying
and
removing
disparate
impact
in
proceedings
of
the
21th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
pp
259
268
acm
2015
29
ferraro
serra
bauer
break
the
loop
gender
imbalance
in
music
recommenders
in
scholer
thomas
elsweiler
joho
kando
smith
eds
chiir
21
acm
sigir
conference
on
human
information
interaction
and
retrieval
canberra
act
australia
march
14
19
2021
pp
249
254
acm
2021
30
freund
iyer
schapire
singer
an
efficient
boosting
algorithm
for
combining
preferences
in
proceedings
of
the
fifteenth
international
conference
on
machine
learning
icml
1998
pp
170
178
morgan
kaufmann
1998
31
friedler
scheidegger
venkatasubramanian
on
the
im
possibility
of
fairness
corr
abs
1609.07236
2016
http
arxiv
org
abs
1609.07236
32
friedler
scheidegger
venkatasubramanian
the
im
possibility
of
fairness
different
value
systems
require
different
mechanisms
for
fair
decision
making
commun
acm
64
136
143
2021
33
friedler
scheidegger
venkatasubramanian
choudhary
hamilton
roth
comparative
study
of
fairnessenhancing
interventions
in
machine
learning
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
fat
2019
pp
329
338
acm
2019
34
galhotra
brun
meliou
fairness
testing
testing
software
for
discrimination
in
proceedings
of
the
2017
11th
joint
meeting
on
foundations
of
software
engineering
esec
fse
2017
pp
498
510
acm
2017
35
geyik
ambler
kenthapadi
fairness-aware
ranking
in
search
recommendation
systems
with
application
to
linkedin
talent
search
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
kdd
2019
pp
2221
2231
acm
2019
36
ghodsi
zaharia
hindman
konwinski
shenker
stoica
dominant
resource
fairness
fair
allocation
of
multiple
resource
types
in
proceedings
of
the
8th
usenix
symposium
on
networked
systems
design
and
implementation
nsdi
2011
usenix
association
2011
37
grgic-hlaca
redmiles
gummadi
weller
human
perceptions
of
fairness
in
algorithmic
decision
making
457
case
study
of
criminal
risk
prediction
in
proceedings
of
the
2018
world
wide
web
conference
on
world
wide
web
www
2018
pp
903
912
acm
2018
38
gupta
mehta
wang
dayal
fair
effective
efficient
and
differentiated
scheduling
in
an
enterprise
data
warehouse
in
edbt
2009
12th
international
conference
on
extending
database
technology
acm
international
conference
proceeding
series
vol
360
pp
696
707
acm
2009
39
hainmueller
entropy
balancing
for
causal
effects
multivariate
reweighting
method
to
produce
balanced
samples
in
observational
studies
polit
anal
20
25
46
2012
40
hardt
price
srebro
equality
of
opportunity
in
supervised
learning
in
nips
pp
3315
3323
2016
41
ho
imai
king
stuart
matchit
nonparametric
preprocessing
for
parametric
causal
inference
stat
softw
42
28
2011
42
hu
chen
short-term
intervention
for
long-term
fairness
in
the
labor
market
in
proceedings
of
the
2018
world
wide
web
conference
on
world
wide
web
www
2018
lyon
france
april
23
27
2018
pp
1389
1398
acm
2018
43
imana
korolova
heidemann
auditing
for
discrimination
in
algorithms
delivering
job
ads
in
proceedings
of
the
web
conference
2021
www
21
2021
44
ingold
soper
amazon
doesn
consider
the
race
of
its
customers
should
it
in
bloomberg
2016
45
ingold
soper
amazon
doesn
consider
the
race
of
its
customers
should
it
2016
46
jameson
smyth
recommendation
to
groups
in
the
adaptive
web
methods
and
strategies
of
web
personalization
lecture
notes
in
computer
science
vol
4321
pp
596
627
springer
2007
47
kamiran
calders
classifying
without
discriminating
in
2009
2nd
international
conference
on
computer
control
and
communication
pp
2009
48
kamiran
calders
data
preprocessing
techniques
for
classification
without
discrimination
knowl
inf
syst
33
33
2011
49
kamishima
akaho
asoh
sakuma
recommendation
independence
in
conference
on
fairness
accountability
and
transparency
fat
2018
proceedings
of
machine
learning
research
vol
81
pp
187
201
pmlr
2018
50
kay
matuszek
munson
unequal
representation
and
gender
stereotypes
in
image
search
results
for
occupations
in
proceedings
of
the
33rd
annual
acm
conference
on
human
factors
in
computing
systems
chi
2015
pp
3819
3828
acm
2015
51
kendall
new
measure
of
rank
correlation
biometrika
30
81
93
1938
52
kingma
welling
auto-encoding
variational
bayes
in
2nd
international
conference
on
learning
representations
iclr
2014
2014
53
kleinberg
mullainathan
raghavan
inherent
tradeoffs
in
the
fair
determination
of
risk
scores
in
8th
innovations
in
theoretical
computer
science
conference
itcs
2017
lipics
vol
67
pp
43
43
23
schloss
dagstuhl
leibniz-zentrum
für
informatik
2017
54
krasanakis
spyromitros-xioufis
papadopoulos
kompatsiaris
adaptive
sensitive
reweighting
to
mitigate
bias
in
fairness-aware
classification
pp
853
862
2018
55
kuhlman
rundensteiner
rank
aggregation
algorithms
for
fair
consensus
proc
vldb
endow
13
11
2706
2719
2020
56
kunjir
fain
munagala
babu
robus
fair
cache
allocation
for
data-parallel
workloads
in
proceedings
of
the
2017
acm
international
conference
on
management
of
data
sigmod
conference
2017
pp
219
234
acm
2017
57
kusner
loftus
russell
silva
counterfactual
fairness
in
advances
in
neural
information
processing
systems
123
458
30
annual
conference
on
neural
information
processing
systems
2017
pp
4066
4076
2017
58
liang
krishnan
hoffman
jebara
variational
autoencoders
for
collaborative
filtering
in
proceedings
of
the
2018
world
wide
web
conference
on
world
wide
web
www
2018
pp
689
698
acm
2018
59
lin
zhang
zhang
gu
liu
ma
fairnessaware
group
recommendation
with
pareto-efficiency
in
proceedings
of
the
eleventh
acm
conference
on
recommender
systems
recsys
2017
pp
107
115
acm
2017
60
liu
learning
to
rank
for
information
retrieval
springer
2011
61
lum
johndrow
statistical
framework
for
fair
predictive
algorithms
corr
abs
1610.08077
2016
http://arxiv.org/
abs
1610.08077
62
machado
stefanidis
fair
team
recommendations
for
multidisciplinary
projects
in
2019
ieee
wic
acm
international
conference
on
web
intelligence
wi
2019
thessaloniki
greece
october
14
17
2019
pp
293
297
acm
2019
63
martin
how
social
media
has
changed
how
we
consume
news
in
forbes
2018
https://www.forbes.com/sites/nicolemartin1/
2018
11
30
how-social-media-has-changed-how-we-consumenews
18ae4c093c3c
64
ntoutsi
stefanidis
nørvåg
kriegel
fast
group
recommendations
by
applying
user
clustering
in
conceptual
modeling
31st
international
conference
er
2012
proceedings
lecture
notes
in
computer
science
vol
7532
pp
126
140
springer
2012
65
olsson
huhtamäki
kärkkäinen
directions
for
professional
social
matching
systems
commun
acm
63
60
69
2020
66
oosterhuis
jagerman
de
rijke
unbiased
learning
to
rank
counterfactual
and
online
approaches
in
seghrouchni
sukthankar
liu
van
steen
eds
companion
of
the
2020
web
conference
2020
pp
299
300
acm
iw3c2
2020
67
du
pin
calmon
wei
vinzamuri
ramamurthy
varshney
optimized
pre-processing
for
discrimination
prevention
in
advances
in
neural
information
processing
systems
30
annual
conference
on
neural
information
processing
systems
2017
pp
3992
4001
2017
68
pitoura
social-minded
measures
of
data
quality
fairness
diversity
and
lack
of
bias
acm
data
inf
qual
12
12
12
2020
69
plane
redmiles
mazurek
tschantz
exploring
user
perceptions
of
discrimination
in
online
targeted
advertising
in
26th
usenix
security
symposium
usenix
security
2017
pp
935
951
usenix
association
2017
70
ramakrishnan
gehrke
database
management
systems
edn
mcgraw-hill
2003
71
rastegarpanah
gummadi
crovella
fighting
fire
with
fire
using
antidote
data
to
improve
polarization
and
fairness
of
recommender
systems
in
proceedings
of
the
twelfth
acm
international
conference
on
web
search
and
data
mining
wsdm
2019
pp
231
239
acm
2019
72
roy
amer-yahia
chawla
das
yu
space
efficiency
in
group
recommendation
vldb
19
877
900
2010
73
sacharidis
top-n
group
recommendations
with
fairness
in
proceedings
of
the
34th
acm
sigapp
symposium
on
applied
computing
sac
2019
pp
1663
1670
acm
2019
74
salimi
rodriguez
howe
suciu
interventional
fairness
causal
database
repair
for
algorithmic
fairness
in
proceedings
of
the
2019
international
conference
on
management
of
data
sigmod
conference
2019
pp
793
810
acm
2019
75
saxena
huang
de
filippis
radanovic
parkes
liu
how
do
fairness
definitions
fare
examining
public
123
pitoura
et
al
attitudes
towards
algorithmic
definitions
of
fairness
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
aies
2019
pp
99
106
acm
2019
76
serbos
qi
mamoulis
pitoura
tsaparas
fairness
in
package-to-group
recommendations
in
proceedings
of
the
26th
international
conference
on
world
wide
web
www
2017
pp
371
379
acm
2017
77
singh
joachims
fairness
of
exposure
in
rankings
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
kdd
2018
pp
2219
2228
acm
2018
78
steck
calibrated
recommendations
in
proceedings
of
the
12th
acm
conference
on
recommender
systems
recsys
2018
pp
154
162
acm
2018
79
stoyanovich
howe
abiteboul
miklau
sahuguet
weikum
fides
towards
platform
for
responsible
data
science
in
proceedings
of
the
29th
international
conference
on
scientific
and
statistical
database
management
ssdbm
2017
pp
26
26
acm
2017
80
stratigi
nummenmaa
pitoura
stefanidis
fair
sequential
group
recommendations
in
sac
20
the
35th
acm
sigapp
symposium
on
applied
computing
pp
1443
1452
acm
2020
81
sweeney
discrimination
in
online
ad
delivery
commun
acm
56
44
54
2013
82
tijdeman
the
chairman
assignment
problem
discret
math
32
323
330
1980
83
tsioutsiouliklis
pitoura
tsaparas
kleftakis
mamoulis
fairness-aware
pagerank
in
www
21
the
web
conference
2021
virtual
event
ljubljana
slovenia
april
19
23
2021
pp
3815
3826
acm
iw3c2
2021
84
verma
rubin
fairness
definitions
explained
in
proceedings
of
the
international
workshop
on
software
fairness
fairware
icse
2018
pp
acm
2018
85
weber
dwoskin
are
workplace
personality
tests
fair
wall
street
2014
86
xie
lakshmanan
wood
composite
recommendations
from
items
to
packages
front
comput
sci
264
277
2012
87
yang
stoyanovich
measuring
fairness
in
ranked
outputs
in
proceedings
of
the
29th
international
conference
on
scientific
and
statistical
database
management
ssdbm
2017
pp
22
22
acm
2017
88
yao
huang
beyond
parity
fairness
objectives
for
collaborative
filtering
in
advances
in
neural
information
processing
systems
30
annual
conference
on
neural
information
processing
systems
2017
pp
2921
2930
2017
89
zehlike
bonchi
castillo
hajian
megahed
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
in
proceedings
of
the
2017
acm
on
conference
on
information
and
knowledge
management
cikm
2017
pp
1569
1578
acm
2017
90
zehlike
diehn
castillo
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
in
the
web
conference
www
acm
2020
91
zehlike
yang
stoyanovich
fairness
in
ranking
survey
corr
abs
2103.14000
2021
92
zemel
wu
swersky
pitassi
dwork
learning
fair
representations
in
proceedings
of
the
30th
international
conference
on
machine
learning
icml
2013
jmlr
workshop
and
conference
proceedings
vol
28
pp
325
333
jmlr
org
2013
publisher
note
springer
nature
remains
neutral
with
regard
to
jurisdictional
claims
in
published
maps
and
institutional
affiliations