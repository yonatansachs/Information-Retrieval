ranking
individual
group
fairness
simultaneously
sruthi
gorantla
amit
deshpande
anand
louis
arxiv
2010
06986v1
cs
ir
24
sep
2020
indian
institute
science
bangalore
india
gorantlas
anandl
iisc
ac
microsoft
research
bangalore
india
amitdesh@microsoft.com
abstract
search
recommendation
systems
search
engines
recruiting
tools
online
marketplaces
news
social
media
output
ranked
lists
content
products
sometimes
people
credit
ratings
standardized
tests
risk
assessments
output
score
also
used
implicitly
ranking
bias
ranking
systems
especially
among
top
ranks
can
worsen
social
economic
inequalities
polarize
opinions
reinforce
stereotypes
hand
bias
correction
minority
groups
can
cause
harm
perceived
favoring
group-fair
outcomes
meritocracy
paper
study
trade-off
individual
fairness
group
fairness
ranking
define
individual
fairness
based
close
predicted
rank
item
true
rank
prove
lower
bound
trade-off
achievable
simultaneous
individual
group
fairness
ranking
give
fair
ranking
algorithm
takes
given
ranking
outputs
another
ranking
simultaneous
individual
group
fairness
guarantees
comparable
lower
bound
prove
algorithm
can
used
preprocess
training
data
well
post-process
output
existing
ranking
algorithms
experimental
results
show
algorithm
performs
better
state-of-the-art
fair
learning
rank
fair
post-processing
baselines
introduction
search
recommendation
systems
revolutionized
way
consume
overwhelming
amount
data
find
relevant
information
quickly
bp98
at05
help
us
find
relevant
documents
news
media
people
places
products
rank
based
interests
intent
klh16
pzz
19
examples
include
rankings
recommendations
online
marketplaces
recruitments
college
admissions
news
social
media
rankings
can
also
implicit
systems
screen
rate
people
products
places
well
social
economic
exchange
goods
money
information
downstream
application
uses
scores
ratings
ranking
credit
ratings
standardized
test
scores
health
risk
assessment
scores
common
examples
information
presented
ranked
lists
influences
worldview
par11
tav20
rankings
influence
users
consume
also
act
vehicles
opportunities
items
ranked
biased
ranking
news
people
products
raises
ethical
concerns
can
potentially
cause
long-term
economic
societal
harm
demographics
businesses
nob18
many
state-of-the-art
rankings
maximize
utility
relevance
reflect
existing
societal
biases
often
oblivious
societal
harm
may
cause
amplifying
biases
systems
amplify
societal
biases
observed
training
data
worsen
social
economic
inequalities
polarize
opinions
reinforce
stereotypes
n16
addition
ethical
concerns
also
legal
obligations
remove
bias
disparate
impact
laws
prohibit
even
unintentional
biased
outcomes
employment
housing
many
areas
one
group
people
belonging
protected
group
adversely
affected
compared
another
bs16
protected
groups
vary
specific
statutes
include
race
gender
age
religion
national
origin
etc
group-fairness
machine
learning
literature
focused
outcome-based
proportion-based
definitions
fairness
demographic
parity
equality
opportunity
classification
ranking
selection
problems
hps16
bhn19
another
notion
fairness
studied
fairness
literature
individual
fairness
achieving
individual
fairness
classification
often
means
similar
predictions
two
similar
individuals
two
similar
data
points
terms
features
risks
dhp
12
cdpf
17
group-fairness
desirable
goal
arbitrary
corrections
achieve
group-fairness
can
cause
harm
perceived
individually
unfair
cro04
classification
well
ranking
consider
individual
fairness
average
closely
tied
overall
accuracy
relevance
however
remove
aggregation
averaging
focus
parts
individual-fairness
really
matters
always
accuracy
relevance
recent
work
pointed
subtleties
group-fairness
individual-fairness
classification
ranking
bgw18
bin20
krw17
fairness
ranking
fairness
ranking
three
broad
requirements
sufficient
presence
items
belonging
different
groups
consistent
treatment
similar
individuals
individual
fairness
proper
representation
avoid
representational
harm
members
protected
groups
cas19a
first
third
requirements
group-fairness
whereas
second
requirement
individual-fairness
example
diversity
alone
top
ranks
satisfies
sufficient
presence
user
consumes
ranking
need
provide
consistent
treatment
proper
representation
way
items
ranked
fair
ranking
algorithms
can
divided
two
categories
first
re-ranking
algorithms
modify
given
ranking
high
utility
incorporate
fairness
constraints
trying
preserve
utility
second
learning-to-rank
algorithms
incorporate
fairness
utility
objectives
learning
ranker
training
data
re-ranking
can
used
post-process
prediction
given
ranker
well
pre-process
training
data
given
ranker
survey
previous
work
fair
ranking
distinction
mind
fair
ranking
can
framed
integer
optimization
problem
csv18
given
set
items
along
group
memberships
single
item
can
belong
multiple
groups
matrix
whose
entries
wij
indicate
utility
assigning
rank
item
objective
maximize
total
utility
rank
assignments
satisfying
given
group-fairness
constraints
top
positions
consider
group-fairness
constraints
lower
upper
bounds
group-wise
utilities
top
positions
allow
constraints
values
matrices
corresponding
practical
utility
metrics
discounted
cumulative
gain
dcg
greedy
assignment
highest
valued
item
available
rank
maximizes
total
utility
maximum
number
groups
item
belongs
fair
greedy
re-ranking
gives
approximation
group-fair
ranking
maximum
utility
csv18
fair
top-k
selection
problem
gives
another
formulation
fair
re-ranking
zbc
17
given
list
items
numerical
quality
value
assigned
item
objective
fair
top-k
selection
problem
select
items
maximize
utility
ensuring
minimum
proportion
protected
group
topl
ranks
authors
divide
utility
two
objectives
selection
ordering
selection
utility
quantifies
every
candidate
top-k
qualified
rest
ordering
utility
quantifies
every
pair
top-k
ranked
according
numerical
quality
values
give
efficient
algorithm
called
fa
ir
solve
fair
top-k
selection
problem
re-ranking
given
true
color-blind
ranking
orders
items
numerical
quality
values
fair
ranking
problem
can
also
defined
learning-to-rank
ltr
setting
model
trained
maxi2
mize
utility
subject
fairness
constraints
ltr
setting
ranking
probabilistic
fairness
guarantees
often
average
given
query-document
pair
probability
document
ranked
top-1
called
exposure
listnet
neural
network
model
trained
rank
list
documents
minimizing
loss
function
based
true
predicted
exposure
cql
07
building
upon
deltr
zc20
learns
fair
ranking
via
multi-objective
optimization
maximizes
utility
minimizes
disparate
exposure
different
groups
items
group-fairness
different
items
individual-fairness
general
learning-to-rank
framework
facilitates
optimizing
multiple
utility
metrics
satisfying
equal
exposure
fair-pg-ltr
sj19
learns
ranking
satisfies
fairness
exposure
experimental
results
fair
ltr
algorithms
give
better
fairness
utility
compared
post-processing
algorithms
real-world
datasets
yahoo
ltr
germancredit
data
related
work
defining
maximizing
various
group-fairness
metrics
overall
top-l
prefixes
top-k
ranks
ys17
given
using
optimization
algorithm
learn
fair
representations
zws
13
also
measures
group-fairness
ranking
based
pairwise
comparisons
ncgw20
bcd
19
recent
work
also
studied
fairness-aware
ranking
search
recommendations
real-world
recruitment
tools
using
fairness
metrics
based
skew
top-k
normalized
discounted
kl-divergence
ndkl
divergence
gak19
intersectional
fairness
items
belong
one
group
counterfactually
fair
ranking
measured
group
fairness
demographic
parity
top-k
equal
opportunity
top-k
ranking
utility
utility
loss
top-k
average
precision
top-k
studied
yls20
best
knowledge
existing
fair
ranking
algorithms
guarantee
group
fairness
can
provide
aggregate
guarantee
individual
fairness
study
trade-offs
group
fairness
utility
fair
rankings
provide
guarantees
worst-case
individual
fairness
work
address
gap
fair
ranking
literature
group-fairness
definition
ensures
sufficient
presence
groups
similar
previous
work
give
new
natural
definition
individual-fairness
main
contributions
can
summarized
follows
define
individual-fairness
based
worst-case
deviation
re-ranking
true
merit-based
color-blind
ranking
top-k
items
directly
captures
loss
visibility
suffered
items
high
merit
may
get
ranked
lower
order
achieve
high
group-fairness
prove
lower
bound
trade-off
achievable
individual-fairness
group-fairness
simultaneously
propose
fair
individual
group-fair
ranking
figr
algorithm
takes
given
merit-based
color-blind
ranking
outputs
another
ranking
simultaneous
individual
group-fairness
guarantees
comparable
lower
bound
mentioned
algorithm
can
used
pre-process
training
data
well
post-process
output
existing
ranking
algorithms
extensive
experiments
show
algorithm
performs
better
state-of-the-art
fair
ltr
fair
post-processing
baselines
standard
real-world
datasets
compas
recidivism
german
credit
risk
chilesat
used
fair
ranking
literature
individual
group
fair
rankings
rest
paper
say
rank
lower
rank
say
rank
higher
rank
now
formally
define
notion
group
fairness
definition
similar
notions
studied
literature
cas19a
csv18
definition
2.1
group
fairness
ranking
said
satisfy
group
fairness
consecutive
ranks
Î±k
items
group
notion
group
fairness
desirable
property
even
low
ranked
items
removed
ranking
remaining
ranking
still
satisfies
group
fairness
conditions
case
given
true
ranking
items
doesn
already
satisfy
group
fairness
conditions
re-ranking
algorithms
rearrange
items
true
ranking
group
fairness
conditions
satisfied
using
notion
individual
fairness
like
capture
much
item
displaced
true
rank
re-ranking
group
fairness
definition
2.2
individual
fairness
ranking
said
satisfy
individual
fairness
rank
item
times
true
rank
remark
unless
true
ranking
satisfies
group
fairness
conditions
items
high
merit
must
suffer
loss
visibility
process
re-ranking
group
fairness
output
group
fair
ranking
strictly
less
individual
fairness
manifests
trade-off
group
fairness
individual
fairness
ranking
also
note
true
ranking
always
available
real-world
datasets
experiments
use
natural
substitutes
true
ranking
see
section
details
closely
related
individual
fairness
well
studied
notion
precision
ranking
jk00
mrs08
zc20
given
ranking
precision
defined
number
items
top
ranks
true
ranking
also
appear
top
ranks
given
ranking
get
following
relation
individual
fairness
precision
corollary
2.3
ranking
satisfying
individual
fairness
also
precision
least
Î±k
proof
fix
ranking
individual
fairness
definition
top
Î±k
items
true
ranking
get
displaced
rank
Î±k
hence
least
top
Î±k
items
true
ranking
also
top
ranks
individually
fair
ranking
therefore
precision
least
Î±k
first
main
result
lower
bound
trade-off
achievable
simultaneous
individual
group
fairness
ranking
theorem
2.4
fix
every
n0
exists
n0
exists
true
ranking
2n
items
grouped
two
groups
items
following
holds
ranking
satisfying
individual
fairness
true
ranking
group
fairness
first
ranks
must
proof
let
set
integer
multiple
ak
n0
consider
true
ranking
items
group
placed
first
ranks
followed
items
group
now
consider
ranking
items
satisfying
individual
fairness
group
fairness
first
ranks
observe
choice
parameters
integer
definition
individual
fairness
get
first
ranks
must
contain
items
group
since
ranking
satisfies
group
fairness
consecutive
ranks
Î²k
items
group
implies
consecutive
ck
ranks
Î²ck
items
group
choice
parameters
Î±k
integer
let
algorithm
figr
algorithm
input
ranking
items
parameters
satisfying
conditions
theorem
2.5
set
Ç«k
min
Ç«k
Ç«k
move
item
rank
Ç«k
rank
Ç«k
end
end
Ç«k
Ç«k
Ç«k
rank
unoccupied
10
move
first
item
rank
higher
can
moved
rank
without
violating
Ç«k
group
fairness
constraints
among
items
ranks
Ç«k
Ç«k
element
available
11
end
12
end
13
end
14
15
rank
unoccupied
16
move
rank
first
item
rank
higher
17
end
18
end
19
output
final
ranking
Î±k
therefore
first
ck
ranks
contain
Î²ck
Î²n
items
group
first
ranks
contain
strictly
less
elements
group
contradiction
therefore
must
next
main
result
fair
ranking
algorithm
takes
given
ranking
outputs
another
ranking
individual
group
fairness
guarantees
comparable
theorem
2.4
theorem
2.5
given
true
ranking
items
grouped
disjoint
groups
group
least
items
fairness
parameters
exists
polynomial
time
algorithm
compute
ranking
satisfying
individual
fairness
k2
group
fairness
first
ranks
rest
section
let
k2
let
ith
block
ranks
refer
ranks
Ç«k
Ç«k
lemma
2.6
ranking
output
algorithm
satisfies
Ç«k
individual
fairness
proof
fix
item
true
rank
end
step
rank
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
subsequent
steps
increase
ranking
item
lemma
2.7
end
step
13
positions
first
Ç«k
ranks
will
empty
proof
consider
step
10
algorithm
rank
will
left
unoccupied
either
group
already
Ç«k
elements
block
containing
rank
ii
groups
less
Ç«k
elements
block
containing
rank
elements
ranks
higher
choice
parameters
Ç«k
1â
1â
Ç«k
Ç«k
1â
Ç«k
Ç«k
Ç«k
Ç«k
therefore
group
Ç«k
elements
block
containing
rank
every
rank
block
occupied
therefore
case
can
happen
block
first
Ç«k
elements
intermediate
ranking
including
final
ranking
contain
Ç«k
items
group
since
least
elements
group
long
satisfies
Ç«k
will
least
one
element
available
group
move
empty
spot
first
blocks
without
violating
Ç«k
group
fairness
constraints
first
blocks
thus
first
blocks
will
filled
end
step
13
therefore
number
ranks
filled
least
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
Ç«k
therefore
case
ii
will
happen
first
Ç«k
ranks
thus
end
step
13
positions
first
Ç«k
ranks
will
empty
lemma
2.8
end
step
13
block
Ç«k
items
particular
group
proof
block
observe
end
step
block
size
Ç«k
Ç«k
nonempty
positions
therefore
Ç«k
items
particular
group
step
10
ensures
algorithm
terminates
block
Ç«k
items
particular
group
lemma
2.9
ranking
output
algorithm
satisfies
group
fairness
first
Ç«k
ranks
proof
lemma
2.7
shows
none
first
Ç«k
ranks
will
empty
end
step
13
therefore
ranks
will
remain
unchanged
steps
step
13
def
def
consider
consecutive
ranks
let
i1
Ç«k
i2
Ç«k
construction
blocks
i1
i2
fully
contained
ranks
number
items
group
ranks
number
items
group
blocks
i1
i2
using
lemma
2.8
get
Î±k
Ç«k
note
bound
also
holds
cases
i2
i1
i2
i1
proof
theorem
2.5
follows
choice
lemma
2.6
lemma
2.7
lemma
2.9
also
obtain
slightly
stronger
guarantees
need
group
fairness
blocks
size
instead
group
fairness
guarantees
consecutive
ranks
theorem
2.10
given
true
ranking
items
grouped
disjoint
groups
group
least
items
fairness
parameters
exists
polynomial
time
algorithm
compute
ranking
satisfying
k1
individual
fairness
group
fairness
first
Î±k
blocks
size
proof
use
algorithm
now
ith
block
size
Ç«k
fix
item
true
ranking
proof
lemma
2.6
shows
final
rank
will
Ç«k
k1
equality
follows
choice
hence
ranking
output
algorithm
satisfies
k1
individual
fairness
lemma
2.8
shows
end
step
13
block
Î±k
items
particular
group
Î±â
Î±k
Î±k
1â
therefore
block
contains
Î±k
items
groups
can
empty
ranks
consequently
long
Î±k
items
fom
group
available
blocks
will
contain
empty
ranks
since
least
items
group
rank
first
blocks
will
empty
satisfies
Î±k
hence
blocks
size
contain
Î±k
items
group
first
Î±k
Î±k
blocks
size
satisfy
group
fairness
experimental
validation
section
study
trade-off
individual
group
fairness
achieved
figr
various
realworld
datasets
also
compare
results
fair
post-processing
fair
ltr
baselines
3.1
datasets
consider
two
types
datasets
experiments
first
type
global
ranking
entire
dataset
calculated
using
subset
attributes
bias
one
attributes
might
introduce
bias
ranking
output
hence
fair
post-processing
algorithm
can
used
correct
biases
global
ranking
however
listwise
ltr
model
listnet
cql
07
requires
training
sample
consists
list
items
ranking
available
hence
experiments
based
ltr
model
performed
second
type
datasets
datasets
query-document
format
support
training
listwise
ltr
model
german
credit
risk
dataset
dataset
credit
risk
scoring
adult
german
residents
dg17
consists
set
1000
candidates
various
demographics
applying
loan
features
candidate
include
demographic
information
personal
status
gender
age
etc
well
financial
status
credit
history
property
housing
job
etc
schufa
scores
individuals
used
get
global
ranking
dataset
similar
zbc
17
ys17
cas19b
observed
schufa
scoring
biased
young
adults
hence
divide
dataset
protected
non-protected
groups
based
age
consider
two
cases
age
25
protected
group
ii
age
35
protected
group
similar
zbc
17
compas
recidivism
dataset
dataset
consists
violent
recidivism
assessment
nearly
7000
criminal
defendants
correctional
offender
management
profiling
alternative
sanctions
compas
tool
based
answers
questionnaire
consisting
137
questions
dataset
curated
almk16
analyze
biases
tool
study
points
racial
well
gender
biases
predictions
compas
experiments
consider
ranking
based
recidivism
score
individual
highest
recidivism
ranked
top-1
protected
groups
gender
female
ii
race
african
american
similar
zbc
17
chilesat
dataset
dataset
consists
chilean
university
admission
test
scores
students
admitted
highschool
grades
score
based
academic
performance
one
year
university
given
test
scores
highschool
grades
ltr
model
predict
advance
ranking
students
performance
end
first
academic
year
dataset
query-document
format
since
academic
year
can
considered
query
students
academic
year
list
candidates
ranked
around
450
600
student
records
academic
years
dataset
experiments
perform
fold
cross
validation
four
academic
years
training
one
validating
non-binary
genders
annotated
datasets
used
paper
ltr
models
consider
two
protected
groups
dataset
gender
female
high-school
type
public
study
behaviour
figr
two
different
types
bias
exist
biases
also
studied
zc20
experiments
use
processed
subsets
chilesat
engineering
students
german
credit
risk
compas
recidivism
datasets3
3.2
experiments
compare
results
baselines
listnet
cql
07
listwise
ltr
model
ranks
list
documents
based
relevance
scores
respect
query
ii
deltr
zc20
in-processing
ltr
model
trained
listnet
objective
along
group
fairness
constraint
fairness
exposure
iii
fa
ir
zbc
17
post-processing
algorithm
re-ranks
given
ranking
maintain
significant
proportions
protected
group
every
prefix
ranking
brevity
omit
detailing
intricacies
baseline
algorithms
following
brief
explanation
experimental
setup4
color-blind
ltr
listnet
trained
colorblind
fashion
access
protected
sensitive
attributes
color-aware
ltr
listnet
trained
attributes
deltr
trained
100k
refer
zc20
details
fa
ir
pre
fa
ir
parameter
used
pre-process
training
data
listnet
trained
data
fa
ir
post
listnet
predictions
test
data
post-processed
using
fa
ir
parameter
fa
ir
fa
ir
parameter
used
re-rank
global
ranking
compas
recidivism
german
credit
risk
datasets
figr
pre
figr
parameter
max
1â
used
pre-process
training
data
add
small
number
since
figr
requires
strictly
greater
listnet
trained
data
figr
post
listnet
predictions
test
data
post-processed
using
figr
parameter
max
1â
figr
figr
parameter
max
1â
used
re-rank
global
ranking
compas
recidivism
german
credit
risk
datasets
training
details
fa
ir
parameter
chosen
proportion
protected
group
0.1
0.1
parameter
setting
adopted
zc20
experiments
number
groups
protected
non-protected
hence
use
0.01
figr
values
parameter
figr
will
max
0.51
datasets
experiments
set
value
100
reasonable
choice
since
number
ranked
items
least
500
datasets
ltr
models
listnet
deltr
use
parameter
hyper-parameter
settings
zc20
report
average
results
across
folds
chilesat
dataset
account
variability
learning
parameters
ltr
run
ltr
based
expreiments
times
report
final
average5
https://github.com/milkalichtblau/deltr-experiments/tree/master/data/engineeringstudents
https://github.com/dataresponsibly/fairrank/tree/master/datasets
code
figr
baselines
available
https://github.com/sruthigorantla/figr
runs
showed
minor
variations
learned
ltr
parameters
almost
ranking
predictions
protected
group
age
35
proportion
protected
group
proportion
protected
group
protected
group
age
25
0.6
0.5
0.4
0.3
0.2
0.1
0.6
0.5
0.4
0.3
0.2
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
0.1
figr
fa
ir
100
200
300
400
500
600
700
800
900
1000
figr
fa
ir
fa
ir
true
1.0
min
min
1.0
min
min
top-k
figr
0.8
0.8
0.6
0.6
item
binsofofsize
size100
100
blocks
10
blocks
size
100
10
figure
trade-offs
individual
group
fairness
german
credit
risk
dataset
experiments
figr
parameter
settings
0.01
100
max
0.51
reading
plots
every
combination
dataset
protected
group
show
pair
plots
understand
better
trade-off
group
individual
fairness
real-world
datasets
example
figure
figure
show
group
individual
fairness
respectively
german
credit
risk
dataset
age
25
protected
group
figure
x-axis
represents
rank
y-axis
shows
proportion
protected
group
items
top-k
ranks
plot
line
shows
proportion
protected
group
entire
training
data
whereas
line
true
represents
proportion
protected
group
top-k
ranks
true
ranking
two
lines
serve
guidelines
understand
behavior
various
algorithms
pick
best
performing
algorithm
figure
x-axis
represents
blocks
size
100
ith
block
consists
items
100
100i
true
ranking
item
block
Î±j
pred
pred
rank
item
output
ranking
y-axis
shows
minimum
Î±j
among
items
block
call
min
higher
value
min
higher
individual
fairness
according
definition
2.2
since
measure
individual
fairness
respect
true
ranking
line
true
individual
fairness
say
trade-off
group
individual
fairness
algorithms
placing
higher
proportions
protected
groups
top-k
ranks
consistently
suffer
lesser
individual
fairness
ranks
following
section
study
trade-offs
achieved
figr
baselines
real-world
datasets
10
protected
group
african
american
proportion
protected
group
proportion
protected
group
protected
group
female
0.6
0.5
0.4
0.3
0.2
0.1
0.6
0.5
0.4
0.3
0.2
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
figr
fa
ir
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
figr
figr
fa
ir
fa
ir
1.0
min
min
1.0
min
min
true
0.8
0.8
0.6
0.6
item
binsofofsize
size
100
100
blocks
10
item
binsofofsize
size
100
100
blocks
10
figure
trade-offs
individual
group
fairness
compas
recidivism
dataset
experiments
figr
parameter
settings
0.01
100
max
0.51
show
reults
top
1000
ranks
3.3
figr
vs
fair
post-processing
methods
german
credit
risk
compas
datasets
figures
show
experimental
results
german
credit
risk
dataset
protected
group
age
25
younger
adults
significantly
underrepresented
dataset
0.15
see
figure
moreover
true
proportions
given
schufa
score
based
ranking
top-400
ranks
even
less
indicates
bias
younger
adults
ranking
three
variants
figr
allocate
almost
1.5
times
true
proportion
increase
representation
younger
adults
top-400
ranks
figr
also
achieves
approximately
70individual
fairness
reasonable
trade-off
group
fairness
case
fa
ir
hand
fails
correct
biases
even
fails
achieve
significant
improvement
representation
younger
adults
case
protected
group
age
35
young
adults
although
representation
entire
dataset
almost
11
1.0
0.45
0.8
min
min
proportion
protected
group
pre-processing
vs
deltr
vs
ltr
0.50
0.40
0.6
0.35
0.4
0.30
0.2
100
0.45
0.45
0.40
200
300
400
500
color-blind
ltr
color-aware
ltr
deltr
top-k
figr
pre
figr
pre
figr
pre
blocks
size
100
0.50
fa
ir
pre
fa
ir
pre
0.45
fa
ir
pre
true
figure
trade-offs
individual
group
fairness
chilesat
dataset
public
school
protected
group
comparision
pre-processig
in-processing
standard
ltr
methods
experiments
figr
parameter
settings
0.01
100
max
0.51
see
table
appendix
tabulated
results
half
0.55
ranked
tail
true
ranking
shows
strong
bias
young
adults
see
figure
fa
ir
stays
close
true
proportions
hand
slightly
larger
values
fa
ir
clearly
overcompensate
lack
representation
protected
group
cost
low
individual
fairness
0.50
0.65
also
reduces
proportion
non-protected
group
much
lower
true
proportion
leading
inversion
bias
figr
design
avoids
problem
values
higher
set
1â
small
value
0.01
experiments
hence
representation
protected
non-protected
groups
bounded
see
theorem
2.5
also
evident
results
figr
achieves
results
cases
set
0.51
figr
parameter
settings
finds
best
trade-off
group
fairness
45
top
100
positions
individual
fairness
0.75
top
100
positions
figures
show
experimental
results
compas
recidivism
dataset
females
dataset
0.19
face
bias
top-200
ranks
due
biases
introduced
compas
tool
see
figure
fa
ir
stays
close
true
proportions
improves
female
representation
top-400
ranks
figr
also
shows
trends
fa
ir
figr
trade
individual
fairness
group
fairness
nevertheless
doesn
cause
inversion
bias
output
ranking
protected
group
race
african
american
substantially
underrepresented
dataset
top-500
positions
even
though
representation
entire
data
high
0.51
see
figure
shows
bias
protected
group
true
ranking
based
recidivism
score
since
case
bias
similar
bias
towards
young
adults
german
credit
risk
dataset
see
similar
results
figr
improves
representation
top-500
ranks
achieving
reasonable
individual
fairness
fa
ir
close
competitor
figr
best
choice
algorithm
however
fa
ir
overcompensate
lack
representation
african-americans
top-1000
positions
since
actually
majority
group
data
12
1.0
0.45
0.8
min
min
proportion
protected
group
post-processing
vs
deltr
vs
ltr
0.50
0.40
0.6
0.4
0.35
0.2
0.30
100
200
300
400
top-k
0.45
0.45
color-blind
ltr
color-aware
ltr
deltr
0.40
500
figrpost
pre
figr
figr
figrpost
pre
figr
figrpost
pre
blocks
size
100
0.50
fa
ir
pre
postp
fa
ir
fa
ir
post
pre
pp
0.45
fa
ir
post
pre
pp
true
figure
trade-offs
individual
group
fairness
chilesat
dataset
public
school
protected
group
comparision
post-processig
in-processing
standard
ltr
methods
experiments
figr
parameter
settings
0.01
100
max
0.51
see
table
appendix
tabulated
results
3.4
figr
vs
fair
ltr
methods
chilesat
dataset
figure
figure
show
group
individual
fairness
trade-offs
achieved
ltr
models
deltr
pre-processing
training
data
figr
fa
ir
whereas
figure
figure
show
results
ltr
deltr
models
compared
post-processing
methods
applied
listnet
predictions
representation
students
public
school
top-300
ranks
higher
total
representation
0.34
see
figure
means
students
public
schools
test
scores
private
school
students
indeed
caliber
perform
better
university
hence
evident
test
scores
biased
students
public
school
biases
naturally
adjusted
color-aware
ltr
whereas
color-blind
ltr
re-inforces
biases
see
figure
deltr
adjust
ranks
interestingly
pre-processing
three
variants
figr
fa
ir
rank
higher
proportions
public
school
students
top-200
ranks
time
achieve
high
individual
fairness
compared
ltr
models
see
figure
although
color-aware
ltr
corrects
biases
impact
limited
deltr
hand
limits
achieving
higher
group
fairness
well
individual
fairness
since
fairness
exposure
already
satisfied
case
post-processing
listnet
predictions
figr
achieves
similar
results
see
figure
still
much
individual
fairness
others
female
students
substantially
under-represented
dataset
0.20
color-blind
ltr
ranks
almost
proportion
females
top-k
ranks
true
proportions
see
figure
means
academic
performance
females
first
year
university
bad
case
color-aware
ltr
will
learn
discriminate
females
case
placing
females
top
ranks
hurt
utility
expected
color-aware
ltr
stays
color-blind
ltr
deltr
acts
difference
exposure
groups
hence
places
number
females
top-200
ranks
purely
achieve
fairness
exposure
deltr
oblivious
impact
individual
fairness
although
13
0.35
1.0
0.30
0.8
min
min
proportion
protected
group
pre-processing
vs
deltr
vs
ltr
0.25
0.20
0.6
0.4
0.15
0.2
0.10
100
200
300
400
top-k
0.45
0.45
color-blind
ltr
color-aware
ltr
deltr
0.40
500
figr
pre
figr
pre
figr
pre
blocks
size
100
0.50
fa
ir
pre
fa
ir
pre
0.45
fa
ir
pre
true
figure
trade-offs
individual
group
fairness
chilesat
dataset
female
protected
group
comparision
pre-processig
in-processing
standard
ltr
methods
experiments
figr
parameter
settings
0.01
100
max
0.51
see
table
appendix
tabulated
results
pre-processing
figr
achieves
less
individual
fairness
deltr
post-processing
figr
places
highest
number
females
top-200
achieving
much
individual
fairness
deltr
see
figure
cases
figr
parameter
settings
achieves
best
group
fairness
well
individual
fairness
even
cases
loses
individual
fairness
compared
baselines
trade-off
minimal
contrast
fair
ltr
fair
post-processing
baselines
achieve
trade-offs
conclusion
fair
ranking
crucial
search
recommendations
matter
global
concern
quest
towards
responsible
ai
studied
group
individual
fairness
notions
ranking
defined
individual
fairness
based
close
predicted
rank
item
true
rank
proved
lower
bound
trade-off
achievable
simultaneous
individual
group
fairness
ranking
works
csv18
etc
studied
aggregate
forms
individual
fairness
best
knowledge
work
first
give
provable
guarantees
worst-case
displacement
item
output
ranking
respect
true
rank
presented
first
best
knowledge
algorithm
takes
given
ranking
outputs
another
ranking
simultaneous
individual
group
fairness
guarantees
comparable
lower
bound
proved
algorithm
performed
better
state-of-the-art
fair
learning
rank
fair
post-processing
baselines
one
limitation
work
re-ranking
algorithms
requires
true
ranking
input
theoretical
guarantees
respect
true
ranking
practice
true
merit-based
ranking
may
debatable
unavailable
due
incomplete
data
unobserved
features
legal
ethical
considerations
behind
downstream
application
rankings
etc
14
0.35
1.0
0.30
0.8
min
min
proportion
protected
group
post-processing
vs
deltr
vs
ltr
0.25
0.20
0.6
0.4
0.15
0.2
0.10
100
200
300
400
500
0.45
0.45
0.40
color-blind
ltr
color-aware
ltr
deltr
top-k
figrpost
pre
figr
figr
figrpost
pre
figr
figrpost
pre
blocks
size
100
0.50
fa
ir
pre
postp
fa
ir
fa
ir
post
pre
pp
0.45
fa
ir
post
pre
pp
true
figure
trade-offs
individual
group
fairness
chilesat
dataset
female
protected
group
comparision
post-processig
in-processing
standard
ltr
methods
experiments
figr
parameter
settings
0.01
100
max
0.51
see
table
appendix
tabulated
results
acknowledgements
al
supported
part
serb
award
ecr
2017
003296
pratiksha
trust
young
investigator
award
al
also
grateful
microsoft
research
supporting
collaboration
references
almk16
julia
angwin
jeff
larson
surya
mattu
lauren
kirchner
machine
bias
2016
at05
adomavicius
tuzhilin
toward
next
generation
recommender
systems
survey
state-of-the-art
possible
extensions
ieee
transactions
knowledge
data
engineering
17
734
749
2005
bcd
19
alex
beutel
chen
doshi
qian
wei
wu
heldt
zhe
zhao
hong
ed
huai
hsin
chi
cristos
goodrow
fairness
recommendation
ranking
pairwise
comparisons
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
2019
bgw18
asia
biega
krishna
gummadi
gerhard
weikum
equity
attention
amortizing
individual
fairness
rankings
41st
international
acm
sigir
conference
research
development
information
retrieval
sigir
18
page
405414
new
york
ny
usa
2018
association
computing
machinery
bhn19
solon
barocas
moritz
hardt
arvind
narayanan
fairness
machine
learning
fairmlbook
org
2019
http://www.fairmlbook.org.
15
bin20
reuben
binns
apparent
conflict
individual
group
fairness
fat
20
conference
fairness
accountability
transparency
barcelona
spain
january
27
30
2020
pages
514
524
acm
2020
bp98
sergey
brin
lawrence
page
anatomy
large-scale
hypertextual
web
search
engine
proceedings
seventh
international
conference
world
wide
web
www7
page
107117
nld
1998
elsevier
science
publishers
bs16
solon
barocas
andrew
selbst
104
671
732
2016
cas19a
carlos
castillo
fairness
transparency
ranking
sigir
forum
52
6471
january
2019
cas19b
carlos
castillo
fairness
transparency
ranking
volume
52
page
6471
new
york
ny
usa
january
2019
association
computing
machinery
big
data
disparate
impact
california
law
review
cdpf
17
sam
corbett-davies
emma
pierson
avi
feller
sharad
goel
aziz
huq
algorithmic
decision
making
cost
fairness
proceedings
23rd
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
17
page
797806
new
york
ny
usa
2017
association
computing
machinery
cql
07
zhe
cao
tao
qin
tie-yan
liu
ming-feng
tsai
hang
li
learning
rank
pairwise
approach
listwise
approach
proceedings
24th
international
conference
machine
learning
icml
07
page
129136
new
york
ny
usa
2007
association
computing
machinery
cro04
crosby
affirmative
action
dead
long
live
affirmative
action
current
perspectives
psychology
yale
university
press
2004
csv18
elisa
celis
damian
straszak
nisheeth
vishnoi
ranking
fairness
constraints
ioannis
chatzigiannakis
christos
kaklamanis
da
niel
marx
donald
sannella
editors
45th
international
colloquium
automata
languages
programming
icalp
2018
july
13
2018
prague
czech
republic
volume
107
lipics
pages
28
28
15
schloss
dagstuhl
leibnizzentrum
fu
informatik
2018
dg17
dheeru
dua
casey
graff
uci
machine
learning
repository
2017
dhp
12
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
richard
zemel
fairness
awareness
proceedings
3rd
innovations
theoretical
computer
science
conference
itcs
12
page
214226
new
york
ny
usa
2012
association
computing
machinery
gak19
sahin
cem
geyik
stuart
ambler
krishnaram
kenthapadi
fairness-aware
ranking
search
recommendation
systems
application
linkedin
talent
search
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
19
page
22212231
new
york
ny
usa
2019
association
computing
machinery
hps16
hardt
price
nathan
srebro
equality
opportunity
supervised
learning
nips
2016
jk00
kalervo
ja
rvelin
jaana
keka
la
inen
ir
evaluation
methods
retrieving
highly
relevant
documents
proceedings
23rd
annual
international
acm
sigir
conference
research
development
information
retrieval
sigir
00
page
4148
new
york
ny
usa
2000
association
computing
machinery
16
klh16
christoph
kofler
martha
larson
alan
hanjalic
user
intent
multimedia
search
survey
state
art
future
challenges
acm
comput
surv
49
august
2016
krw17
michael
kearns
aaron
roth
zhiwei
steven
wu
meritocratic
fairness
cross-population
selection
volume
70
proceedings
machine
learning
research
pages
1828
1836
international
convention
centre
sydney
australia
06
11
aug
2017
pmlr
mrs08
christopher
manning
prabhakar
raghavan
hinrich
schu
tze
introduction
information
retrieval
cambridge
university
press
usa
2008
ncgw20
harikrishna
narasimhan
andy
cotter
maya
gupta
serena
lutong
wang
pairwise
fairness
ranking
regression
33rd
aaai
conference
artificial
intelligence
2020
nob18
safiya
umoja
noble
algorithms
oppression
search
engines
reinforce
racism
nyu
press
2018
n16
cathy
neil
weapons
math
destruction
big
data
increases
inequality
threatens
democracy
crown
publishing
group
usa
2016
par11
eli
pariser
filter
bubble
internet
hiding
penguin
group
2011
pzz
19
changhua
pei
yi
zhang
yongfeng
zhang
fei
sun
xiao
lin
hanxiao
sun
jian
wu
peng
jiang
junfeng
ge
wenwu
ou
dan
pei
personalized
re-ranking
recommendation
proceedings
13th
acm
conference
recommender
systems
recsys
19
page
311
new
york
ny
usa
2019
association
computing
machinery
sj19
ashudeep
singh
thorsten
joachims
policy
learning
fairness
ranking
neurips
2019
tav20
herman
tavani
search
engines
ethics
edward
zalta
editor
stanford
encyclopedia
philosophy
metaphysics
research
lab
stanford
university
fall
2020
edition
2020
yls20
ke
yang
joshua
loftus
julia
stoyanovich
causal
intersectionality
fair
ranking
arxiv
abs
2006.08688
2020
ys17
ke
yang
julia
stoyanovich
measuring
fairness
ranked
outputs
proceedings
29th
international
conference
scientific
statistical
database
management
ssdbm
17
new
york
ny
usa
2017
association
computing
machinery
zbc
17
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
ricardo
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
proceedings
2017
acm
conference
information
knowledge
management
cikm
17
page
15691578
new
york
ny
usa
2017
association
computing
machinery
zc20
meike
zehlike
carlos
castillo
reducing
disparate
exposure
ranking
learning
rank
approach
proceedings
web
conference
2020
2020
zws
13
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
cynthia
dwork
learning
fair
representations
sanjoy
dasgupta
david
mcallester
editors
proceedings
30th
international
conference
machine
learning
volume
28
proceedings
machine
learning
research
pages
325
333
atlanta
georgia
usa
17
19
jun
2013
pmlr
appendix
experimental
results
17
top-k
ranks
method
true
color-blind
ltr
color-aware
ltr
deltr
fa
ir
pre
fa
ir
pre
fa
ir
pre
figr
pre
figr
pre
figr
pre
fa
ir
post
fa
ir
post
fa
ir
post
figr
post
figr
post
figr
post
100
grp
ind
0.42
0.32
0.17
0.38
0.18
0.41
0.19
0.39
0.18
0.45
0.2
0.39
0.18
0.45
0.2
0.49
0.21
0.43
0.19
0.38
0.18
0.4
0.17
0.38
0.18
0.41
0.19
0.45
0.18
0.41
0.18
200
grp
ind
0.38
0.32
0.46
0.38
0.4
0.41
0.42
0.39
0.4
0.45
0.54
0.39
0.4
0.45
0.54
0.49
0.52
0.44
0.54
0.38
0.4
0.41
0.46
0.38
0.4
0.41
0.47
0.44
0.46
0.41
0.47
300
grp
ind
0.36
0.32
0.63
0.37
0.72
0.39
0.76
0.38
0.64
0.43
0.65
0.38
0.61
0.43
0.63
0.45
0.65
0.41
0.62
0.37
0.72
0.41
0.7
0.37
0.72
0.41
0.67
0.45
0.71
0.4
0.65
400
grp
ind
0.35
0.33
0.88
0.36
0.87
0.36
0.77
0.36
0.87
0.38
0.82
0.36
0.87
0.38
0.82
0.39
0.82
0.37
0.82
0.35
0.87
0.41
0.84
0.35
0.87
0.39
0.83
0.41
0.87
0.39
0.83
500
grp
ind
0.31
0.3
0.31
0.31
0.31
0.32
0.31
0.32
0.32
0.32
0.31
0.34
0.31
0.33
0.34
0.33
table
individual
group
fairness
results
chilesat
dataset
public
school
protected
group
plots
results
shown
figure
figure
column
grp
shows
proportion
protected
group
items
top-k
ranks
column
ind
shows
min
ranks
99
note
99
100
th
block
size
100
shown
plots
18
top-k
ranks
method
true
color-blind
ltr
color-aware
ltr
deltr
fa
ir
pre
fa
ir
pre
fa
ir
pre
figr
pre
figr
pre
figr
pre
fa
ir
post
fa
ir
post
fa
ir
post
figr
post
figr
post
figr
post
100
grp
ind
0.1
0.13
0.17
0.1
0.17
0.18
0.16
0.12
0.16
0.23
0.14
0.11
0.17
0.27
0.14
0.35
0.13
0.27
0.14
0.15
0.16
0.25
0.15
0.11
0.17
0.3
0.13
0.35
0.13
0.31
0.13
200
grp
ind
0.14
0.14
0.46
0.12
0.46
0.22
0.49
0.14
0.47
0.27
0.39
0.12
0.45
0.3
0.38
0.35
0.35
0.31
0.36
0.17
0.42
0.26
0.45
0.12
0.46
0.3
0.46
0.35
0.44
0.31
0.52
300
grp
ind
0.17
0.17
0.63
0.15
0.59
0.22
0.6
0.17
0.72
0.26
0.71
0.15
0.62
0.27
0.62
0.3
0.63
0.29
0.65
0.17
0.75
0.27
0.6
0.15
0.6
0.27
0.61
0.3
0.6
0.28
0.6
400
grp
ind
0.19
0.18
0.88
0.16
0.86
0.21
0.85
0.18
0.9
0.23
0.96
0.17
0.88
0.23
0.95
0.24
0.92
0.23
0.91
0.18
0.92
0.24
0.88
0.16
0.86
0.24
0.87
0.24
0.89
0.22
0.85
500
grp
ind
0.17
0.17
0.16
0.18
0.16
0.19
0.16
0.19
0.19
0.19
0.16
0.2
0.16
0.2
0.2
0.18
table
individual
group
fairness
results
chilesat
dataset
female
protected
group
plots
results
shown
figure
figure
column
grp
shows
proportion
protected
group
items
top-k
ranks
column
ind
shows
min
ranks
99
note
th
block
size
100
shown
plots
99
100
19