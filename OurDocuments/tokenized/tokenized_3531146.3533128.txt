subverting
fair
image
search
with
generative
adversarial
perturbations
avijit
ghosh
matthew
jagielski
christo
wilson
ghosh.a@northeastern.edu
northeastern
university
boston
ma
usa
jagielski@google.com
google
research
mountain
view
ca
usa
cbw@ccs.neu.edu
northeastern
university
boston
ma
usa
abstract
in
this
work
we
explore
the
intersection
fairness
and
robustness
in
the
context
of
ranking
when
ranking
model
has
been
calibrated
to
achieve
some
definition
of
fairness
is
it
possible
for
an
external
adversary
to
make
the
ranking
model
behave
unfairly
without
having
access
to
the
model
or
training
data
to
investigate
this
question
we
present
case
study
in
which
we
develop
and
then
attack
stateof-the-art
fairness-aware
image
search
engine
using
images
that
have
been
maliciously
modified
using
generative
adversarial
perturbation
gap
model
75
these
perturbations
attempt
to
cause
the
fair
re-ranking
algorithm
to
unfairly
boost
the
rank
of
images
containing
people
from
an
adversary-selected
subpopulation
we
present
results
from
extensive
experiments
demonstrating
that
our
attacks
can
successfully
confer
significant
unfair
advantage
to
people
from
the
majority
class
relative
to
fairly-ranked
baseline
search
results
we
demonstrate
that
our
attacks
are
robust
across
number
of
variables
that
they
have
close
to
zero
impact
on
the
relevance
of
search
results
and
that
they
succeed
under
strict
threat
model
our
findings
highlight
the
danger
of
deploying
fair
machine
learning
algorithms
in-the-wild
when
the
data
necessary
to
achieve
fairness
may
be
adversarially
manipulated
and
the
models
themselves
are
not
robust
against
attacks
the
machine
learning
ml
community
has
awoken
to
concerns
about
the
fairness
of
ml
models
the
elimination
of
unjustified
bias
against
specific
groups
of
people
from
models
there
is
now
extensive
literature
documenting
unfairness
in
deployed
ml
systems
21
as
well
as
techniques
for
training
fair
classification
42
47
50
65
and
ranking
28
86
98
models
companies
are
adopting
and
deploying
fair
ml
systems
in
many
real-world
contexts
10
96
most
ml
systems
that
strive
to
achieve
demographic
fairness
are
dependent
on
high-quality
demographic
data
to
control
for
unjustified
biases
13
94
recent
work
has
highlighted
how
critical
this
dependency
is
by
showing
how
unintentional
errors
in
demographic
data
can
dramatically
undermine
the
objectives
of
fair
ranking
algorithms
39
another
serious
concern
in
the
ml
community
is
model
robustness
especially
in
the
face
of
clever
and
dedicated
adversaries
the
field
of
adversarial
ml
has
demonstrated
that
seemingly
accurate
models
are
brittle
when
presented
with
maliciously
crafted
inputs
24
89
and
that
these
attacks
impact
models
across
variety
of
contexts
12
31
46
89
the
existence
of
adversarial
ml
challenges
the
use
of
models
in
real-world
deployments
in
this
work
we
explore
the
intersection
of
these
two
concerns
fairness
and
robustness
in
the
context
of
ranking
when
ranking
model
has
been
carefully
calibrated
to
achieve
some
definition
of
fairness
is
it
possible
for
an
external
adversary
to
make
the
ranking
model
behave
unfairly
without
having
access
to
the
model
or
training
data
in
other
words
can
attackers
intentionally
weaponize
demographic
markers
in
data
to
subvert
fairness
guarantees
to
investigate
this
question
we
present
case
study
in
which
we
develop
and
then
attack
fairness-aware
image
search
engine
using
images
that
have
been
maliciously
modified
with
adversarial
perturbations
we
chose
this
case
study
because
image
retrieval
based
on
text
queries
is
popular
real-world
use
case
for
neural
models
google
image
search
istock
getty
images
etc
and
because
prior
work
has
shown
that
these
models
can
potentially
be
fooled
using
adversarial
perturbations
101
although
not
in
the
context
of
fairness
to
strengthen
our
case
study
we
adopt
strict
threat
model
under
which
the
adversary
cannot
poison
training
data
48
for
the
ranking
model
and
has
no
knowledge
of
the
ranking
model
or
fairness
algorithm
used
by
the
victim
search
engine
instead
the
adversary
can
only
add
images
into
the
victim
database
after
the
image
retrieval
model
is
trained
for
our
experiments
we
develop
an
image
search
engine
that
uses
state-of-the-art
multimodal
transformer
mmt
37
retrieval
model
and
fair
re-ranking
algorithm
fmmr
51
that
aims
to
achieve
demographic
group
fairness
on
the
ranked
list
of
image
ccs
concepts
information
systems
retrieval
models
and
ranking
security
and
privacy
keywords
information
retrieval
fair
ranking
adversarial
machine
learning
demographic
inference
acm
reference
format
avijit
ghosh
matthew
jagielski
and
christo
wilson
2022
subverting
fair
image
search
with
generative
adversarial
perturbations
in
2022
acm
conference
on
fairness
accountability
and
transparency
facct
22
june
21
24
2022
seoul
republic
of
korea
acm
new
york
ny
usa
14
pages
https://doi.org/10.1145/3531146.3533128
permission
to
make
digital
or
hard
copies
of
part
or
all
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
for
profit
or
commercial
advantage
and
that
copies
bear
this
notice
and
the
full
citation
on
the
first
page
copyrights
for
third-party
components
of
this
work
must
be
honored
for
all
other
uses
contact
the
owner
author
facct
22
june
21
24
2022
seoul
republic
of
korea
2022
copyright
held
by
the
owner
author
acm
isbn
978
4503
9352
22
06
https://doi.org/10.1145/3531146.3533128
637
introduction
facct
22
june
21
24
2022
seoul
republic
of
korea
tennis
player
avijit
ghosh
matthew
jagielski
and
christo
wilson
crawl
query
image
corpus
retrieval
model
gap
tennis
player
fairnessaware
ranker
deepface
prediction
light-skinned
male
gap
perturbation
magniﬁed
5x
deepface
prediction
dark-skinned
male
figure
diagram
showing
our
attack
approach
shows
example
search
results
from
an
image
search
engine
for
the
query
tennis
player
this
search
engine
attempts
to
provide
demographically-fair
results
and
at
this
point
no
images
in
the
corpus
have
been
adversarially
perturbed
as
this
search
engine
crawls
and
indexes
new
images
from
the
web
it
collects
images
that
have
been
adversarially
perturbed
using
gap
model
we
show
real
example
of
one
image
before
and
after
applying
the
generated
perturbation
which
causes
the
deepface
model
90
to
misclassify
this
person
skin
tone
in
response
to
future
query
for
tennis
player
the
retrieval
model
will
identify
relevant
images
some
of
which
are
perturbed
the
fairness-aware
ranker
the
target
of
the
attack
highlighted
in
red
mistakenly
elevates
the
rank
of
an
image
containing
light-skinned
male
also
highlighted
in
red
because
it
misclassifies
them
as
dark-skinned
due
to
the
perturbations
query
results
without
ever
explicitly
using
demographic
labels
under
normal
circumstances
where
the
images
are
unperturbed
our
search
engine
returns
demographically
balanced
sets
of
images
in
response
to
free
text
queries
we
then
train
generative
adversarial
perturbation
gap
model
75
that
learns
from
pretrained
demographic
classifiers
to
strategically
insert
human-imperceptible
perturbations
into
images
these
perturbations
attempt
to
cause
fmmr
to
unfairly
boost
the
rank
of
images
containing
people
from
an
adversary-selected
subpopulation
light-skinned
men
figure
shows
example
image
search
results
produced
by
our
search
engine
in
response
to
the
query
tennis
player
with
and
without
our
attack
we
present
results
from
extensive
experiments
demonstrating
that
our
attacks
can
successfully
confer
significant
unfair
advantage
to
people
from
the
majority
class
light-skinned
men
in
our
case
in
terms
of
their
overall
representation
and
position
in
search
results
relative
to
fairly-ranked
baseline
search
results
we
demonstrate
that
our
attack
is
robust
across
number
of
variables
including
the
length
of
search
result
lists
the
fraction
of
images
that
the
adversary
is
able
to
perturb
the
fairness
algorithm
used
by
the
search
engine
the
image
embedding
algorithm
used
by
the
search
engine
the
demographic
inference
algorithm
used
to
train
the
gap
models
and
the
training
objective
of
the
gap
models
additionally
our
attacks
are
stealthy
they
have
close
to
zero
impact
on
the
relevance
of
search
results
in
summary
we
show
that
gaps
can
be
used
to
subvert
fairness
guarantees
in
the
context
of
fair
image
retrieval
further
our
attack
is
successful
under
highly
restricted
threat
model
which
suggests
that
more
powerful
adversaries
will
also
be
able
to
implement
successful
attacks
we
hypothesize
that
similar
attacks
may
be
possible
against
other
classes
of
ml-based
systems
that
rely
on
highly
parameterized
models
and
make
fairness
decisions
for
inputs
that
are
based
on
data
controlled
by
adversaries
638
the
goal
of
our
work
is
not
to
hinder
or
deter
the
adoption
of
fair
ml
techniques
we
argue
that
fair
ml
techniques
must
be
adopted
in
practice
rather
our
goal
is
to
demonstrate
that
fairness
guarantees
can
potentially
be
weaponized
so
that
the
research
community
will
be
energized
to
develop
mitigations
by
making
models
more
robust
and
by
adopting
high-quality
sources
of
demographic
data
that
are
resistant
to
manipulation
to
facilitate
mitigation
development
without
arming
attackers
we
plan
to
release
our
code
and
data
to
researchers
on
request
background
2.1
fairness
in
ranking
algorithmic
decision
making
is
permeating
modern
life
including
high-stakes
decisions
like
credit
lending
11
bail
granting
hiring
96
etc
while
these
systems
are
great
at
scaling
up
processes
with
human
bottlenecks
they
also
have
the
unintended
property
of
embedding
and
entrenching
unfair
social
biases
sexism
and
homophobia
in
response
there
is
growing
body
of
academic
work
on
ways
to
detect
algorithmic
bias
40
and
develop
classes
of
fair
algorithms
for
instance
classification
42
47
50
65
causal
inference
62
70
word
embeddings
18
20
regression
15
and
retrieval
ranking
28
86
98
there
is
also
growing
body
of
legal
work
and
legislative
action
around
the
globe
30
33
53
73
to
tackle
algorithmic
bias
in
this
study
we
focus
on
fair
information
retrieval
ir
algorithms
class
of
algorithms
that
have
received
comparatively
less
attention
than
classification
algorithms
in
the
literature
initial
studies
that
examined
fair
ir
proposed
to
solve
this
in
binary
context
make
ranked
list
fair
between
two
groups
28
98
subsequent
work
uses
constrained
learning
to
solve
ranking
problems
using
classic
optimization
methods
86
there
are
also
methods
that
use
pairwise
comparisons
16
and
describe
methods
to
achieve
fairness
in
learning-to-rank
contexts
68
99
subverting
fair
image
search
with
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
of
korea
in
industrial
settings
researchers
at
linkedin
have
proposed
an
algorithm
that
uses
re-ranking
in
post-processing
to
achieve
representational
parity
38
however
recent
work
by
ghosh
et
al
39
shows
how
uncertainty
due
to
incorrect
inference
of
protected
demographic
attributes
can
undermine
fairness
guarantees
in
ir
contexts
fairness
methods
that
do
not
require
explicit
demographic
labels
at
runtime
are
as
of
this
writing
an
emerging
area
of
focus
in
classification
55
and
ranking
51
79
one
example
that
has
been
studied
at
large-scale
is
shopify
fair
maximal
marginal
relevance
fmmr
algorithm
51
which
we
describe
in
more
detail
in
3.2
in
this
study
we
examine
the
robustness
of
shopify
and
linkedin
fair
ranking
algorithms
2.2
adversarial
machine
learning
adversarial
ml
is
growing
field
of
research
that
aims
to
develop
methods
and
tools
that
can
subvert
the
objectives
of
ml
algorithms
for
example
prior
research
has
highlighted
that
deep
learning
models
are
often
not
robust
when
presented
with
inputs
that
have
been
intentionally
maliciously
crafted
24
43
67
74
85
95
97
several
proposed
defenses
against
state-of-the-art
adversarial
ml
attacks
have
been
defeated
91
and
adversarial
examples
maliciously
crafted
inputs
have
been
shown
to
transfer
across
models
performing
similar
tasks
60
92
the
most
promising
defense
method
adversarial
training
is
computationally
expensive
and
imperfect
it
results
in
decreased
standard
accuracy
while
still
having
somewhat
low
adversarial
accuracy
35
54
84
as
such
adversarial
ml
presents
significant
hurdle
to
deploying
neural
models
in
sensitive
real-world
systems
our
work
considers
adversarial
ml
attacks
on
ir
systems
previous
work
has
demonstrated
successful
attacks
on
image-to-image
search
systems
61
101
allowing
an
adversary
to
control
the
results
of
targeted
image
queries
using
localized
patches
19
or
universal
adversarial
perturbations1
57
other
work
has
demonstrated
attacks
on
text-to-text
retrieval
systems
77
and
personalized
ranking
systems
46
work
by
zhou
et
al
101
hypothesized
that
targeted
attacks
on
selected
items
in
ranked
list
might
be
possible
using
universal
adversarial
perturbations
none
of
these
works
consider
compromising
text-to-image
models
or
group
fairness
objectives
as
we
do
in
this
study
prior
work
has
demonstrated
adversarial
ml
attacks
against
fairness
objectives
of
ml
systems
at
training
time
in
these
attacks
the
adversary
supplies
poisoned
training
data
which
then
results
in
models
that
either
compromise
the
accuracy
of
targeted
subgroups
48
64
or
exacerbate
pre-existing
unfairness
between
subgroups
29
87
specific
to
classification
there
exists
theoretical
work
that
shows
how
to
learn
fair
models
when
sensitive
attributes
are
noisy
25
or
corrupted
in
poisoning
attack
27
but
they
do
not
consider
ranking
adversarial
ml
attacks
at
test
time
after
training
model
using
non-malicious
data
that
we
consider
in
this
work
are
relatively
unexplored
in
fairness
settings
nanda
et
al
71
show
that
adversarial
ml
attacks
can
harm
certain
subpopulations
more
than
others
in
classification
tasks
however
while
this
is
an
important
observation
the
harms
suggested
by
this
work
may
be
difficult
to
realize
in
practice
as
they
only
involve
disparity
between
examples
that
are
adversarially
corrupted
by
contrast
our
work
shows
that
test-time
attacks
can
harm
fairness
for
benign
data
when
launched
in
ranking
setting
methodology
we
now
present
the
plan
for
our
study
first
we
introduce
our
application
context
and
the
threat
model
under
which
an
attacker
will
attempt
to
compromise
the
application
second
we
discuss
the
ir
models
and
algorithms
underlying
our
fairness-aware
image
search
engine
third
we
discuss
our
strategy
for
attacking
this
search
engine
using
gaps
3.1
context
and
threat
model
in
this
study
we
consider
the
security
of
fairness-aware
image
search
engine
this
search
engine
indexes
images
from
around
the
web
either
automatically
via
crawler
or
from
user-provided
submissions
and
provides
free-text
interface
to
query
the
image
database
examples
of
image
search
engines
include
google
image
search
istock
and
getty
images
and
giphy
in
our
case
the
image
search
engine
attempts
to
produce
results
that
are
both
relevant
to
given
query
and
fair
according
to
some
fairness
objective
one
example
fairness
objective
is
demographic
representativeness
for
search
results
that
contain
images
of
people
we
consider
malicious
image
curator
imgur
4chan
or
similar
with
large
database
of
perturbed
images
that
are
eventually
scraped
or
uploaded
into
the
victim
image
search
engine
index
our
adversarial
image
curator
goal
is
to
perturb
the
images
in
their
database
to
subvert
the
fairness
guarantees
of
the
downstream
retrieval
system
we
assume
that
the
adversary
does
not
have
any
knowledge
of
the
internals
of
the
ranking
system
what
retrieval
model
is
used
other
images
in
the
index
or
which
fairness
algorithm
is
used
this
threat
model
constitutes
strict
but
realistic
limitation
on
our
adversary
notice
that
this
threat
model
would
also
apply
if
the
image
search
engine
was
compromised
giving
the
adversary
access
to
underlying
models
and
the
entire
dataset
of
images
we
consider
both
adversaries
in
our
experiments
we
also
note
that
if
the
adversary
only
seeks
to
target
small
set
of
queries
they
need
only
control
fraction
of
the
images
matching
each
query
rather
than
fraction
of
the
entire
image
database
this
is
useful
for
the
adversary
in
the
case
that
not
all
queries
are
equally
sensitive
3.2
building
an
image
search
engine
we
now
turn
our
attention
to
building
realistic
image
search
engine
that
will
serve
as
the
victim
for
our
attacks
3.2
image
retrieval
from
text
queries
the
first
choice
we
make
for
this
study
is
to
select
an
image
retrieval
model
there
are
several
frameworks
for
image
retrieval
in
the
literature
starting
from
an
adversarial
image
curator
is
also
the
threat
model
assumed
for
clean
label
poi1
universal
adversarial
perturbation
uap
is
an
adversarial
perturbation
that
soning
attacks
83
93
this
adversarial
image
curator
may
perturb
copies
of
images
taken
from
the
web
or
original
images
that
they
author
this
setup
is
also
used
by
85
as
defensive
method
against
unauthorized
models
generalizes
to
all
classes
of
target
classifier
one
patch
to
untargeted
attack
as
many
classes
as
possible
639
facct
22
june
21
24
2022
seoul
republic
of
korea
avijit
ghosh
matthew
jagielski
and
christo
wilson
tag-based
matching
56
to
state-of-the-art
vision-language
transformers
58
63
for
the
purpose
of
this
paper
we
used
multimodal
transformer
mmt
37
based
text-image
retrieval
model
this
model
consists
of
two
components
fast
although
somewhat
lower
quality
retrieval
step
that
identifies
large
set
of
relevant
images
followed
by
re-ranking
step
that
selects
the
best
images
from
the
retrieved
set
concretely
the
user
provides
string
that
queries
into
database
of
images
for
the
retrieval
step
the
query
string
is
encoded
with
an
embedding
function
fq
to
produce
an
embedding
vq
and
all
images
in
have
pre-computed
embeddings
from
an
embedding
function
the
cosine
distance
between
vq
and
all
embeddings
of
are
computed
to
collect
some
large
set
dq
of
size
plausible
image
matches
these
images
are
then
ranked
according
to
joint
model
that
takes
both
the
indicating
query
and
an
image
as
input
returning
scores
si
how
well
each
image
dq
matches
the
query
these
scores
are
used
to
produce
the
final
ranking
note
that
the
mmt
model
is
not
designed
to
be
fair
in
any
normative
sense
to
achieve
fairness
results
from
the
model
must
be
re-ranked
which
we
describe
in
the
next
section
thus
the
mmt
model
is
not
the
target
of
our
attacks
since
it
is
not
responsible
for
implementing
any
fairness
objectives
3.2
fairness-aware
re-ranking
the
second
choice
we
make
for
this
study
is
selecting
an
algorithm
that
takes
the
output
of
the
image
retrieval
model
as
input
and
produces
fair
re-ranking
of
the
items
in
fairness-aware
re-ranking
ranking
function
fr
is
post-processed
to
achieve
fairness
according
to
some
subgroup
where
denotes
the
score
labels
on
the
dataset
si
th
of
the
item
the
heuristic
score
according
to
which
the
list
is
sorted
and
denotes
the
item
to
be
ranked
the
re-ranking
algorithm
we
adopt
is
fair
maximal
marginal
relevance
fmmr
51
which
was
developed
and
used
at
shopify
for
representative
ranking
of
images
fmmr
builds
on
the
maximal
marginal
relevance
23
technique
in
ir
that
seeks
to
maximize
the
information
in
ranked
list
by
choosing
the
next
retrieved
item
in
the
list
to
be
as
dissimilar
to
the
current
items
present
in
the
list
as
possible
mmr
introduces
hyperparameter
that
allows
the
operator
to
choose
the
trade-off
between
similarity
and
relevance
fmmr
modifies
the
similarity
heuristic
from
mmr
to
encode
for
similarity
in
terms
of
demographics
with
the
idea
being
that
the
next
relevant
item
chosen
to
be
placed
in
the
re-ranked
list
will
be
as
demographically
different
from
the
existing
images
as
possible
similarity
is
calculated
using
image
embeddings
for
which
we
examine
three
models
faster
r-cnn
78
inceptionv3
88
and
resnet18
45
we
fix
the
trade-off
parameter
at
0.14
as
that
is
the
value
used
by
karako
and
manggala
51
in
their
fmmr
paper
it
is
notable
that
fmmr
does
not
require
demographic
labels
of
people
in
images
to
perform
fair
re-ranking
since
it
uses
heuristic
that
only
relies
on
embeddings
indeed
fmmr
comes
from
class
of
fair
ranking
algorithms
that
all
use
the
inherent
latent
representations
of
the
objects
for
their
re-ranking
strategy
51
79
that
said
since
fmmr
attempts
to
maximize
the
distance
from
the
centroids
of
the
embeddings
of
different
demographic
groups
it
can
be
thought
of
as
performing
indirect
demographic
inference
on
individuals
in
images
640
additionally
we
also
evaluated
our
attacks
against
second
fairness-aware
re-ranking
algorithm
detconstsort
38
developed
by
and
deployed
at
linkedin
in
their
talent
search
system
unlike
fmmr
detconstsort
explicitly
requires
demographic
labels
for
the
items
it
is
trying
to
fairly
re-rank
however
prior
work
39
shows
that
detconstsort
has
significant
limitations
when
demographic
inference
is
used
rather
than
ground-truth
demographic
labels
making
it
unfair
even
without
perturbed
images
as
result
evaluating
an
attack
against
detconstsort
is
not
meaningful
and
we
defer
our
discussion
of
detconstsort
to
3.3
attack
construction
having
described
our
search
engine
we
are
ready
to
turn
our
attention
to
our
attack
first
we
introduce
the
demographic
inference
models
deepface
90
and
fairface
52
that
we
use
to
train
our
attack
next
we
describe
how
we
generate
adversarial
perturbations
from
demographic
inference
model
modifying
images
in
way
that
is
imperceptible
to
human
eyes
yet
significant
enough
to
fool
the
fair
re-ranking
algorithm
of
our
search
engine
3.3
demographic
inference
algorithms
for
large-scale
datasets
such
as
images
scraped
from
the
web
demographic
meta-data
for
people
in
the
images
is
not
readily
available
and
prohibitively
expensive
to
collect
through
manual
annotation
17
pipelines
using
demographic
inference
are
commonly
used
in
practice
when
demographic
labels
are
not
available
for
example
the
bayesian
improved
surname
geocoding
bisg
tool
is
used
to
measure
fairness
violations
in
lending
decisions
22
and
it
relies
on
inferred
demographic
information
this
makes
attacks
on
demographic
inference
models
natural
candidate
for
adversely
affecting
ranking
fairness
we
consider
two
image
demographic
inference
models
to
train
our
attacks
deepface
90
is
face
recognition
model
for
gender
and
race
inference
developed
by
facebook
we
use
its
public
wrapper
82
which
includes
models
fine
tuned
on
roughly
22
000
samples
for
race
and
gender
classification
fairface
52
is
model
designed
for
race
and
gender
inference
trained
on
diverse
set
of
108
000
images
since
both
of
these
models
infer
race
ethnicity
we
used
mapping
to
infer
skin
tone
since
we
could
not
find
commercially
available
algorithms
to
infer
skin
tones
from
human
images
we
also
use
these
models
to
infer
demographics
as
input
to
the
detconstsort
algorithm
matching
the
pipeline
of
39
which
we
discuss
in
3.3
subpopulation
generative
adversarial
perturbations
recall
our
adversarial
image
curator
goal
to
produce
database
of
malicious
images
that
when
indexed
by
our
image
search
engine
undermine
its
purported
fairness
guarantees
concretely
this
means
fooling
the
fair
re-ranker
such
that
it
believes
given
set
of
search
results
is
fair
across
two
or
more
subgroups
when
in
fact
the
results
are
unfair
because
some
subgroups
are
under
or
over-represented
additionally
these
malicious
images
must
retain
their
relevance
the
mapping
we
used
is
white
east
asian
middle
eastern
light
and
black
south
asian
hispanic
dark
we
acknowledge
that
this
is
crude
mapping
but
it
enabled
us
to
train
successful
attack
subverting
fair
image
search
with
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
of
korea
to
given
query
and
not
be
perceived
as
manipulated
to
human
users
of
the
search
engine
prior
work
see
2.2
has
demonstrated
that
neural
image
classification
models
can
be
fooled
by
adding
adversarial
perturbations
to
images
at
high-level
the
adversary
goal
is
to
train
model
that
can
add
noise
to
images
such
that
specific
latent
characteristics
of
the
images
are
altered
in
our
case
these
altered
characteristics
should
impact
the
image
embeddings
calculated
by
the
image
embedding
model
inceptionv3
that
fmmr
relies
upon
to
do
fair
re-ranking
running
an
adversarial
perturbation
algorithm
on
each
of
the
images
in
the
adversary
database
would
be
prohibitive
as
these
algorithms
involve
computationally
expensive
optimization
algorithms
that
are
not
practical
at
the
scale
of
an
entire
database
we
avoid
this
limitation
by
training
generative
adversarial
perturbation
gap
model
75
gap
model
gap
takes
clean
image
as
input
and
returns
perturbed
image
that
is
misclassified
by
some
target
model
targ
this
replaces
the
per-image
optimization
problem
with
much
less
expensive
forward
pass
of
gap
training
the
gap
is
one
time
expense
for
the
adversary
amortized
over
the
large
number
of
image
perturbations
done
later
universal
adversarial
perturbations
uaps
66
are
another
approach
to
amortizing
runtime
but
require
all
images
to
be
the
same
dimensions
an
unrealistic
assumption
for
real-world
image
databases
having
motivated
the
choice
of
gap
model
for
our
attack
we
now
consider
the
problem
of
impacting
fairness
by
attacking
the
fair
re-ranking
algorithm
used
by
victim
search
engine
we
choose
to
design
gap
to
target
demographic
inference
model
di
this
will
produce
perturbations
that
to
deep
image
model
make
an
image
of
person
from
one
demographic
group
appear
to
be
from
different
demographic
group
this
attack
would
heavily
impact
demographic-aware
re-ranking
algorithm
such
as
detconstsort
38
see
if
it
used
an
accurate
demographic
inference
algorithm
to
produce
annotations
although
fmmr
does
not
use
annotations
we
show
in
that
our
attack
is
still
successful
at
compromising
fmmr
fairness
guarantees
our
attack
can
be
seen
as
an
application
of
the
transferability
property
of
adversarial
examples
additionally
training
our
gap
against
demographic
inference
model
causes
our
attack
to
be
independent
of
the
ranking
algorithm
and
image
corpus
used
by
the
victim
search
engine
both
of
which
are
strong
adversarial
assumptions
in
designing
our
gap
to
compromise
fairness
we
first
note
that
an
attack
that
simply
forces
di
to
make
arbitrarily
many
errors
may
not
impact
fairness
for
example
suppose
the
image
database
contained
two
subpopulations
the
advantaged
class
and
the
disadvantaged
class
suppose
the
attack
causes
di
to
misclassify
all
members
of
as
and
all
members
of
as
this
is
the
best
possible
result
of
an
attack
on
the
demographic
inference
algorithm
but
results
in
no
changes
to
fair
ranking
algorithm
it
will
simply
consider
to
be
the
disadvantaged
class
and
thus
produce
the
same
ranking
for
this
reason
our
adversary
must
incorporate
subpopulations
into
the
attack
to
do
so
we
propose
the
class-targeted
generative
adversarial
perturbation
cgap
definition
cgap
we
consider
loss
function
target
model
targ
distribution
over
inputs
and
outputs
the
adversary
provides
source
class
ys
and
target
class
yt
then
the
cgap
model
cgap
is
model
that
takes
as
input
an
image
and
returns
an
image
minimizing
the
following
loss
functions
ℓcgap
cgap
yt
targ
ys
ℓcgap
cgap
targ
ys
that
is
the
cgap
should
force
the
demographic
inference
model
to
misclassify
samples
of
class
ys
to
class
yt
while
maintaining
its
performance
for
samples
not
from
class
ys
we
also
consider
two
extensions
of
this
definition
first
we
permit
the
adversary
to
target
multiple
classes
at
once
in
the
extreme
an
adversary
may
want
all
samples
to
be
classified
to
the
same
class
this
approach
is
proposed
by
75
for
demographic
inference
algorithm
all
samples
having
the
same
demographic
label
will
cause
the
fair
re-ranking
system
to
have
similar
performance
to
an
unfair
ranking
system
as
all
points
will
appear
to
fall
into
the
same
subpopulation
the
second
extension
is
the
untargeted
attack
where
the
cgap
simply
increases
loss
for
points
from
class
ys
inducing
arbitrary
misclassifications
simultaneously
making
both
relaxations
recovers
the
original
untargeted
gap
approach
we
experiment
with
both
relaxations
independently
as
well
as
multiple
instantiations
of
cgap
as
defined
above
experiments
in
this
section
we
introduce
the
dataset
we
used
for
our
evaluation
describe
the
setup
for
our
experiments
and
define
the
metrics
we
use
to
evaluate
our
attacks
4.1
dataset
annotation
and
preprocessing
we
use
microsoft
common
objects
in
context
ms-coco
59
as
our
retrieval
dataset
since
it
contains
variety
of
images
with
variable
dimensions
and
depths
this
closely
mimics
what
realworld
image
search
dataset
might
contain
to
specifically
measure
for
demographic
bias
we
filter
the
dataset
keeping
only
images
that
contain
people
we
also
need
the
images
to
have
demographic
annotations
for
fair
ranking
so
we
use
an
annotated
subset
of
the
coco
2014
dataset
constructed
by
zhao
et
al
100
similar
to
prior
work
26
39
zhao
et
al
crowdsource
skin
color
on
the
fitzpatrick
skin
type
scale
which
the
authors
simplified
to
light
and
dark
and
binary
perceived
gender
expression
for
15
762
images
for
the
purposes
of
our
experiments
we
only
considered
the
692
images
that
contain
one
person
after
filtering
our
final
dataset
consisted
of
216
light
men
536
light
women
714
dark
men
and
226
dark
women
4.2
experimental
setup
as
starting
point
for
our
experiments
we
need
to
collect
ranked
lists
from
our
baseline
unfair
retrieval
system
as
described
in
3.2
to
do
so
we
run
three
different
search
queries
on
the
retrieval
system
tennis
player
person
eating
pizza
and
person
at
table
we
chose
these
queries
because
they
all
reference
human
uap
can
also
be
seen
as
gap
where
gap
for
fixed
therefore
we
expect
that
gap
will
perform
strictly
better
than
uap
recall
that
per
our
threat
model
in
3.1
the
attacker
does
not
know
what
fair
re-ranking
algorithm
is
used
by
the
victim
and
thus
cannot
train
against
it
directly
641
facct
22
june
21
24
2022
seoul
republic
of
korea
avijit
ghosh
matthew
jagielski
and
christo
wilson
search
queries
attack
training
embedding
training
objective
attack
probability
top
tennis
player
person
eating
pizza
person
at
table
deepface
fairface
f-rcnn
inceptionv3
resnet
any
light
men
light
men
any
dark
men
light
men
light
men
dark
men
0.2
0.5
0.7
1.0
10
15
20
45
50
table
variables
and
hyperparameters
we
used
for
evaluating
our
attack
being
are
ethnicity
and
gender
neutral
and
are
well-supported
in
the
coco
dataset
we
picked
popular
object
tags
see
we
set
the
upper
bound
in
the
baseline
retrieval
system
to
be
200
images
the
three
queries
return
131
75
and
124
images
respectively
along
with
their
relevance
scores
we
show
the
distribution
of
the
relevance
scores
and
the
skin
color
gender
distributions
of
the
images
within
the
top
40
search
results
for
each
query
in
figure
as
also
shown
by
zhao
et
al
100
light
men
comprise
the
overwhelming
majority
in
all
three
lists
and
they
also
have
high
relevance
scores
across
the
board
meaning
that
the
retrieval
system
places
light
men
near
the
top
of
the
search
results
we
call
these
lists
the
baseline
lists
we
also
need
to
produce
fair
versions
of
the
baseline
lists
to
do
so
we
pass
the
baseline
lists
for
each
of
our
three
queries
through
fmmr
with
the
three
embedding
algorithms
without
any
adversarial
perturbations
we
refer
to
the
nine
lists
obtained
via
the
fair
re-ranker
three
queries
times
three
image
embedding
models
as
the
oracle
lists
to
train
our
adversarial
attacks
we
first
remove
the
330
images
in
our
oracle
lists
from
the
original
dataset
leaving
362
images
these
362
images
were
then
split
randomly
into
training
and
testing
sets
in
an
ratio
to
train
cgap
models
for
all
possible
combinations
of
training
objectives
and
demographic
inference
algorithms
di
described
in
detail
below
we
ran
our
experiments
on
pytorch
with
cuda
backend
on
two
nvidia
rtx-a6000
gpus
and
trained
cgap
models
for
10
epochs
each
with
the
norm7
bound
set
to
10
we
describe
our
different
training
and
inference
combinations
below
table
shows
summary
of
the
different
settings
involved
during
the
training
and
testing
of
our
cgap
attacks
4.2
embedding
algorithm
as
we
discuss
in
3.2
fmmr
requires
image
embeddings
the
authors
of
the
original
paper
used
pretrained
inceptionv3
model
which
we
also
adopt
additionally
we
test
the
performance
of
fmmr
using
embeddings
generated
by
pretrained
faster
r-cnn
and
resnet
models
these
models
are
trained
for
standard
image
classification
tasks
and
have
no
inherent
concept
of
demographic
groups
4.2
attack
training
algorithm
as
detailed
in
3.3
we
train
cgap
models
to
induce
adversary-selected
misclassifications
in
two
target
demographic
inference
models
denoted
as
di
deepface
90
and
fairface
82
these
models
are
trained
for
demographic
inference
and
so
do
not
overlap
in
training
objective
with
the
image
embedding
models
for
fmmr
the
only
similarity
in
architecture
between
the
demographic
inference
and
fmmr
embedding
models
is
that
fairface
uses
resnet
architecture
4.2
training
objectives
as
discussed
in
3.3
we
select
certain
subpopulations
to
be
systematically
misclassified
by
the
two
di
described
above
the
four
cgaps
we
train
induce
misclassifications
with
the
following
source-target
pairs
any
light
men
where
every
subgroup
was
perturbed
to
be
predicted
as
light
men
light
men
any
where
only
light
men
are
arbitrarily
misclassified
dark
men
light
men
where
only
dark
men
are
misclassified
as
light
men
and
light
men
dark
men
where
only
light
men
are
misclassified
as
dark
men
4.2
attack
probability
pr
it
is
strong
assumption
that
an
adversary
can
perturb
the
entire
image
database
of
victim
search
engine
this
is
only
possible
if
the
search
engine
itself
is
malicious
or
it
is
utterly
compromised
instead
we
measure
the
effect
of
our
attack
when
the
attacker
may
perturb
pr
20
50
70
and
100
of
the
image
database
relevant
to
each
query
if
small
number
of
queries
are
targeted
only
few
images
are
required
to
run
the
attack
4.2
top
ranking
is
very
sensitive
to
position
bias
39
80
so
we
measure
with
different
lengths
of
the
top
list
ranging
from
top
10
to
top
50
to
gauge
our
attack
impact
on
the
fair
ranking
algorithms
as
final
list
sizes
vary
4.3
evaluation
metrics
to
evaluate
the
impact
of
our
attacks
we
use
three
metrics
that
aim
to
measure
representation
bias
attention
or
exposure
bias
and
loss
in
ranking
utility
due
to
re-ranking
additionally
we
introduce
summarizing
meta-metric
that
enables
us
to
clearly
present
the
impact
of
our
attacks
with
respect
to
each
metric
4.3
skew
the
metric
we
use
to
measure
the
bias
in
representation
is
called
skew
38
39
for
ranked
list
the
skew
for
attribute
value
ai
at
position
is
defined
as
pτ
ai
skewai
pq
ai
pτ
ai
represents
the
fraction
of
members
having
the
attribute
ai
among
the
top
items
in
and
pq
ai
represents
the
fraction
of
members
from
subgroup
ai
in
the
overall
population
in
an
ideal
fair
representation
the
skew
value
for
all
subgroups
is
equal
to
indicating
that
their
representation
among
the
top
items
exactly
matches
their
proportion
in
the
overall
population
dark-skinned
women
do
appear
in
the
search
results
for
the
query
female
tennis
player
this
seems
to
reflect
stereotypical
bias
36
within
the
learned-word
representations
in
the
mmt
model
is
the
absolute
distance
in
pixel
space
any
one
pixel
is
changed
pixel
can
at
most
change
by
value
of
10
in
each
color
channel
642
4.3
attention
even
if
all
subgroups
were
fairly
represented
in
the
top
ranked
items
of
list
the
relative
position
of
the
ranked
items
adds
another
dimension
of
bias
unequal
exposure
previous
5.0
2.60
4.5
2.55
2.50
facct
22
june
21
24
2022
seoul
republic
of
korea
3.4
3.3
utility
score
2.65
utility
score
utility
score
subverting
fair
image
search
with
generative
adversarial
perturbations
4.0
3.5
3.0
2.45
query
tennis
player
light
men
3.1
3.0
2.9
2.5
2.40
3.2
2.8
query
person
eating
pizza
query
person
at
table
light
women
dark
women
dark
men
figure
utility
relevance
score
and
group
size
distribution
within
the
top
40
baseline
search
results
for
three
queries
the
black
dots
represent
the
average
utility
score
for
that
group
while
the
circle
size
represents
the
group
size
no
dark-skinned
women
appear
in
the
top
40
baseline
results
for
the
tennis
player
query
studies
69
72
have
shown
that
people
attention
rapidly
decreases
as
they
scan
down
list
with
more
attention
given
to
the
higher
ranking
items
ultimately
dropping
to
zero
attention
in
this
study
we
model
attention
decay
using
the
geometric
distribution
as
done
in
prior
work
by
sapiezynski
et
al
80
we
compute
attention
at
the
rank
as
attentionp
100
useful
since
group
sizes
vary
second
comparing
to
the
group
that
gets
the
minimum
boost
ensures
that
the
metric
presents
the
widest
fairness
disparity
regardless
of
the
total
number
of
groups
for
the
purposes
of
this
paper
we
set
as
light
men
because
they
are
socially
and
historically
the
most
advantaged
group
and
large
for
light
men
indicates
that
the
attack
causes
their
ranking
to
be
unfairly
boosted
relative
to
the
least
privileged
group
to
make
sure
that
the
fairness
impacts
we
observe
are
due
to
the
effectiveness
of
our
attack
on
the
re-ranking
algorithms
only
the
values
and
the
change
in
ndcg
are
all
measured
against
the
oracle
fairly
re-ranked
lists
because
we
compare
against
the
oracle
list
all
results
with
attack
probability
pr
will
have
where
is
the
fraction
of
total
attention
given
to
the
top
search
result
the
choice
of
is
application
specific
for
this
paper
we
fixed
to
be
0.36
based
on
study
44
that
reported
that
the
top
result
on
google
search
receives
36.4
of
the
total
clicks
we
then
calculate
the
average
attention
per
subgroup
average
attentionai
att
where
aτk
ai
ai
ideally
in
perfectly
fair
ranked
list
all
subgroups
should
receive
equal
average
attention
4.3
normalized
discounted
cumulative
gain
ndcg
is
widely
used
measure
in
ir
to
evaluate
the
quality
of
search
rankings
49
it
is
defined
as
ndcg
siτ
1x
loд2
5.1
where
siτ
is
the
utility
score
from
the
mmt
retrieval
model
of
the
th
element
in
the
ranked
list
and
loд
ndcg
scores
range
from
to
with
the
latter
capturing
ideal
search
results
4.3
summarizing
metric
for
the
purpose
of
quantifying
how
much
unfair
advantage
our
attacks
confer
on
members
of
the
majority
class
relative
to
all
other
classes
we
define
new
meta-metric
called
attack
effectiveness
for
given
metric
skew
attention
and
subgroup
it
is
defined
as
change
in
for
subgroup
minimum
change
in
over
other
subgroups
we
chose
this
formulation
of
for
two
reasons
first
comparing
percentage
changes
makes
the
metric
scale
invariant
which
is
643
results
in
this
section
we
evaluate
the
impact
of
our
attacks
on
the
fairness
guarantees
of
fmmr
for
each
set
of
results
we
examine
how
attack
effectiveness
varies
for
one
particular
variable
top
image
embedding
model
etc
as
the
attack
probability
pr
the
fraction
of
images
under
adversarial
control
varies
when
focusing
on
particular
variable
we
present
results
that
are
averaged
across
all
other
variables
and
all
three
of
our
queries
top
and
pr
we
begin
by
evaluating
the
impact
of
our
attacks
as
we
vary
the
length
of
the
top
list
and
the
fraction
of
images
in
the
query
list
under
adversarial
control
pr
plotted
in
figure
varying
pr
has
the
expected
effect
as
the
adversary
has
more
control
over
the
image
database
attacks
become
more
effective
for
skew
and
attention
increase
when
the
adversary
is
able
to
control
100
of
images
in
the
query
list
attacks
are
especially
strong
increasing
attention
unfairness
by
over
50
for
some
values
of
even
with
only
20
control
the
adversary
can
increase
attention
unfairness
by
30
recall
that
pr
measures
the
fraction
of
each
query
list
that
is
compromised
so
as
few
as
35
images
can
be
compromised
at
pr
0.5
for
the
person
eating
pizza
query
varying
also
impacts
ranking
fairness
as
increases
attention
unfairness
increases
modestly
and
skew
unfairness
decreases
that
skew
unfairness
decreases
with
indicates
that
the
composition
of
items
in
the
search
results
becomes
fairer
as
the
length
of
the
list
avijit
ghosh
matthew
jagielski
and
christo
wilson
30
20
10
0.1
50
change
ndcg
attention
light
men
skew
light
men
facct
22
june
21
24
2022
seoul
republic
of
korea
40
30
0.2
0.3
0.4
0.5
0.6
10
20
30
40
50
10
20
30
top
40
50
10
20
30
top
skew
40
50
top
attention
ndcg
attack
probability
0.2
0.5
0.7
1.0
figure
attack
effectiveness
as
function
of
attack
probability
pr
and
list
length
higher
is
more
effective
attack
the
search
results
are
more
favorable
to
light-skinned
men
unfairness
increases
as
pr
increases
yet
there
is
almost
no
impact
on
ranking
quality
ndcg
as
increases
skew
is
less
impacted
but
attention
is
impacted
somewhat
more
14
12
10
0.2
0.5
0.7
1.0
0.2
70
60
0.0
change
ndcg
attention
light
men
skew
light
men
16
50
40
30
20
0.2
0.4
0.6
10
attack
probability
0.8
0.2
0.5
0.7
1.0
0.2
attack
probability
skew
attention
f-rcnn
0.5
0.7
1.0
attack
probability
ndcg
inceptionv3
resnet
figure
attack
effectiveness
is
stable
when
the
model
used
for
the
fmmr
embedding
is
changed
resnet
embeddings
are
slightly
more
robust
to
attack
and
f-rcnn
are
slightly
less
robust
interestingly
the
resnet
robustness
is
in
spite
of
it
having
the
most
similar
model
architecture
to
fairface
grows
however
our
attack
is
able
to
cause
fmmr
to
reorder
the
list
such
the
top-most
items
remain
unfair
regardless
of
which
is
why
attention
unfairness
exhibits
less
dependency
on
lastly
we
observe
that
our
attacks
are
stealthy
regardless
of
or
pr
ndcg
never
changes
more
than
0.7
meaning
that
our
attack
had
effectively
zero
impact
on
search
result
relevance
5.2
both
cases
light
men
end
up
with
an
significant
unfair
advantage
we
explain
this
seeming
contradiction
with
an
example
in
figure
in
essence
using
gap
to
misclassify
people
from
minority
group
into
the
majority
group
reduces
the
minority
group
overall
share
of
the
population
since
group
fairness
in
this
case
is
based
on
the
overall
population
distribution
this
causes
fmmr
to
rerank
fewer
minority
group
members
into
the
top
of
the
search
results
based
on
the
results
in
figure
it
appears
that
there
is
no
way
to
advantage
minority
group
with
our
attacks
choice
of
training
objective
we
evaluate
our
attack
impact
on
fairness
with
four
cgap
models
one
that
misclassifies
dark
men
as
light
men
one
for
misclassifying
light
men
as
dark
men
and
relaxed
cgap
models
that
misclassify
all
people
as
light
men
and
all
light
men
as
other
groups
we
show
these
attacks
effectiveness
in
figure
each
of
these
attacks
performs
similarly
well
at
harming
fairness
in
terms
of
skew
and
attention
and
remaining
stealthy
in
terms
of
ndcg
one
surprising
observation
is
that
misclassifying
dark
men
as
light
men
performs
similarly
to
the
exact
opposite
attack
in
644
5.3
choice
of
attack
training
algorithm
we
measure
our
attacks
effectiveness
when
the
gap
models
are
trained
on
deepface
and
fairface
demographic
inference
models
we
observe
that
attack
effectiveness
is
largely
independent
of
the
choice
of
inference
model
and
all
attacks
remain
stealthy
we
defer
plot
of
the
results
to
the
supplementary
material
in
figure
subverting
fair
image
search
with
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
of
korea
0.1
14
12
10
0.2
0.5
0.7
1.0
50
0.0
change
ndcg
attention
light
men
skew
light
men
16
40
30
20
10
0.2
0.3
0.4
0.5
0.6
0.2
attack
probability
0.5
0.7
1.0
0.2
attack
probability
skew
any
light
men
0.1
attention
light
men
any
0.5
0.7
1.0
attack
probability
ndcg
dark
men
light
men
light
men
dark
men
figure
attack
effectiveness
is
relatively
stable
when
the
gap
training
objective
is
changed
top
skew
1.50
1.50
1.25
0.5
light
dark
light
dark
dark
light
figure
an
example
showing
how
incorrect
group
allocation
in
any
direction
always
harms
the
minority
group
members
in
fair
ranking
shows
baseline
unfair
list
with
all
people
sorted
by
relevance
to
the
query
and
no
dark
people
in
the
top
shows
the
fair
ranking
produced
by
fmmr
with
the
same
proportion
of
light
and
dark
people
in
the
top
as
the
overall
population
in
light
people
images
are
perturbed
using
gap
so
that
half
of
them
are
grouped
with
dark
people
fmmr
moves
the
most
relevant
dark
people
into
the
top
to
make
the
list
fair
but
in
this
case
the
most
relevant
dark
people
are
really
light
skinned
in
half
of
the
dark
people
are
perturbed
using
gap
to
be
grouped
as
light
people
to
fmmr
this
appears
to
reduce
the
overall
population
of
dark
people
so
it
only
needs
to
move
one
dark
person
into
the
top
to
make
the
list
proportionally
fair
note
that
if
all
light
people
were
grouped
as
dark
or
all
dark
people
were
grouped
as
light
the
ranking
would
remain
the
unfair
baseline
shown
in
5.4
choice
of
query
lastly
we
examine
the
effectiveness
of
our
attacks
against
three
different
queries
and
plot
the
results
in
figure
we
observe
that
all
attacks
were
successful
but
that
effectiveness
varies
by
query
the
differences
in
attack
effectiveness
are
explained
by
the
underlying
distributions
of
population
and
utility
scores
see
figure
the
tennis
results
exhibit
the
most
unfairness
post-attack
because
they
were
most
unfair
to
begin
with
the
difference
in
utility
scores
between
light
and
dark
skinned
people
was
greatest
in
the
tennis
results
as
compared
to
the
other
queries
in
contrast
the
pizza
results
exhibit
the
most
robustness
to
attack
in
terms
of
attention
645
because
these
were
the
only
results
among
the
queries
where
minority
people
had
higher
utility
scores
than
majority
people
in
the
baseline
results
figure
discussion
in
this
study
we
develop
novel
adversarial
ml
attack
against
fair
ranking
algorithms
and
use
fairness-aware
text-to-image
retrieval
as
case
study
to
demonstrate
our
attack
effectiveness
unfortunately
we
find
that
our
attack
is
very
successful
at
subverting
the
fairness
algorithm
of
the
search
engine
across
an
extensive
set
of
attack
variations
while
having
almost
zero
impact
on
search
result
relevance
facct
22
june
21
24
2022
seoul
republic
of
korea
avijit
ghosh
matthew
jagielski
and
christo
wilson
15
10
0.1
80
0.0
change
ndcg
attention
light
men
skew
light
men
20
60
40
20
0.1
0.2
0.3
0.4
0.5
0.6
0.2
0.5
0.7
1.0
attack
probability
skew
query
tennis
player
0.2
0.5
0.7
1.0
attack
probability
0.2
0.5
0.7
1.0
attack
probability
attention
ndcg
query
person
eating
pizza
query
person
at
table
figure
attacks
are
effective
against
all
three
of
our
queries
but
the
effectiveness
varies
in
relation
to
the
underlying
population
and
utility
score
distributions
see
figure
although
we
present
single
case
study
we
argue
that
our
attack
is
likely
to
generalize
we
adopt
strong
threat
model
and
demonstrate
that
our
attacks
succeed
even
when
the
attacker
cannot
poison
training
data
access
the
victim
whole
image
corpus
or
know
what
models
are
used
by
the
victim
thus
our
attack
is
highly
likely
to
succeed
in
cases
where
the
threat
model
is
more
relaxed
when
the
fairness
algorithm
used
by
the
victim
is
known
alarmingly
our
work
shows
that
an
adversary
can
attack
fairness
algorithm
like
fmmr
even
when
it
does
not
explicitly
rely
on
demographic
inference
thus
it
is
highly
likely
that
our
attack
will
also
succeed
against
any
ranking
algorithm
that
does
rely
on
demographic
inference
model
even
if
that
model
is
highly
accurate
we
explore
this
possibility
to
the
best
of
our
ability
in
we
hope
that
this
research
will
raise
awareness
and
spur
further
research
into
vulnerabilities
in
fair
algorithms
our
results
highlight
how
in
the
absence
of
safeguards
fairness
interventions
can
potentially
be
weaponized
by
malicious
parties
as
tool
of
oppression
in
the
absence
of
fair
ml
development
methods
and
algorithms
that
are
robust
to
adversarial
attacks
it
may
not
be
possible
for
policymakers
to
safely
mandate
the
use
of
fair
ml
algorithms
in
practice
we
believe
that
future
work
is
needed
to
develop
more
robust
fair
ml
interventions
we
adopt
broad
view
of
possible
mitigations
spanning
from
value
sensitive
design
34
methods
that
help
developers
preemptively
identify
attack
surface
and
plan
defenses
32
to
models
that
are
hardened
against
adversarial
perturbation
techniques
43
to
auditing
checklists
76
and
tools
that
help
developers
notice
and
triage
attacks
above
all
this
work
highlights
that
achieving
demographic
fairness
requires
high-quality
demographic
data
allowing
an
adversary
to
influence
demographic
meta-data
is
the
underlying
flaw
that
enables
our
attack
to
succeed
demographic
data
may
be
sourced
from
data
subjects
themselves
with
full
knowledge
and
consent
or
from
human
labelers
10
with
the
caveat
that
these
labels
themselves
will
need
to
be
de-biased
100
6.1
limitations
our
study
has
number
of
limitations
first
our
analysis
is
limited
to
two
discrete
racial
and
two
discrete
gender
categories
although
646
our
cgap
attack
could
be
tailored
to
select
any
group
it
is
unclear
how
well
our
attack
would
perform
in
situations
with
discrete
protected
groups
groups
with
continuous
attributes
people
with
multiple
or
partial
group
memberships
or
with
population
distributions
that
varied
significantly
from
our
dataset
second
while
our
dataset
is
sufficiently
large
to
demonstrate
our
attack
it
is
smaller
than
the
databases
that
real-world
image
search
engines
retrieve
from
third
our
proof-of-concept
was
tuned
to
attack
fairface
and
deepface
it
is
unclear
how
well
our
cgap
attack
would
generalize
to
other
models
or
real-world
deployed
systems
fourth
as
we
observe
in
figure
our
attack
is
only
successful
at
generating
unfairness
in
favor
of
already-advantaged
groups
while
this
is
limitation
it
in
no
way
diminishes
the
potential
real-world
harm
our
attack
could
inflict
on
marginalized
populations
finally
as
shown
in
figure
our
attack
effectiveness
varies
by
query
in
real
world
scenarios
an
attacker
could
mitigate
this
to
some
extent
by
devoting
more
of
their
resources
towards
perturbing
images
that
are
relevant
to
high-value
queries
it
is
unclear
how
much
the
attackers
effort
would
need
to
vary
in
practice
given
that
we
make
no
attempt
to
attack
deployed
search
engines
6.2
ethics
in
this
work
we
present
concrete
attack
against
fair
ranking
system
like
all
adversarial
attack
research
our
methods
can
potentially
be
misused
by
bad
actors
however
this
also
necessitates
our
research
since
documenting
vulnerabilities
is
the
first
step
towards
mitigating
them
to
the
best
of
our
knowledge
with
the
exception
of
shopify
and
linkedin
few
services
are
known
to
employ
fair
ranking
systems
in
practice
meaning
there
currently
exists
window
of
opportunity
to
preemptively
identify
attacks
raise
awareness
and
deploy
mitigations
prior
work
on
adversarial
ml
attacks
against
fairness
made
their
source
code
publicly
available
71
however
because
attack
tools
are
dual-use
we
have
opted
to
take
more
conservative
approach
we
will
only
share
source
code
with
researchers
from
research
universities
as
identified
by
taxonomies
like
the
carnegie
classification
and
companies
that
develop
potentially
vulnerable
products
given
that
our
attack
can
be
used
for
legitimate
blackbox
algorithm
auditing
purposes
we
opt
to
restrict
who
may
access
subverting
fair
image
search
with
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
of
korea
our
source
code
rather
than
the
uses
it
may
be
put
towards
in
our
opinion
this
process
will
facilitate
follow-up
research
mitigation
development
and
algorithm
auditing
without
supplying
bad
actors
with
ready-made
attack
tools
like
many
works
in
the
computer
vision
field
we
rely
on
images
with
crowdsourced
and
inferred
demographic
labels
both
processes
have
been
criticized
for
their
lack
of
consent
41
the
way
they
operationalize
identity
81
and
the
harm
they
may
cause
through
mis-identification
14
these
problems
reinforce
the
need
for
highquality
consensual
demographic
data
as
means
to
improve
ethical
norms
and
defend
against
adversarial
ml
attacks
acknowledgments
funding
support
this
work
was
supported
by
2019
sloan
fellowship
and
nsf
grants
iis-1553088
and
iis-1910064
any
opinions
findings
conclusions
or
recommendations
expressed
herein
are
those
of
the
authors
and
do
not
necessarily
reflect
the
views
of
the
funders
the
legal
department
of
google
participated
in
the
review
and
approval
of
the
manuscript
they
asked
that
the
authors
add
additional
examples
of
image
search
engines
besides
google
image
search
and
we
complied
aside
from
the
authors
google
had
no
role
in
the
design
and
conduct
of
the
study
access
and
collection
of
data
analysis
and
interpretation
of
data
or
preparation
of
the
manuscript
the
authors
thank
jane
adams
michael
davinroy
jeffrey
gleason
and
the
anonymous
reviewers
for
their
comments
references
116th
congress
2019
2020
2231
algorithmic
accountability
act
of
2019
https://www.congress.gov/bill/116th-congress/house-bill/2231.
dzifa
adjaye-gbewonyo
robert
bednarczyk
robert
davis
and
saad
omer
2014
using
the
bayesian
improved
surname
geocoding
method
bisg
to
create
working
classification
of
race
and
ethnicity
in
diverse
managed
care
population
validation
study
health
services
research
49
2014
268
283
alekh
agarwal
miroslav
dudík
and
zhiwei
steven
wu
2019
fair
regression
quantitative
definitions
and
reduction-based
algorithms
arxiv
preprint
arxiv
1905.12843
2019
facebook
ai
2021
how
we
re
using
fairness
flow
to
help
build
ai
that
works
better
for
everyone
facebook
ai
https://ai.facebook.com/blog/how-wereusing-fairness-flow-to-help-build-ai-that-works-better-for-everyone/.
naveed
akhtar
jian
liu
and
ajmal
mian
2018
defense
against
universal
adversarial
perturbations
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
3389
3398
mckane
andrus
elena
spitzer
jeffrey
brown
and
alice
xiang
2021
what
we
can
measure
we
can
understand
challenges
to
demographic
data
procurement
in
the
pursuit
of
fairness
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
virtual
event
canada
facct
21
association
for
computing
machinery
new
york
ny
usa
249
260
https
doi
org
10.1145
3442188.3445888
julia
angwin
jeff
larson
surya
mattu
and
lauren
kirchner
2019
machine
bias
there
software
used
across
the
country
to
predict
future
criminals
and
it
biased
against
blacks
2016
url
https://www.
propublica
org
article
machinebias-risk-assessments-in-criminal-sentencing
2019
anish
athalye
nicholas
carlini
and
david
wagner
2018
obfuscated
gradients
give
false
sense
of
security
circumventing
defenses
to
adversarial
examples
in
international
conference
on
machine
learning
pmlr
274
283
solon
barocas
and
andrew
selbst
2016
big
data
disparate
impact
calif
rev
104
2016
671
10
sid
basu
ruthie
berman
adam
bloomston
john
campbell
anne
diaz
nanako
era
benjamin
evans
sukhada
palkar
and
skyler
wharton
2020
measuring
discrepancies
in
airbnb
guest
acceptance
rates
using
anonymized
demographic
data
airbnb
https://news.airbnb.com/wp-content/uploads/sites/4/2020/06/
project-lighthouse-airbnb-2020-06-12
pdf
11
thorsten
beck
patrick
behr
and
andreas
madestam
2018
sex
and
credit
is
there
gender
bias
in
lending
journal
of
banking
and
finance
87
2018
647
12
melika
behjati
seyed-mohsen
moosavi-dezfooli
mahdieh
soleymani
baghshah
and
pascal
frossard
2019
universal
adversarial
attacks
on
text
classifiers
in
icassp
2019
2019
ieee
international
conference
on
acoustics
speech
and
signal
processing
icassp
ieee
7345
7349
13
rachel
ke
bellamy
kuntal
dey
michael
hind
samuel
hoffman
stephanie
houde
kalapriya
kannan
pranay
lohia
jacquelyn
martino
sameep
mehta
aleksandra
mojsilović
et
al
2019
ai
fairness
360
an
extensible
toolkit
for
detecting
and
mitigating
algorithmic
bias
ibm
journal
of
research
and
development
63
2019
14
cynthia
bennett
cole
gleason
morgan
klaus
scheuerman
jeffrey
bigham
anhong
guo
and
alexandra
to
2021
it
complicated
negotiating
accessibility
and
mis
representation
in
image
descriptions
of
race
gender
and
disability
in
proceedings
of
the
2021
chi
conference
on
human
factors
in
computing
systems
yokohama
japan
chi
21
association
for
computing
machinery
new
york
ny
usa
article
375
19
pages
https://doi.org/10.1145/3411764.3445498
15
richard
berk
hoda
heidari
shahin
jabbari
matthew
joseph
michael
kearns
jamie
morgenstern
seth
neel
and
aaron
roth
2017
convex
framework
for
fair
regression
arxiv
preprint
arxiv
1706.02409
2017
16
alex
beutel
jilin
chen
tulsee
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
and
cristos
goodrow
2019
fairness
in
recommendation
ranking
through
pairwise
comparisons
in
kdd
https
arxiv
org
pdf
1903.00780
pdf
17
miranda
bogen
aaron
rieke
and
shazeda
ahmed
2020
awareness
in
practice
tensions
in
access
to
sensitive
attribute
data
for
antidiscrimination
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
492
500
18
tolga
bolukbasi
kai-wei
chang
james
zou
venkatesh
saligrama
and
adam
kalai
2016
man
is
to
computer
programmer
as
woman
is
to
homemaker
debiasing
word
embeddings
in
advances
in
neural
information
processing
systems
4349
4357
19
tom
brown
dandelion
mané
aurko
roy
martín
abadi
and
justin
gilmer
2017
adversarial
patch
arxiv
preprint
arxiv
1712.09665
2017
20
marc-etienne
brunet
colleen
alkalay-houlihan
ashton
anderson
and
richard
zemel
2019
understanding
the
origins
of
bias
in
word
embeddings
in
international
conference
on
machine
learning
803
811
21
joy
buolamwini
and
timnit
gebru
2018
gender
shades
intersectional
accuracy
disparities
in
commercial
gender
classification
in
conference
on
fairness
accountability
and
transparency
pmlr
77
91
22
consumer
financial
protection
bureau
2014
using
publicly
available
information
to
proxy
for
unidentified
race
and
ethnicity
report
available
at
https
files
consumerfinance
gov
201409_cfpb_report_proxy-methodology
pdf
2014
23
jaime
carbonell
and
jade
goldstein
1998
the
use
of
mmr
diversity-based
reranking
for
reordering
documents
and
producing
summaries
in
proceedings
of
the
21st
annual
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
335
336
24
nicholas
carlini
and
david
wagner
2017
towards
evaluating
the
robustness
of
neural
networks
in
2017
ieee
symposium
on
security
and
privacy
sp
ieee
39
57
25
elisa
celis
lingxiao
huang
vijay
keswani
and
nisheeth
vishnoi
2021
fair
classification
with
noisy
protected
attributes
framework
with
provable
guarantees
in
international
conference
on
machine
learning
pmlr
1349
1361
26
elisa
celis
and
vijay
keswani
2020
implicit
diversity
in
image
summarization
proceedings
of
the
acm
on
human-computer
interaction
cscw2
2020
28
27
elisa
celis
anay
mehrotra
and
nisheeth
vishnoi
2021
fair
classification
with
adversarial
perturbations
arxiv
preprint
arxiv
2106.05964
2021
28
elisa
celis
damian
straszak
and
nisheeth
vishnoi
2018
ranking
with
fairness
constraints
in
45th
international
colloquium
on
automata
languages
and
programming
icalp
2018
schloss
dagstuhl-leibniz-zentrum
fuer
informatik
29
hongyan
chang
ta
duy
nguyen
sasi
kumar
murakonda
ehsan
kazemi
and
reza
shokri
2020
on
adversarial
bias
and
the
robustness
of
fair
machine
learning
arxiv
preprint
arxiv
2006.08669
2020
30
european
commission
proposal
for
regulation
laying
down
harmonised
rules
on
artificial
intelligence
artificial
intelligence
act
https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-layingdown-harmonised-rules-artificial-intelligence-artificial-intelligence.
31
hanjun
dai
hui
li
tian
tian
xin
huang
lin
wang
jun
zhu
and
le
song
2018
adversarial
attack
on
graph
structured
data
in
international
conference
on
machine
learning
pmlr
1115
1124
32
tamara
denning
batya
friedman
and
tadayoshi
kohno
2013
the
security
cards
security
threat
brainstorming
toolkit
university
of
washington
https://securitycards.cs.washington.edu/.
33
uk
office
for
artificial
intelligence
ethics
transparency
and
accountability
framework
for
automated
decision-making
https://www.gov.uk/government/publications/ethics-transparency-andaccountability-framework-for-automated-decision-making.
34
batya
friedman
and
david
hendry
2019
value
sensitive
design
shaping
technology
with
moral
imagination
mit
press
facct
22
june
21
24
2022
seoul
republic
of
korea
avijit
ghosh
matthew
jagielski
and
christo
wilson
35
yaroslav
ganin
evgeniya
ustinova
hana
ajakan
pascal
germain
hugo
larochelle
françois
laviolette
mario
marchand
and
victor
lempitsky
2016
domain-adversarial
training
of
neural
networks
the
journal
of
machine
learning
research
17
2016
2096
2030
36
nikhil
garg
londa
schiebinger
dan
jurafsky
and
james
zou
2018
word
embeddings
quantify
100
years
of
gender
and
ethnic
stereotypes
proceedings
of
the
national
academy
of
sciences
115
16
2018
e3635
e3644
37
gregor
geigle
jonas
pfeiffer
nils
reimers
ivan
vulić
and
iryna
gurevych
2021
retrieve
fast
rerank
smart
cooperative
and
joint
approaches
for
improved
cross-modal
retrieval
arxiv
preprint
abs
2103.11920
2021
arxiv
2103.11920
http://arxiv.org/abs/2103.11920
38
sahin
cem
geyik
stuart
ambler
and
krishnaram
kenthapadi
2019
fairnessaware
ranking
in
search
recommendation
systems
with
application
to
linkedin
talent
search
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
2221
2231
39
avijit
ghosh
ritam
dutt
and
christo
wilson
2021
when
fair
ranking
meets
uncertain
inference
association
for
computing
machinery
new
york
ny
usa
1033
1043
https://doi.org/10.1145/3404835.3462850
40
avijit
ghosh
lea
genuit
and
mary
reagan
2021
characterizing
intersectional
group
fairness
with
worst-case
comparisons
in
proceedings
of
2nd
workshop
on
diversity
in
artificial
intelligence
aidbei
proceedings
of
machine
learning
research
vol
142
deepti
lamba
and
william
hsu
eds
pmlr
22
34
https://proceedings.mlr.press/v142/ghosh21a.html
41
william
gies
james
overby
nick
saraceno
jordan
frome
emily
york
and
ahmad
salman
2020
restricting
data
sharing
and
collection
of
facial
recognition
data
by
the
consent
of
the
user
systems
analysis
in
2020
systems
and
information
engineering
design
symposium
sieds
https
doi
org
10.1109
sieds49339
2020.9106661
42
naman
goel
mohammad
yaghini
and
boi
faltings
2018
non-discriminatory
machine
learning
through
convex
fairness
criteria
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
43
ian
goodfellow
jonathon
shlens
and
christian
szegedy
2014
explaining
and
harnessing
adversarial
examples
arxiv
preprint
arxiv
1412.6572
2014
44
danny
goodwin
2011
top
google
result
gets
36.4
of
clicks
study
search
engine
watch
https://www.searchenginewatch.com/2011/04/21/top-googleresult-gets-36-4-of-clicks-study/.
45
kaiming
he
xiangyu
zhang
shaoqing
ren
and
jian
sun
2016
deep
residual
learning
for
image
recognition
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
770
778
46
xiangnan
he
zhankui
he
xiaoyu
du
and
tat-seng
chua
2018
adversarial
personalized
ranking
for
recommendation
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
355
364
47
lingxiao
huang
and
nisheeth
vishnoi
2019
stable
and
fair
classification
arxiv
preprint
arxiv
1902.07823
2019
48
matthew
jagielski
giorgio
severi
niklas
pousette
harger
and
alina
oprea
2020
subpopulation
data
poisoning
attacks
arxiv
preprint
arxiv
2006.14026
2020
49
kalervo
järvelin
and
jaana
kekäläinen
2002
cumulated
gain-based
evaluation
of
ir
techniques
acm
transactions
on
information
systems
tois
20
2002
422
446
50
toshihiro
kamishima
shotaro
akaho
hideki
asoh
and
jun
sakuma
2012
fairness-aware
classifier
with
prejudice
remover
regularizer
in
joint
european
conference
on
machine
learning
and
knowledge
discovery
in
databases
springer
35
50
51
chen
karako
and
putra
manggala
2018
using
image
fairness
representations
in
diversity-based
re-ranking
for
recommendations
in
adjunct
publication
of
the
26th
conference
on
user
modeling
adaptation
and
personalization
23
28
52
kimmo
karkkainen
and
jungseock
joo
2021
fairface
face
attribute
dataset
for
balanced
race
gender
and
age
for
bias
measurement
and
mitigation
in
proceedings
of
the
ieee
cvf
winter
conference
on
applications
of
computer
vision
1548
1558
53
alistair
knott
moving
towards
responsible
government
use
of
ai
in
new
zealand
https://digitaltechitp.nz/2021/03/22/moving-towardsresponsible-government-use-of-ai-in-new-zealand/.
54
alexey
kurakin
ian
goodfellow
and
samy
bengio
2016
adversarial
machine
learning
at
scale
arxiv
preprint
arxiv
1611.01236
2016
55
preethi
lahoti
alex
beutel
jilin
chen
kang
lee
flavien
prost
nithum
thain
xuezhi
wang
and
ed
chi
2020
fairness
without
demographics
through
adversarially
reweighted
learning
2020
56
kristina
lerman
anon
plangprasopchok
and
chio
wong
2007
personalizing
image
search
results
on
flickr
intelligent
information
personalization
2007
57
jie
li
rongrong
ji
hong
liu
xiaopeng
hong
yue
gao
and
qi
tian
2019
universal
perturbation
attack
against
image
retrieval
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
4899
4908
58
xiujun
li
xi
yin
chunyuan
li
pengchuan
zhang
xiaowei
hu
lei
zhang
lijuan
wang
houdong
hu
li
dong
furu
wei
et
al
2020
oscar
objectsemantics
aligned
pre-training
for
vision-language
tasks
in
european
conference
on
computer
vision
springer
121
137
648
59
tsung-yi
lin
michael
maire
serge
belongie
james
hays
pietro
perona
deva
ramanan
piotr
dollár
and
lawrence
zitnick
2014
microsoft
coco
common
objects
in
context
in
european
conference
on
computer
vision
springer
740
755
60
yanpei
liu
xinyun
chen
chang
liu
and
dawn
song
2016
delving
into
transferable
adversarial
examples
and
black-box
attacks
arxiv
preprint
arxiv
1611.02770
2016
61
zhuoran
liu
zhengyu
zhao
and
martha
larson
2019
who
afraid
of
adversarial
queries
the
impact
of
image
modifications
on
content-based
image
retrieval
in
proceedings
of
the
2019
on
international
conference
on
multimedia
retrieval
ottawa
on
canada
icmr
19
association
for
computing
machinery
new
york
ny
usa
306
314
https://doi.org/10.1145/3323873.3325052
62
joshua
loftus
chris
russell
matt
kusner
and
ricardo
silva
2018
causal
reasoning
for
algorithmic
fairness
arxiv
preprint
arxiv
1805.05859
2018
63
jiasen
lu
dhruv
batra
devi
parikh
and
stefan
lee
2019
vilbert
pretraining
task-agnostic
visiolinguistic
representations
for
vision-and-language
tasks
arxiv
preprint
arxiv
1908.02265
2019
64
ninareh
mehrabi
muhammad
naveed
fred
morstatter
and
aram
galstyan
2020
exacerbating
algorithmic
bias
through
fairness
attacks
arxiv
preprint
arxiv
2012.08723
2020
65
aditya
krishna
menon
and
robert
williamson
2018
the
cost
of
fairness
in
binary
classification
in
conference
on
fairness
accountability
and
transparency
107
118
66
seyed-mohsen
moosavi-dezfooli
alhussein
fawzi
omar
fawzi
and
pascal
frossard
2017
universal
adversarial
perturbations
arxiv
1610.08401
cs
cv
67
seyed-mohsen
moosavi-dezfooli
alhussein
fawzi
and
pascal
frossard
2016
deepfool
simple
and
accurate
method
to
fool
deep
neural
networks
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
2574
2582
68
marco
morik
ashudeep
singh
jessica
hong
and
thorsten
joachims
2020
controlling
fairness
and
bias
in
dynamic
learning-to-rank
arxiv
preprint
arxiv
2005.14713
2020
69
ankan
mullick
sayan
ghosh
ritam
dutt
avijit
ghosh
and
abhijnan
chakraborty
2019
public
sphere
2.0
targeted
commenting
in
online
news
media
in
european
conference
on
information
retrieval
springer
180
187
70
razieh
nabi
and
ilya
shpitser
2018
fair
inference
on
outcomes
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
aaai
conference
on
artificial
intelligence
vol
2018
nih
public
access
1931
71
vedant
nanda
samuel
dooley
sahil
singla
soheil
feizi
and
john
dickerson
2021
fairness
through
robustness
investigating
robustness
disparity
in
deep
learning
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
466
477
72
jakob
nielsen
2003
usability
101
introduction
to
usability
jakob
nielsen
alertbox
73
government
of
canada
responsible
use
of
artificial
intelligence
ai
https://www.canada.ca/en/government/system/digital-government/
digital-government-innovations
responsible-use-ai
html
74
nicolas
papernot
fartash
faghri
nicholas
carlini
ian
goodfellow
reuben
feinman
alexey
kurakin
cihang
xie
yash
sharma
tom
brown
aurko
roy
et
al
2016
technical
report
on
the
cleverhans
v2
1.0
adversarial
examples
library
arxiv
preprint
arxiv
1610.00768
2016
75
omid
poursaeed
isay
katsman
bicheng
gao
and
serge
belongie
2018
generative
adversarial
perturbations
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
4422
4431
76
inioluwa
deborah
raji
andrew
smart
rebecca
white
margaret
mitchell
timnit
gebru
ben
hutchinson
jamila
smith-loud
daniel
theron
and
parker
barnes
2020
closing
the
ai
accountability
gap
defining
an
end-to-end
framework
for
internal
algorithmic
auditing
in
proc
of
fat
77
nisarg
raval
and
manisha
verma
2020
one
word
at
time
adversarial
attacks
on
retrieval
models
arxiv
preprint
arxiv
2008.02197
2020
78
shaoqing
ren
kaiming
he
ross
girshick
and
jian
sun
2015
faster
r-cnn
towards
real-time
object
detection
with
region
proposal
networks
advances
in
neural
information
processing
systems
28
2015
91
99
79
alexey
romanov
maria
de-arteaga
hanna
wallach
jennifer
chayes
christian
borgs
alexandra
chouldechova
sahin
geyik
krishnaram
kenthapadi
anna
rumshisky
and
adam
tauman
kalai
2019
what
in
name
reducing
bias
in
bios
without
access
to
protected
attributes
arxiv
preprint
arxiv
1904.05233
2019
80
piotr
sapiezynski
wesley
zeng
ronald
robertson
alan
mislove
and
christo
wilson
2019
quantifying
the
impact
of
user
attentionon
fair
group
representation
in
ranked
lists
in
companion
proceedings
of
the
2019
world
wide
web
conference
553
562
81
morgan
klaus
scheuerman
kandrea
wade
caitlin
lustig
and
jed
brubaker
2020
how
we
ve
taught
algorithms
to
see
identity
constructing
race
and
gender
in
image
databases
for
facial
analysis
cscw1
article
058
may
2020
35
pages
https://doi.org/10.1145/3392866
82
sefik
ilkin
serengil
and
alper
ozpinar
2020
lightface
hybrid
deep
face
recognition
framework
in
2020
innovations
in
intelligent
systems
and
applications
conference
asyu
ieee
subverting
fair
image
search
with
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
of
korea
83
ali
shafahi
ronny
huang
mahyar
najibi
octavian
suciu
christoph
studer
tudor
dumitras
and
tom
goldstein
2018
poison
frogs
targeted
clean-label
poisoning
attacks
on
neural
networks
arxiv
preprint
arxiv
1804.00792
2018
84
ali
shafahi
mahyar
najibi
amin
ghiasi
zheng
xu
john
dickerson
christoph
studer
larry
davis
gavin
taylor
and
tom
goldstein
2019
adversarial
training
for
free
arxiv
preprint
arxiv
1904.12843
2019
85
shawn
shan
emily
wenger
jiayun
zhang
huiying
li
haitao
zheng
and
ben
zhao
2020
fawkes
protecting
privacy
against
unauthorized
deep
learning
models
in
29th
usenix
security
symposium
usenix
security
20
1589
1604
86
ashudeep
singh
and
thorsten
joachims
2018
fairness
of
exposure
in
rankings
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
2219
2228
87
david
solans
battista
biggio
and
carlos
castillo
2020
poisoning
attacks
on
algorithmic
fairness
arxiv
preprint
arxiv
2004.07401
2020
88
christian
szegedy
wei
liu
yangqing
jia
pierre
sermanet
scott
reed
dragomir
anguelov
dumitru
erhan
vincent
vanhoucke
and
andrew
rabinovich
2015
going
deeper
with
convolutions
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
89
christian
szegedy
wojciech
zaremba
ilya
sutskever
joan
bruna
dumitru
erhan
ian
goodfellow
and
rob
fergus
2013
intriguing
properties
of
neural
networks
arxiv
preprint
arxiv
1312.6199
2013
90
yaniv
taigman
ming
yang
marc
aurelio
ranzato
and
lior
wolf
2014
deepface
closing
the
gap
to
human-level
performance
in
face
verification
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
1701
1708
91
florian
tramer
nicholas
carlini
wieland
brendel
and
aleksander
madry
2020
on
adaptive
attacks
to
adversarial
example
defenses
arxiv
preprint
arxiv
2002.08347
2020
92
florian
tramèr
nicolas
papernot
ian
goodfellow
dan
boneh
and
patrick
mcdaniel
2017
the
space
of
transferable
adversarial
examples
arxiv
2017
https://arxiv.org/abs/1704.03453
93
alexander
turner
dimitris
tsipras
and
aleksander
madry
2018
clean-label
backdoor
attacks
2018
94
sriram
vasudevan
and
krishnaram
kenthapadi
2020
lift
scalable
framework
for
measuring
fairness
in
ml
applications
in
proceedings
of
the
29th
acm
international
conference
on
information
knowledge
management
2773
2780
95
yevgeniy
vorobeychik
and
murat
kantarcioglu
2018
adversarial
machine
learning
synthesis
lectures
on
artificial
intelligence
and
machine
learning
12
2018
169
96
christo
wilson
avijit
ghosh
shan
jiang
alan
mislove
lewis
baker
janelle
szary
kelly
trindel
and
frida
polli
2021
building
and
auditing
fair
algorithms
case
study
in
candidate
screening
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
666
677
97
zuxuan
wu
ser-nam
lim
larry
davis
and
tom
goldstein
2020
making
an
invisibility
cloak
real
world
adversarial
attacks
on
object
detectors
in
european
conference
on
computer
vision
springer
17
98
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
and
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
in
proceedings
of
the
2017
acm
on
conference
on
information
and
knowledge
management
1569
1578
99
meike
zehlike
and
carlos
castillo
2020
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
in
proceedings
of
the
web
conference
2020
2849
2855
100
dora
zhao
angelina
wang
and
olga
russakovsky
2021
understanding
and
evaluating
racial
biases
in
image
captioning
in
international
conference
on
computer
vision
iccv
101
mo
zhou
zhenxing
niu
le
wang
qilin
zhang
and
gang
hua
2020
adversarial
ranking
attack
and
defense
arxiv
preprint
arxiv
2002.11293
2020
supplementary
material
comparison
between
detconstsort
and
fmmr
in
this
section
we
compare
the
performance
of
two
fair
re-rankers
in
the
presence
of
our
gap
attack
we
have
already
described
the
details
of
the
first
algorithm
fmmr
in
3.2
1.1
detconstsort
the
second
algorithm
detconstsort
38
was
developed
by
and
is
currently
deployed
at
linkedin
in
their
talent
search
system
unlike
fmmr
detconstsort
requires
access
to
the
demographic
labels
of
the
items
it
is
trying
to
fairly
re-rank
detconstsort
rearranges
given
list
of
items
such
that
for
any
particular
rank
and
for
any
attribute
the
attribute
is
present
at
least
pa
times
in
the
ranked
list
where
pa
is
the
proportion
of
items
in
the
list
that
have
the
attribute
detconstsort
also
re-sorts
the
items
within
the
relevance
criteria
so
that
items
with
better
utility
scores
are
placed
higher
in
the
ranked
list
as
much
as
possible
while
maintaining
the
desired
attribute
ratio
it
thus
aims
to
solve
deterministic
interval
constrained
sorting
problem
if
ground-truth
demographic
labels
are
unavailable
detconstsort
may
instead
utilize
labels
sourced
from
demographic
inference
model
recent
work
however
has
shown
that
detconstsort
is
sensitive
to
errors
in
demographic
labels
with
one
example
of
such
errors
being
inaccurate
inferences
39
1.2
evaluation
results
we
present
the
results
of
our
gap
attacks
against
our
search
engine
when
it
uses
detconstsort
and
fmmr
as
the
fair
re-ranker
respectively
in
figure
as
in
these
results
are
averaged
across
three
queries
multiple
values
of
etc
for
detconstsort
the
skew
and
attention
metrics
are
not
impacted
by
our
attack
this
can
be
clearly
seen
by
comparing
the
values
when
pr
there
are
no
perturbed
images
to
other
values
of
pr
for
detconstsort
for
skew
and
attention
starts
high
unfair
when
pr
and
does
not
change
as
pr
increases
the
correct
interpretation
of
these
results
is
not
that
detconstsort
is
resilient
to
our
attack
rather
the
correct
interpretation
is
that
detconstsort
starts
off
unfair
due
to
the
use
of
inaccurate
inferred
demographic
data
39
and
our
attack
is
unable
to
make
the
unfairness
worse
thus
we
find
that
prerequisite
for
evaluating
the
success
of
our
attacks
on
detconstsort
is
an
accurate
demographic
inference
model
developing
such
models
is
still
an
active
area
of
research
and
is
out-of-scope
for
our
work
should
more
accurate
demographic
inference
model
be
designed
in
the
future
however
it
must
be
designed
with
adversarial
robustness
in
mind
to
prevent
our
attacks
choice
of
queries
to
facilitate
our
experiments
we
chose
to
select
search
query
terms
that
would
provide
sizeable
list
of
images
to
do
so
we
looked
at
the
list
of
terms
in
the
coco
image
captions
excluding
english
stop
words
and
words
related
to
ethnicity
or
gender
the
following
table
shows
some
top
terms
from
this
information
we
composed
our
three
queries
given
that
sitting
tennis
table
person
pizza
etc
were
among
the
most
popular
terms
649
facct
22
june
21
24
2022
seoul
republic
of
korea
avijit
ghosh
matthew
jagielski
and
christo
wilson
60
12
10
0.2
0.5
0.7
0.1
40
30
20
10
1.0
0.0
50
change
ndcg
attention
light
men
skew
light
men
14
0.2
0.3
0.4
0.5
0.2
attack
probability
0.5
0.7
1.0
0.2
attack
probability
skew
attention
deepface
0.5
0.7
1.0
attack
probability
ndcg
fairface
figure
gap
models
trained
on
different
demographic
inference
algorithms
offer
similar
attack
effectiveness
60
30
25
0.0
0.2
0.5
0.7
attack
probability
skew
1.0
80
0.2
change
ndcg
attention
light
men
skew
light
men
150
120
90
60
40
20
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.5
0.7
1.0
attack
probability
0.2
0.5
0.7
attack
probability
attention
detconstsort
0.0
ndcg
fmmr
figure
detconstsort
has
poor
performance
even
without
an
attack
making
our
results
uninteresting
term
count
sitting
standing
people
holding
large
person
street
table
small
tennis
riding
train
young
red
baseball
pizza
55084
44121
42133
29055
25305
25123
21609
20775
20661
19718
18809
18287
17767
17522
15362
11163
table
the
most
common
gender
or
race
unrelated
caption
terms
in
the
evaluation
dataset
650
1.0