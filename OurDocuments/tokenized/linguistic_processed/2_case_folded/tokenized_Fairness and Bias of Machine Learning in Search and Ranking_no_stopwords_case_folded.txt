santa
clara
university
scholar
commons
engineering
ph
theses
student
scholarship
2024
fairness
bias
machine
learning
search
ranking
yuan
wang
follow
additional
works
https://scholarcommons.scu.edu/eng_phd_theses
fairness
bias
machine
learning
search
ranking
yuan
wang
dissertation
submitted
partial
fulﬁllment
requirements
degree
doctor
philosophy
computer
science
engineering
school
engineering
santa
clara
university
2024
santa
clara
california
dedicated
family
iii
acknowledgements
first
foremost
extend
deepest
gratitude
advisor
professor
yi
fang
whose
unwavering
guidance
support
instrumental
doctoral
journey
professor
fang
believed
potential
also
supported
patience
kindness
unmatched
dedication
excellence
mentorship
transcended
academic
instruction
oﬀering
personal
support
invaluable
life
lessons
shaped
scholar
individual
teaches
keep
trying
curious
work
hard
want
keep
things
job
learning
really
special
chance
thankful
like
thank
doctoral
committee
consisting
prof
zhiqiang
tao
prof
david
anastasiu
prof
sean
choi
prof
haibing
lu
time
suggestions
make
thesis
better
like
thank
lab
mates
travis
ebesu
xuyang
wu
zhiyuan
peng
suthee
chaidaroon
supported
diﬀerent
parts
journey
lastly
like
thank
family
parents
brother
given
endless
love
support
throughout
entire
journey
also
want
thank
ﬁancée
yijia
love
patience
iv
fairness
bias
machine
learning
search
ranking
yuan
wang
department
computer
science
engineering
santa
clara
university
santa
clara
california
2024
abstract
recent
advancements
information
retrieval
ir
machine
learning
significantly
improved
ranking
search
system
performance
however
data-driven
approaches
often
suﬀer
inherent
biases
present
training
datasets
leading
unfair
treatment
certain
demographic
groups
contributing
systematic
discrimination
based
race
gender
geographic
location
research
aims
address
fairness
bias
issue
ranking
search
systems
proposing
innovative
frameworks
mitigate
data
bias
ensure
equitable
representation
exposure
across
diverse
groups
introduce
two
novel
frameworks
meta-learning
based
fair
ranking
mfr
model
meta
curriculum-based
fair
ranking
mcfr
framework
designed
alleviate
dataset
bias
automatically-weighted
loss
functions
curriculum
learning
strategies
respectively
approaches
utilize
meta-learning
adjust
ranking
loss
focusing
particularly
improving
fairness
metrics
minority
groups
maintaining
competitive
ranking
performance
additionally
conduct
empirical
evaluation
large
language
models
llms
text-ranking
tasks
revealing
biases
handling
queries
documents
related
binary
protected
attributes
analysis
oﬀers
benchmark
assessing
llms
fairness
highlights
necessity
equitable
representation
search
outcomes
furthermore
explore
challenge
data
selection
bias
multi-stage
recommendation
systems
particularly
online
advertising
contexts
like
pinterest
multi-cascade
ads
ranking
system
comprehensive
experiments
assess
various
state-ofthe-art
methods
ﬁndings
demonstrate
eﬀectiveness
modiﬁed
version
unsupervised
domain
adaptation
muda
mitigating
selection
bias
collectively
work
contributes
development
fairer
ranking
search
systems
addressing
bias
source
employing
meta-learning
curriculum
learning
techniques
pave
way
equitable
transparent
ir
systems
serve
diverse
user
bases
without
discrimination
contents
acknowledgements
iv
abstract
contents
vii
list
figures
list
tables
xiii
introduction
1.1
motivation
1.2
overview
1.3
contributions
1.4
outline
16
16
18
19
21
related
work
2.1
fairness
ranking
2.2
meta-learning
fairness
2.3
fairness
llms
2.4
selection
bias
23
23
24
25
26
meta-learning
approach
fair
ranking
3.1
introduction
3.2
meta-learning
based
fair
ranking
3.3
experiments
3.4
conclusion
28
28
32
36
41
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
42
4.1
introduction
42
vii
contents
viii
4.2
meta
curriculum-based
fair
ranking
4.2
problem
setting
4.2
uniﬁed
mcfr
framework
4.2
parameter
update
4.2
ranking
fairness
loss
4.2
4.1
ranking
terms
4.2
4.2
fairness
terms
4.2
curriculum
sampling
experiments
4.3
experimental
setting
4.3
1.1
baselines
4.3
1.2
implementation
details
4.3
fair
ranking
performance
4.3
ablation
studies
4.3
3.1
ranking
terms
analysis
4.3
3.2
fairness
terms
analysis
4.3
3.3
curriculum
sampling
analysis
4.3
3.4
data
eﬃciency
4.3
3.5
training
inference
eﬃciency
conclusion
47
48
48
51
53
53
54
55
59
59
61
62
63
65
66
67
68
69
69
70
empirical
study
fairness
llms
rankers
5.1
introduction
5.2
llm
fair
ranking
5.2
datasets
5.2
listwise
evaluation
5.2
2.1
data
construction
5.2
2.2
metrics
5.2
pairwise
evaluation
5.2
3.1
data
construction
5.2
3.2
metrics
5.3
results
analysis
5.3
0.1
eﬀect
window
step
size
5.3
listwise
evaluation
results
5.3
1.1
item-side
analysis
5.3
1.2
query-side
analysis
5.3
pairwise
evaluation
results
5.3
overall
evaluation
5.4
enhancing
fairness
lora
5.5
conclusion
71
71
74
75
75
76
77
78
79
79
79
80
80
82
84
85
86
87
88
empirical
study
selection
bias
pinterest
ads
retrieval
90
4.3
4.4
contents
6.1
6.2
6.3
6.4
6.5
ix
introduction
90
bias
pinterest
ads
96
6.2
datasets
training
pipeline
96
6.2
selection
bias
98
6.2
problem
formulation
99
solution
100
6.3
naive
method
binary
classiﬁcation
100
6.3
in-batch
negative
classiﬁcation
100
6.3
knowledge
distillation
101
6.3
transfer
learning
101
6.3
adversarial
regularization
102
6.3
unsupervised
domain
adaptation
103
6.3
6.1
naive
uda
103
6.3
6.2
modiﬁed
uda
104
experiments
results
106
6.4
datasets
106
6.4
experimental
setting
107
6.4
evaluation
metrics
109
6.4
oﬄine
evaluation
110
6.4
online
experiments
111
6.4
5.1
overall
evaluation
111
6.4
5.2
evaluation
ads
objective
type
114
6.4
5.3
conversion
ads
116
6.4
variants
muda
117
conclusion
120
conclusion
121
bibliography
123
list
figures
3.1
3.2
3.3
4.1
4.2
illustration
predicted
rankings
distribution
protected
groups
female
students
african
american
students
two
diﬀerent
datasets
report
kendall
tau
ranking
metric
proposed
mfr
model
ranks
items
protected
groups
higher
compared
listnet
17
indicates
mfr
improves
protected
attribute
exposure
unbiased
ranking
performance
29
mfr
learning
algorithm
ﬂowchart
steps
algorithm
note
ranking
model
meta-learner
batch
size
training
dataset
batch
size
meta-dataset
learning
rates
iteration
ﬁrstly
update
meta-learner
using
eq
meta-dataset
update
ranking
model
using
eq
training
dataset
32
plot
variation
learned
weight
two
training
datasets
weight
diﬀerence
computed
φdiﬀ
φi
φi
plot
φdiﬀ
training
epochs
shown
plot
weighting
function
converging
diﬀerent
values
weights
epoch
decreasing
0.0
40
illustration
predicted
rankings
distribution
two
protected
attributes
four
datasets
law
student
gender
82
law
student
race
82
compas
engineering
student
89
report
kendall
tau
48
ranking
performance
mcfr
mfr
80
improve
protected
attributes
ranking
realizing
competitive
ranking
performance
compared
listnet
17
demonstrating
approach
increase
exposure
minority
43
mcfr
learning
algorithm
ﬂowchart
steps
algorithm
note
ranking
model
meta
learner
batch
size
training
dataset
batch
size
meta-dataset
learning
rates
iteration
ﬁrstly
update
meta
learner
using
eq
meta-dataset
sampled
curriculum
sampling
update
sampling
diﬃculty
epoch
update
ranking
model
using
eq
training
dataset
47
list
figures
4.3
4.4
5.1
5.2
5.3
5.4
6.1
xi
curriculum
sampling
strategy
illustrated
engineering
student
gender
dataset
use
ratio
unprotected
group
protected
group
meta-dataset
training
dataset
beginning
training
epoch
gradually
decrease
ratio
training
epoch
increase
ratio
becomes
shows
balanced
metadataset
55
evaluation
results
down-sampling
experiments
conduct
experiment
law
students
gender
law
students
race
datasets
down-sample
training
data
rate
0.1
0.9
results
show
mcfr
better
data
eﬃciency
achieve
better
fairness
metrics
similar
ranking
performance
mfr
autodebias
diﬀerent
down-sampling
rate
68
illustration
two
evaluation
methods
listwise
evaluation
pairwise
evaluation
document
associated
binary
protected
attribute
used
fairness
evaluation
metrics
proposed
evaluation
framework
schematic
diagram
represents
dual
evaluation
methodology
top
sequence
depicts
listwise
ranking
process
items
protected
unprotected
groups
presented
various
llms
gpt-3
gpt-4
mistral-7b
llama2
evaluated
utility
group
exposure
metrics
bottom
sequence
illustrates
pairwise
ranking
approach
contrasts
ranking
preference
llms
items
protected
unprotected
groups
quantifying
bias
percentage
unprotected
group
items
ranked
higher
predicted
rankings
distribution
protected
groups
trec
datasets
using
listwise
evaluation
plots
reveal
ranking
variability
potential
biases
gender
geographic
attributes
highlighting
areas
improvement
fairness
across
llms
impact
lora
fine-tuning
mistral-7b
fairness
figure
shows
percentage
ﬁrst-ranked
items
protected
unprotected
groups
figure
demonstrates
resulting
fairness
ratios
loraadjusted
model
yields
ratios
closer
ideal
fairness
benchmark
1.0
across
trec
datasets
72
74
81
87
life
cycle
online
ads
delivery
high
level
ads
request
triggered
user
opens
pinterest
app
starts
new
session
ads
request
will
sent
ads
delivery
system
query
dozen
ads
ads
delivery
backend
ad
candidates
inventory
will
ﬂow
various
stages
like
targeting
retrieval
ranking
auction
sends
auction
winners
back
mobile
app
selected
ads
will
visible
user
91
list
figures
6.2
xii
distribution
features
labels
across
three
ads
datasets
related
retrieval
modeling
shows
ﬂow
major
ad
candidates
along
ads
delivery
funnel
shows
distribution
empirical
vtcvr
one
key
retrieval
model
features
across
three
datasets
retrieval
training
serving
shows
distribution
empirical
good
click
rate
one
key
retrieval
model
features
across
three
datasets
retrieval
training
serving
shows
distribution
ranking
model
predictions
used
pseudo
label
retrieval
model
training
across
three
datasets
note
exact
values
x-axes
hidden
conﬁdentiality
reasons
95
list
tables
3.1
experimental
results
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
bold
text
indicates
model
best
performance
results
show
mfr
model
better
fairness
metrics
comparable
performance
ranking
metrics
state-of-the-art
models
39
4.1
summary
ranking
fairness
terms
used
loss
function
loss
function
used
framework
γu
can
insert
exposure
terms
ranking
loss
terms
needed
note
denotes
number
candidates
per
query
summary
dataset
statistics
report
average
counts
total
unprotected
items
per
query
w3c
experts
engineering
students
datasets
provide
exact
item
counts
law
students
compas
datasets
contains
one
query
experimental
results
hinge
exposure
89
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
bold
text
indicates
model
best
performance
results
show
mcfr
model
better
fairness
metrics
comparable
performance
ranking
metrics
state-of-the-art
models
ablation
study
results
rankmse
11
ablation
study
results
ranknet
15
ablation
study
results
listnet
17
experimental
results
total
convergence
time
seconds
shows
total
convergence
time
diﬀerent
algorithms
deltr
mfr
mcfr
across
various
datasets
scenarios
based
table
mcfr
framework
generally
comparable
convergence
time
two
algorithms
4.2
4.3
4.4
4.5
4.6
4.7
xiii
51
58
64
66
66
67
69
list
tables
5.1
5.2
5.3
6.1
6.2
6.3
6.4
6.5
xiv
evaluation
results
diﬀerent
choices
window
step
sizes
results
show
signiﬁcant
diﬀerences
ranking
fairness
metrics
select
window
size
step
size
listwise
evaluation
experiments
80
listwise
evaluation
results
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
closer
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
precision
10
10
scores
indicate
better
performance
82
pairwise
evaluation
results
table
displays
fairness
metrics
llms
ranking
relevant
irrelevant
item
pairs
one
protected
unprotected
groups
includes
percentages
items
ranked
ﬁrst
group
ratio
reﬂecting
fairness
varying
levels
fairness
across
llms
particularly
irrelevant
pairings
highlight
importance
enhancing
fairness
llms
85
auc-roc
evaluation
dataset
models
knowledge
distillation
adversarial
learning
binary
classiﬁcation
trained
auction
winners
dataset
usually
better
oﬄine
evaluation
results
110
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
various
models
types
ads
in-batch
negative
knowledge
distillation
methods
improve
gctr30
cost
impression
drop
muda
method
recommend
ads
higher
quality
observed
increased
gctr30
without
impression
drop
112
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
two
promising
models
type
awareness
traﬃc
web-conversion
ads
in-batch
negative
classiﬁcation
model
works
better
traﬃc
ads
muda
model
helps
web-conversion
ads
114
online
metrics
performance
in-batch
negative
classiﬁcation
muda
models
web-conversion
ads
in-batch
negative
classiﬁcation
model
leads
lower
conversion
probability
ads
impression
icvr
thus
higher
cpa
cost
advertisers
contrast
muda
model
recommended
ad
candidates
higher
conversion
rate
therefore
lower
cpa
cost
116
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
various
muda
variants
types
ads
muda
v1
achieves
highest
gain
ads
engagement
ctr
gctr30
muda
v3
achieves
balanced
gain
across
diﬀerent
metrics
good
gctr30
impression
lift
117
list
tables
6.6
6.7
xv
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
muda
variants
type
awareness
traﬃc
web-conversion
ads
muda
v3
shows
best
balanced
impression
gains
among
118
online
lifts
ads
hide
rate
hdr
re-pin
rate
rpr
observed
muda
variants
types
ads
muda
v3
achieves
balanced
performance
fewer
ads
hidden
ads
repined
users
119
chapter
introduction
1.1
motivation
quest
fairness
information
retrieval
ir
systems
gaining
unprecedented
attention
digital
era
demands
equity
across
platforms
services
central
pursuit
challenge
mitigating
systematic
biases
within
data-driven
ranking
models
biases
often
reﬂection
historical
discrimination
manifest
unfair
treatment
towards
underrepresented
groups
leading
disparate
exposure
unequal
opportunities
various
real-world
applications
expert
search
job
recommendations
essence
fairness
ir
extends
beyond
mere
algorithmic
adjustments
ensuring
demographic
groups
equal
visibility
representation
outcomes
search
recommendation
systems
16
chapter
introduction
17
address
inherent
biases
datasets
used
training
machine
learning
models
developed
novel
frameworks
frameworks
including
meta-learning
based
fair
ranking
mfr
meta
curriculum-based
fair
ranking
mcfr
represent
signiﬁcant
strides
towards
achieving
equitable
treatment
across
protected
attributes
re-weighting
ranking
losses
incorporating
curriculum
learning
meta
dataset
construction
models
aim
balance
exposure
advantaged
disadvantaged
groups
oﬀering
nuanced
approach
fairness
transcends
traditional
mitigation
strategies
integration
large
language
models
llms
ranking
tasks
complicates
landscape
fairness
ir
despite
superior
performance
understanding
processing
natural
language
llms
immune
fairness
concerns
empirical
scrutiny
models
fairness
benchmarks
reveals
pressing
need
evaluate
ﬁne-tune
focus
equity
ensuring
deployment
perpetuate
existing
biases
lastly
realm
online
advertising
particularly
multi-stage
recommendation
systems
like
used
ad
retrieval
underscores
pervasive
challenge
selection
bias
area
might
seem
tangential
shares
core
issue
bias
mitigation
broader
ir
systems
eﬃciently
managing
diversity
quality
ads
upper
funnel
stages
without
succumbing
biases
crucial
maintaining
integrity
fairness
digital
advertising
ecosystems
summary
motivation
thesis
stems
urgent
need
address
chapter
introduction
18
rectify
fairness
issues
ir
systems
comprehensive
exploration
innovative
frameworks
meticulous
evaluation
llms
consideration
selection
bias
online
advertising
work
aims
contribute
meaningful
solutions
overarching
challenge
ensuring
fairness
digital
information
landscape
1.2
overview
thesis
presents
comprehensive
exploration
fairness
ranking
search
systems
addressing
multifaceted
challenge
bias
information
retrieval
ir
series
innovative
approaches
methodologies
across
four
distinct
interconnected
studies
delve
complexities
data
bias
selection
bias
ethical
implications
large
language
models
llms
text
ranking
providing
holistic
examination
fairness
digital
information
landscape
ﬁrstly
introduces
meta-learning
based
fair
ranking
mfr
model
advanced
framework
designed
mitigate
data
bias
re-weighting
ranking
losses
bilevel
optimization
process
model
enhances
fairness
metrics
also
maintains
competitive
ranking
performance
oﬀering
scalable
solution
equitable
ir
systems
building
foundation
proposes
meta
curriculum-based
fair
ranking
mcfr
framework
addresses
data
bias
integrating
in-processing
pre-processing
techniques
curriculum
learning
mcfr
demonstrates
remarkable
chapter
introduction
19
versatility
eﬀectiveness
improving
fairness
metrics
across
various
ranking
loss
functions
showcasing
potential
generic
framework
fair
ranking
focus
evaluation
fairness
llms
text
ranking
establishing
benchmark
incorporates
listwise
pairwise
evaluation
methods
focused
binary
protected
attributes
extensive
experimentation
reveal
inherent
fairness
issues
llms
propose
ﬁne-tuning
strategy
using
low-rank
adaptation
lora
mitigate
issues
marking
signiﬁcant
step
towards
equitable
llm-based
ranking
systems
finally
tackles
selection
bias
multi-cascade
advertisement
recommendation
systems
surveying
state-of-the-art
modeling
strategies
introducing
modiﬁed
unsupervised
domain
adaptation
muda
approach
muda
outperforms
contemporary
models
existing
production
model
online
settings
highlighting
eﬀectiveness
addressing
selection
bias
enhancing
fairness
eﬃciency
recommendation
systems
1.3
contributions
contribution
thesis
can
summarized
follows
introduce
meta-learning
based
fair
ranking
mfr
model
novel
approach
addresses
data
bias
ranking
systems
automatically
adjusting
ranking
losses
mfr
model
framed
bilevel
optimization
problem
chapter
introduction
20
solved
innovative
gradients-through-gradients
technique
demonstrates
robustness
eﬀectiveness
real-world
datasets
results
highlight
mfr
capacity
achieve
competitive
ranking
performance
signiﬁcantly
enhancing
fairness
metrics
marking
critical
advancement
pursuit
fair
information
retrieval
systems
present
meta
curriculum-based
fair
ranking
mcfr
framework
innovative
approach
mitigates
data
bias
blending
in-processing
preprocessing
techniques
curriculum
learning
mcfr
formulated
bilevel
optimization
problem
solved
via
gradients-through-gradients
proves
versatile
across
various
ranking
loss
functions
fairness
metrics
empirical
studies
across
public
datasets
aﬃrm
mcfr
eﬀectiveness
matching
existing
ranking
performances
signiﬁcantly
advancing
fairness
metrics
notably
mcfr
enhances
fairness
eﬃciently
requiring
less
data
achieving
fast
convergence
positioning
highly
adaptable
impactful
framework
promoting
fairness
ranking
systems
create
benchmark
evaluating
fairness
large
language
models
llms
text
ranking
focusing
binary
protected
attributes
listwise
pairwise
methods
extensive
experiments
real-world
datasets
reveal
fairness
issues
llms
prompting
us
propose
ﬁne-tuning
strategy
using
lowrank
adaptation
lora
speciﬁcally
designed
address
concerns
dual
approach
identifying
mitigating
fairness
problems
marks
signiﬁcant
advancement
improving
llms
performance
ranking
tasks
chapter
introduction
21
address
selection
bias
advertisement
recommendation
systems
characterizing
issue
evaluating
various
modeling
strategies
exploration
leads
development
modiﬁed
unsupervised
domain
adaptation
muda
approach
stands
superior
performance
online
settings
outperforming
contemporary
models
existing
production
model
study
advances
mitigation
selection
bias
showcasing
muda
eﬀectiveness
enhancing
recommendation
fairness
eﬃciency
1.4
outline
thesis
structured
follows
chapter
reviews
existing
literature
fairness
information
retrieval
highlighting
signiﬁcance
addressing
biases
ranking
models
evolving
strategies
mitigate
challenges
chapter
details
meta-learning
based
fair
ranking
mfr
model
focusing
innovative
approach
enhance
fairness
adjusting
training
losses
improved
minority
group
exposure
validation
real-world
datasets
chapter
discusses
meta
curriculumbased
fair
ranking
mcfr
framework
integrates
meta-learning
curriculum
learning
counteract
data
bias
showcases
eﬀectiveness
traditional
fairness
models
chapter
explores
fairness
large
language
models
ranking
tasks
presenting
empirical
study
biases
introducing
mitigation
strategy
via
lora
ﬁne-tuning
promote
equitable
outcomes
chapter
investigates
selection
bias
pinterest
advertising
system
proposes
modiﬁed
unsupervised
domain
22
adaptation
muda
model
demonstrating
capacity
improve
recommendation
performance
advertising
eﬃciency
chapter
concludes
thesis
summarizing
key
contributions
reﬂecting
impact
work
fairness
search
ranking
suggesting
future
research
directions
advance
ﬁeld
chapter
related
work
2.1
fairness
ranking
zehlike
et
al
92
categorized
fair
ranking
models
score-based
supervised
learning
models
score-based
models
modify
score
outcomes
distributions
enhanced
fairness
notable
contributions
include
works
yang
et
al
86
87
celis
et
al
18
stoyanovich
et
al
72
kleinberg
et
al
47
asudeh
et
al
supervised
fairness
models
ranking
span
pre-processing
in-processing
postprocessing
approaches
pre-processing
models
exempliﬁed
lahoti
et
al
49
work
deriving
fair
training
data
in-processing
models
zehlike
et
al
deltr
89
address
fairness
training
focusing
exposure
bias
similarly
beutel
et
al
introduced
pairwise
ranking
loss
function
fairness
regularizer
ma
et
al
52
23
chapter
related
work
24
tackled
fairness
query
generation
haak
et
al
39
aimed
search
query
bias
identiﬁcation
chu
et
al
23
highlighted
biases
neural
architecture
search
evaluations
importantly
chen
et
al
20
proposed
meta-learning-based
debiasing
framework
recommendations
post-processing
models
conversely
reﬁne
model
outputs
post-training
fairness
among
zehlike
et
al
works
90
91
like
fa
ir
ensure
representation
protected
groups
oﬀer
continuous
fairness
interpolation
additionally
biega
et
al
10
developed
algorithm
optimizing
equity
user
attention
relevance
loss
function
2.2
meta-learning
fairness
meta-learning
ﬁeld
study
aims
improve
learning
ability
models
adapting
new
tasks
environments
divided
two
main
categories
model-based
30
learning
algorithm-based
addition
tasks
fewshot
learning
43
continual
learning
60
hyperparameter
optimization
32
fairness
important
ﬁeld
zhao
et
al
98
presented
follow
fair
meta
leader
ffml
learns
online
fair
classiﬁcation
model
primal
delivering
accuracy
fairness
subsequent
work
zhao
et
al
97
emphasized
primal-dual
fair
meta-learning
targeting
optimal
initialization
base
model
weights
rapidly
adjust
new
fairness
tasks
advanced
research
96
creating
few-shot
discrimination
chapter
related
work
25
prevention
model
unbiased
multi-class
classiﬁcation
rooted
maml
framework
concurrently
slack
et
al
71
introduced
fair-maml
designed
derive
fair
models
minimal
data
emerging
tasks
model
like
zhao
built
upon
maml
framework
incorporates
fairness
regularization
speciﬁc
fairness
hyperparameter
recommender
systems
chen
et
al
20
applied
meta-learning
principles
autodebias
framework
2.3
fairness
llms
research
fairness
llms
gained
considerable
traction
driven
realization
biases
present
pretraining
corpora
can
lead
llms
generate
content
harmful
also
oﬀensive
often
resulting
discrimination
marginalized
groups
heightened
awareness
spurred
increased
research
eﬀorts
aimed
understanding
origins
bias
addressing
detrimental
aspects
llms
68
13
initiatives
like
reinforcement
learning
human
feedback
58
reinforcement
learning
ai
fairness
seek
mitigate
reinforcement
existing
stereotypes
generation
demeaning
content
beyond
existing
literature
fairllm
95
critically
evaluates
recllm
fairness
highlighting
biases
chatgpt
recommendations
user
attributes
concurrently
eﬀorts
reﬁne
llm
fairness
assessments
gaining
traction
within
nlp
community
22
66
studies
like
12
expose
biases
gpt-3
content
generation
chapter
related
work
26
latter
noting
violent
bias
muslims
benchmarks
bbq
61
crowspairs
54
realtoxicityprompts
34
holistic
evaluations
50
analysis
across
various
llms
decodingtrust
77
extends
detailed
fairness
exploration
chatgpt
gpt-4
2.4
selection
bias
research
selection
bias
recommendation
systems
increasing
exploring
methods
reduce
bias
enhance
system
performance
one
approaches
re-sampling
techniques
includes
methods
undersampling
63
42
smote
synthetic
minority
over-sampling
technique
19
14
aims
balance
distribution
data
across
diﬀerent
classes
another
popular
approach
use
cost-sensitive
learning
methods
assign
diﬀerent
costs
diﬀerent
types
errors
order
balance
trade-oﬀ
diﬀerent
types
bias
example
method
adversarial
learning
94
24
aims
minimize
bias
adding
adversarial
term
loss
function
encourages
model
produce
fair
predictions
another
area
research
focuses
use
debiasing
techniques
representation
learning
process
fair
representation
learning
93
learns
representations
invariant
certain
sensitive
attributes
also
recent
studies
address
selection
bias
using
counterfactual
data
augmentation
cfda
81
creates
new
hypothetical
data
points
increase
diversity
training
set
can
done
generating
synthetic
data
points
similar
original
data
points
27
diﬀerent
sensitive
attributes
addition
meta-learning
21
78
applied
debiasing
recommendation
systems
multi-stage
cascade
systems
qin
et
al
64
proposed
rankflow
solve
selection
bias
joint-training
system
expensive
deploy
production
system
work
aims
solve
selection
bias
issue
independent-training
models
cascade
system
chapter
meta-learning
approach
fair
ranking
3.1
introduction
recently
fairness
information
retrieval
ir
system
attracted
attention
92
86
87
ranking
models
aim
give
relevant
scores
items
query
top
items
highest
scores
will
delivered
users
ranking
models
generally
data-driven
means
models
will
observe
particular
patterns
training
dataset
make
predictions
based
however
subject
ranking
problem
expert
search
job
recommendation
systematic
biases
dataset
usually
stemming
biased
data
distribution
will
introduce
unfairness
trained
model
example
28
chapter
meta-learning
approach
fair
ranking
law
students
race
29
law
students
gender
figure
3.1
illustration
predicted
rankings
distribution
protected
groups
female
students
african
american
students
two
diﬀerent
datasets
report
kendall
tau
ranking
metric
proposed
mfr
model
ranks
items
protected
groups
higher
compared
listnet
17
indicates
mfr
improves
protected
attribute
exposure
unbiased
ranking
performance
traditional
ltr
model
listnet
17
will
discriminately
assign
lower
weights
minority
group
due
data
bias
see
fig
3.1
addressed
friedman
33
historic
discrimination
socially
underrepresented
group
dataset
will
make
way
model
pattern
will
observed
training
process
unfairness
problem
summarized
disparate
exposure
89
disadvantaged
protected
group
treated
equally
advantaged
group
dataset
disparate
exposure
lead
negative
impact
many
realworld
ranking
problems
unequal
opportunity
job
market
underrepresented
group
solve
unfairness
problem
tremendous
research
eﬀorts
made
designing
fairness-aware
algorithms
among
fairness
ranking
models
can
categorized
score-based
supervised
ones
score-based
models
chapter
meta-learning
approach
fair
ranking
30
rank-aware
proportional
representation
86
constrained
ranking
maximization
18
etc
score-based
models
aim
correct
bias
training
data
others
aim
adjust
prediction
scores
better
fairness
also
supervised
models
deltr
89
fa
ir
90
etc
learn
fair
model
biased
dataset
general
ranking
models
focus
diﬀerent
mitigation
points
post
pre-processing
model
training
although
in-processing
models
achieved
good
performance
fairness
metric
still
limitation
model
learned
biased
dataset
thus
meta-learning
beneﬁt
aforementioned
problem
training
meta-learner
meta-dataset
meta-dataset
collected
uniformly
without
bias
train
fair
metalearner
ranking
model
learn
general
fairness
problems
training
classiﬁcation
model
biased
dataset
researchers
applied
model-agnostic
meta-learning
maml
31
example
meta-weight-net
69
proposed
explicitly
learn
weighting
function
meta-dataset
updated
simultaneously
classiﬁer
however
meta-learning
still
under-explored
fairness-aware
ranking
problems
study
propose
meta-learning
framework
formulate
fairness-aware
ranking
task
bilevel
optimization
problem
upper-level
meta-trainer
lower-level
ranking
model
can
train
meta-learner
meta-dataset
help
ranking
model
learn
fairly
biased
dataset
meta-dataset
small
unbiased
dataset
collected
uniformly
sampling
training
dataset
queries
protected
group
chapter
meta-learning
approach
fair
ranking
31
unprotected
group
detail
training
iteration
use
ranking
model
ranking
loss
function
compute
loss
values
data
sample
training
dataset
train
multi-layer
neural
network
weighting
function
re-weight
loss
values
weighting
function
optimized
weighted
loss
values
meta-dataset
since
weighting
function
meta-learner
subject
ranking
models
goal
optimize
loss
weights
given
meta-learner
achieve
fairness
training
dataset
intuitively
can
see
loss
weight
hyperparameter
learned
train
metalearner
tune
hyperparameter
meta-dataset
training
process
also
referred
bilevel
optimization
learned
parameters
ranking
model
depend
parameters
meta-learner
best
knowledge
propose
ﬁrst
meta-learning
approach
fair
ranking
summary
work
makes
following
contributions
propose
general
meta-learning
framework
fairness
ranking
called
meta-learning
based
fair
ranking
mfr
addresses
data
bias
automatically
re-weighting
ranking
losses
formulate
mfr
bilevel
optimization
problem
solve
using
gradients
gradients
experiments
real-world
datasets
demonstrate
proposed
method
achieves
comparable
ranking
performance
signiﬁcantly
improves
fairness
metric
compared
state-of-the-art
methods
chapter
meta-learning
approach
fair
ranking
32
biased
unprotected
group
protected
group
unbiased
figure
3.2
mfr
learning
algorithm
ﬂowchart
steps
algorithm
note
ranking
model
meta-learner
batch
size
training
dataset
batch
size
meta-dataset
learning
rates
iteration
ﬁrstly
update
meta-learner
using
eq
meta-dataset
update
ranking
model
using
eq
training
dataset
3.2
meta-learning
based
fair
ranking
aim
train
fairness-aware
ranking
model
achieve
good
performance
utility
fairness
metrics
tune
ranking
model
loss
weights
values
make
model
emphasize
protected
group
unprotected
one
ranking
inference
instead
using
ﬁxed
weights
utilize
metadataset
sampled
original
training
dataset
unbiased
distribution
smaller
size
train
meta-learner
weighting
function
meta-learner
guide
ranking
model
learn
fairly
given
training
dataset
set
queries
qtrain
qtrain
set
items
dtrain
dtrain
query
qtrain
associated
list
item
candidates
dtrain
item
represented
feature
vector
xi
query
feature
vector
associated
relevance
score
let
ranking
model
represent
learnable
parameters
chapter
meta-learning
approach
fair
ranking
33
output
ranking
model
denoted
generally
learn
optimized
parameters
minw
m1
yi
used
ranking
loss
functions
however
equally
treating
sample
lead
ranking
model
unfair
minority
groups
since
heavy
data
bias
issue
training
dataset
address
challenge
introduce
meta-learner
parameterized
adaptively
tune
loss
weights
sample
achieve
fair
exposure
diversity
thus
rewrite
training
loss
following
ltrain
φi
li
yi
3.1
xi
represents
model
output
φi
represents
i-th
sample
loss
weight
given
proposed
meta-learner
notably
ltrain
governed
meta-learner
output
weights
conditioning
ﬁxed
used
updating
ranking
model
parameter
convenience
denote
li
original
loss
value
i-th
training
data
sample
output
ranking
loss
following
69
develop
meta-learner
multi-layer
neural
network
takes
input
loss
value
instantiate
φi
li
li
3.2
sample
either
training
dataset
meta-dataset
set
last-layer
activation
function
sigmoid
range
output
chapter
meta-learning
approach
fair
ranking
34
algorithm
mfr
learning
algorithm
input
training
dataset
qtrain
dtrain
meta-dataset
qmeta
dmeta
batch
size
max
iterations
output
classiﬁer
network
parameter
initialize
ranking
model
parameter
meta-learner
parameter
xqmeta
qmeta
sampleminibatch
qmeta
dmeta
xqtrain
qtrain
sampleminibatch
qtrain
dtrain
update
wˆ
eq
3.4
xqtrain
qtrain
update
eq
3.9
xqmeta
qmeta
update
eq
3.10
xqtrain
qtrain
end
lies
eventually
deﬁne
meta
training
loss
function
li
lmeta
3.3
update
parameters
ranking
network
gradient
decent
batch
training
data
loss
function
eq
3.1
can
deﬁne
ltrain
ltrain
3.4
train
meta-learner
need
sample
small
meta-dataset
qmeta
dmeta
meta-dataset
represents
meta-knowledge
true
distribution
protected
group
group
qmeta
dmeta
meta-dataset
denote
feature
vector
item
qmeta
relevance
score
qmeta
given
query
qmeta
qmeta
similar
ltrain
denote
lmeta
loss
value
meta-dataset
sample
goal
meta-learner
leverage
unbiased
meta-dataset
learn
re-weight
loss
values
train
chapter
meta-learning
approach
fair
ranking
35
model
biased
dataset
since
function
naturally
formulate
proposed
mfr
bilevel
optimization
problem
give
objective
function
min
lmeta
3.5
arg
min
train
loss
functions
proposed
mfr
jointly
considers
utility
fairness
metrics
developing
listwise
ranking
loss
exposure
term
following
deltr
loss
89
given
γu
3.6
listwise
fairness
measurement
listwise
loss
based
cross
entropy
17
balancing
parameter
obtain
optimal
parameters
minimize
training
loss
φi
ltrain
3.7
meta
3.8
arg
min
train
loss
meta-learner
arg
min
meta
parameters
update
step
compute
weighted
loss
values
θt
wt
update
loss
ranking
model
meta-dataset
chapter
meta-learning
approach
fair
ranking
36
following
lmeta
3.9
learning
rate
batch
size
meta-dataset
update
following
φi
ltrain
3.10
learning
rate
batch
size
training
dataset
adopt
alternating
optimization
strategy
69
75
88
implement
eq
3.9
eq
3.10
instead
using
nested
optimization
loops
whole
training
process
summarized
algorithm
although
consider
deltr
loss
objective
function
ranking
model
also
use
fair
ranking
losses
besides
disparate
exposure
biases
common
ranking
dataset
selection
bias
position
bias
model
aims
provide
general
meta-learning
framework
can
handle
fair
ranking
problems
3.3
experiments
experiments
train
evaluate
model
three
real-world
datasets
used
deltr
89
study
ranking
fairness
metrics
approach
compared
baseline
models
baseline
models
include
following
chapter
meta-learning
approach
fair
ranking
37
listnet
17
ii
lambdamart
16
iii
deltr
model
γsmall
γlarge
setting
89
iv
fa
ir
90
pre-processing
approach
creates
fair
dataset
trains
fa
ir
post-processing
approach
reorders
prediction
results
ensure
fairness
vi
mfr
diﬀerent
diﬀerent
dataset
vii
mfr
listnet
loss
mfr-listnet
code
available
https://github.com/ywang4/a-meta-learning-approach-to-fair-ranking.
fair
comparison
follow
settings1
described
deltr
89
split
dataset
generate
item
features
use
following
datasets
w3c
experts
gender
ii
engineering
students
high
school
iii
engineering
students
gender
iv
law
students
gender
law
students
race
w3c
experts
dataset
task
expert
search
originated
trec
2005
enterprise
track
26
protected
attribute
female
200
items
per
query
average
21.5
items
protected
group
engineering
students
dataset
task
academic
performance
prediction
dataset
contains
anonymized
historical
information
college
students
high
school
dataset
protected
attribute
public
high
school
480.6
items
per
query
167.6
items
protected
group
average
gender
dataset
protected
attribute
female
480.6
items
per
query
97.6
items
protected
group
average
law
students
dataset
task
also
academic
performance
prediction
gender
dataset
protected
attribute
female
total
21791
items
9537
items
protected
group
race
dataset
protected
https://github.com/milkalichtblau/deltr-experiments
chapter
meta-learning
approach
fair
ranking
38
attribute
black
total
19567
items
1282
protected
group
queries
technical
topics
w3c
dataset
academic
years
datasets
fair
comparison
adapt
evaluation
metrics
89
split
datasets
50
queries
training
10
queries
testing
w3c
dataset
queries
training
query
testing
engineering
students
dataset
80
training
20
testing
law
students
dataset
use
precision
10
10
w3c
dataset
kendall
tau
datasets
evaluate
ranking
performance
measure
fairness
compute
exposure
ratio
protected
non-protected
group
thus
fairness
metric
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
described
sec
3.2
meta-dataset
required
approach
since
protected
attribute
datasets
binary
perform
random
uniform
sampling
collect
meta-dataset
speciﬁcally
randomly
sample
amount
data
items
query
protected
group
non-protected
group
settings
general
weighting
function
set
update
frequency
parameter
θto
per
steps
optimizer
sgd
momentum
0.98
learning
rate
0.02
hidden
layer
dimension
30
number
hidden
layers
ranking
model
set
learning
rate
datasets
0.005
except
w3c
data
0.0005
optimizer
sgd
momentum
0.95
weight
decay
0.005
values
training
epoch
vary
diﬀerent
datasets
w3c
dataset
uses
500
100
epochs
engineering
students
high
school
uses
5000
500
epochs
engineering
students
gender
uses
chapter
meta-learning
approach
fair
ranking
listnet
17
lambdamart
16
deltr
γsmall
89
deltr
γlarge
89
fa
ir
post
90
fa
ir
pre
90
mfr-listnet
mfr
w3c
experts
gender
10
fairness
0.178
0.759
0.095
0.738
0.178
0.785
0.180
0.827
0.178
0.824
0.180
0.770
0.115
0.775
0.126
0.830
engineering
students
high
school
type
tau
fairness
0.390
1.070
0.355
1.002
0.390
1.075
0.391
1.075
0.390
1.070
0.374
1.020
0.385
0.990
0.391
1.086
engineering
students
gender
tau
fairness
0.384
0.858
0.326
0.907
0.384
0.860
0.370
0.976
0.384
0.886
0.360
0.942
0.385
0.855
0.352
1.052
39
law
students
gender
tau
fairness
0.202
0.931
0.199
0.979
0.201
0.958
0.188
0.993
0.182
0.965
0.203
0.931
0.225
0.901
0.225
1.015
law
students
race
tau
fairness
0.184
0.853
0.156
0.847
0.173
0.874
0.130
1.014
0.140
0.944
0.161
0.895
0.182
0.848
0.184
1.654
table
3.1
experimental
results
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
bold
text
indicates
model
best
performance
results
show
mfr
model
better
fairness
metrics
comparable
performance
ranking
metrics
state-of-the-art
models
500
100
epochs
law
students
gender
uses
1200
3000
epochs
law
students
race
uses
50000
100
epochs
results
analysis
shown
tab
3.1
approach
performs
better
terms
fairness
metrics
datasets
deltr
γsmall
deltr
γlarge
deltr
γsmall
deltr
γlarge
models
use
diﬀerent
scales
values
weight
exposure
measure
loss
function
meta
learner
can
achieve
higher
fairness
metrics
re-weighting
loss
distribution
training
process
intuition
behind
observation
imbalanced
pattern
among
training
data
observed
corrected
meta
learner
ranking
metrics
similar
better
results
datasets
except
w3c
dataset
since
listnet
lambdamart
consider
fairness
measure
training
results
expected
fairness
metrics
worse
fairness
ranking
models
addition
train
mfr-listnet
standard
listwise
ranking
loss
framework
evaluation
results
show
worse
performance
ranking
chapter
meta-learning
approach
fair
ranking
40
engineering
students
high
school
law
students
gender
figure
3.3
plot
variation
learned
weight
two
training
datasets
weight
diﬀerence
computed
φtdiﬀ
φi
φi
plot
φdiﬀ
training
epochs
shown
plot
weighting
function
converging
diﬀerent
values
weights
epoch
decreasing
0.0
fairness
metrics
listwise
loss
consider
exposure
measure
metadataset
diﬀerent
data
distribution
training
dataset
negative
eﬀect
meta-learner
re-weighting
process
thus
conclude
meta-learning
approach
help
model
improve
fairness
metrics
compare
model
deltr
loss
function
fig
3.1
plot
histogram
ranks
protected
attributes
different
models
plot
can
see
distribution
predicted
ranks
shifts
right
left
indicates
mfr
model
generally
ranks
items
protected
group
higher
compared
listnet
note
plot
means
top
rank
data
samples
fall
bins
left
items
receive
higher
ranks
plot
also
agrees
evaluation
results
see
large
diﬀerence
fig
1b
fairness
metric
mfr
law
students
race
dataset
two
times
listnet
41
fig
3.3
plot
variation
learned
weight
training
data
plots
show
weighting
function
converging
diﬀerent
values
weights
epoch
decreasing
suggested
meta-weight-net
69
use
multi-layer
neural
network
weighting
function
multi-layer
neural
network
known
universal
approximator
continuous
functions
convergence
shown
plots
indicates
successful
learning
process
weighting
function
3.4
conclusion
work
proposed
meta-learning
based
fair
ranking
mfr
model
improve
minority
group
exposure
experiments
real-world
datasets
demonstrate
approach
achieve
better
fairness
metrics
compared
fair
ranking
model
without
meta-learning
part
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
4.1
introduction
fairness
search
engines
important
topic
focuses
training
unbiased
ranking
model
towards
protected
attributes
typically
user
query
given
ranking
model
predicts
relevant
scores
among
candidate
items
returns
items
highest
scores
users
data-driven
ranking
model
usually
trained
large
datasets
thus
ranker
will
learn
user
item
patterns
training
dataset
make
predictions
based
however
many
cases
systematic
biases
42
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
african
american
female
african
american
43
female
figure
4.1
illustration
predicted
rankings
distribution
two
protected
attributes
four
datasets
law
student
gender
82
law
student
race
82
compas
engineering
student
89
report
kendall
tau
48
ranking
performance
mcfr
mfr
80
improve
protected
attributes
ranking
realizing
competitive
ranking
performance
compared
listnet
17
demonstrating
approach
increase
exposure
minority
exposure
bias
70
dataset
will
cause
unfairness
ranking
model
historical
discrimination
socially
underrepresented
group
33
will
make
way
model
pattern
will
observed
training
process
unfairness
problem
summarized
disparate
exposure
70
leading
negative
impact
many
real-world
ranking
problems
disparate
exposure
prevalent
information
retrieval
instance
expert
search
job
recommendation
systems
historically
underrepresented
minority
groups
like
females
african
americans
consequently
traditional
learning
rank
ltr
models
listnet
17
often
rank
groups
lower
due
data
biases
fig
4.1
shows
ranking
scores
diﬀerent
models
four
datasets
highlighting
unfairness
disparate
exposure
implies
uneven
group
visibility
algorithm
outcomes
especially
linked
attributes
like
gender
race
distinct
biases
like
selection
conformity
challenge
algorithmic
fairness
eﬃciency
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
44
reduce
disparate
exposure
ranking
context
many
research
works
proposed
recently
designing
fairness-aware
algorithms
can
divided
two
categories
score-based
models
supervised-learning
models
score-based
models
86
87
18
72
47
compute
ranking
scores
ﬂy
given
candidates
list
return
sorted
candidates
model
outcome
supervised-learning
models
generally
solve
ranking
prediction
problem
focus
diﬀerent
mitigation
strategies
post
49
89
52
39
23
27
20
pre-processing
90
91
10
model
training
although
in-processing
models
achieved
promising
performance
fairness
ranking
metrics
learning
biased
datasets
still
under-explored
challenging
due
unbalanced
distributions
protected
attributes
public
training
datasets
one
possible
way
alleviate
system
discrimination
inherited
data
bias
dynamically
re-weighting
minority
groups
contribute
penalties
computing
ranking
loss
end
meta-learning
31
emerges
eﬀective
way
enable
learning-to-weight
approach
leveraging
small
unbiased
dataset
meta
dataset
fairness-aware
ranking
problem
propose
mitigate
exposure
issue
biased
dataset
learning
weighting
model
meta-learner
re-weight
loss
ranking
model
biased
dataset
meta-learner
will
optimized
meta
dataset
unbiased
weighted
loss
training
dataset
biased
will
used
optimize
ranking
model
however
due
distribution
shift
biased
unbiased
datasets
non-trivial
directly
train
meta-learner
base
learner
two
datasets
large
training
loss
may
impair
ranking
utility
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
45
burden
convergence
speed
propose
adopt
curriculum
learning
gradually
increase
diﬃculty
training
meta-learners
address
challenge
speciﬁcally
deﬁne
diﬃculty
exposure
protected
groups
dataset
ﬁrst
randomly
sample
meta
dataset
exposure
training
dataset
continually
increase
protected
groups
exposure
meta
dataset
sampling
candidates
group
ongoing
epoch
uniform
distribution
equal
exposure
achieved
sensitive
attributes
intuitively
incremental
concept
learning
good
ﬁt
solve
distribution
shift
problem
meta-learners
trained
samples
biased
dataset
early
epochs
means
less
distribution
shift
meta-dataset
training
dataset
experimental
results
demonstrate
eﬀectiveness
curriculum
learning
improved
data
eﬃciency
training
study
propose
uniﬁed
meta-learning
framework
curriculum
learning
formulate
fairness-aware
ranking
task
bilevel
optimization
problem
upper
level
focuses
learning-to-weight
mitigate
biased
exposure
protected
attributes
lower
level
solves
learning-to-rank
dynamic
loss
governed
meta
learner
speciﬁcally
alleviate
data
bias
issue
protected
groups
automatically
weighted
loss
contributions
work
follows
propose
novel
meta
curriculum-based
fair
ranking
framework
namely
mcfr
addresses
data
bias
automatically
re-weighting
ranking
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
46
losses
proposed
mcfr
formulated
bilevel
optimization
problem
solved
using
gradients
gradients
proposed
fair
ranking
algorithm
marries
in-processing
methods
preprocessing
techniques
seamlessly
incorporating
curriculum
learning
construction
process
meta
datasets
develop
mcfr
general
framework
applicable
various
ranking
loss
functions
fairness
metrics
systematic
empirical
study
provided
show
versatility
proposed
framework
diﬀerent
ranking
fairness
criteria
experiments
public
datasets
show
method
matches
existing
ranking
performance
enhances
fairness
metrics
additionally
evaluations
conﬁrm
mcfr
improves
fairness
less
training
data
achieves
comparable
convergence
times
work
oﬀers
ﬁrst
fair
ranking
framework
utilize
pre-processing
in-processing
methods
new
approach
enhances
model
adaptability
robustness
allowing
broader
range
loss
functions
dynamically
adjusting
meta-datasets
training
additionally
framework
demonstrates
data
eﬃciency
comparative
experiments
ve
also
conducted
comprehensive
tests
incorporating
additional
baseline
models
performing
ablation
study
various
fairness
terms
ranking
losses
lastly
ve
updated
manuscript
include
recent
related
works
providing
fuller
understanding
fairness
ranking
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
meta
model
single
step
scheduler
47
ranking
model
curriculum
sampling
easier
biased
distribution
harder
unbiased
distribution
ranking
loss
unprotected
group
protected
group
mini
batch
sampling
fairness
term
rankmse
squared
exposure
ranknet
hinge
exposure
listnet
figure
4.2
mcfr
learning
algorithm
ﬂowchart
steps
algorithm
note
ranking
model
meta
learner
batch
size
training
dataset
batch
size
meta-dataset
learning
rates
iteration
ﬁrstly
update
meta
learner
using
eq
meta-dataset
sampled
curriculum
sampling
update
sampling
diﬃculty
epoch
update
ranking
model
using
eq
training
dataset
4.2
meta
curriculum-based
fair
ranking
section
will
explain
proposed
meta
curriculum-based
fair
ranking
framework
detail
mcfr
framework
will
train
unbiased
ranking
model
using
meta-leaner
re-weight
ranking
losses
formulate
bilevel
optimization
problem
solve
using
gradients
gradients
also
show
framework
trained
various
ranking
loss
functions
fairness
terms
finally
describe
design
curriculum
sampling
strategy
meta
dataset
address
bias
datasets
traditional
methods
utilized
pre-processing
in-processing
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
48
post-processing
techniques
92
28
model
combines
pre-processing
inprocessing
introducing
meta
curriculum-based
fair
ranking
framework
derive
smaller
dataset
meta-learner
training
assigns
weights
emphasize
protected
group
training
curriculum
learning
adjusts
dataset
distribution
ratio
epochs
facilitating
smoother
meta-learner
training
integrates
ranking
loss
fairness
regularization
using
meta-learner
guide
model
training
depicted
fig
4.2
4.2
problem
setting
denote
set
queries
training
dataset
qtrain
size
qtrain
set
items
dtrain
dtrain
query
qtrain
list
item
candidates
dtrain
pair
query
item
represented
feature
vector
xi
associated
relevance
score
yi
dataset
candidates
binary
attribute
speciﬁes
whether
candidate
belongs
protected
group
non-protected
group
example
binary
attribute
represent
gender
race
systematic
bias
exists
dataset
collection
4.2
uniﬁed
mcfr
framework
address
fairness
problem
train
meta
learner
meta-dataset
help
train
fair
ranking
model
biased
training
dataset
ranking
model
learnable
parameters
denote
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
49
output
model
generally
model
parameter
optimized
minw
m1
yi
minimize
given
ranking
loss
function
pairwise
loss
listwise
loss
however
loss
functions
treat
sample
equally
ranking
model
will
unfair
heavy
data
bias
issue
towards
minority
groups
training
dataset
mitigate
problem
introduce
meta
learner
learnable
parameters
adaptively
tune
loss
weights
sample
achieve
fair
exposure
diversity
rewrite
training
loss
following
ltrain
φi
li
yi
4.1
xi
denotes
model
output
φi
denotes
i-th
sam
ple
loss
weight
given
aforementioned
meta
learner
notably
ltrain
governed
meta
learner
output
weights
depends
ﬁxed
used
updating
ranking
model
parameter
short
write
li
original
loss
value
i-th
training
data
sample
output
ranking
loss
meta
learner
use
multi-layer
perceptron
network
proposed
69
takes
loss
values
input
output
weighted
loss
φi
li
li
4.2
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
50
algorithm
parameter
update
algorithm
mcfr
input
batch
training
data
xqtrain
qtrain
batch
meta-dataset
xqtrain
qtrain
ranking
model
parameter
meta
learner
parameter
output
ranking
model
parameter
update
update
eq
4.5
xqtrain
qtrain
update
eq
4.8
xqmeta
qmeta
update
eq
4.9
xqtrain
qtrain
sample
training
dataset
meta-dataset
use
sigmoid
last-layer
activation
function
deﬁne
meta
training
loss
function
li
lmeta
4.3
qmeta
goal
meta
learner
leverage
meta-dataset
learn
re-weight
loss
values
train
model
biased
dataset
indicating
relationship
meta-learner
plays
pivotal
role
directing
tuning
ranking
model
parameters
inherently
making
function
since
function
naturally
formulate
proposed
mcfr
bilevel
optimization
problem
give
objective
function
min
lmeta
arg
min
ltrain
4.4
illustrated
fig
4.2
proposed
mcfr
model
takes
advantage
sampled
meta-dataset
learn
unbiased
ranking
model
meta-dataset
guide
meta
learner
reweight
training
loss
helps
ranking
model
focus
candidates
protected
group
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
type
formula
hinge
exposure
89
max
exposure
g0
exposure
g1
squared
exposure
exposure
g0
exposure
g1
rankmse
11
n1
ranknet
15
n1
listnet
17
fairness
ranking
51
yi
log
exp
yi
py
log
py
table
4.1
summary
ranking
fairness
terms
used
loss
function
loss
function
used
framework
γu
can
insert
exposure
terms
ranking
loss
terms
needed
note
denotes
number
candidates
per
query
4.2
parameter
update
since
formulate
framework
bilevel
optimization
problem
challenging
calculating
optimal
parameters
requires
two
nested
loops
optimization
following
well-known
maml
works
69
75
88
adopt
online
strategy
single
optimization
loop
update
ranking
model
meta-learner
parameters
guarantee
training
eﬃciency
update
parameters
ranking
network
using
gradient
decent
batch
training
data
loss
function
eq
4.1
deﬁne
update
ltrain
ltrain
4.5
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
52
step
update
ranking
model
parameters
step
obtain
optimal
parameters
minimize
training
loss
φi
ltrain
4.6
meta
4.7
arg
min
ltrain
loss
meta
learner
arg
min
lmeta
given
eq
4.5
update
loss
ranking
model
meta-dataset
following
lmeta
4.8
learning
rate
batch
size
meta-dataset
update
following
φi
ltrain
4.9
learning
rate
batch
size
training
dataset
adopt
alternating
optimization
strategy
69
75
88
implement
eq
4.8
eq
4.9
instead
using
nested
optimization
loops
one
step
update
algorithm
summarised
alg
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
4.2
53
ranking
fairness
loss
proposed
mcfr
serves
uniﬁed
framework
aims
improve
ranking
fairness
metrics
given
ranking
fairness
objectives
achieve
goal
propose
include
two
terms
loss
functions
similar
in-processing
fairness
methods
deltr
89
develop
loss
functions
ranking
term
fairness
term
given
γu
4.10
fairness
term
ranking
loss
term
balancing
parameter
4.2
4.1
ranking
terms
ranking
loss
use
following
loss
functions
experiments
rankmse
11
ranknet
15
listnet
17
rankmse
pointwise
loss
based
least
mean
squared
regression
ranknet
proposed
ﬁrst
pairwise
cross
entropy
loss
consider
preference
relationships
documents
however
possible
correctly
predict
document
order
cases
listnet
aims
directly
compute
ranking
loss
query
candidates
list
instead
computing
pairwise
loss
one
pair
one
pair
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
54
worth
noting
ranking
losses
also
applicable
mcfr
provide
general
framework
improve
ranking
metrics
4.2
4.2
fairness
terms
work
focus
disparate
exposure
fairness
term
candidates
two
diﬀerent
groups
non-protected
group
g0
protected
group
g1
candidates
g1
belong
discriminated
group
female
african
american
signiﬁcant
disadvantages
datasets
following
deﬁnition
singh
et
al
70
exposure
candidate
ranked
list
generated
probabilistic
ranking
given
exposure
xi
pi
va
4.11
va
position
bias
position
follow
implementation
zelike
el
al
89
consider
position
bias
position
v1
average
exposure
candidates
group
written
exposure
exposure
xi
4.12
xi
exposure
term
deﬁned
can
introduce
fairness
measure
minimizing
diﬀerence
exposure
g0
exposure
g1
experiments
use
two
exposure
measurements
hinge
exposure
calculates
hinge
squared
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
55
figure
4.3
curriculum
sampling
strategy
illustrated
engineering
student
gender
dataset
use
ratio
unprotected
group
protected
group
meta-dataset
training
dataset
beginning
training
epoch
gradually
decrease
ratio
training
epoch
increase
ratio
becomes
shows
balanced
meta-dataset
loss
exposure
diﬀerence
two
groups
square
exposure
computes
squared
exposure
diﬀerence
ranking
loss
terms
exposure
terms
used
arbitrary
combination
framework
improve
fairness
ranking
metrics
given
diﬀerent
combinations
ranking
terms
fairness
terms
summarised
table
4.1
4.2
curriculum
sampling
training
data
shows
systematic
bias
fewer
candidates
protected
groups
unprotected
ones
address
issue
trained
meta
learner
using
unbiased
meta-dataset
since
real
unbiased
data
rare
autodebias
20
previously
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
56
tackled
similar
issue
recommendation
systems
ﬁt
ranking-focused
needs
another
approach
used
mfr
80
equally
samples
candidates
group
however
method
creates
meta-dataset
may
fall
short
accurately
capturing
real
biased
data
tasks
like
ranking
order
relevance
items
crucial
mismatch
data
distribution
can
signiﬁcantly
hinder
model
ability
provide
fair
eﬀective
rankings
practical
applications
biased
situations
end
adopt
curriculum
learning
method
starts
easier
less
biased
samples
gradually
introduces
complex
ones
mimics
natural
learning
helping
model
adapt
better
become
robust
designed
ease
model
understanding
correcting
biases
ensuring
performs
well
fairly
real-world
applications
even
underlying
biases
data
trained
detail
want
downsample
meta-dataset
similar
distribution
training
dataset
early
training
epochs
gradually
change
ratio
number
candidates
protected
unprotected
groups
1.0
since
collect
real
unbiased
dataset
deﬁne
1.0
unbiased
ratio
number
candidates
two
diﬀerent
groups
dunprotected
vs
dprotected
means
equal
number
candidates
group
downsampling
ratio
deﬁned
dunprotected
dprotected
underlying
assumption
behind
curriculum
sampling
strategy
easier
train
model
metadataset
training
dataset
similar
distribution
diﬃcult
optimize
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
57
algorithm
mcfr
learning
algorithm
input
training
dataset
qtrain
dtrain
batch
size
max
iterations
output
ranking
model
parameter
initialize
ranking
model
parameter
meta
learner
parameter
xqmeta
qmeta
curriculumsampling
qtrain
dtrain
xqtrain
qtrain
sampleminibatch
qtrain
dtrain
update
alg
end
parameters
ranking
model
meta
learner
sees
diﬀerent
metadataset
compared
training
dataset
shown
fig
4.3
illustrate
change
distribution
two
groups
meta-dataset
diﬀerent
training
epochs
train
meta
learner
use
curriculum
sampled
data
xqmeta
qmeta
meta-dataset
represents
meta-knowledge
true
distribution
protected
group
group
qmeta
dmeta
metadataset
denote
feature
vector
item
qmeta
relevance
score
qmeta
given
query
qmeta
qmeta
similar
ltrain
denote
lmeta
loss
value
meta-dataset
sample
thus
deﬁne
curriculumsampling
qtrain
dtrain
following
1.0
4.13
ratio
sampled
candidates
group
query
note
single
step
scheduler
ratio
updated
epoch
executing
curriculumsampling
epoch
sampling
meta-dataset
xqmeta
qmeta
property
dunprotected
dprotected
intuitively
curriculumsampling
decreases
ratio
epoch
epoch
biased
ratio
1.0
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
58
described
section
4.2
meta-dataset
important
part
model
training
key
data
guide
meta
learner
since
meta
learner
aims
reweight
loss
ranking
model
well
meta
learner
trained
determine
performance
ranking
model
curriculum
sampling
decrease
training
diﬃculty
meta
learner
compared
mfr
80
uses
one
sampled
unbiased
dataset
meta
learner
progressively
trained
unbiased
meta-dataset
epoch
increases
improve
meta
learner
performance
lead
better
overall
performance
ranking
model
whole
training
process
summarized
algorithm
items
query
protected
query
w3c
experts
gender
200
21.5
engineering
students
high
school
type
480.6
167.6
engineering
students
gender
480.6
97.6
law
students
law
students
compas
gender
race
race
21791
19567
6889
9537
1282
3528
table
4.2
summary
dataset
statistics
report
average
counts
total
unprotected
items
per
query
w3c
experts
engineering
students
datasets
provide
exact
item
counts
law
students
compas
datasets
contains
one
query
framework
provides
ﬂexibility
solve
diﬀerent
ranking
problems
listnet
17
may
work
ranking
problems
cases
fairness
terms
also
switched
using
diﬀerent
fairness
metrics
diﬀerent
formula
compute
disparate
exposure
exposure
issue
fairness
problem
mcfr
capable
optimized
fairness
terms
position
bias
conformity
bias
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
4.3
59
experiments
experiments
train
evaluate
model
four
real-world
public
datasets
study
ranking
fairness
metrics
approach
compared
baseline
models
also
conduct
ablation
study
eﬀectiveness
framework
changing
ranking
loss
term
disparate
exposure
term
repeat
experiment
datasets
diﬀerent
settings
loss
functions
evaluate
proposed
framework
comparing
baseline
models
analysis
following
questions
answered
proposed
mcfr
performance
compared
baseline
models
mcfr
improve
ranking
fairness
metrics
diﬀerent
loss
functions
eﬀects
curriculum
sampling
4.3
experimental
setting
train
evaluate
model
four
real-world
public
datasets
engineering
student
ii
law
student
iii
w3c
experts
iv
compas
correctional
oﬀender
management
proﬁling
alternative
sanctions
statistics
dataset
summarized
table
4.2
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
60
w3c
experts
dataset
dataset
originates
trec
2005
enterprise
track
26
involves
searching
experts
based
topic
using
features
emails
designate
gender
protected
attribute
technical
topics
queries
context
females
protected
group
males
non-protected
query
200
items
averaging
21.5
protected
group
given
original
dataset
ranks
retrieved
experts
equally
adopt
delter
experiments
setting
89
categorizing
expert
candidates
male
experts
female
experts
male
non-experts
female
non-experts
candidate
features
utilize
elasticsearch
learning
rank
plug-in1
query-candidate
pair
text
features
law
student
dataset
dataset
82
collected
determine
lsat
law
school
admission
test
us
biased
ethnic
minorities
dataset
contains
information
ﬁrst-year
law
students
protected
attributes
gender
race
query
academic
year
task
retrieve
students
good
lsat
scores
since
problem
setting
focused
one
protected
attribute
time
two
datasets
law
students
gender
law
students
race
law
students
gender
dataset
females
protected
group
among
21
791
candidates
537
female
law
students
race
dataset
african
americans
protected
group
19
567
candidates
282
group
engineering
students
dataset
89
contains
information
ﬁrst-year
students
chilean
university
qualiﬁcation
features
include
admission
test
results
mathematics
language
science
students
high
school
grades
number
https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
61
credits
taken
university
task
predict
academic
performance
protected
attributes
high
school
type
gender
similarly
two
datasets
engineering
students
high
school
type
engineering
students
gender
engineering
students
datasets
one
focuses
high
school
type
public
high
school
students
protected
group
averaging
167.6
480.6
items
per
query
considers
gender
females
protected
group
averaging
97.6
480.6
items
per
query
compas
compas
correctional
oﬀender
management
proﬁling
alternative
sanctions
commercial
algorithm
scoring
criminal
defendant
likelihood
recidivism
compas
dataset
observed
algorithm
biased
towards
african
american
candidates
dataset
task
predict
recidivism
score
protected
attribute
race
889
candidates
total
528
african
americans
4.3
1.1
baselines
integrated
several
baseline
models
implementation
listnet
17
introduces
listwise
loss
function
lambdamart
16
combines
mart
lambdarank
transforming
ranking
tasks
gradient
boosting
decision
trees
deltr
89
oﬀers
ltr
strategy
listwise
fairness
metrics
fa
ir
90
applies
pre
post-processing
techniques
enhanced
fairness
autodebias
20
presents
debiasing
method
recommendation
systems
fairgbm
27
delivers
fairness-centric
classiﬁcation
model
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
62
gbdt
mfr
80
employs
meta-learning
fair
ltr
notably
listnet
lambdamart
focus
solely
ranking
metrics
deltr
mfr
emphasizing
fairness-aware
ranking
4.3
1.2
implementation
details
split
datasets
50
queries
training
10
queries
testing
w3c
dataset
queries
training
query
testing
engineering
students
dataset
80
training
20
testing
law
students
dataset
compas
dataset
use
precision
10
10
38
w3c
dataset
kendall
tau
48
datasets
evaluate
ranking
performance
kendall
tau
assesses
correlation
two
ranking
sets
calculating
diﬀerence
number
concordant
discordant
pairs
divided
total
number
pairs
ranges
indicating
perfect
agreement
correlation
perfect
disagreement
rankings
respectively
details
kendall
tau
calculated
following
kendall
tau
4.14
number
concordant
pairs
number
discordant
pairs
number
ties
ground
truth
rankings
number
ties
predicted
rankings
measure
fairness
compute
exposure
ratio
protected
non-protected
group
89
thus
fairness
metric
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
63
training
set
update
frequency
weighting
model
parameter
per
steps
optimizer
sgd
74
momentum
0.98
learning
rate
0.022
hidden
layer
dimension
30
number
hidden
layers
ranking
model
set
learning
rate
0.005
optimizer
sgd
momentum
0.95
weight
decay
0.005
set
diﬀerent
values
training
epoch
diﬀerent
dataset
w3c
dataset
uses
500
100
epochs
engineering
students
high
school
uses
000
280
epochs
engineering
students
gender
uses
400
150
epochs
law
students
gender
uses
200
550
epochs
law
students
race
uses
50
000
110
epochs
compas
race
uses
500
45
epochs
ablation
study
evaluate
eﬀectiveness
framework
use
hyperparameters
described
ranking
losses
rankmse
ranknet
experiment
collect
results
combinations
ranking
losses
fairness
terms
4.3
fair
ranking
performance
table
4.3
detail
performance
baseline
fair
ranking
models
trained
hinge
exposure
proposed
mcfr
outperforms
baseline
models
fairness
metrics
across
datasets
compared
listnet
lambdamart
models
like
deltr
mfr
fa
ir
autodebias
fairgbm
mcfr
show
enhanced
results
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
w3c
experts
gender
precision
10
fairness
listnet
17
lambdamart
16
deltr
89
fa
ir
pre
90
fa
ir
post
90
autodebias
20
fairgbm
27
mfr
mcfr
0.178
0.095
0.180
0.180
0.180
0.033
0.087
0.126
0.118
0.759
0.738
0.827
0.770
0.827
0.829
0.941
0.830
0.843
law
students
gender
kendall
tau
fairness
listnet
17
lambdamart
16
deltr
89
fa
ir
pre
90
fa
ir
post
90
autodebias
20
fairgbm
27
mfr
mcfr
0.202
0.199
0.188
0.203
0.182
0.222
0.141
0.225
0.225
0.931
0.979
0.993
0.931
0.965
0.894
0.998
1.015
1.023
engineering
students
high
school
type
kendall
tau
fairness
0.390
0.355
0.391
0.374
0.391
0.372
0.338
0.391
0.390
1.070
1.002
1.075
1.020
1.075
0.955
0.909
1.086
1.088
law
students
race
kendall
tau
fairness
0.184
0.156
0.130
0.161
0.140
0.135
0.210
0.184
0.182
0.853
0.847
1.014
0.895
0.944
1.009
1.116
1.654
1.671
64
engineering
students
gender
kendall
tau
fairness
0.384
0.326
0.370
0.360
0.370
0.372
0.336
0.352
0.350
0.858
0.907
0.976
0.942
0.976
0.955
0.892
1.052
1.055
compas
race
kendall
tau
fairness
0.639
0.542
0.576
0.557
0.557
0.644
0.550
0.644
0.644
0.836
0.956
0.970
1.039
1.040
1.136
0.917
1.138
1.144
table
4.3
experimental
results
hinge
exposure
89
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
greater
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
bold
text
indicates
model
best
performance
results
show
mcfr
model
better
fairness
metrics
comparable
performance
ranking
metrics
state-of-the-art
models
due
inclusion
fairness
measures
training
notably
mcfr
use
curriculum
sampling
meta-dataset
allows
surpass
mfr
fairness
metrics
meta-learner
adeptly
adjusts
loss
distribution
mcfr
training
curriculum
sampling
creates
meta-dataset
meta
model
w3c
dataset
limited
items
protected
group
hinder
signiﬁcant
distribution
shifts
meta-dataset
sampling
aﬀecting
ranking
performance
constraint
primarily
contributes
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
65
decreasing
ranking
performance
observed
model
trained
w3c
data
except
w3c
dataset
mcfr
competitive
results
ranking
metrics
compared
baseline
models
indicating
training
mcfr
focus
solely
fairness
metrics
listnet
results
also
expected
optimize
ranking
metrics
better
performance
ranking
metrics
engineering
students
gender
law
students
race
since
autodebias
fairgbm
tailored
recommendation
classiﬁcation
tasks
respectively
limited
performance
ranking
problems
expected
fig
4.1
also
plot
histogram
ranks
protected
attributes
diﬀerent
models
plot
can
see
distribution
predicted
ranks
shifts
right
left
indicating
mcfr
model
generally
ranks
items
protected
group
higher
compared
listnet
mfr
plot
x-axis
indicates
top
rank
candidates
falling
bins
left
means
candidates
receive
higher
ranks
ranking
algorithms
mcfr
enhances
visibility
underrepresented
protected
groups
however
fairness
doesn
mean
maximizing
exposure
expense
non-protected
group
visibility
4.3
ablation
studies
present
ablation
study
results
mcfr
oﬀers
ﬂexibility
choosing
loss
functions
fairness
terms
generalized
framework
mcfr
consistently
enhances
ranking
fairness
metrics
across
various
loss
functions
exposure
formulas
employed
rankmse
ranknet
listnet
representatives
pointwise
pairwise
listwise
losses
serve
baseline
models
table
4.4
4.5
4.6
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
exposure
type
rankmse
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
rankmse
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
66
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.121
0.770
0.187
0.800
0.376
0.836
0.115
0.781
0.384
1.049
0.357
1.010
0.115
0.782
0.384
1.052
0.353
1.020
0.115
0.780
0.384
1.045
0.360
0.982
0.115
0.782
0.384
1.045
0.360
0.990
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.213
0.874
0.190
0.847
0.493
0.768
0.225
0.910
0.191
0.847
0.634
0.911
0.226
0.920
0.190
0.851
0.634
0.911
0.223
1.010
0.139
0.992
0.633
0.911
0.225
1.023
0.138
0.996
0.630
0.928
table
4.4
ablation
study
results
rankmse
11
exposure
type
ranknet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
ranknet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.121
0.770
0.131
0.806
0.190
0.800
0.121
0.774
0.126
0.925
0.188
0.810
0.123
0.775
0.131
0.867
0.186
0.820
0.121
0.774
0.126
0.925
0.188
0.810
0.121
0.774
0.131
0.867
0.186
0.812
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.093
0.942
0.105
0.866
0.128
0.768
0.131
1.033
0.140
1.284
0.373
0.839
0.132
1.036
0.152
1.370
0.375
0.840
0.173
1.033
0.105
0.866
0.352
0.832
0.220
1.050
0.105
0.866
0.352
0.832
table
4.5
ablation
study
results
ranknet
15
4.3
3.1
ranking
terms
analysis
first
analyze
performance
mcfr
using
diﬀerent
ranking
terms
loss
functions
using
listnet
mcfr
worse
ranking
performance
w3c
experts
gender
engineering
students
gender
datasets
listnet
model
datasets
mcfr
listnet
model
similar
ranking
performance
note
law
students
gender
dataset
mcfr
also
improves
ranking
metrics
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
exposure
type
listnet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
listnet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
67
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.178
0.759
0.390
1.070
0.384
0.858
0.126
0.830
0.391
1.086
0.352
1.052
0.118
0.843
0.390
1.088
0.350
1.055
0.118
0.803
0.330
1.005
0.358
1.006
0.118
0.803
0.341
1.005
0.342
1.018
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.202
0.931
0.184
0.853
0.639
0.836
0.225
1.015
0.184
1.654
0.644
1.138
0.225
1.023
0.182
1.671
0.644
1.144
0.223
1.010
0.113
1.166
0.340
0.828
0.225
1.014
0.079
1.115
0.632
1.068
table
4.6
ablation
study
results
listnet
17
using
rankmse
similar
pattern
observed
ranknet
mcfr
achieves
similar
ranking
performance
w3c
experts
gender
dataset
improves
ranking
metrics
law
students
gender
law
students
race
datasets
addition
fairness
metrics
consistent
improvement
ranking
metrics
shows
proposed
mcfr
generalized
framework
can
adapt
many
ranking
loss
functions
4.3
3.2
fairness
terms
analysis
second
evaluate
diﬀerent
fairness
terms
loss
functions
using
listnet
ranking
loss
term
mcfr
greatly
improves
fairness
metrics
w3c
experts
gender
engineering
students
gender
datasets
datasets
mcfr
outperforms
listnet
model
fairness
metrics
similar
ranking
performance
using
rankmse
mcfr
also
improves
fairness
metrics
law
students
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
68
gender
law
students
race
datasets
see
mcfr
can
improve
fairness
metrics
various
ranking
loss
terms
4.3
3.3
curriculum
sampling
analysis
moreover
compare
performance
mcfr
mfr
show
eﬀectiveness
curriculum
learning
using
diﬀerent
losses
note
mfr
use
settings
loss
functions
mcfr
fair
comparison
using
hinge
exposure
mcfr
usually
better
fairness
performance
minor
trade-oﬀs
ranking
metrics
except
w3c
experts
gender
dataset
using
listnet
using
squared
exposure
except
law
students
race
dataset
mcfr
improves
ranking
fairness
metrics
compared
mfr
results
demonstrate
eﬀectiveness
curriculum
learning
fairness
metric
gender
ranking
metric
gender
fairness
metric
race
ranking
metric
race
figure
4.4
evaluation
results
down-sampling
experiments
conduct
experiment
law
students
gender
law
students
race
datasets
downsample
training
data
rate
0.1
0.9
results
show
mcfr
better
data
eﬃciency
achieve
better
fairness
metrics
similar
ranking
performance
mfr
autodebias
diﬀerent
down-sampling
rate
chapter
uniﬁed
meta-learning
framework
fair
ranking
curriculum
learning
4.3
3.4
69
data
eﬃciency
assess
curriculum
learning
eﬀect
data
eﬃciency
compare
mcfr
mfr
autodebias
using
down-sampled
training
data
varying
10
90
original
data
figure
4.4
illustrates
mcfr
outperforms
mfr
autodebias
across
sampling
rates
fairness
gender-related
data
achieving
fair
metrics
close
1.0
maintaining
high
ranking
performance
mcfr
demonstrates
superior
fairness
reduced
training
data
race-related
data
mcfr
achieves
better
ranking
performance
higher
fairness
metrics
indicating
curriculum
strategy
eﬀectively
enhances
fairness
protected
groups
even
less
data
deltr
mfr
mcfr
w3c
experts
gender
43.69
21.16
171.37
engineering
students
high
school
type
14.09
15.24
92.57
engineering
students
gender
40.92
17.24
91.64
law
students
law
students
compas
gender
race
race
14.35
17.70
19.67
51.29
49.72
76.88
294.42
293.92
352.96
table
4.7
experimental
results
total
convergence
time
seconds
shows
total
convergence
time
diﬀerent
algorithms
deltr
mfr
mcfr
across
various
datasets
scenarios
based
table
mcfr
framework
generally
comparable
convergence
time
two
algorithms
4.3
3.5
training
inference
eﬃciency
enhance
ranking
fairness
mcfr
sought
balance
fairness
eﬃciency
shown
table
4.7
mcfr
training
complexity
comparable
methods
like
deltr
curriculum
sampling
extends
training
time
linearly
sampling
rounds
notably
inference
mcfr
mfr
deltr
will
show
consistent
eﬃciency
since
algorithms
share
base
ranking
model
number
parameters
layers
one
forward
pass
70
predictions
table
4.7
shows
mcfr
extended
convergence
time
due
curriculum
sampling
added
epochs
mcfr
fairness
beneﬁts
clear
yet
value
eﬃciency
time-sensitive
applications
overall
results
demonstrate
curriculum
learning
mcfr
enhances
fairness
without
compromising
ranking
performance
also
making
training
eﬃcient
4.4
conclusion
study
introduced
meta
curriculum-based
fair
ranking
mcfr
framework
address
data
bias
search
problems
employing
meta-learner
trained
curriculum-learning-sampled
meta-dataset
approach
re-weights
training
loss
target
ranker
biased
data
re-weighted
loss
aids
developing
unbiased
ranking
model
enhancing
exposure
minority
groups
comparative
experiments
real-world
datasets
conﬁrm
mcfr
superiority
fair
ranking
models
lacking
meta-learning
curriculum
learning
components
chapter
empirical
study
fairness
llms
rankers
5.1
introduction
emergence
large
language
models
llms
like
gpt
models
12
57
llama2
76
marks
signiﬁcant
trend
multiple
ﬁelds
ranging
natural
language
processing
information
retrieval
ranking
challenges
llms
shown
demonstrated
performance
research
exempliﬁed
projects
like
rankgpt
73
65
highlights
proﬁciency
gpt
models
delivering
competitive
ranking
results
surpassing
traditional
neural
ranking
models
precision
relevance
growing
popularity
llms
assessing
fairness
become
crucial
evaluating
eﬀectiveness
71
chapter
empirical
study
fairness
llms
rankers
72
search
query
agriculture
rank
passages
based
relevance
search
query
hana
meisel
female
agronomist
thomas
giles
male
pastoralist
theodor
bergmann
male
agronomist
thomas
giles
male
pastoralist
theodor
bergmann
male
agronomist
hana
meisel
female
agronomist
listwise
evaluation
search
query
agriculture
rank
two
passages
based
relevance
search
query
hana
meisel
female
agronomist
thomas
giles
male
pastoralist
thomas
giles
male
pastoralist
hana
meisel
female
agronomist
pairwise
evaluation
figure
5.1
illustration
two
evaluation
methods
listwise
evaluation
pairwise
evaluation
document
associated
binary
protected
attribute
used
fairness
evaluation
metrics
recent
research
primarily
concentrated
eﬃciency
accuracy
llms
ranking
tasks
increasing
concern
fairness
concern
particularly
highlighted
given
signiﬁcant
impact
easy
accessibility
models
prior
studies
natural
language
processing
41
62
recommendation
systems
95
shown
unfair
treatment
towards
underrepresented
groups
llms
although
fairness
issues
traditional
search
engines
extensively
explored
notable
gap
examining
llms
rankers
search
systems
study
seeks
address
gap
conducting
in-depth
audit
various
llms
including
gpt
models
open-source
alternatives
work
conduct
empirical
study
assesses
llms
text
ranker
user
item
perspectives
evaluate
fairness
investigate
chapter
empirical
study
fairness
llms
rankers
73
models
despite
trained
vast
varied
datasets
might
unintentionally
mirror
social
biases
ranking
outcomes
concentrate
various
binary
protected
attributes
frequently
underrepresented
search
results
examining
llms
rank
documents
associated
attributes
response
diverse
user
queries
speciﬁcally
examine
llms
using
listwise
pairwise
evaluation
methods
aiming
provide
comprehensive
study
fairness
models
furthermore
mitigate
pairwise
fairness
issue
ﬁne-tuning
llms
unbiased
dataset
experimental
results
show
improvement
evaluation
best
knowledge
work
presents
ﬁrst
benchmark
results
investigating
fairness
issue
llms
rankers
summary
work
makes
contribution
follows
build
ﬁrst
llm
fair
ranking
benchmark
llms
text
ranker
incorporates
listwise
pairwise
evaluation
methods
consideration
binary
protected
attributes
conduct
extensive
comprehensive
experiments
revealing
fairness
problem
llms
real-word
datasets
propose
mitigation
strategy
involving
ﬁne-tuning
open-source
llms
using
lora
40
address
fairness
issue
observed
pairwise
evaluation
chapter
empirical
study
fairness
llms
rankers
74
listwise
ranking
gpt-3
gpt-4
protected
group
mistral
unprotected
group
llama2
rank
rank
rank
rank
rank
rank
utility
group
exposure
pairwise
ranking
gpt-3
gpt-4
mistral
llama2
items
ranked
higher
llms
percentage
unprotected
group
figure
5.2
proposed
evaluation
framework
schematic
diagram
represents
dual
evaluation
methodology
top
sequence
depicts
listwise
ranking
process
items
protected
unprotected
groups
presented
various
llms
gpt-3
gpt-4
mistral-7b
llama2
evaluated
utility
group
exposure
metrics
bottom
sequence
illustrates
pairwise
ranking
approach
contrasts
ranking
preference
llms
items
protected
unprotected
groups
quantifying
bias
percentage
unprotected
group
items
ranked
higher
5.2
llm
fair
ranking
deﬁne
set
queries
dataset
consisting
queries
set
items
comprising
items
query
exists
list
item
candidates
represent
i-th
query-item
pair
text
token
vector
xi
associated
relevance
score
yi
importantly
item
candidates
annotated
binary
attribute
indicating
classiﬁcation
either
belonging
protected
group
non-protected
group
attribute
representing
aspects
like
gender
race
crucial
highlights
potential
exposure
bias
present
ranking
prediction
process
next
present
evaluation
benchmark
dataset
introduce
two
fairness
evaluation
methods
listwise
pairwise
evaluation
chapter
empirical
study
fairness
llms
rankers
5.2
75
datasets
benchmark
leverage
datasets
trec
fair
ranking
track
29
years
2021
2022
primarily
focus
task
wikiproject
coordinators
search
relevant
articles
2022
dataset
containing
44
queries
2021
dataset
57
query
select
200
items
english
wikipedia
apply
deltr
89
experiment
methodology
introduce
discriminatory
pattern
sorting
candidates
categorizing
four
groups
experts
non-protected
group
experts
protected
group
non-experts
non-protected
group
non-experts
protected
group
speciﬁc
experts
deﬁned
relevant
candidates
given
query
non-experts
irrelevant
candidates
randomly
selected
relevant
candidates
queries
benchmark
includes
three
datasets
diﬀerentiated
protected
groups
trec
2022
gender
females
considered
protected
group
males
non-protected
trec
2022
location
trec
2021
location
non-europeans
designated
protected
group
europeans
serving
non-protected
group
5.2
listwise
evaluation
listwise
evaluation
assesses
fairness
two
perspectives
query-side
itemside
focusing
attributes
like
gender
measure
well
llms
integrate
underrepresented
groups
rankings
using
group
fairness
visibility
precision
chapter
empirical
study
fairness
llms
rankers
76
utility
query-side
analysis
checks
biases
protected
attributes
contrasting
gender-neutral
gender-sensitive
queries
verify
consistent
rankings
across
groups
methods
together
ensure
thorough
fairness
evaluation
llm
rankings
5.2
2.1
data
construction
fairness
evaluation
dataset
leverage
rankgpt
73
approach
standard
prompt
template
capture
user
instructions
detailing
preferences
task
details
dataset
incorporates
item-side
protected
groups
introduces
neutral
sensitive
query
templates
former
without
demographic
markers
latter
including
speciﬁc
references
attributes
like
gender
geography
queryside
fairness
assessment
speciﬁcally
template
neutral
sensitive
instructions
following
neutral
search
system
wikiproject
coordinators
users
goal
search
relevant
articles
produce
ranked
list
articles
needing
work
editors
can
consult
looking
work
search
query
query
rank
passages
based
relevance
search
query
item
d1
dn
sensitive
search
system
query-side
sensitive
attribute
wikiproject
coordinators
users
goal
search
relevant
articles
produce
ranked
list
articles
needing
work
editors
can
consult
looking
chapter
empirical
study
fairness
llms
rankers
77
work
search
query
query
rank
passages
based
relevance
search
query
item
d1
dn
5.2
2.2
metrics
group
exposure
ratio
listwise
fairness
evaluation
deﬁne
two
groups
candidates
within
non-protected
group
g0
protected
group
g1
latter
representing
historically
discriminated
groups
females
non-europeans
often
underrepresented
datasets
following
methodology
introduced
singh
joachims
70
measure
exposure
candidate
represented
text
token
xi
ranked
list
generated
probabilistic
ranking
model
expressed
exposure
xi
pi
va
5.1
pi
probability
places
document
rank
va
represents
position
bias
position
va
log
following
zehlike
castillo
89
focus
position
bias
top
position
v1
average
exposure
candidates
group
exposure
exposure
xi
5.2
xi
exposure
finally
deﬁne
group
exposure
ratio
exposure
g1
ratio
closer
1.0
indicates
fairer
ranking
list
chapter
empirical
study
fairness
llms
rankers
5.2
78
pairwise
evaluation
pairwise
evaluation
method
delve
item-side
fairness
presenting
pairs
items
llms
one
protected
group
one
non-protected
group
method
includes
two
distinct
tasks
relevant
items
comparison
provide
llms
pair
randomly
selected
relevant
items
prompting
determine
item
relevant
fairness
assessment
hinges
balance
number
items
recognized
relevant
groups
nearly
equal
count
signiﬁes
fairness
indicates
unbiased
relevance
assessment
fairness
quantiﬁed
ratio
recognized
relevance
groups
ratio
close
1.0
signaling
greater
fairness
irrelevant
items
comparison
similarly
present
pairs
irrelevant
items
follow
procedure
scenario
fair
llm
exhibit
similar
indiﬀerence
irrelevance
items
groups
reﬂected
ratio
approaching
1.0
pairwise
evaluation
employed
detect
biases
llm
rankings
towards
protected
unprotected
groups
directly
contrasting
items
varying
groups
method
uncovers
potential
group
preferences
within
llms
oﬀering
clear
view
fairness
diﬀerent
ranking
scenarios
chapter
empirical
study
fairness
llms
rankers
5.2
3.1
79
data
construction
pairwise
evaluation
use
ﬁxed
prompt
template
pairs
relevant
irrelevant
items
containing
one
protected
group
one
unprotected
group
mitigate
position
bias
two
items
pair
queried
twice
order
protected
unprotected
items
alternated
speciﬁcally
template
following
search
system
wikiproject
coordinators
users
goal
search
relevant
articles
produce
ranked
list
articles
needing
work
editors
can
consult
looking
work
rank
two
passages
based
relevance
query
query
item
d1
d2
5.2
3.2
metrics
pairwise
evaluation
metrics
calculate
proportion
times
items
protected
unprotected
groups
ranked
ﬁrst
additionally
compute
ratio
number
times
protected
group
items
ranked
ﬁrst
number
times
unprotected
group
items
ranked
ﬁrst
ratio
near
1.0
indicates
higher
fairness
5.3
results
analysis
benchmark
carefully
evaluate
popular
llms
including
gpt-3
gpt-4
llama2-13b
mistral-7b
44
section
details
analysis
performance
chapter
empirical
study
fairness
llms
rankers
80
across
listwise
pairwise
evaluations
5.3
0.1
eﬀect
window
step
size
window
10
10
20
step
10
20
0.1261
0.1295
0.1227
0.1205
fairness
0.9881
0.9634
0.9777
0.9628
table
5.1
evaluation
results
diﬀerent
choices
window
step
sizes
results
show
signiﬁcant
diﬀerences
ranking
fairness
metrics
select
window
size
step
size
listwise
evaluation
experiments
shown
table
5.1
conduct
additional
experiments
evaluate
diﬀerent
sets
window
sizes
step
sizes
experiments
conducted
listwise
evaluation
2022
gender
datasets
neutral
query
using
mistral-7b
model
set
window
size
ranging
20
step
size
10
following
sliding
window
strategy
provided
rankgpt
73
empirically
observe
signiﬁcant
diﬀerences
ranking
fairness
metrics
thus
adopted
small
window
step
size
window
size
step
size
accounting
less
gpu
memory
save
computation
resources
5.3
listwise
evaluation
results
listwise
evaluation
adopt
rankgpt
methodology
using
sliding
window
strategy
extract
ranking
lists
llms
use
window
size
step
size
across
tested
llms
given
models
trained
chapter
empirical
study
fairness
llms
rankers
81
trec
2022
gender
trec
2022
location
trec
2021
location
figure
5.3
predicted
rankings
distribution
protected
groups
trec
datasets
using
listwise
evaluation
plots
reveal
ranking
variability
potential
biases
gender
geographic
attributes
highlighting
areas
improvement
fairness
across
llms
extensive
internet
corpora
trec
datasets
derived
wikipedia
input
wikipedia
page
titles
approach
leverages
llms
inherent
knowledge
base
topics
additionally
include
two
neural
rankers
monot5
56
monobert
55
baseline
models
unlike
llms
use
full
text
wikipedia
webpages
input
neural
rankers
chapter
empirical
study
fairness
llms
rankers
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.1852
0.9964
0.1761
0.9559
0.1227
0.9919
0.1239
0.9955
0.1261
0.9881
0.1216
1.0304
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.2110
0.9739
0.1980
1.0031
0.1440
0.9308
0.1240
0.9268
0.1230
0.9426
0.1280
0.9607
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.2018
1.0406
0.1974
1.0340
0.1184
0.9820
0.1167
0.9850
0.1430
0.9856
0.1211
0.9634
male
20
fairness
0.0830
0.7809
0.1000
0.8101
0.0841
0.9463
0.1080
0.9504
0.0966
0.9382
0.0920
0.9661
82
female
20
fairness
0.5239
1.9402
0.5102
1.7475
0.1705
1.2186
0.1761
1.2576
0.2102
1.4879
0.1614
1.2550
trec
2022
gender
european
20
fairness
0.2800
0.8543
0.2860
0.8890
0.1500
0.8846
0.1510
0.8889
0.1490
0.8895
0.1340
0.9130
non-european
20
fairness
0.0180
1.4682
0.0370
1.3201
0.1480
0.9368
0.1420
0.9432
0.0930
1.1073
0.1030
1.0227
trec
2022
location
european
20
fairness
0.3035
0.8483
0.2658
0.9254
0.1421
0.9173
0.1544
0.9071
0.1614
0.9142
0.1105
0.9247
non-european
20
fairness
0.0158
1.5039
0.0728
1.3143
0.1228
0.9841
0.1325
0.9877
0.0684
1.1448
0.1105
1.0325
trec
2021
location
table
5.2
listwise
evaluation
results
measure
fairness
compute
exposure
ratio
protected
non-protected
group
values
closer
1.0
indicate
greater
visibility
protected
group
vice
versa
ranking
metric
higher
precision
10
10
scores
indicate
better
performance
5.3
1.1
item-side
analysis
table
5.2
monot5
monobert
exhibit
robust
precision
20
scores
reﬂecting
eﬀectiveness
ranking
however
fairness
metrics
reveal
gap
equitable
gender
representation
monot5
slightly
outperforming
monobert
front
chapter
empirical
study
fairness
llms
rankers
83
performance
discrepancy
likely
models
utilize
complete
text
wikipedia
pages
providing
wealth
features
represent
items
comprehensively
hand
llms
face
constraints
due
maximum
token
limits
input
limiting
capacity
fully
exploit
extensive
textual
information
available
trec
datasets
thereby
impacting
ranking
capability
among
llms
including
gpt-3
gpt-4
mistral-7b
llama2-13b
precision
20
scores
comparatively
lower
neural
ranking
models
may
reﬂect
generative
models
broader
focus
beyond
just
ranking
tasks
fairness
metrics
llms
varied
gpt-3
gpt-4
manage
stay
closer
ideal
fairness
ratio
indicating
balanced
treatment
gender
groups
mistral-7b
maintaining
similar
precision
falls
behind
fairness
indicating
potential
gender
bias
ranking
llama2-13b
although
consistent
approach
fairness
reveals
room
improvement
precision
contrasting
neural
rankers
llms
becomes
apparent
although
neural
rankers
demonstrate
higher
precision
necessarily
outperform
llms
terms
fairness
observation
underscores
importance
considering
fairness
particularly
users
prioritize
precision
speciﬁc
applications
within
llm
group
uniformity
achieving
fairness
suggesting
models
training
design
inherent
biases
may
inﬂuence
ability
rank
fairly
chapter
empirical
study
fairness
llms
rankers
5.3
1.2
84
query-side
analysis
analyzing
query-side
fairness
table
5.2
focus
whether
llms
provide
similar
ranking
performance
diﬀerent
query
attributes
male
vs
female
european
vs
non-european
reveals
consistent
trend
across
neural
ranking
models
llms
tend
favor
female
european
queries
male
non-european
ones
fairness
metrics
llms
like
gpt-3
gpt-4
mistral-7b
llama2-13b
relatively
close
indicating
attempt
balanced
treatment
precision
20
scores
suggest
diﬀerent
story
clear
skew
towards
female
european
queries
observed
pattern
evident
monot5
monobert
points
underlying
bias
persists
despite
eﬀorts
achieve
equitable
treatment
across
query
attributes
underscoring
need
enhanced
model
training
fairness
optimization
figure
5.3
plot
predicted
ranking
protected
groups
highlights
distinct
patterns
fairness
ranking
performance
neural
rankers
llms
llms
demonstrate
tighter
rank
distributions
exhibit
biases
toward
certain
query
attributes
example
disparities
observed
treatment
gender
geographic
attributes
monot5
monobert
often
ranking
female
european
queries
favorably
trend
also
noted
varying
degrees
within
llms
suggests
neural
rankers
may
excel
precision
llms
oﬀer
consistent
rankings
though
neither
group
devoid
fairness
issues
ﬁndings
emphasize
necessity
tuning
bias
mitigation
neural
rankers
llms
chapter
empirical
study
fairness
llms
rankers
85
ensure
equitable
treatment
across
query
attributes
5.3
pairwise
evaluation
results
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2407
0.2453
0.2275
0.2496
0.2366
0.0995
0.1227
0.2293
ratio
1.0190
1.0971
0.4206
1.8694
irrelevant
items
unprotected
protected
0.1797
0.2979
0.2033
0.2939
0.1335
0.1160
0.0920
0.2913
ratio
1.6580
1.4430
0.8689
3.1643
trec
2022
gender
females
protected
group
males
non-protected
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2638
0.2537
0.2347
0.2878
0.2484
0.4168
0.1521
0.2290
ratio
0.9615
1.2262
1.6779
1.5052
irrelevant
items
unprotected
protected
0.3199
0.2245
0.2759
0.2401
0.1876
0.1928
0.2444
0.1643
ratio
0.7500
0.8701
1.0277
0.6725
trec
2022
location
non-europeans
protected
europeans
non-protected
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2117
0.3150
0.2148
0.3125
0.2582
0.4137
0.1490
0.2688
ratio
1.4877
1.4545
1.6019
1.8035
irrelevant
items
unprotected
protected
0.2385
0.2616
0.2428
0.2598
0.2516
0.1628
0.2540
0.1752
ratio
1.0968
1.0701
0.6471
0.6898
trec
2021
location
non-europeans
protected
europeans
non-protected
table
5.3
pairwise
evaluation
results
table
displays
fairness
metrics
llms
ranking
relevant
irrelevant
item
pairs
one
protected
unprotected
groups
includes
percentages
items
ranked
ﬁrst
group
ratio
reﬂecting
fairness
varying
levels
fairness
across
llms
particularly
irrelevant
pairings
highlight
importance
enhancing
fairness
llms
pairwise
evaluations
detailed
table
5.3
focus
assessing
fairness
various
llms
studying
rank
pairs
items
considered
relevant
irrelevant
analysis
aims
reveal
whether
models
display
biases
toward
items
speciﬁc
groups
gpt-3
consistently
shows
preference
female
items
scenarios
inclination
pronounced
irrelevant
items
chapter
empirical
study
fairness
llms
rankers
86
suggesting
bias
favor
female
items
similarly
gpt-4
displays
moderate
bias
towards
female
items
ratios
indicating
stronger
bias
irrelevant
contexts
observed
trend
across
models
datasets
signals
area
improvement
pointing
need
balanced
algorithms
favor
one
group
another
particularly
situations
item
relevance
neutral
contrastingly
mistral-7b
shows
distinct
bias
towards
male
items
relevant
pairs
notably
trec
2022
gender
dataset
raising
questions
model
decisionmaking
process
suggesting
algorithm
may
weigh
male
items
heavily
relevant
however
bias
diminishes
irrelevant
pairs
indicating
diﬀerent
algorithmic
behavior
contexts
llama2-13b
hand
presents
signiﬁcant
bias
towards
female
items
across
datasets
relevant
irrelevant
pairs
concerning
overall
fairness
overall
llms
show
nuanced
biases
others
like
llama2-13b
require
interventions
ensure
fair
equitable
treatment
across
group
attributes
5.3
overall
evaluation
overall
analyzing
listwise
pairwise
evaluation
results
table
5.2
table
5.3
observe
complex
picture
fairness
listwise
evaluation
based
group
exposure
ratios
suggests
fair
representation
diﬀerent
groups
pairwise
evaluation
reveals
unfairness
llms
inconsistency
particularly
evident
chapter
empirical
study
fairness
llms
rankers
87
llms
rank
pairs
relevant
irrelevant
items
protected
unprotected
groups
5.4
enhancing
fairness
lora
percentage
protected
vs
unprotected
group
items
ranked
ﬁrst
across
diﬀerent
trec
datasets
ratio
protected
unprotected
group
across
diﬀerent
trec
datasets
figure
5.4
impact
lora
fine-tuning
mistral-7b
fairness
figure
shows
percentage
ﬁrst-ranked
items
protected
unprotected
groups
figure
demonstrates
resulting
fairness
ratios
lora-adjusted
model
yields
ratios
closer
ideal
fairness
benchmark
1.0
across
trec
datasets
employed
lora
40
ﬁne-tune
mistral-7b
model
approach
involves
creating
balanced
training
dataset
equal
representation
responses
protected
unprotected
groups
balanced
dataset
aims
steer
model
towards
fairer
rankings
evaluating
pairs
relevant
irrelevant
items
diverse
groups
implementation
lora
module
facilitated
using
peft
53
package
aligning
parameter-eﬃcient
methodology
outlined
original
lora
study
speciﬁcally
focuses
adapting
attention
weights
simplify
enhance
parameter-eﬃciency
opted
freeze
parameters
case
set
optimal
rank
deeming
low-rank
adaptation
matrix
adequate
chosen
88
learning
rate
0.003
batch
size
set
conﬁgurations
selected
based
considerations
speciﬁc
study
dataset
comprising
approximately
140
000
item
pairs
randomly
sampled
trec
dataset
facilitate
comprehensive
training
process
conducted
nvidia
a100
80gb
needs
approximately
30
hours
split
queries
training
testing
using
80
training
remaining
20
testing
results
ﬁne-tuning
mistral-7b
lora
illustrated
figure
5.4
posttuning
noticeable
reduction
consistent
responses
model
queried
twice
reversed
item
orders
indicates
increase
response
variability
positive
indicator
fairness
less
predictability
responses
can
mitigate
systematic
bias
improvement
fairness
supported
figure
4b
outcomes
post-lora
ﬁne-tuning
show
ratios
approaching
1.0
indicating
equitable
treatment
protected
unprotected
groups
model
5.5
conclusion
conclusion
in-depth
analysis
reveals
intricate
biases
present
large
language
models
evaluated
fairness
listwise
pairwise
methods
listwise
evaluations
painted
picture
relative
fairness
deeper
investigation
via
pairwise
evaluations
uncovered
subtler
profound
biases
often
favored
certain
protected
groups
implementation
lora
ﬁne-tuning
mistral-7b
model
89
yielded
encouraging
strides
towards
rectifying
biases
demonstrating
enhanced
fairness
model
output
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
6.1
introduction
pinterest
visual
discovery
platform
allows
users
discover
save
ideas
various
interests
fashion
home
decor
travel
become
popular
destination
users
search
discover
new
products
ideas
inspiration
result
also
become
attractive
advertising
platform
businesses
looking
reach
engage
target
audience
support
growing
demand
online
advertising
pinterest
developed
large-scale
advertisement
serving
platform
using
multi-cascade
ranking
system
51
deliver
relevant
ads
users
90
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
ads
request
91
ads
inventory
ads
targeting
ads
retrieval
ads
ranking
auction
winners
pinterest
app
auction
ads
delivery
stack
figure
6.1
life
cycle
online
ads
delivery
high
level
ads
request
triggered
user
opens
pinterest
app
starts
new
session
ads
request
will
sent
ads
delivery
system
query
dozen
ads
ads
delivery
backend
ad
candidates
inventory
will
ﬂow
various
stages
like
targeting
retrieval
ranking
auction
sends
auction
winners
back
mobile
app
selected
ads
will
visible
user
like
many
online
advertising
platforms
multi-cascade
recommendation
system
contains
several
stages
ﬁlter
rank
ads
based
various
business
logic
modeling
signals
shown
figure
6.1
typical
ads
serving
system
four
main
stages
ads
targeting
ads
retrieval
ads
ranking
ads
auction
ads
targeting
ﬁrst
stage
stage
selects
ads
meet
targeting
criterion
preset
advertisers
ads
retrieval
second
stage
right
ads
targeting
stage
various
mechanisms
including
retrieval
models
models
used
retrieval
stage
used
select
smaller
subset
ad
candidates
millions
candidates
received
targeting
stage
selected
ad
candidates
passed
ads
ranking
stage
comprehensive
scoring
ranking
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
92
ads
ranking
stage
set
sophisticated
models
developed
accurately
score
speciﬁc
objectives
ctr
cvr
relevance
etc
ad
candidate
selected
retrieval
stage
model
prediction
stage
will
directly
impact
many
key
aspects
quality
delivered
ads
result
stage
able
score
limited
number
ad
candidates
spends
much
allotted
time
budget
score
ad
candidate
using
complex
performant
models
ensure
prediction
accuracy
ads
auction
last
stage
serving
stack
main
objective
make
ﬁnal
decision
auction
candidate
whether
candidate
delivered
user
position
targeting
surface
candidate
inserted
afterwards
winning
candidates
will
delivered
user
device
inserted
corresponding
position
user
will
see
ads
respond
ads
various
user
actions
discussed
ads
retrieval
second
stage
delivery
system
responsible
retrieving
valuable
ads
large
set
ad
candidates
query
goal
stage
retrieve
relevant
ads
also
minimizing
number
irrelevant
low-quality
ones
requires
use
machine
learning
models
can
eﬃciently
predict
relevance
quality
ads
candidate
based
variety
features
signals
diﬃcult
problem
retrieval
stage
eﬃciently
fulﬁll
mission
due
several
key
challenges
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
93
selected
subset
candidates
high
quality
avoid
wasting
capacity
expensive
full
ads
ranking
low
quality
ads
size
selected
candidates
small
enough
subsequent
comprehensive
ranking
ads
ranking
stage
can
handle
ad
candidates
retrieval
models
required
score
rank
post
targeting
ad
candidates
order
millions
retrieval
models
will
accessible
lot
ml
signals
especially
expensive
real-time
ones
will
also
able
leverage
sophisticated
model
architectures
due
scalability
consideration
discussed
previous
point
result
building
performant
retrieval
models
constraints
challenging
problem
machine
learning
domain
currently
retrieval
models
ads
platforms
use
two-tower
model
architecture
proposed
covington
et
al
25
among
challenges
associated
retrieval
model
development
optimization
selection
bias
training
data
long-lasting
problem
impairing
performance
models
work
focus
issue
data
selection
bias
ads
retrieval
stage
pinterest
multi-cascade
ads
ranking
system
training
data
used
train
model
reﬂects
real
user
preferences
also
includes
production
model
personalized
recommendations
means
training
data
representative
overall
population
advertisements
can
lead
inaccurate
results
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
94
addition
distribution
discrepancy
training
data
observed
user
actions
true
labels
inference
data
composed
ad
candidates
targeting
stage
can
impact
model
performance
address
data
selection
bias
ads
retrieval
funnel
ﬁrst
investigated
data
distribution
across
various
types
ad
candidates
datasets
assessed
various
ml
techniques
including
unsupervised
domain
adaptation
uda
83
improve
performance
retrieval
models
number
ad
candidates
real
user
action
small
will
beneﬁcial
model
training
leverage
unlabeled
ad
candidates
data
particularly
ones
similar
distribution
inference
data
one
diﬃculty
model
training
strategy
determining
eﬀectively
use
unlabeled
data
points
consistent
distribution
compared
model
inference
data
work
leveraged
various
state-of-the-art
sota
methods
incorporate
unlabeled
data
training
retrieval
models
additionally
developed
modiﬁed
version
uda
muda
improve
performance
naive
implementation
uda
retrieval
model
training
online
experimental
results
show
couple
methods
potentially
improve
performance
ads
ranking
system
compared
knowledge
distilled
model
current
production
environment
methods
thus
contribution
summarized
following
identiﬁed
characterized
selection
bias
issue
upper
funnel
multi-cascade
advertisement
recommendation
system
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
95
surveyed
series
sota
modeling
strategies
evaluated
performance
oﬄine
online
settings
proposed
modiﬁed
version
unsupervised
domain
adaptation
muda
provides
best
online
performance
among
modeling
strategies
examined
online
experiments
show
muda
also
outperforms
current
production
model
post-targeting
candidates
ads
inventory
retrieval
model
post-retrieval
candidates
targeting
auction
candidates
ranking
filtering
training
pipeline
auction
winners
auction
training
data
figure
6.2
distribution
features
labels
across
three
ads
datasets
related
retrieval
modeling
shows
ﬂow
major
ad
candidates
along
ads
delivery
funnel
shows
distribution
empirical
vtcvr
one
key
retrieval
model
features
across
three
datasets
retrieval
training
serving
shows
distribution
empirical
good
click
rate
one
key
retrieval
model
features
across
three
datasets
retrieval
training
serving
shows
distribution
ranking
model
predictions
used
pseudo
label
retrieval
model
training
across
three
datasets
note
exact
values
x-axes
hidden
conﬁdentiality
reasons
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
6.2
96
bias
pinterest
ads
illustrated
figure
6.1
pinterest
ads
serving
system
consists
four
stages
ads
targeting
ads
retrieval
ads
ranking
ads
auction
stage
scores
ﬁlters
ad
candidates
based
request
ads
content
features
given
ad
request
ads
retrieval
narrows
millions
ad
candidates
couple
thousands
candidates
sent
ads
ranking
accurate
prediction
user
action
well
ﬁltering
finally
run
ads
auctions
survivors
determine
auction
winners
based
predeﬁned
utility
function
advertiser
bid
retrieval
stage
latency
limit
crucial
large
number
ad
candidates
database
adopt
two-tower
dnn
structure
25
candidate
embedding
computed
oﬄine
serving
model
will
produce
score
ad
candidate
calculating
dot-product
precomputed
candidate
embedding
query
embedding
computed
on-the-ﬂy
request
6.2
datasets
training
pipeline
mentioned
earlier
ads
serving
system
consists
targeting
retrieval
ranking
auction
shown
ﬁgure
2a
millions
candidates
ads
inventory
will
ﬂow
various
stages
across
ads
delivery
funnel
small
set
valuable
ads
will
survive
delivered
users
speciﬁcally
initial
ads
inventory
candidates
will
selected
ads
targeting
reﬁne
set
ad
candidates
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
97
post-targeting
candidates
will
scored
ranked
retrieval
models
selected
retrieval
models
survivors
post-retrieval
candidates
will
ﬁltered
selected
various
business
logics
models
ranking
stage
leads
new
set
ad
candidates
auction
candidates
will
evaluated
auction
stage
auction
stage
will
pick
dozen
winners
auction
candidates
deliver
ﬁnal
survivors
auction
winners
pinterest
users
existing
retrieval
models
two
types
training
data
collected
auction
candidates
auction
winners
latter
dataset
includes
observed
user
actions
true
labels
former
one
includes
ranking
model
predictions
pseudo
labels
ranking
model
predictions
used
auction
stage
determine
winning
ads
auction
candidates
pool
currently
use
ranking
model
predictions
pseudo
labels
train
retrieval
models
aim
maximize
funnel
eﬃciency
deliver
valuable
ad
candidates
pinterest
users
ensure
model
freshness
retrieval
models
continuously
trained
evaluated
daily
basis
speciﬁcally
model
snapshot
trained
day
data
loaded
train
day
data
newly
trained
model
evaluated
day
data
daily
training
setup
enables
model
capture
recent
patterns
keeping
responsive
new
trends
second-day
evaluation
allows
detection
possible
overﬁt
abnormal
behavior
serving
production
traﬃc
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
6.2
98
selection
bias
mentioned
retrieval
models
currently
trained
auction
candidates
auction
winners
ranking
model
predictions
used
pseudo
labels
setup
inevitably
introduces
data
selection
bias
particularly
inconsistency
dataset
training
serving
79
serving
time
however
model
needs
make
predictions
post-targeting
ad
candidates
auction
candidates
winners
small
subset
post-targeting
candidates
generated
various
business
logics
ranking
models
distribution
datasets
will
inconsistent
model
training
serving
figure
2a
illustrates
concept
inconsistency
ads
datasets
used
training
inferencing
cycle
retrieval
models
demonstrate
bias
analyzed
distributions
pseudo
labels
two
important
retrieval
model
features
across
three
diﬀerent
datasets
post-targeting
candidates
auction
candidates
auction
winners
figure
2b
2c
2d
demonstrates
distributions
diﬀerent
across
three
datasets
distribution
diﬀerence
much
signiﬁcant
two
datasets
used
current
retrieval
model
training
one
used
retrieval
model
serving
simplicity
rest
work
will
interchangeably
use
following
terms
post-targeting
candidates
serving
datasets
retrieval
models
unbiased
dataset
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
6.2
99
problem
formulation
simplicity
represent
data
record
tuple
three
elements
feature
request
containing
user
proﬁle
features
context
features
search
term
search
surface
advertisement
candidate
features
groundtruth
label
observed
user
actions
additionally
let
represent
distribution
request
features
advertisement
features
inventory
represent
full
distribution
request
ad
candidates
pairs
finally
let
fθ
represent
model
trainable
parameter
loss
function
want
minimize
model
maps
request
candidate
features
numeric
value
function
maps
two
numeric
values
scalar
loss
value
ideally
want
minimize
training
loss
unbiased
data
min
lideal
fθ
fθ
6.1
reality
impossible
calculate
loss
function
unbiased
dataset
true
labels
available
result
leverage
biased
dataset
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
100
whose
true
labels
available
us
next
section
will
describe
series
methods
use
biased
unbiased
datasets
build
model
score
post-targeting
ad
candidates
system
6.3
solution
6.3
naive
method
binary
classiﬁcation
naive
method
train
simple
classiﬁcation
model
common
way
training
click
classiﬁcation
model
based
dataset
observed
user
actions
ones
user
clicks
treated
positive
examples
ones
clicks
treated
negatives
naive
method
will
optimize
following
loss
function
min
lnaive
fθ
fθ
6.2
dataset
denotes
set
request
auction
winners
pairs
observed
user
actions
6.3
in-batch
negative
classiﬁcation
similar
naive
classiﬁcation
method
will
build
classiﬁcation
model
based
biased
dataset
observed
user
actions
true
labels
real-world
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
101
advertising
system
viewed
ads
without
user
clicks
necessarily
reliable
negative
examples
users
still
ﬁnd
ads
valuable
even
take
actions
moment
diﬀerent
naive
classiﬁcation
method
generate
negative
examples
introducing
ad
candidates
requests
training
batch
current
request
following
common
setup
35
45
84
85
speciﬁcally
delivered
ads
user
clicks
included
training
data
clicked
ads
diﬀerent
requests
batch
treated
negative
examples
6.3
knowledge
distillation
ranking
models
trained
complex
architectures
numerous
input
features
contrast
retrieval
models
limit
architecture
two-tower
dnn
well
available
features
due
demanding
requirement
scalability
low
serving
latency
minimize
performance
loss
knowledge
distillation
kd
37
adopted
means
retrieval
models
trained
ranking
model
predictions
pseudo
labels
formally
denoting
ranking
model
optimize
following
loss
function
min
lkd
fθ
6.3
fθ
6.3
transfer
learning
core
idea
transfer
learning
train
model
source
domain
data
ﬁne
tune
part
parameters
target
domain
particularly
dnn
model
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
102
early
layers
usually
ﬁxed
ﬁne
tuning
shown
represent
primitive
general
features
59
case
retrieval
model
two-tower
dnn
data
distribution
discrepancy
across
diﬀerent
datasets
ad
candidates
result
use
unbiased
data
ﬁne
tune
ad
embedding
tower
keep
query
tower
unchanged
6.3
adversarial
regularization
another
view
bias
issue
representation
learned
biased
data
general
enough
applied
unbiased
dataset
leading
performance
degradation
can
therefore
add
regularization
learning
intermediate
output
model
information
indicating
data
source
technique
known
adversarial
adv
learning
36
dnn
model
can
split
two
parts
former
one
takes
raw
input
gives
intermediate
output
latter
one
takes
intermediate
output
gives
ﬁnal
prediction
adversarial
regularization
trains
data
source
classiﬁer
intermediate
output
negative
whose
loss
function
added
original
one
regularization
formally
let
f1
f2
denote
two
parts
dnn
denotes
classiﬁer
loss
function
data
source
classiﬁer
deﬁned
equation
6.4
lcls
log
f1
log
f1
6.4
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
103
ﬁnal
loss
function
adversarial
regularization
shown
equation
6.5
ladv
ltarget
f2
f1
λlcls
6.5
ltarget
original
loss
function
trains
target
model
hyper
parameter
weighting
regularization
goal
minimize
ladv
regarding
f1
f2
lcls
regards
6.3
unsupervised
domain
adaptation
uda
unsupervised
domain
adaptation
technique
train
model
works
well
target
domain
unlabeled
data
using
labeled
samples
source
domain
uda
method
applied
situation
feature
distribution
data
labeling
diﬀerent
source
target
domains
pinterest
ads
system
source
domain
biased
dataset
labels
target
domain
unbiased
dataset
without
labels
result
data
selection
bias
formulated
uda
problem
83
6.3
6.1
naive
uda
naive
method
directly
train
model
unbiased
dataset
will
inconsistency
training
serving
ground
truth
labels
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
104
unbiased
dataset
missing
pseudo
labels
will
generated
separate
model
trained
biased
dataset
source
domain
ground
truth
label
available
following
annotation
scheme
let
denotes
ranking
model
used
generate
pseudo
labels
unbiased
dataset
source
domain
optimization
goal
becomes
following
min
lnaiveu
da
fθ
fθ
6.6
data
source
domain
however
method
many
drawbacks
reality
unbiased
data
sampling
volume
small
due
infra
cost
might
lead
performance
degradation
additionally
high-quality
candidates
might
suﬃciently
representative
training
data
source
domain
will
discuss
performance
experiment
section
6.3
6.2
modiﬁed
uda
uda
quality
pseudo
labels
critical
performance
trained
models
naive
uda
mechanism
guarantee
quality
pseudo
labels
especially
pseudo
label
generating
model
remains
suﬃciently
accurate
previously
saito
et
al
67
proposed
use
asymmetric
tri-training
method
two
separate
pseudo
label
generating
models
used
mechanism
ensure
pseudo
label
quality
however
requirement
maintain
second
pseudo
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
105
label
generating
model
reasonable
performance
will
costly
real-world
advertising
system
tens
even
hundreds
retrieval
models
needed
retrained
daily
basis
additionally
will
inhibitively
costly
pseudo
label
derived
set
models
second
set
several
models
will
required
developed
maintained
leverage
tri-training
method
address
pseudo
label
quality
issue
real-world
ads
retrieval
transform
original
numeric
pseudo
label
prediction
ranking
model
binary
classiﬁcation
label
based
carefully
chosen
thresholds
formally
let
δl
δh
denote
two
thresholds
δl
δh
shown
equation
6.8
numeric
pseudo
labels
lower
ﬁrst
threshold
treated
negative
higher
second
threshold
treated
positive
data
records
numeric
pseudo
labels
falling
two
thresholds
removed
training
dataset
rationale
behind
keep
records
ranking
model
conﬁdent
discard
ones
close
hyperplane
ranking
classiﬁer
now
optimization
goal
training
retrieval
model
becomes
following
min
lm
da
fθ
δl
δh
φδδhl
fθ
6.7
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
106
φδδhl
pseudo
classiﬁcation
label
indicator
converting
ranking
model
predictions
binary
label
according
given
thresholds
shown
equation
φδδhl
δh
δl
6.8
select
thresholds
adopt
data
driven
method
particularly
bucketize
ranking
model
prediction
check
corresponding
empirical
click
rate
bucket
thresholds
chosen
sudden
change
empirical
click
rates
6.4
experiments
results
section
will
ﬁrst
describe
model
training
details
introduce
evaluation
settings
metrics
will
present
discuss
results
oﬄine
online
experiments
compare
performance
proposed
solutions
6.4
datasets
described
section
6.2
two
existing
training
data
sources
auction
candidates
auction
winners
biased
datasets
section
6.2
introduced
unbiased
dataset
randomly
sampled
post-targeting
dataset
unbiased
dataset
required
scored
ranked
retrieval
models
production
system
taking
consideration
infrastructure
cost
volume
resulting
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
107
dataset
sample
100
000
queries
000
advertisement
candidates
query
create
unbiased
dataset
every
day
6.4
experimental
setting
examine
performance
de-biasing
methods
pinterest
ads
dataset
implement
models
conduct
systematic
experiments
collect
evaluation
results
real-world
production
system
binary
classiﬁcation
models
trained
auction
winners
real
user
actions
true
labels
aims
provide
supplemental
evidence
indicate
reason
current
production
model
directly
trained
real
user
actions
following
describes
details
baseline
models
binary
classiﬁcation
since
regression
model
trained
pseudo
labels
generated
ads
ranking
models
performance
classiﬁcation
model
directly
trained
user
actions
worth
examining
train
model
use
auction
winner
training
dataset
labels
deﬁned
section
6.3
binary
cross
entropy
bce
loss
function
in-batch
negative
classiﬁcation
also
train
classiﬁcation
model
inbatch
negative
sampling
uses
candidates
batch
data
negative
samples
given
query
use
1000
batch
size
use
batch
size
number
hard
negatives
loss
function
model
trained
auction
winner
dataset
use
candidates
user
clicks
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
108
knowledge
distillation
current
production
model
training
use
ads
ranking
model
output
pseudo
label
mean
absolute
logarithmic
error
logmae
loss
function
production
model
training
dataset
includes
auction
candidates
auction
winners
besides
production
model
also
train
another
one
auction
winners
evaluation
refer
ﬁrst
one
production
model
second
one
knowledge
distillation
model
summarize
implementation
details
debiasing
model
following
transfer
learning
transfer
learning
model
use
biased
unbiased
dataset
also
use
ranking
model
predictions
pseudo
labels
logmae
loss
function
train
retrieval
model
adversarial
learning
adversarial
learning
model
implement
data
source
discriminator
one-layer
mlp
sigmoid
activation
function
biased
unbiased
datasets
used
train
retrieval
model
ads
ranking
model
used
generate
pseudo
labels
training
datasets
naive
unsupervised
domain
adaptation
uda
train
naive
uda
model
use
unbiased
dataset
pseudo
labels
generated
ads
ranking
model
predictions
logmae
loss
function
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
109
modiﬁed
unsupervised
domain
adaptation
muda
use
unbiased
dataset
pseudo
labels
derived
discussed
section
6.3
6.2
transforming
ranking
model
predictions
binary
classes
bce
loss
function
model
training
hyper-parameters
use
6144
batch
size
0.0001
learning
rate
unless
deﬁned
speciﬁcally
two-tower
model
use
four
fully
connected
layers
ﬁnal
layer
output
dimension
32
use
sigmoid
activation
function
output
layer
use
selu
46
layers
6.4
evaluation
metrics
oﬄine
evaluation
metrics
use
auc-roc
score
classiﬁcation
regression
models
evaluate
models
one
day
auction
winners
dataset
online
experiments
compare
models
production
model
report
change
total
impressions
numbers
δimp
click
rate
δctr
30
seconds
click
rate
δgctr30
ads
evaluation
besides
user-side
metrics
mentioned
also
report
metrics
relate
advertiser
experience
metrics
impression
conversion
rate
ratio
icvr
measures
eﬀectiveness
ad
campaign
converting
impressions
conversions
cost
per
action
cpa
measures
cost
advertiser
positive
user
action
currently
exclusively
applied
conversion
ads
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
110
due
information
conﬁdentiality
report
lift
metrics
compared
current
production
model
6.4
oﬄine
evaluation
models
production
model
binary
classiﬁcation
in-batch
negative
knowledge
distillation
transfer
learning
adversarial
learning
naive
uda
muda
auc-roc
0.895
0.895
0.701
0.896
0.890
0.896
0.841
0.844
table
6.1
auc-roc
evaluation
dataset
models
knowledge
distillation
adversarial
learning
binary
classiﬁcation
trained
auction
winners
dataset
usually
better
oﬄine
evaluation
results
oﬄine
evaluation
evaluate
regression
classiﬁcation
models
using
auc-roc
evaluation
dataset
use
auction
winners
contain
real
user
clicks
shown
table
6.1
compared
production
model
models
knowledge
distillation
transfer
learning
binary
classiﬁcation
adversarial
models
similar
performance
terms
auc-roc
score
results
expected
training
datasets
include
auction
winners
models
in-batch
negative
model
trained
positive
candidates
auction
winners
dataset
perform
well
oﬄine
evaluation
negative
candidates
included
training
dataset
native
uda
muda
models
trained
post-targeting
datasets
feature
distribution
discrepancy
figure
2a
leads
lower
performance
models
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
111
summarize
oﬄine
evaluation
expected
see
models
trained
evaluated
source
data
better
performance
trained
diﬀerent
sources
data
however
oﬄine
evaluation
necessarily
reﬂect
true
model
performance
production
system
especially
serving
data
used
online
experiments
completely
diﬀerent
distribution
thus
following
sections
conduct
systematic
online
experiments
compare
performance
aforementioned
models
6.4
online
experiments
6.4
5.1
overall
evaluation
table
6.2
shows
overall
online
evaluation
results
models
among
metrics
will
focus
change
gctr30
models
optimized
towards
objective
binary
classiﬁcation
model
decreased
gctr30
indicating
signiﬁcant
drop
quality
user
engagement
recommended
ads
also
shows
largest
decrease
ctr
highest
increase
impressions
means
ads
delivered
users
fewer
got
clicked
contrast
in-batch
negative
knowledge
distillation
models
positive
changes
gctr30
however
decrease
impression
main
reason
gctr30
increase
less
ads
shown
users
although
three
models
binary
classiﬁcation
in-batch
negative
knowledge
distillation
suﬀer
selection
bias
training
dataset
latter
two
perform
better
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
112
binary
classiﬁcation
model
training
negative
candidates
always
query
whereas
in-batch
negative
classiﬁcation
model
trained
random
sampled
candidates
diﬀerent
queries
within
batch
diﬀerence
source
negative
candidates
provides
model
diverse
informative
training
data
results
overﬁtting
speciﬁc
query
knowledge
distillation
training
data
labels
ranking
model
predictions
whose
values
contain
richer
information
raw
binary
click-or-not
labels
models
binary
classiﬁcation
in-batch
negative
knowledge
distillation
transfer
learning
adversarial
learning
naive
uda
muda
δimp
0.95
2.25
3.26
0.43
0.28
0.45
0.92
δctr
5.51
4.45
0.25
1.88
0.45
3.05
0.47
δgctr30
12.66
4.68
5.97
4.35
0.66
4.80
5.07
table
6.2
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
various
models
types
ads
in-batch
negative
knowledge
distillation
methods
improve
gctr30
cost
impression
drop
muda
method
recommend
ads
higher
quality
observed
increased
gctr30
without
impression
drop
transfer
learning
model
small
increase
impressions
also
negative
change
gctr30
warm
start
weights
transfer
learning
model
similar
results
decrease
user
engagements
case
problem
transfer
learning
model
ﬁne
tuning
unbiased
dataset
candidates
unbiased
dataset
randomly
sampled
query
high
quality
ones
might
underrepresented
adversarial
model
similar
results
decrease
gctr30
slight
increase
impressions
compared
transfer
learning
model
adversarial
model
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
113
better
performance
user
engagements
adversarial
model
classiﬁer
serves
regularizer
prevent
embedding
tower
learning
domain
speciﬁc
embedding
certain
log
source
unlike
transfer
learning
model
debiasing
technique
adversarial
model
rely
quality
unbiased
training
data
training
classiﬁer
unsupervised
use
log
source
auction
winner
auction
candidates
ground
truth
label
able
successfully
train
classiﬁer
classify
log
source
classiﬁer
used
adversarial
regularizer
help
train
unbiased
embedding
model
however
compared
production
model
decrease
gctr30
may
indicate
restriction
embedding
learning
makes
model
drop
information
critical
online
evaluations
naive
uda
model
average
performance
compared
baseline
models
naive
uda
model
trained
unbiased
dataset
contains
pseudo
label
generated
ranking
model
reason
naive
uda
model
performs
badly
similar
reason
transfer
learning
model
performed
poorly
since
unbiased
dataset
collected
random
sampling
post-targeting
ad
candidates
addition
existing
queries
sampled
candidates
mostly
negative
samples
help
train
good
retrieval
model
contrast
modiﬁed
uda
muda
model
much
higher
gctr30
production
model
number
impressions
increases
higher
user
engagement
suggests
muda
model
delivers
ads
higher
quality
users
compared
naive
uda
model
muda
model
transforms
numerical
pseudo
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
114
labels
generated
ranking
model
binary
classes
determined
certain
thresholds
model
also
uses
bce
loss
lift
user
engagement
metrics
suggests
label
transformation
improves
quality
pseudo
labels
used
muda
transforming
numerical
pseudo
labels
binary
ones
prevented
model
overly
ﬁtting
ranking
model
prediction
every
single
candidate
rank
ranking
model
high
conﬁdence
6.4
5.2
evaluation
ads
objective
type
models
in-batch
negative
muda
δimp
8.70
0.32
awareness
δctr
δgctr30
2.41
13.74
2.71
1.97
δimp
1.03
0.43
traﬃc
δctr
δgctr30
1.16
2.56
4.28
3.07
web
conversion
δimp
δctr
δgctr30
1.31
1.69
0.39
3.15
5.19
8.88
table
6.3
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
two
promising
models
type
awareness
traﬃc
web-conversion
ads
in-batch
negative
classiﬁcation
model
works
better
traﬃc
ads
muda
model
helps
web-conversion
ads
overall
evaluation
in-batch
negative
muda
two
methods
demonstrate
promising
metrics
table
6.3
show
evaluation
results
two
methods
broken
diﬀerent
ads
objective
types
awareness
traﬃc
web
conversion
ads
awareness
ads
aim
increase
visibility
brand
product
analyzing
performance
awareness
ads
helps
understand
eﬀectiveness
brand
marketing
strategy
shown
table
6.3
in-batch
negative
model
signiﬁcant
increase
gctr30
compared
models
awareness
ads
however
boost
might
due
huge
decrease
impressions
in-batch
negative
model
trained
candidates
user
long
clicks
awareness
ads
essentially
lower
chance
clicked
types
since
main
goal
increase
visibility
brand
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
115
result
in-batch
negative
model
bias
towards
ads
types
leading
huge
impression
drop
awareness
ads
traﬃc
ads
designed
drive
traﬃc
speciﬁc
website
landing
page
typically
used
increase
brand
awareness
generate
leads
drive
sales
analyzing
metrics
ctr
gctr30
businesses
can
determine
whether
ads
resonating
target
audience
whether
successfully
achieving
advertising
goals
can
seen
in-batch
negative
model
ones
yield
increase
ads
impression
gctr30
traﬃc
ads
aim
attract
users
click
occupy
big
portion
records
positive
user
actions
therefore
in-batch
negative
model
training
dataset
includes
candidates
positive
user
actions
may
higher
proportion
candidates
well-suited
driving
traﬃc
result
model
better
identify
candidates
likely
drive
traﬃc
resulting
improvement
gctr30
metric
traﬃc
ads
web-conversion
ads
aim
drive
users
take
speciﬁc
action
website
making
purchase
ads
can
provide
insight
measuring
success
online
advertising
campaign
shown
table
6.3
muda
model
favors
webconversion
ads
objective
type
highest
improvement
ctr
gctr30
among
models
objective
type
in-batch
negative
model
also
performs
well
web-conversion
ads
improvements
ctr
gctr30
muda
may
favor
web-conversion
ads
pseudo
labels
generated
ads
ranking
model
may
favor
web-conversion
ads
designed
attract
users
stay
target
websites
longer
potential
conversion
behaviors
additionally
threshold
selection
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
116
strategy
used
muda
model
may
eﬀective
identifying
high-quality
candidates
web-conversion
ads
also
contribute
better
performance
type
ad
6.4
5.3
conversion
ads
models
in-batch
negative
muda
δicvr
2.55
1.89
δcpa
1.11
4.40
table
6.4
online
metrics
performance
in-batch
negative
classiﬁcation
muda
models
web-conversion
ads
in-batch
negative
classiﬁcation
model
leads
lower
conversion
probability
ads
impression
icvr
thus
higher
cpa
cost
advertisers
contrast
muda
model
recommended
ad
candidates
higher
conversion
rate
therefore
lower
cpa
cost
table
6.4
show
performance
in-batch
negative
muda
models
regard
conversion
related
metrics
two
show
good
performance
webconversion
ads
general
in-batch
negative
model
decreased
icvr
increased
cpa
favorable
advertiser
increases
costs
measured
cpa
hand
muda
model
shows
opposite
result
increased
icvr
decreased
cpa
reducing
ads
campaign
cost
advertisers
metrics
indicate
increase
long
clicks
muda
model
performs
much
better
generating
conversions
increased
long
clicks
one
reason
muda
model
performs
better
model
improve
performance
identifying
high-quality
candidates
likely
lead
conversions
thus
decrease
cost
per
action
advertisers
additionally
fact
muda
model
trained
unbiased
data
pseudo
labels
generated
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
117
ads
ranking
model
impact
pseudo
labels
may
capture
relevant
information
users
behaviors
preferences
leading
better
performance
terms
cpa
6.4
variants
muda
muda
method
believe
diﬀerent
threshold
selection
mechanisms
impact
quality
binary
pseudo
labels
result
investigate
impact
different
thresholding
mechanisms
performance
trained
retrieval
models
unbiased
dataset
ﬁrst
bucketize
candidates
according
numerical
pseudo
labels
gctr30
predicted
ranking
model
compute
percentile
labels
use
adjacent
percentile
create
buckets
bucket
adapt
following
two
strategies
calculate
empirical
gctr30
compute
gctr30
candidates
real
user
actions
divide
number
true
good
clicks
number
candidates
bucket
models
muda
v1
muda
v2
muda
v3
δimp
0.07
0.56
0.92
δctr
11.26
3.52
0.47
δgctr30
30.78
13.04
5.07
table
6.5
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
various
muda
variants
types
ads
muda
v1
achieves
highest
gain
ads
engagement
ctr
gctr30
muda
v3
achieves
balanced
gain
across
diﬀerent
metrics
good
gctr30
impression
lift
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
118
select
threshold
determining
elbow
point
graph
example
sudden
drop
true
good
clicks
good
clicks
rate
two
adjacent
bins
use
one
bins
negative
threshold
means
label
transformation
treat
candidates
pseudo
labels
smaller
threshold
negative
samples
positive
labels
check
sudden
increase
true
good
clicks
good
clicks
rate
bins
study
diﬀerent
threshold
selection
strategy
propose
three
variants
muda
models
v1
train
muda
model
biased
unbiased
datasets
ﬁrst
threshold
selection
strategy
v2
train
muda
model
unbiased
datasets
ﬁrst
threshold
selection
strategy
v3
train
muda
model
unbiased
datasets
second
threshold
selection
strategy
models
muda
v1
muda
v2
muda
v3
δimp
2.13
0.10
0.32
awareness
δctr
δgctr30
2.77
7.97
1.72
5.97
2.71
1.97
δimp
5.22
1.53
0.43
traﬃc
δctr
δgctr30
0.47
17.34
2.69
1.18
4.28
3.07
web
conversion
δimp
δctr
δgctr30
12.98
21.52
29.63
5.16
11.14
17.83
3.15
5.19
8.88
table
6.6
online
lifts
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
muda
variants
type
awareness
traﬃc
web-conversion
ads
muda
v3
shows
best
balanced
impression
gains
among
table
6.5
shows
overall
performance
three
variants
uda
models
measured
several
evaluation
metrics
impression
click-through
rate
ctr
ﬁrst
glance
v1
model
may
seem
work
best
hugely
increasing
user
engagement
chapter
empirical
study
selection
bias
pinterest
ads
retrieval
119
keeping
impression
neutral
however
broken
ad
types
mode
actually
leads
large
impression
shift
awareness
2.13
traﬃc
ads
5.12
toward
web
conversion
ads
12.98
shown
table
6.6
observation
may
indicate
training
uda
models
biased
data
make
model
favor
webconversion
ads
others
comparing
v2
v3
models
latter
one
shows
better
balanced
impression
gains
across
ad
objective
types
due
second
strategy
calculating
approximate
gctr30
unbiased
dataset
strategy
may
better
represent
true
performance
candidates
result
accurate
threshold
selection
leading
improved
performance
muda
model
models
muda
v1
muda
v2
muda
v3
δhdr
4.80
6.35
2.81
δrpr
13.88
4.43
1.43
table
6.7
online
lifts
ads
hide
rate
hdr
re-pin
rate
rpr
observed
muda
variants
types
ads
muda
v3
achieves
balanced
performance
fewer
ads
hidden
ads
repined
users
better
understand
performance
muda
variants
also
measure
online
performance
two
useful
engagement
metrics
hide
rate
hdr
repin
rate
rpr
note
re-pin
user
action
indicating
user
saves
ad
pinterest
board
table
6.7
show
change
two
metrics
compared
production
model
although
rpr
increased
muda
v1
v2
models
recommend
ads
will
hidden
users
suggesting
recommended
ads
models
provide
good
user
experience
contrast
muda
v3
model
generally
balanced
improvement
across
metrics
shows
120
positive
lift
user
engagement
reduction
unwanted
user
experience
hdr
6.5
conclusion
conclusion
work
analyzed
impact
selection
bias
pinterest
online
advertising
system
propose
evaluate
several
debiasing
methods
mitigate
negative
impacts
selection
bias
recommender
performance
results
experiments
show
proposed
methods
speciﬁcally
muda
model
can
eﬀectively
improve
performance
advertising
systems
handling
selection
bias
additionally
online
experiment
shows
model
also
improves
cost
eﬃciency
ad
campaigns
ﬁndings
demonstrate
importance
addressing
selection
bias
recommendation
systems
provide
valuable
insights
practitioners
ﬁeld
chapter
conclusion
conclusions
presented
works
collectively
highlight
signiﬁcant
strides
addressing
fairness
within
ranking
search
systems
alongside
mitigating
selection
bias
online
advertising
platforms
innovative
approaches
like
meta-learning
based
fair
ranking
mfr
meta
curriculum-based
fair
ranking
mcfr
frameworks
demonstrated
potential
signiﬁcantly
improve
fairness
metrics
minority
group
exposure
re-weighting
training
losses
employing
metalearning
techniques
curriculum
learning
methods
shown
promising
results
real-world
datasets
underscoring
eﬀectiveness
traditional
fair
ranking
models
furthermore
exploration
large
language
models
llms
uncovered
biases
challenge
fairness
prompting
development
ﬁne-tuning
strategies
lora
foster
equitable
outcomes
ranking
tasks
research
also
121
122
delves
issue
selection
bias
pinterest
multi-cascade
advertising
recommendation
system
presenting
debiasing
methodologies
like
modiﬁed
unsupervised
domain
adaptation
muda
model
enhances
recommendation
system
performance
also
boosts
ad
campaign
cost-eﬃciency
future
directions
body
work
include
reﬁning
meta-dataset
collection
methods
meta-learning
expanding
applicability
fairness
frameworks
accommodate
multiple
protected
attributes
exploring
diverse
ranking
tasks
datasets
moreover
eﬀorts
will
focus
balancing
accuracy
equity
llm
applications
improved
ranking
performance
fairness
strategies
additionally
insights
garnered
mitigating
selection
bias
online
advertising
systems
pave
way
innovation
addressing
biases
across
recommendation
systems
contributing
broader
discourse
fairness
transparency
machine
learning
ai
applications
bibliography
abubakar
abid
maheen
farooqi
james
zou
persistent
anti-muslim
bias
large
language
models
marion
fourcade
benjamin
kuipers
seth
lazar
deirdre
mulligan
editors
aies
21
aaai
acm
conference
ai
ethics
society
virtual
event
usa
may
19
21
2021
pages
298
306
acm
2021
abubakar
abid
maheen
farooqi
james
zou
large
language
models
associate
muslims
violence
nature
machine
intelligence
461
463
06
2021
doi
10.1038
s42256-021-00359-2
marcin
andrychowicz
misha
denil
sergio
gómez
colmenarejo
matthew
hoﬀman
david
pfau
tom
schaul
brendan
shillingford
nando
de
freitas
learning
learn
gradient
descent
gradient
descent
proceedings
30th
international
conference
neural
information
processing
systems
page
3988
3996
red
hook
ny
usa
2016
curran
associates
inc
antreas
antoniou
harrison
edwards
amos
storkey
train
maml
international
conference
learning
representations
2019
123
bibliography
124
abolfazl
asudeh
jagadish
julia
stoyanovich
gautam
das
designing
fair
ranking
schemes
proceedings
2019
international
conference
management
data
page
1259
1276
new
york
ny
usa
2019
association
computing
machinery
isbn
9781450356435
yuntao
bai
saurav
kadavath
sandipan
kundu
amanda
askell
jackson
kernion
andy
jones
anna
chen
anna
goldie
azalia
mirhoseini
cameron
mckinnon
carol
chen
catherine
olsson
christopher
olah
danny
hernandez
dawn
drain
deep
ganguli
dustin
li
eli
tran-johnson
ethan
perez
jamie
kerr
jared
mueller
jeﬀrey
ladish
joshua
landau
kamal
ndousse
kamile
lukosuite
liane
lovitt
michael
sellitto
nelson
elhage
nicholas
schiefer
noemi
mercado
nova
dassarma
robert
lasenby
robin
larson
sam
ringer
scott
johnston
shauna
kravec
sheer
el
showk
stanislav
fort
tamera
lanham
timothy
telleen-lawton
tom
conerly
tom
henighan
tristan
hume
samuel
bowman
zac
hatﬁelddodds
ben
mann
dario
amodei
nicholas
joseph
sam
mccandlish
tom
brown
jared
kaplan
constitutional
ai
harmlessness
ai
feedback
2022
matias
barenstein
propublica
compas
data
revisited
arxiv
e-prints
art
arxiv
1906.04711
jun
2019
yoshua
bengio
jérôme
louradour
ronan
collobert
jason
weston
curriculum
learning
proceedings
26th
annual
international
conference
machine
learning
page
41
48
new
york
ny
usa
2009
acm
bibliography
125
alex
beutel
jilin
chen
tulsee
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
cristos
goodrow
fairness
recommendation
ranking
pairwise
comparisons
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
page
2212
2220
new
york
ny
usa
2019
association
computing
machinery
isbn
9781450362016
10
asia
biega
krishna
gummadi
gerhard
weikum
equity
attention
amortizing
individual
fairness
rankings
41st
international
acm
sigir
conference
research
development
information
retrieval
page
405
414
new
york
ny
usa
2018
association
computing
machinery
isbn
9781450356572
11
christopher
bishop
pattern
recognition
machine
learning
springer
2006
12
tom
brown
benjamin
mann
nick
ryder
melanie
subbiah
jared
kaplan
prafulla
dhariwal
arvind
neelakantan
pranav
shyam
girish
sastry
amanda
askell
sandhini
agarwal
ariel
herbert-voss
gretchen
krueger
tom
henighan
rewon
child
aditya
ramesh
daniel
ziegler
jeﬀrey
wu
clemens
winter
chris
hesse
mark
chen
eric
sigler
mateusz
litwin
scott
gray
benjamin
chess
jack
clark
christopher
berner
sam
mccandlish
alec
radford
ilya
sutskever
dario
amodei
language
models
few-shot
learners
larochelle
ranzato
hadsell
balcan
lin
editors
advances
neural
information
processing
systems
volume
33
pages
1877
1901
curran
associates
inc
2020
bibliography
126
13
sébastien
bubeck
varun
chandrasekaran
ronen
eldan
johannes
gehrke
eric
horvitz
ece
kamar
peter
lee
yin
tat
lee
yuanzhi
li
scott
lundberg
harsha
nori
hamid
palangi
marco
túlio
ribeiro
yi
zhang
sparks
artiﬁcial
general
intelligence
early
experiments
gpt-4
corr
abs
2303.12712
2023
14
chumphol
bunkhumpornpat
krung
sinapiromsaran
chidchanok
lursinsap
dbsmote
density-based
synthetic
minority
over-sampling
technique
applied
intelligence
36
664
684
2012
15
chris
burges
tal
shaked
erin
renshaw
ari
lazier
matt
deeds
nicole
hamilton
greg
hullender
learning
rank
using
gradient
descent
proceedings
22nd
international
conference
machine
learning
page
89
96
new
york
ny
usa
2005
acm
16
christopher
jc
burges
ranknet
lambdarank
lambdamart
overview
learning
11
23
581
81
2010
17
zhe
cao
tao
qin
tie-yan
liu
ming-feng
tsai
hang
li
learning
rank
pairwise
approach
listwise
approach
proceedings
24th
international
conference
machine
learning
pages
129
136
2007
18
elisa
celis
damian
straszak
nisheeth
vishnoi
ranking
fairness
constraints
ioannis
chatzigiannakis
christos
kaklamanis
dániel
marx
donald
sannella
editors
45th
international
colloquium
automata
languages
programming
2018
july
13
2018
prague
czech
republic
volume
107
pages
28
28
15
schloss
dagstuhl
leibniz-zentrum
für
informatik
2018
bibliography
127
19
nitesh
chawla
kevin
bowyer
lawrence
hall
philip
kegelmeyer
smote
synthetic
minority
over-sampling
technique
artif
int
res
16
321
357
jun
2002
issn
1076
9757
20
jiawei
chen
hande
dong
yang
qiu
xiangnan
xin
xin
liang
chen
guli
lin
keping
yang
autodebias
learning
debias
recommendation
proceedings
44th
international
acm
sigir
conference
research
development
information
retrieval
page
21
30
new
york
ny
usa
2021
association
computing
machinery
21
jiawei
chen
hande
dong
yang
qiu
xiangnan
xin
xin
liang
chen
guli
lin
keping
yang
autodebias
learning
debias
recommendation
proceedings
44th
international
acm
sigir
conference
research
development
information
retrieval
sigir
21
page
21
30
new
york
ny
usa
2021
association
computing
machinery
22
myra
cheng
esin
durmus
dan
jurafsky
marked
personas
using
natural
language
prompts
measure
stereotypes
language
models
anna
rogers
jordan
boyd-graber
naoaki
okazaki
editors
proceedings
61st
annual
meeting
association
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
1504
1532
association
computational
linguistics
2023
23
xiangxiang
chu
bo
zhang
ruijun
xu
fairnas
rethinking
evaluation
fairness
weight
sharing
neural
architecture
search
international
conference
bibliography
128
computer
vision
2021
24
daniel
cohen
bhaskar
mitra
katja
hofmann
bruce
croft
cross
domain
regularization
neural
ranking
models
using
adversarial
learning
41st
international
acm
sigir
conference
research
development
information
retrieval
sigir
18
page
1025
1028
new
york
ny
usa
2018
association
computing
machinery
25
paul
covington
jay
adams
emre
sargin
deep
neural
networks
youtube
recommendations
proceedings
10th
acm
conference
recommender
systems
recsys
16
page
191
198
new
york
ny
usa
2016
association
computing
machinery
26
nick
craswell
arjen
de
vries
ian
soboroﬀ
overview
trec
2005
enterprise
track
trec
volume
pages
2005
27
andré
cruz
catarina
belém
joão
bravo
pedro
saleiro
pedro
bizarro
fairgbm
gradient
boosting
fairness
constraints
eleventh
international
conference
learning
representations
2023
28
michael
ekstrand
anubrata
das
robin
burke
fernando
diaz
et
al
fairness
information
access
systems
foundations
trends
information
retrieval
16
177
2022
29
michael
ekstrand
graham
mcdonald
amifa
raj
isaac
johnson
overview
trec
2021
fair
ranking
track
thirtieth
text
retrieval
conference
trec
2021
proceedings
2022
bibliography
129
30
chelsea
finn
pieter
abbeel
sergey
levine
model-agnostic
meta-learning
fast
adaptation
deep
networks
proceedings
34th
international
conference
machine
learning
volume
70
page
1126
1135
jmlr
org
2017
31
chelsea
finn
pieter
abbeel
sergey
levine
model-agnostic
meta-learning
fast
adaptation
deep
networks
doina
precup
yee
whye
teh
editors
proceedings
34th
international
conference
machine
learning
volume
70
proceedings
machine
learning
research
pages
1126
1135
pmlr
06
11
aug
2017
32
luca
franceschi
paolo
frasconi
saverio
salzo
riccardo
grazzi
massimiliano
pontil
bilevel
programming
hyperparameter
optimization
meta-learning
international
conference
machine
learning
pages
1568
1577
pmlr
2018
33
batya
friedman
helen
nissenbaum
bias
computer
systems
acm
trans
inf
syst
14
330
347
jul
1996
34
samuel
gehman
suchin
gururangan
maarten
sap
yejin
choi
noah
smith
realtoxicityprompts
evaluating
neural
toxic
degeneration
language
models
findings
2020
35
daniel
gillick
sayali
kulkarni
larry
lansing
alessandro
presta
jason
baldridge
eugene
ie
diego
garcia-olano
learning
dense
representations
entity
retrieval
proceedings
23rd
conference
computational
natural
language
learning
conll
pages
528
537
hong
kong
china
november
2019
association
computational
linguistics
bibliography
130
36
ian
goodfellow
jean
pouget-abadie
mehdi
mirza
bing
xu
david
warde-farley
sherjil
ozair
aaron
courville
yoshua
bengio
generative
adversarial
nets
ghahramani
welling
cortes
lawrence
weinberger
editors
advances
neural
information
processing
systems
volume
27
curran
associates
inc
2014
37
jianping
gou
baosheng
yu
stephen
maybank
dacheng
tao
knowledge
distillation
survey
international
journal
computer
vision
129
1789
1819
2021
38
cyril
goutte
eric
gaussier
probabilistic
interpretation
precision
recall
f-score
implication
evaluation
david
losada
juan
fernández-luna
editors
advances
information
retrieval
pages
345
359
berlin
heidelberg
2005
springer
berlin
heidelberg
39
fabian
haak
philipp
schaer
auditing
search
query
suggestion
bias
recursive
algorithm
interrogation
14th
acm
web
science
conference
2022
page
219
227
new
york
ny
usa
2022
acm
40
edward
hu
yelong
shen
phillip
wallis
zeyuan
allen-zhu
yuanzhi
li
shean
wang
lu
wang
weizhu
chen
lora
low-rank
adaptation
large
language
models
international
conference
learning
representations
2022
41
ben
hutchinson
vinodkumar
prabhakaran
emily
denton
kellie
webster
yu
zhong
stephen
denuyl
social
biases
nlp
models
barriers
persons
disabilities
dan
jurafsky
joyce
chai
natalie
schluter
joel
bibliography
131
tetreault
editors
proceedings
58th
annual
meeting
association
computational
linguistics
pages
5491
5501
online
july
2020
association
computational
linguistics
42
gert
jacobusse
cor
veenman
selection
bias
imbalanced
classes
toon
calders
michelangelo
ceci
donato
malerba
editors
discovery
science
pages
325
340
cham
2016
springer
international
publishing
43
muhammad
abdullah
jamal
guo-jun
qi
task
agnostic
meta-learning
fewshot
learning
proceedings
ieee
cvf
conference
computer
vision
pattern
recognition
pages
11719
11727
2019
44
albert
jiang
alexandre
sablayrolles
arthur
mensch
chris
bamford
devendra
singh
chaplot
diego
de
las
casas
florian
bressand
gianna
lengyel
guillaume
lample
lucile
saulnier
lélio
renard
lavaud
marie-anne
lachaux
pierre
stock
teven
le
scao
thibaut
lavril
thomas
wang
timothée
lacroix
william
el
sayed
mistral
7b
2023
45
vladimir
karpukhin
barlas
oguz
sewon
min
patrick
lewis
ledell
wu
sergey
edunov
danqi
chen
wen-tau
yih
dense
passage
retrieval
open-domain
question
answering
proceedings
2020
conference
empirical
methods
natural
language
processing
emnlp
online
november
2020
association
computational
linguistics
46
günter
klambauer
thomas
unterthiner
andreas
mayr
sepp
hochreiter
selfnormalizing
neural
networks
proceedings
31st
international
conference
bibliography
132
neural
information
processing
systems
nips
17
page
972
981
red
hook
ny
usa
2017
curran
associates
inc
47
jon
kleinberg
manish
raghavan
selection
problems
presence
implicit
bias
anna
karlin
editor
9th
innovations
theoretical
computer
science
conference
volume
94
leibniz
international
proceedings
informatics
pages
33
33
17
dagstuhl
germany
2018
schloss
dagstuhl
leibniz-zentrum
fuer
informatik
48
william
knight
computer
method
calculating
kendall
tau
ungrouped
data
journal
american
statistical
association
61
314
436
439
1966
49
preethi
lahoti
gerhard
weikum
krishna
gummadi
ifair
learning
individually
fair
data
representations
algorithmic
decision
making
2019
ieee
35th
international
conference
data
engineering
pages
1334
1345
2019
50
percy
liang
rishi
bommasani
tony
lee
dimitris
tsipras
dilara
soylu
michihiro
yasunaga
yian
zhang
deepak
narayanan
yuhuai
wu
ananya
kumar
benjamin
newman
binhang
yuan
bobby
yan
ce
zhang
christian
cosgrove
christopher
manning
christopher
ré
diana
acosta-navas
drew
hudson
eric
zelikman
esin
durmus
faisal
ladhak
frieda
rong
hongyu
ren
huaxiu
yao
jue
wang
keshav
santhanam
laurel
orr
lucia
zheng
mert
yuksekgonul
mirac
suzgun
nathan
kim
neel
guha
niladri
chatterji
omar
khattab
peter
henderson
qian
huang
ryan
chi
sang
michael
xie
shibani
santurkar
surya
ganguli
bibliography
133
tatsunori
hashimoto
thomas
icard
tianyi
zhang
vishrav
chaudhary
william
wang
xuechen
li
yifan
mai
yuhui
zhang
yuta
koreeda
holistic
evaluation
language
models
2023
51
shichen
liu
fei
xiao
wenwu
ou
luo
si
cascade
ranking
operational
e-commerce
search
proceedings
23rd
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
17
page
1557
1565
new
york
ny
usa
2017
association
computing
machinery
52
hanchao
ma
sheng
guan
christopher
toomey
yinghui
wu
diversiﬁed
subgraph
query
generation
group
fairness
proceedings
fifteenth
acm
international
conference
web
search
data
mining
page
686
694
new
york
ny
usa
2022
acm
53
sourab
mangrulkar
sylvain
gugger
lysandre
debut
younes
belkada
sayak
paul
benjamin
bossan
peft
state-of-the-art
parameter-eﬃcient
ﬁne-tuning
methods
https://github.com/huggingface/peft,
2022
54
nikita
nangia
clara
vania
rasika
bhalerao
samuel
bowman
crowspairs
challenge
dataset
measuring
social
biases
masked
language
models
bonnie
webber
trevor
cohn
yulan
yang
liu
editors
proceedings
2020
conference
empirical
methods
natural
language
processing
emnlp
pages
1953
1967
online
november
2020
association
computational
linguistics
55
rodrigo
nogueira
kyunghyun
cho
passage
re-ranking
bert
2020
bibliography
134
56
rodrigo
nogueira
zhiying
jiang
ronak
pradeep
jimmy
lin
document
ranking
pretrained
sequence-to-sequence
model
trevor
cohn
yulan
yang
liu
editors
findings
association
computational
linguistics
emnlp
2020
pages
708
718
online
november
2020
association
computational
linguistics
57
openai
gpt-4
technical
report
2023
58
long
ouyang
jeﬀ
wu
xu
jiang
diogo
almeida
carroll
wainwright
pamela
mishkin
chong
zhang
sandhini
agarwal
katarina
slama
alex
ray
john
schulman
jacob
hilton
fraser
kelton
luke
miller
maddie
simens
amanda
askell
peter
welinder
paul
christiano
jan
leike
ryan
lowe
training
language
models
follow
instructions
human
feedback
2022
59
sinno
jialin
pan
qiang
yang
survey
transfer
learning
ieee
transactions
knowledge
data
engineering
22
10
1345
1359
2010
60
german
parisi
ronald
kemker
jose
part
christopher
kanan
stefan
wermter
continual
lifelong
learning
neural
networks
review
neural
networks
113
54
71
2019
issn
0893
6080
61
alicia
parrish
angelica
chen
nikita
nangia
vishakh
padmakumar
jason
phang
jana
thompson
phu
mon
htut
samuel
bowman
bbq
hand-built
bias
benchmark
question
answering
smaranda
muresan
preslav
nakov
bibliography
135
aline
villavicencio
editors
findings
association
computational
linguistics
acl
2022
dublin
ireland
may
22
27
2022
pages
2086
2105
association
computational
linguistics
2022
62
ethan
perez
saﬀron
huang
francis
song
trevor
cai
roman
ring
john
aslanides
amelia
glaese
nat
mcaleese
geoﬀrey
irving
red
teaming
language
models
language
models
yoav
goldberg
zornitsa
kozareva
yue
zhang
editors
proceedings
2022
conference
empirical
methods
natural
language
processing
pages
3419
3448
abu
dhabi
united
arab
emirates
december
2022
association
computational
linguistics
63
andrea
dal
pozzolo
olivier
caelen
reid
johnson
gianluca
bontempi
calibrating
probability
undersampling
unbalanced
classiﬁcation
2015
ieee
symposium
series
computational
intelligence
pages
159
166
new
york
ny
usa
2015
ieee
64
jiarui
qin
jiachen
zhu
bo
chen
zhirong
liu
weiwen
liu
ruiming
tang
rui
zhang
yong
yu
weinan
zhang
rankﬂow
joint
optimization
multi-stage
cascade
ranking
systems
ﬂows
sigir
22
page
814
824
new
york
ny
usa
2022
association
computing
machinery
65
zhen
qin
rolf
jagerman
kai
hui
honglei
zhuang
junru
wu
jiaming
shen
tianqi
liu
jialu
liu
donald
metzler
xuanhui
wang
michael
bendersky
large
language
models
eﬀective
text
rankers
pairwise
ranking
prompting
2023
bibliography
136
66
aida
ramezani
yang
xu
knowledge
cultural
moral
norms
large
language
models
anna
rogers
jordan
boyd-graber
naoaki
okazaki
editors
proceedings
61st
annual
meeting
association
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
428
446
association
computational
linguistics
2023
67
kuniaki
saito
yoshitaka
ushiku
tatsuya
harada
asymmetric
tri-training
unsupervised
domain
adaptation
proceedings
34th
international
conference
machine
learning
volume
70
icml
17
page
2988
2997
jmlr
org
2017
68
sebastin
santy
jenny
liang
ronan
le
bras
katharina
reinecke
maarten
sap
nlpositionality
characterizing
design
biases
datasets
models
anna
rogers
jordan
boyd-graber
naoaki
okazaki
editors
proceedings
61st
annual
meeting
association
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
9080
9102
association
computational
linguistics
2023
69
jun
shu
qi
xie
lixuan
yi
qian
zhao
sanping
zhou
zongben
xu
deyu
meng
meta-weight-net
learning
explicit
mapping
sample
weighting
advances
neural
information
processing
systems
32
2019
70
ashudeep
singh
thorsten
joachims
fairness
exposure
rankings
proceedings
24th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
page
2219
2228
new
york
ny
usa
2018
acm
bibliography
137
71
dylan
slack
sorelle
friedler
emile
givental
fairness
warnings
fairmaml
learning
fairly
minimal
data
page
200
209
new
york
ny
usa
2020
association
computing
machinery
72
julia
stoyanovich
ke
yang
hv
jagadish
online
set
selection
fairness
diversity
constraints
proceedings
edbt
conference
2018
73
weiwei
sun
lingyong
yan
xinyu
ma
shuaiqiang
wang
pengjie
ren
zhumin
chen
dawei
yin
zhaochun
ren
chatgpt
good
search
investigating
large
language
models
re-ranking
agents
houda
bouamor
juan
pino
kalika
bali
editors
proceedings
2023
conference
empirical
methods
natural
language
processing
pages
14918
14937
singapore
december
2023
association
computational
linguistics
74
ilya
sutskever
james
martens
george
dahl
geoﬀrey
hinton
importance
initialization
momentum
deep
learning
international
conference
machine
learning
pages
1139
1147
pmlr
2013
75
zhiqiang
tao
yaliang
li
bolin
ding
ce
zhang
jingren
zhou
yun
fu
learning
mutate
hypergradient
guided
population
advances
neural
information
processing
systems
volume
33
pages
17641
17651
curran
associates
inc
2020
bibliography
138
76
hugo
touvron
louis
martin
kevin
stone
peter
albert
amjad
almahairi
yasmine
babaei
nikolay
bashlykov
soumya
batra
prajjwal
bhargava
shruti
bhosale
dan
bikel
lukas
blecher
cristian
canton
ferrer
moya
chen
guillem
cucurull
david
esiobu
jude
fernandes
jeremy
fu
wenyin
fu
brian
fuller
cynthia
gao
vedanuj
goswami
naman
goyal
anthony
hartshorn
saghar
hosseini
rui
hou
hakan
inan
marcin
kardas
viktor
kerkez
madian
khabsa
isabel
kloumann
artem
korenev
punit
singh
koura
marie-anne
lachaux
thibaut
lavril
jenya
lee
diana
liskovich
yinghai
lu
yuning
mao
xavier
martinet
todor
mihaylov
pushkar
mishra
igor
molybog
yixin
nie
andrew
poulton
jeremy
reizenstein
rashi
rungta
kalyan
saladi
alan
schelten
ruan
silva
eric
michael
smith
ranjan
subramanian
xiaoqing
ellen
tan
binh
tang
ross
taylor
adina
williams
jian
xiang
kuan
puxin
xu
zheng
yan
iliyan
zarov
yuchen
zhang
angela
fan
melanie
kambadur
sharan
narang
aurelien
rodriguez
robert
stojnic
sergey
edunov
thomas
scialom
llama
open
foundation
ﬁne-tuned
chat
models
2023
77
boxin
wang
weixin
chen
hengzhi
pei
chulin
xie
mintong
kang
chenhui
zhang
chejian
xu
zidi
xiong
ritik
dutta
rylan
schaeﬀer
sang
truong
simran
arora
mantas
mazeika
dan
hendrycks
zinan
lin
yu
cheng
sanmi
koyejo
dawn
song
bo
li
decodingtrust
comprehensive
assessment
trustworthiness
gpt
models
corr
abs
2306.11698
2023
78
xiaojie
wang
rui
zhang
yu
sun
jianzhong
qi
combating
selection
biases
recommender
systems
unbiased
ratings
proceedings
14th
bibliography
139
acm
international
conference
web
search
data
mining
wsdm
21
page
427
435
new
york
ny
usa
2021
association
computing
machinery
79
xuanhui
wang
michael
bendersky
donald
metzler
marc
najork
learning
rank
selection
bias
personal
search
proceedings
39th
international
acm
sigir
conference
research
development
information
retrieval
sigir
16
page
115
124
new
york
ny
usa
2016
association
computing
machinery
80
yuan
wang
zhiqiang
tao
yi
fang
meta-learning
approach
fair
ranking
45th
international
acm
sigir
conference
research
development
information
retrieval
page
2539
2544
new
york
ny
usa
2022
acm
81
zhenlei
wang
jingsen
zhang
hongteng
xu
xu
chen
yongfeng
zhang
wayne
xin
zhao
ji-rong
wen
counterfactual
data-augmented
sequential
recommendation
proceedings
44th
international
acm
sigir
conference
research
development
information
retrieval
sigir
21
page
347
356
new
york
ny
usa
2021
association
computing
machinery
82
linda
wightman
lsac
national
longitudinal
bar
passage
study
lsac
research
report
series
1998
83
garrett
wilson
diane
cook
survey
unsupervised
deep
domain
adaptation
acm
trans
intell
syst
technol
11
jul
2020
issn
2157
6904
bibliography
140
84
ledell
wu
fabio
petroni
martin
josifoski
sebastian
riedel
luke
zettlemoyer
scalable
zero-shot
entity
linking
dense
entity
retrieval
proceedings
2020
conference
empirical
methods
natural
language
processing
emnlp
pages
6397
6407
online
november
2020
association
computational
linguistics
85
chenyan
xiong
zhuyun
dai
jamie
callan
zhiyuan
liu
russell
power
endto-end
neural
ad-hoc
ranking
kernel
pooling
proceedings
40th
international
acm
sigir
conference
research
development
information
retrieval
sigir
17
page
55
64
new
york
ny
usa
2017
association
computing
machinery
86
ke
yang
julia
stoyanovich
measuring
fairness
ranked
outputs
proceedings
29th
international
conference
scientiﬁc
statistical
database
management
new
york
ny
usa
2017
association
computing
machinery
isbn
9781450352826
87
ke
yang
vasilis
gkatzelis
julia
stoyanovich
balanced
ranking
diversity
constraints
proceedings
twenty-eighth
international
joint
conference
artiﬁcial
intelligence
pages
6035
6042
international
joint
conferences
artiﬁcial
intelligence
organization
2019
88
huaxiu
yao
xian
wu
zhiqiang
tao
yaliang
li
bolin
ding
ruirui
li
zhenhui
li
automated
relational
meta-learning
8th
international
conference
learning
representations
2020
bibliography
141
89
meike
zehlike
carlos
castillo
reducing
disparate
exposure
ranking
learning
rank
approach
page
2849
2855
association
computing
machinery
new
york
ny
usa
2020
isbn
9781450370233
90
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
ricardo
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
proceedings
2017
acm
conference
information
knowledge
management
page
1569
1578
new
york
ny
usa
2017
association
computing
machinery
isbn
9781450349185
91
meike
zehlike
philipp
hacker
emil
wiedemann
matching
code
law
achieving
algorithmic
fairness
optimal
transport
data
min
knowl
discov
34
163
200
jan
2020
issn
1384
5810
92
meike
zehlike
ke
yang
julia
stoyanovich
fairness
ranking
survey
arxiv
preprint
arxiv
2103.14000
2021
93
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
cynthia
dwork
learning
fair
representations
sanjoy
dasgupta
david
mcallester
editors
proceedings
30th
international
conference
machine
learning
volume
28
proceedings
machine
learning
research
pages
325
333
atlanta
georgia
usa
17
19
jun
2013
pmlr
94
brian
hu
zhang
blake
lemoine
margaret
mitchell
mitigating
unwanted
biases
adversarial
learning
proceedings
2018
aaai
acm
conference
bibliography
142
ai
ethics
society
aies
18
page
335
340
new
york
ny
usa
2018
association
computing
machinery
95
jizhi
zhang
keqin
bao
yang
zhang
wenjie
wang
fuli
feng
xiangnan
chatgpt
fair
recommendation
evaluating
fairness
large
language
model
recommendation
proceedings
17th
acm
conference
recommender
systems
recsys
23
page
993
999
new
york
ny
usa
2023
association
computing
machinery
isbn
9798400702419
96
chen
zhao
feng
chen
unfairness
discovery
prevention
few-shot
regression
2020
ieee
international
conference
knowledge
graph
pages
137
144
2020
97
chen
zhao
feng
chen
zhuoyi
wang
latifur
khan
primal-dual
subgradient
approach
fair
meta
learning
2020
ieee
international
conference
data
mining
pages
821
830
ieee
2020
98
chen
zhao
feng
chen
bhavani
thuraisingham
fairness-aware
online
metalearning
proceedings
27th
acm
sigkdd
conference
knowledge
discovery
data
mining
page
2294
2304
new
york
ny
usa
2021
association
computing
machinery
isbn
9781450383325