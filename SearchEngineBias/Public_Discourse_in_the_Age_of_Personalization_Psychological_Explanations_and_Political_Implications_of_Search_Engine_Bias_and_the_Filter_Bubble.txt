Journal of Science Policy & Governance Policy Analysis: Public Discourse in Age of
Personalization

Public Discourse in the Age of
Personalization: Psychological
Explanations and Political Implications of
Search Engine Bias and the Filter Bubble
Audrey B. Carson
Corresponding author: audreycarson@college.harvard.edu
EXECUTIVE SUMMARY: This policy proposal recommends that Google engineer and
implement a new web application, to be embedded directly into the company’s trademark
search engine, that will allow users to manually toggle between results returned through
Google’s new personalization algorithms and results returned through Google’s original
PageRank algorithms. The intent of this policy is to provide users with an interactive
visualization of Google’s various content filters that will increase their awareness,
understanding, and control of these same filters and thus impact how users appreciate and
act upon the information delivered to them by Google’s search engine.1 This policy proposal
is motivated by recent research on the effects of online personalization algorithms,
especially their tendency to trap users in “filter bubbles” – information streams uniquely
tailored to the interests and biases of individual users – without their knowledge or
consent.2 Drawing upon relevant literature in rational choice theory and social psychology,
this policy proposal examines how filter bubbles threaten meaningful public discourse and
effective democratic governance, and presents the aforementioned policy as a solution.
Concerns regarding the policy’s feasibility and functionality are also addressed.
This policy proposal is targeted for Google’s Public Policy and Governmental Affairs team.
This team’s primary responsibility is to convene with government and elected officials to
clarify Google products and promote the growth of the web. The team also works to ensure
that Google’s Code of Conduct – guided by the mantra, don’t be evil – is upheld. The Public
Policy and Governmental Affairs team identifies “…following the law, acting honorably, and
treating each other with respect” as important ways in which Google prioritizes “doing the
right thing.” 3

Sayooran Nagulendra and Julita Vassileva, "Understanding and Controlling the Filter Bubble through Interactive Visualization: A User
Study," Hypertext and Social Media: Proceedings of the 25th ACM Conference (Santiago, Chile: The Association for Computing Machinery
Digital Library, 2014).
2 Eli Pariser, The Filter Bubble: What the Internet Is Hiding from You (New York: Penguin, 2011), 9.
3 "Transparency," Google U.S. Public Policy, http://www.google.com/publicpolicy/transparency.html.
1

www.sciencepolicyjournal.org

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

Policy Analysis: Public Discourse in Age of Personalization

I. PUBLIC DISCOURSE AND ONLINE MEDIA
The roots of democratic theory on the
necessity of a free press and public discourse
can be traced back to Thomas Jefferson, who
believed that an educated citizenry was
requisite for the proper functioning of a free
and enlightened nation. Because a free press
is “the best instrument for enlightening the
mind of man and improving him as a rational,
moral, and social being,” Jefferson explained,
the freedom and quality of the press are
important indicators of the health of the
nation as a whole.4
Jefferson’s argument continues to
influence how political scientists understand
the democratic function of the news media in
modern America. While the most basic
purpose of the media is to inform the public,
it is also responsible for establishing the
“foundation of shared experience and shared
knowledge upon which democracy is built.”5
The news puts people on the same page –
literally and figuratively – by providing a
shared vocabulary and set of facts with which
to debate and reach a consensus on how to
work together to solve common problems. By
writing, reading, and discussing the news,
citizens engage in a dialogue that allows them
to “democratically create their culture and to
calibrate their ideas in the world.”6 Indeed, as
reporter Walter Lippmann intoned, it might
even be that “all that the sharpest critics of
democracy have alleged is true, if there is no
steady supply of trustworthy and relevant
news.” 7 Given the indispensability and
responsibility of a free press to democracy,
the media is a fundamentally political and
ethical enterprise.
In recent decades, distributive and
curatorial power over the news has shifted

from print journalism to online content
suppliers. Cyberphiles in the early days of the
Information Age – like John Perry Barlow,
author of the 1996 treatise “A Declaration of
the Independence of Cyberspace” – regarded
this shift with optimism.8 In large part, they
believed the nation’s well-established
newspapers had failed their democratic
mission, using their exclusive ownership of
expensive printing presses to guard the gates
of public opinion, protect elite interests, and
decide what “the people” should think. The
Internet, these “techno-optimists” argued,
would disintermediate and redemocratize
public discourse by allowing individuals to
autonomously plug into a public sphere that
directly supplied more and “better
information, and the power to act on it.”9
While the Internet has not brought
about a digital democratic utopia, recent
technological innovations have dramatically
reduced the cost to produce, distribute, and
access
diverse
information
and
perspectives.10 As Internet activist Eli Pariser
so powerfully puts it, “whereas once only
those who could buy ink by the barrel could
reach an audience of millions, now anyone
with a laptop and a fresh idea can.”11 As the
cost of producing media plummeted at the
end of the 20th century, the number of blogs
and
online
news
websites
grew
exponentially; the chore of filtering through
vast swaths of cluttered, obscure information
on the Internet for relevant content became
unmanageable.12 Google – which now holds
nearly 70% of U.S. search engine market
share – emerged at the turn of the 21st
century to fill the pressing demand for a tool
that would allow users to effectively search

Thomas Jefferson, and Henry Augustine Washington. The
Writings of Thomas Jefferson: Being His Autobiography,
Correspondence, Reports, Messages, Addresses, and Other
Writings, Official and Private. Published by the Order of the Joint
Committee of Congress on the Library, from the Original
Manuscripts, Deposited in the Department of State (Washington,
D.C.: Taylor & Maury, 1854).
5 Pariser, The Filter Bubble, 50.
6 Ibid, 163.
7 Ibid, 50.

8

4

www.sciencepolicyjournal.org

Jack L. Goldsmith and Tim Wu, Who Controls the Internet?:
Illusions of a Borderless World (New York: Oxford UP, 2006),
18.
9 Ibid, 3.
10 Seth R. Flaxman, Sharad Goel, and Justin M. Rao, "Ideological
Segregation and the Effects of Social Media on News
Consumption," Social Science Research Network. (2014): 2.
11 Pariser, The Filter Bubble, 51.
12 Siva Vaidhyanathan,. The Googlization of Everything: (and
Why We Should Worry) (Berkeley: U of California, 2011), 1.

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

Policy Analysis: Public Discourse in Age of Personalization

3

and organize the web. 13 Since then, the
company has become a lens through which
millions of people view the world and process
knowledge, and one of the single most
important arbiters of “what is important,
relevant, and true on the web and in the
world.”14
It is key to note that Internet search
engines and traditional media broadcasters –
like newspapers and television – draw upon
two very different paradigms of content
distribution. Newspapers and television are
“push technologies” that specialize in
“pushing” generic content at a passive general
audience. Search engines, on the other hand,
are “pull technologies” that specialize in
“pulling” information from various online
servers to answer the specific queries of their
users. 15 Google’s original search algorithm,
PageRank, “pulled” information in a relatively
objective and democratic manner. PageRank
assumed that “if one site was referred to
more than another, it was... more relevant to
users.”16 As a young company, Google listed
websites linked to by many other websites
higher up on the pages of results it returned
to users. Google founder Larry Page believed
this process “utilized the uniquely democratic
structure of the web,” in that it essentially
rewarded those websites with the greatest
number of “votes.”17 With PageRank, identical
searches returned identical results ranked
according to popularity, regardless of who
made the search or from where the search
originated.
It would be fair to say that Google did
not resemble a traditional news source in
1998 when the company’s founders, Larry
Page and Sergey Brin, operated their startup
out of a garage near the Stanford University
campus before moving to a small office on

University Avenue in Palo Alto. 18 Unlike
traditional news sources, Google did not
carefully curate a single “front page” that
featured important – and potentially
unpopular – stories to be distributed
indiscriminately to all of its users. It did not
put all of its users on the “same page.” But the
fledgling company did do what may have
been the only feasible option in an
information ecosystem overflowing with new
content: it routed its users to many pages that
were all the same. Every unique Google
search returned a corresponding and
consistent “front page” on the topic implied
by the search’s keywords. A user employing
Google’s search engine to learn more about
the Iraq War, for instance, would see the
same results, in the same order, as other
users who searched the topic. Thus, while
Google did not generate “shared experience
and shared knowledge” in the same way that
traditional news sources had done, the
company did generate and organize a body of
common information that was equally visible
and accessible to all Internet users.19 Because
Google’s search results reflected a democratic
consensus about which news topics were
most important, the public had the potential
to develop a shared vocabulary and engage in
a common conversation about current events.
A few years after its founding, Google adopted
the slogan, “Democracy on the Web works” as
a guiding principle. 20 By that time, the
company was well on its way to becoming
one of the world’s most important sources of
news.
On December 4, 2009, Google
announced that it had replaced PageRank
with algorithms that “personalized search[es]
for everyone.”21 Google executives publically
explained that the change was an attempt to

Ashley Zeckman,. "Google Search Engine Market Share Nears
68%," Search Engine Watch,
http://searchenginewatch.com/sew/study/2345837/googlesearch-engine-market-share-nears-68.
14 Vaidhyanathan, The Googlization of Everything, xi.
15 Pariser, The Filter Bubble, 67.
16 Vaidhyanathan, The Googlization of Everything, 2.
17 Pariser, The Filter Bubble, 31.

18

13

www.sciencepolicyjournal.org

"Our History in Depth," Google,
http://www.google.com/about/company/history/.
19 Pariser, The Filter Bubble, 50.
20 Pamela Jones Harbour, "The Emperor of All Identities," The
New York Times, 20 May 2015,
http://www.nytimes.com/2012/12/19/opinion/why-googlehas-too-much-power-over-your-private-life.html?_r=0.
21 Pariser, The Filter Bubble, 1.

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

improve the user experience. By installing
“cookies” that recorded online “click signals,”
the search engine could learn users’ specific
needs, goals, interests, and preferences, thus
adapting to provide more personally relevant
content. 22 It was a small step towards
perfecting “the ultimate search engine,”
Google founder Larry Page proclaimed – a
machine that “would understand exactly what
you mean and give back exactly what you
want.”23
But
personalization
was
also
motivated by profit. Google monetized its
operations by selling advertising space on its
search engine. Google could sell more
advertisements at higher prices if its
personalization algorithms ensured that
increasingly targeted audiences would see –
and ultimately click on – increasingly relevant
advertisements. 24 Moreover, the growth of
personal data-aggregation companies willing
to pay to accumulate “click signals” in vast
commercial databases on users’ interests and
identities provided Google with another
incentive to personalize. Such personalization
algorithms increased the likelihood that a
given Google search would return relevant
results and trigger sellable clicks. While
Google does not publish confidential
company reports on the statistical
characteristics of its search results, academic
and industry researchers believe that at least
11.7% of searches conducted with Google’s
personalized search engine – and frequently,
a far larger proportion of such searches –
return individual users with significantly
different results as a result of algorithmically
programed
personalization. 25 Google’s
publicly available annual investor relations
Engin Bozdag and Job Timmermans, "Values in the Filter
Bubble Ethics of Personalization Algorithms in Cloud
Computing," 1st International Workshop on Values in Design –
Building Bridges between RE, HCI and Ethics (Lisbon, Portugal:
Delft University of Technology, 6 September 2011).
23 Pariser, The Filter Bubble, 33.
22

Aniko Hannak, Balachander Krishnamurthy, Piotr
Sapiezynski, David Lazer, Christo Wilson, Arash Molavi Kakhki,
and Alan Mislove, "Measuring Personalization of Web Search,"
World Wide Web Conference, 13 May 2013.
25

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

4

reports do shed some light on the effects of
personalization on the company’s bottom
line. Since 2008 – the year before announced
it would be personalizing search – Google’s
advertising revenues have nearly tripled,
jumping from $21 billion in 2008 to just shy
of $60 billion in 2014.26
The realization that Google’s users are
in fact its products, rather than its customers,
is indeed troubling. This paper, however,
addresses what may be an even more
worrying and pervasive outcome of
personalization algorithms: their tendency to
trap users in filter bubbles – information
streams uniquely tailored to the interests and
biases of individual users, often without their
knowledge or consent. Unlike PageRank,
Google’s new algorithms return personalized
results to individual users by noting their
geographic location and documenting their
“click signals.” These filters screen which
results its user sees and dictate in what order
the user sees them, effectively straining out
content deemed inconsistent with interests
and perspectives that the user has
encountered and endorsed in the past. As
citizens become increasingly dependent on
Google for their view of the world, writer Paul
Boutin notes that the “would-be information
superhighway risks becoming a land of culde-sacs, with each user living in an
individualized bubble created by automated
filters.” 27 Thus, Google’s personalization
algorithms are concerning precisely because
they tend to erode the shared public sphere
that the news media once built and occupied
and instead provide users with “their own
custom versions of the Internet.”28
The damage done to democratic
processes by personalization, Pariser asserts,
"Google's Annual Advertising Revenue 2001-2014," Statista,
24 June 2015,
http://www.statista.com/statistics/266249/advertisingrevenue-of-google/.
27 Paul Boutin, "Your Results May Vary," Wall Street Journal
[New York] 20 May 2011, Eastern ed.: A13, ProQuest Business
Collection.
28 Ibid.
26

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

could be severe. After all, democracy requires
that citizens see the world from one another’s
point of view – a task made easier by a
common collection of shared facts – but users
are now more and more “enclosed in their
own bubbles,” living in “parallel but separate
universes.”29 In the pages that follow, this
paper draws upon two interpretations of
human attention, judgment, and decisionmaking to explain why the filter bubbles
created by personalization algorithms are
socially and politically harmful, and proposes
a possible solution.
II. BIAS AND RATIONALITY:
TWO
COMPETING THEORIES OF HUMAN
INFORMATION-PROCESSING
AND
DECISION-MAKING
Modern social science theories that
have their basis in the ways in how human
beings process information and make
decisions have generally demonstrated a
preference for one of two competing
interpretations: information-processing and
decision-making as a manifestation either of
economic rationality or of psychological bias.
These approaches are introduced below and
discussed in relation to the filter bubble in the
following section.
The economic principle of rational
choice theory posits that all individuals seek
complete information in order to make wellreasoned decisions that will maximize their
total utility according to their own stable,
predetermined preferences. 30 Adam Smith,
commonly referred to as the “father of
modern economics,” held that individuals
contribute to society’s general opulence by
pursuing their own self-interest through
markets of trade and exchange.31 Applying
this same logic to the political realm,

Policy Analysis: Public Discourse in Age of Personalization

5

economist Anthony Downs argued that
individuals promote the health of the polity
when they amass complete information about
their voting options and cast their votes for
the party whose platform promises them the
greatest personal utility. In order to evaluate
the optimal course of action, a citizen
“depends ultimately on the information he
has about policies.”32
To
political
scientist
Joseph
Schumpeter,
rational
decision-making
similarly entailed “sift[ing] critically” through
information in order to gather “the ultimate
data of the democratic process.”33 Exclusive
exposure to “adulterated or selective”
information, he argued, impinges upon a
citizen’s ability to rationally “make up his
mind” and leads him to “exalt certain
propositions into axioms and put others out
of court.” 34 Schumpeter believed that this
type of “associative and affective” thinking
would ultimately detriment the political
community as a whole by allowing “the
people to be ‘fooled’ step by step into
something they do not really want.” 35
Rational choice theory thus posits that
individuals pursue their own self-interest
through well-informed and well-reasoned
decision-making that, in the end, tends to
benefit the larger polity.
A more recent iteration of rational
choice theory, “bounded rationality,” asserts
that while individuals are not perfectly
rational, they do act rationally given certain
restrictions. 36 Proponents of bounded
rationality, like Herbert A. Simon and Gary
Becker, criticized Smith and Downs’ models
of rationality because these models assumed
the existence of an impossibly perfect
“economic man,” blessed with access to
Anthony Downs, An Economic Theory of Democracy (New
York: Harper, 1957), 46.
33 Joseph Schumpeter, Capitalism, Socialism, and Democracy
(New York: Harper Perennial, 1976), “The Classical Doctrine of
Democracy,” 254.
34 Ibid, 264.
35 Ibid.
36 Herbert A. Simon, “A Behavioral Model of Rational Choice,”
The Quarterly Journal of Economics 69, no. 1 (1959).
32

Pariser, The Filter Bubble, 5.
30 Jonathan Levin and Paul Milgrom, Introduction to Choice
Theory, Stanford University, 2004,
http://web.stanford.edu/~jdlevin/Econ%20202/Choice%20T
heory.pdf. 1.
31 Adam. Smith,The Wealth of Nations [1776] (New York:
Penguin Books, 1982), http://nrs.harvard.edu/urn3:HUL.FIG:003625441, 13.
29

www.sciencepolicyjournal.org

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

complete
information
and
limitless
“computational capacity.”37 While Simon and
Becker believe that rational decision-making
occurs when individuals “weigh the
advantages and disadvantages of alternative
actions,” they also believed that rational
decision-making is often constrained by
logistical and cognitive limitations.38 Because
“the information-gathering process is not
costless” and because people suffer from
deficiencies in “income, time, memory and
calculating capacities,” they explained,
perfectly rational decision-making is at times
neither practical nor attainable.39 To Simon
and Becker, the fact that individuals
frequently choose to satisfy rather than
maximize their utility preferences does not
prove that humans are irrational. Rather,
decisions of this type indicate the functioning
of rational decision-making under certain
logistical and cognitive constraints.
While academics and researchers who
believe that human information-processing
and decision-making are primarily governed
by certain psychological biases do not deny
the existence of rational thought and action,
they do claim that conscious, intentional, and
systematic cognitive processing plays a far
more limited role in the day-to-day workings
of human behavior than the rational
approach would suggest. In the 1970s,
psychologist Daniel Kahneman proposed that
the human cognition occurs through the
collaboration two different psychological
systems. The operations of System 1,
Kahneman wrote, are “fast, automatic,
effortless, associative, implicit… and often
emotionally charged; they are also governed
by habit.”40 The operations of System 2, on
the other hand, “are slower, serial, effortful,
Ibid, 99.
38 Gary Becker, “The Economic Way of Looking at Life” Nobel
Lecture, 9 December 1992,
http://www.nobelprize.org/nobel_prizes/economicsciences/laureates/1992/becker- lecture.pdf, 51.
39 Simon, Rational Choice, 106.
40 Daniel Kahneman, “A Perspective on Judgment and Choice:
Mapping Bounded Rationality,” The American Psychologist 58,
no. 9 (September 2003): 697–720,
http://www.ncbi.nlm.nih.gov/pubmed/14584987, 698.
37

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

6

more likely to be consciously monitored or
deliberately controlled.” 41 Because the
processing capacity of System 2 is easily
expended – in other words, because the
cognitive resources necessary for rational
decision-making are in short supply – the
intuitive and automatic functions of System 1
manage “most of moment-to-moment
psychological life.” 42 Indeed, “much of a
person’s everyday life,” Bargh and Chartrand
explained, is determined “by mental
processes that are put into motion by features
of the environment and that operate outside
of conscious awareness and guidance.” 43
Rather than endangering free will and selfdetermination, automatic mental processes
silently carry much of the brain’s cognitive
load so that resources remain for rational
decision-making during the most deserving
and deliberative of circumstances. 44
Unfortunately, precisely because System 1’s
operations are effortless, they also frequently
remain unexamined.45 Thus, while System 1’s
mental shortcuts effectively simplify and
sublimate the complex task of gathering and
processing information, they can also lead to
severe and systematic errors that would not
occur if individuals were fully aware of what
they were doing.46 In 2002, Daniel Kahneman
was awarded a Nobel Prize in Economic
Sciences for challenging the prevailing
assumption of human rationality with his
empirical and theoretical findings on
psychological biases and heuristics. His bestselling book, Thinking, Fast and Slow, was
published in 2011 and summarizes a body of
work
that
has
revolutionized
our
understanding of the ways in which human

Ibid.
J. A. Bargh and T L Chartrand, “The Unbearable Automaticity
of Being,” American Psychologist 54, no. 7 (1999): 462.
43 Ibid.
44 Ibid, 464.
45 Kahneman, Judgment and Choice, 699.
46 Robert Jervis, “The Drunkard’s Search,” Political Psychology,
edited by John T Jost and Jim Sidanius (New York: Psychology
Press, 2004), 259.
41
42

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

begins process
decisions.

information

and

Policy Analysis: Public Discourse in Age of Personalization

7

make

III. APPLICATION: THE FILTER BUBBLE
Before any thorough investigation of
the detriments of personalized filter bubbles,
it is important to note that some filtering,
both online and in person, is necessary for
effective functioning. As discussed above, the
human brain cannot constantly engage in
purposeful, systematic reasoning – the type of
rational decision-making venerated by Adam
Smith and Anthony Downs – because the
mind’s effortful resources are limited. The
human brain lacks the capacity to consciously
and deliberately process all of the physical
and social information presented by the
surrounding environment. Thus, most
moment-to-moment
cognition
occurs
automatically, with conscious awareness only
roused when “there are real options and
choices of which path to take.”47
Importantly, the basic psychological
theory that humans have limited information
processing capacity may explain a large part
of Google’s success. As the Internet expanded,
it became clear to many cybertheorists that
the anarchic cyber utopia extolled by “technooptimists” at the dawn of the Information Age
was becomingly mind-numbingly vast; in this
“interlinked yet unindexed” web of networks
in which “clutter and confusion reigned,” the
sheer amount of data thrust in the faces of
intrepid Internet users threatened to render
the system unnavigable.48 The human brain
was not up to the task of consciously and
deliberately filtering through hundreds of
thousands of websites to find a single piece of
useful information. A direct, disintermediated
web guaranteed an attention crash.
The invaluable insight of Google’s
founders was that in order to be useful, the
Internet required a sorting mechanism that
would reduce the amount of energy and time
required to extract useful information from

the web. In many ways, Google’s search
engine can be thought of as a virtual
extension of the System 1 processing
operations of the human brain. Because there
was far too much data on the Internet for
users to consciously sift through in search of
a few pieces of truly important information –
and because Google initially offered to sift
through and prioritize this data using
seemingly “neutral and democratic” methods
–users came to trust Google’s search engine
to quickly filter through the Web for them
and alert them of any key findings.49 While
users lack the cognitive resources to
deliberately assess every page on the web,
they do have sufficient cognitive capacity to
evaluate the limited number of results
returned to them by a Google search,
especially when these results are ranked in a
meaningful and fair way. This is why “when
personalized filters offer a hand,” Pariser
explains, users “are inclined to take it.
Personalized filters can help users find the
information they need to know and see and
hear, the stuff that really matters.”50 Given the
indispensability of some means of filtering
through the vast amount of information on
the web, this paper does not criticize online
personalization outright. Rather, it suggests
that Google’s information filtering algorithms
may have become “too good.”51
One of the primary functions of
System 2 is to reconcile uncertainty and
consciously decide how to settle the doubt
that arises when one is confronted by
incompatible thoughts or perceptions. 52
System 1 recognizes that uncertainty and
doubt are strategically important to humandecision making and accordingly draws the
attention of System 2 to their existence so
that deliberative reasoning can occur. In his
book, The Sentimental Citizen, George Marcus
argues that emotional anxiety provoked by
perceptions of conflict is the only means of
Ibid, 2.
Pariser, The Filter Bubble, 11.
51 Nagulendra and Vassileva, Interactive Visualization, 107.
52 Kahneman, Judgment and Choice, 702.
49
50

47
48

Bargh and Chartrand, Automaticity of Being, 473.
Vaidhyanathan, The Googlization of Everything, 1.

www.sciencepolicyjournal.org

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

forcing a rational reevaluation of one’s
currents beliefs and habits. Anxiety is
unpleasant, Marcus acknowledges, but “it
frees us from being just stimulus-response
creatures” by recruiting “reason and the
attentive state of mind.” 53 While Google’s
personalization
algorithms
are
extraordinarily good at filtering through the
billions of gigabytes of data stored on the web
and bringing items relevant to users’ search
queries to their attention, it is less clear
whether the items they bring to users’
attention actually foster the doubt,
uncertainty, and anxiety required to trigger
deliberate and conscious evaluation. Because
personalized filters are programmed to
predict users’ interests, habits, and desires by
learning from past online behavior, these
filters become increasingly biased to share
users’ own views. 54 Rather than returning
results that might throw a user’s current
beliefs into question and prompt him or her
to reevaluate his or her perceptions and
rationally decide on a new course of action,
personalized algorithms return results that
“encapsulate users in a bubble of their
comfort” – a friendly world – where they see
“only content related to their interests” and
are “spared of anything else.” 55 While
Google’s filtering algorithms excel at
returning relevant results to users, they omit
those that are in fact most strategically
important: results that threaten a user’s
unexamined assumptions and habits. Thus,
Google’s current search engine approximates
the psychological operations of System 1
while neglecting one of its most vital
functions. By metaphorically “severing the
synapses in the brain,” Pariser warns,
Google’s personalized filters are effectively
performing a “global lobotomy.”56
Google’s
original
PageRank
algorithms, on the other hand, did expose
George E. Marcus, The Sentimental Citizen (Pennsylvania
State University Press, 2002), http://nrs.harvard.edu/urn3:hul.ebookbatch.PMUSE_batch:muse9780271052731
54 Pariser, The Filter Bubble, 3.
55 Nagulendra and Vassileva, Interactive Visualization, 107.
56 Pariser, The Filter Bubble, 19.
53

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

8

users to online media that had the potential
to jeopardize their current worldviews and
thus instigate critical assessment of
previously unexamined beliefs. A staunch
social
conservative
who
Googled
“homosexual” in 2006, for instance, might
have received as his or her top search result a
popular article advocating for civil unions; in
2011, this same person would have much
more likely been directed to sources
consistent with his or her ideological
opposition to homosexuality. Because
PageRank ranked its results according to
relevant websites’ popularity, the algorithm
plunged users into a common media stream
of
“shared
knowledge
and
shared
experience.” There, users were forced to
confront social and political positions that
either commanded general democratic
consensus or demanded attention due to the
discussion and debate they instigated. To the
extent these results challenged rather than
affirmed a user’s preexisting beliefs,
PageRank prompted the same uncertainty,
anxiety, and doubt necessary for rational
reevaluation
and
eventual
working
consensus. In other words, PageRank carried
out the democratic mission of a free press.
While PageRank filtered out much of the web,
it continued to point users’ attention to
strategically important information that, by
threatening users’ automatic and unexamined
points of view, would eventually allow users
to end on the same page.
Conversely,
Google’s
new
personalization algorithms presage far less
promising political outcomes. Personalized
search results turn computer monitors into
one-way mirrors that reflect and exaggerate
users’ own biases and interests. Rather than
pushing users towards some sort of shared
dialogue and common consensus on social
and
political
issues,
personalization
algorithms tend to exacerbate individual
users’ particular predilections by showing
them content and perspectives that echo and
amplify their own beliefs. Because Google’s
personalized filters fail to activate System 2JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

type judgments among users, they increase
the likelihood that users will suffer from
some of the same severe and systematic
biases that an unquestioned reliance on
System 1’s automatic assumptions tends to
generate. In 2014, an interdisciplinary team
of computer and social scientists from the
University of Minnesota empirically affirmed
that “recommender systems” like Google’s
personalized search engine have an outsized
effect on user’s choices – larger even than
that of peers and experts – and that over time,
these systems “strongly push users towards
narrow consumption” of online content.57 The
researchers suggested that recommender
systems display diversity metrics or
summary statistics to alert users of the
declining variety of their returned results, a
recommendation very much aligned with the
one proposed in this paper. A second study,
commissioned by Microsoft Research,
produced similar findings and found that
individuals exhibit “substantially higher”
ideological segregation when they rely on
personalized search engines to deliver their
news.58 Cumulatively, personalization tends
to nurture biases that fracture of the public
sphere into narrow and inflexible interests
that struggle to participate effectively in the
rational, deliberative decision-making of
collective public discourse.
IV. POLICY PROPOSAL AND EVALUATION
Currently, a Google search will return
users with only one set of search results.
These results are determined by Google’s
personalization algorithms, which rely on
users’ “click signals” and other personal data
to refine their predictions and provide
individual users with personally relevant
results. Prior to December 4, 2009, PageRank
– Google’s original search algorithm – indexed
Tien T. Nguyen, Pik-Mai Hui, F. Maxwell Harper, Loren
Terveen, and Joseph A. Constan, "Exploring the Filter Bubble:
The Effect of Using Recommender Systems on Content
Diversity," International World Wide Web Conference, 7 April
2014, 685
58 Seth R. Flaxman, Shared Goel, and Justin M. Rao, Ideological
Segregation.
57

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

9

search results for all of Google’s users using a
rough approximation of relative website
popularity. This approach to search was
democratic because it rewarded websites for
receiving “votes” of confidence from other
websites. Because personalized filters
prioritize relevance and PageRank filters
prioritize popularity, a side-by-side search
conducted by the two algorithms using
identical keywords would return different
results.
This policy proposal recommends a
simple solution to the filter bubble problem:
Google should engineer and implement a new
web application that will allow users to easily
toggle between results returned through
Google’s new personalization algorithms and
results returned through Google’s original
PageRank algorithms. At a functional level,
the toggle feature should be an easy-to-use
interactive tool. It should be easily accessed
through an icon embedded in Google’s search
bar, directly adjacent to the microphone
(“Search by voice”) and magnifying glass
(“Search”) icons. A light switch icon may be a
helpful illustration of the toggle feature’s
main function. When flipped to “Personal,”
the toggle feature should prompt the Google
search engine to display and arrange highly
relevant results that reflect Google’s
perceptions of the individual user. When
flipped to “Popular,” the toggle feature should
prompt the Google search engine to display
and arrange highly popular results ranked
using Google’s original PageRank algorithms.
As the name suggests, a single click of the
mouse on the toggle icon should allow the
user to flip back and forth between results
returned using the “Personal” and “Popular”
filters. While the launch of the toggle feature
should prompt the creation and distribution
of some explanatory user materials –
including an informative text bubble that
should instruct a user on how to use the
feature when he or she hovers his or her
mouse over the toggle icon – because Google
developed its PageRank algorithm a decade
ago, the implementation of the toggle feature
JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

should not require the costly and timeconsuming development of an entirely new
search algorithm. Rather, it simply demands
the reintroduction of the PageRank algorithm
alongside the personalized algorithms
already in place, and the creation of a simple
and intuitive toggle mechanism that will
direct the search engine which algorithm to
use in which situation.
The toggle feature will help users
visualize and understand how their search
results change depending on what type of
filter Google applies, and give users the
opportunity to explore content they would
not have discovered if they only had access to
personalized search results. Thus, the toggle
feature will both educate and empower users.
By allowing users to engage in a side-by-side
comparison of PageRank search results and
personalized search results, the toggle feature
will educate users on how Google’s filters
function and expose them to informative
content they would not have seen had they
only referenced their personalized search
results. The toggle feature will give users the
opportunity to choose which filter – popular
or personalized – they wish to apply when
they use Google search, and thus will
empower them to decide for themselves what
type of results they would like to receive.
Because the toggle feature increases the
likelihood that users will be exposed to
results deemed important by Google’s
PageRank algorithms, it increases also the
likelihood that users will confront challenging
information that is highly relevant to the
democratic community as a whole and thus
instrumental to generating democratic
consensus. Additionally, by drawing users’
attention to the disparity between the results
returned by the “Personal” and “Popular”
filters, the toggle feature will alert users of
their own biases, as they are reflected by
their search engine’s learned biases. As a
result, this policy will reduce the negative
effects of filter bubbles while providing users
with continued access to the highly relevant

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

10

search terms returned
personalization algorithms.

by

Google’s

V. RESPONSE TO POSSIBLE CRITICISMS
The policy proposed is vulnerable to
two major criticisms. First, why would
Google, a company beholden to its
stockholders and motivated by profit, expend
precious resources to implement a web
application that, in the end, may harm its
bottom line? Some users may never explore
or utilize the toggle feature. Many will likely
use the feature occasionally to assess how the
results returned differ depending on the filter
they use. Still, others may use the toggle
feature to opt out of Google’s personalization
algorithms altogether, metaphorically leaving
the switch “off” whenever they use Google’s
search
engine.
Less
personalization
theoretically means less relevance, which
ultimately translates into fewer clicks, fewer
data points to sell to data-aggregation
companies, and fewer advertisers willing to
pay for prime real estate in Google’s search
results. So what reason does Google have to
engineer and implement this policy?
It should be noted that the policy
proposed does not completely eliminate
personalization. Rather, the toggle feature
allows users to switch between results
returned using Google’s new personalization
algorithms and results returned using
Google’s original PageRank algorithms.
Google will therefore still continue to profit
from personalization, though it is true that –
in the short term – profits may fall due to user
defection from Google “Personal” to Google
“Popular,” where advertisements command
lower prices and fewer click signals are
generated for sale to date-aggregation
companies.
In
the
long
run,
however,
implementing this policy may actually
increase Google’s profits in three different
ways. First, by increasing the transparency of
its personalization algorithms, Google will
foster trust from its users. Google currently
refrains from explaining or demonstrating
JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

exactly how its personalization algorithms
function, and the company offers users little
to no control over these settings. By
providing users with knowledge about these
filters and granting them some control over
their functioning, Google will strengthen the
loyalty of its existing users and attract new
users, thus increasing the quantity of
personal data it can collect and the value of
the advertisements it can sell. Second, this
policy will placate political interests that are
currently calling for legislation to more
strictly regulate the personal data market.59 If
legal limitations are placed on what kind of
personal data Google can collect and how it
can be collected, Google’s future inventory of
“click signals” will plummet, and a potential
source of profit will be eliminated. Thus, this
policy functions as a proactive concession to
politicians who seek to regulate the personal
data market; by increasing the transparency
of its personalization algorithms, Google may
be able to convince these interests that
further regulation is not required. Finally, as a
corporation headquartered in the United
States of America, Google will benefit from
this policy because it indirectly enhances the
quality of public debate and democratic
decision-making. By reducing the deleterious
effects of filter bubbles, this policy will
contribute to effective democratic governance
in the United States, which in will improve the
laws passed by federal and state governments
contribute to favorable business conditions.
The second criticism of this proposal
logically follows the first: even if Google did
choose to implement this policy, would
Internet users actually use toggle feature?
After all, personalization increases relevance,
and relevance is akin to convenience. In the
short term, users benefit from personalized
search results because they route users
websites tailored to their location, interests,
and values. As such, this policy may
overestimate the extent to which users will
actually assess the differences between their

Policy Analysis: Public Discourse in Age of Personalization

11

“Personal” and “Popular” search results. If
confronting opinions that disagree with our
own is inherently anxiety provoking, as
George Marcus claimed, is it reasonable to
assume that Internet users will willfully seek
out opinions that challenge their own?
First, it must be acknowledged that
not all Google users will explore and utilize
the toggle feature proposed in this policy
recommendation. The criticism advanced
above is valid: personalization is a useful tool,
and many users appreciate the relevancy of
the search results it returns. This policy does
not propose eliminating personalization, but
rather tempering it. It recommends that users
be given the option to explore how search
personalization works and opt out if they so
choose.
Two
powerful
psychological
tendencies grounded in Daniel Kahneman’s
empirical work on psychological biases and
heuristics predict that users will indeed
experiment with the toggle feature. The first
is that “information gaps” tend to spur
curiosity.60 If users are notified that they are
operating in a filtered environment, they may
feel a sense of deprivation and seek to learn
what is being hidden from them. This same
tendency explains why individuals pursue
additional information even if this
information is noninstrumental to their
decision-making.61 Thus, if a clearly visible
icon reminds users their results are being
filtered, they will be tempted to explore how
personalization changes their search results.
A second explanation for why a user may take
advantage of the toggle feature is that, once
he or she has tried it, his or her attentional
capacity is altered and amplified from a
feeling of empowerment. The tool proposed
here is indeed a powerful one; it gives users
have the ability to manipulate the algorithms
of the world’s largest Internet company and
control their own view of the world.
Pariser, The Filter Bubble, 90.
A. Bastardi and E. Shafir, “On the Pursuit and Misuse of
Useless Information,” Journal of Personality and Social
Psychology 75, no. 1 (July 1998).
60
61

59

Harbour, The Emperor of All Identities.

www.sciencepolicyjournal.org

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

Psychological research shows that the
sensation of power allows individuals to
attend to information more selectively and to
exhibit greater attentional flexibility.62 These
findings suggest that individuals who feel
empowered by the toggle feature will want to
continue using it, thus enhancing its greater
impact.
VI. SUMMARY
This policy proposal began by
examining the roots of democratic theory on
the necessity of free speech and public
discourse. It traces the development of the
news media to the present day, with
particular attention paid to the birth and
expansion of online content distributors,
especially Google. It describes Google’s
gradual transition from PageRank algorithms
to personalized algorithms, noting how
personalization increases the company’s
profits. After briefly outlining the rational and
psychological approaches to human behavior,
it applies these approaches to the filter
bubble problem. This policy proposal
recommends that Google implement a toggle
feature to combat the deleterious effects of
filter bubbles. This feature, to be embedded
directly into Google’s central search bar, will
allows users to switch back and forth
between results returned using Google’s
personalization algorithms and results
returned
using
Google’s
PageRank
algorithms. Although this policy will not
completely solve the filter bubble problem
and may, in the short run, diminish Google’s
profits, it is still defended as a necessary step
to ensure the longevity of effective public
discourse in America.
References
Bargh, John A., and Tanya L. Chartrand. “The
Unbearable Automaticity of Being.” American
Psychologist 54, No. 7 (1999): 462-479.
Ana Guinote, “Power Affects Basic Cognition: Increased
Attentional Inhibition and Flexibility,” Journal of Experimental
Social Psychology 43, no. 5 (September 2007): 685.
62

www.sciencepolicyjournal.org

Policy Analysis: Public Discourse in Age of Personalization

12

Bastardi, Anthony, and Eldar Shafir. “On the
Pursuit
and
Misuse
of
Useless
Information.” Journal of Personality and
Social Psychology 75, No. 1 (1998): 19-32.
Becker, Gary. “The Economic Way of Looking at
Life” Nobel Lecture, December 9, 1992.
Boutin, Paul. "Your Results May Very." Wall Street
Journal [New York] 20 May 2011, Eastern ed.:
A13. ProQuest Business Collection. Web. 7 Dec.
2014.
Bozdag, Engin, and Job Timmermans. "Values in
the Filter Bubble Ethics of Personalization
Algorithms in Cloud Computing." 1st
International Workshop on Values in Design –
Building Bridges between RE, HCI and
Ethics. Lisbon, Portugal. Delft University of
Technology, 6 Sept. 2011. Web. 24 Nov.
2014.
Downs, Anthony. An Economic Theory of
Democracy. New York: Harper, 1957.
Flaxman, Seth R., Sharad Goel, and Justin M. Rao.
"Ideological Segregation and the Effects of
Social Media on News Consumption."
Social Science Research Network. 6 Dec. 2014.
Web. 9 Oct. 2014.
Goldsmith, Jack L., and Tim Wu. Who Controls the
Internet?: Illusions of a Borderless World. New
York: Oxford UP, 2006. Print.
"Google's Annual Advertising Revenue 20012014." Statista. Accessed June 24, 2015.
http://www.statista.com/statistics/2662
49/advertising-revenue-of-google/.
Guinote, Ana. “Power Affects Basic Cognition:
Increased Attentional Inhibition and Flexibility.”
Journal of Experimental Social Psychology
43, No. 5 (September 2007): 685-697.
Harbour, Pamela Jones. "The Emperor of All
Identities." The New York Times. N.p., 20 May
2015.
Web.
22
June
2015.
http://www.nytimes.com/2012/12/19/opinion/
why-googlehas-too-much-power-over-yourprivate-life.html?_r=0.
Hannak, Aniko, Balachander Krishnamurthy, Piotr
Sapiezynski, David Lazer, Christo Wilson, Arash
Molavi Kakhki, and Alan Mislove. "Measuring
Personalization of Web Search." World Wide Web
Conference, May 13, 2013. Accessed June 23, 2015.
Introna, Lucas D., and Helen Nisenbaum. "Shaping
the Web: Why the Politics of Search Engines
Matters." The Information Society 16
(2000): 169-85. Accessed June 23, 2015. Taylor &
Francis.

JSPG., Vol. 7, Issue 1, August 2015

Journal of Science Policy & Governance

Jefferson, Thomas, and Henry Augustine
Washington. The Writings of Thomas Jefferson:
Being His
Autobiography,
Correspondence,
Reports, Messages, Addresses, and Other Writings,
Official and Private. Published by the Order
of the Joint Committee of Congress on the Library,
from the Original Manuscripts, Deposited in the
Department of State.
Washington, D.C.: Taylor
& Maury, 1854. Print.
Jervis, Robert. “The Drunkard’s Search.” In
Political Psychology, edited by John T Jost and Jim
Sidanius, 259–270. New York: Psychology
Press, 2004.
Kahneman, Daniel. “A Perspective on Judgment
and Choice: Mapping Bounded Rationality.”
The American Psychologist 58, No. 9
(September 2003): 697-720.
Levin, Jonathan, and Paul Milgrom. Introduction to
Choice Theory. Stanford University. 2004.
Accessed
June
23,
2015.
http://web.stanford.edu/~jdlevin/Econ
%20202/Choice%20Theory.pdf.
Liao, Q. Vera, and Wai-Tat Fu. "Beyond the Filter
Bubble: Interactive Effects of Perceived Threat
and Topic Involvement on Selective Exposure to
Information." Proc. of
Conference on Human
Factors in Computing Systems, Paris. Association
for Computing Machinery and SIGCHI, 2013.
Web. 7 Dec. 2014.
Marcus, George E. The Sentimental Citizen,
Pennsylvania State University Press, 2002.
Nagulendra, Sayooran, and Julita Vassileva.
"Understanding and Controlling the Filter Bubble
through Interactive Visualization: A User
Study."
Hypertext
and
Social
Media:
Proceedings of the 25th ACM Conference.

Policy Analysis: Public Discourse in Age of Personalization

13

Santiago,
Chile.
The
Association
for
Computing Machinery Digital Library,
2014. Web. 24 Nov. 2014.
Nguyen, Tien T., Pik-Mai Hui, F. Maxwell Harper,
Loren Terveen, and Joseph A. Constan.
"Exploring the Filter Bubble: The Effect of
Using Recommender Systems on Content
Diversity." International World Wide Web
Conference, April 7, 2014, 677-86.
"Our History in Depth." Company. Google. Web. 23
June
2015.
http://www.google.com/about/company
/history/
Pariser, Eli. The Filter Bubble: What the Internet Is
Hiding from You. New York: Penguin, 2011.
Print.
Schumpeter, Joseph. Capitalism, Socialism, and
Democracy. New York: Harper Perennial, 1976).
“The Classical Doctrine of Democracy.” Print.
Simon, Herbert A. “A Behavioral Model of Rational
Choice.” The Quarterly Journal of Economics 69,
No. 1 (1955): 99-118
Smith, Adam. The Wealth of Nations [1776], New
York: Penguin Books, 1982.
"Transparency." U.S. Public Policy. Google. Web. 8
Dec.
2014.
http://www.google.com/publicpolicy/tra
nsparency.html.
Vaidhyanathan, Siva. The Googlization of
Everything: (and Why We Should Worry). Berkeley:
U of California, 2011. Print.
Zeckman, Ashley. "Google Search Engine Market
Share Nears 68%." Search Engine Watch. 20
May 2014. Web. 22 June 2015.
http://searchenginewatch.com/sew/stud
y/2345837/google-search-engine-market-share-

Audrey Carson is an undergraduate at Harvard University concentrating in Government and
pursuing a secondary field in History. She grew up in Oakland, California and is preparing to write a
senior honors thesis on cyberpolitics. Ms. Carson's two primary research interests are the politics
of technology and international development.

www.sciencepolicyjournal.org

JSPG., Vol. 7, Issue 1, August 2015

