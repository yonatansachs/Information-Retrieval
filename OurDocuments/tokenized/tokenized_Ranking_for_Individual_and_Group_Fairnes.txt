ranking
for
individual
and
group
fairness
simultaneously
sruthi
gorantla
amit
deshpande
and
anand
louis
arxiv
2010
06986v1
cs
ir
24
sep
2020
indian
institute
of
science
bangalore
india
gorantlas
anandl
iisc
ac
in
microsoft
research
bangalore
india
amitdesh@microsoft.com
abstract
search
and
recommendation
systems
such
as
search
engines
recruiting
tools
online
marketplaces
news
and
social
media
output
ranked
lists
of
content
products
and
sometimes
people
credit
ratings
standardized
tests
risk
assessments
output
only
score
but
are
also
used
implicitly
for
ranking
bias
in
such
ranking
systems
especially
among
the
top
ranks
can
worsen
social
and
economic
inequalities
polarize
opinions
and
reinforce
stereotypes
on
the
other
hand
bias
correction
for
minority
groups
can
cause
more
harm
if
perceived
as
favoring
group-fair
outcomes
over
meritocracy
in
this
paper
we
study
trade-off
between
individual
fairness
and
group
fairness
in
ranking
we
define
individual
fairness
based
on
how
close
the
predicted
rank
of
each
item
is
to
its
true
rank
and
prove
lower
bound
on
the
trade-off
achievable
for
simultaneous
individual
and
group
fairness
in
ranking
we
give
fair
ranking
algorithm
that
takes
any
given
ranking
and
outputs
another
ranking
with
simultaneous
individual
and
group
fairness
guarantees
comparable
to
the
lower
bound
we
prove
our
algorithm
can
be
used
to
both
preprocess
training
data
as
well
as
post-process
the
output
of
existing
ranking
algorithms
our
experimental
results
show
that
our
algorithm
performs
better
than
the
state-of-the-art
fair
learning
to
rank
and
fair
post-processing
baselines
introduction
search
and
recommendation
systems
have
revolutionized
the
way
we
consume
an
overwhelming
amount
of
data
and
find
relevant
information
quickly
bp98
at05
they
help
us
find
relevant
documents
news
media
people
places
products
and
rank
them
based
on
our
interests
and
intent
klh16
pzz
19
examples
of
these
include
rankings
and
recommendations
in
online
marketplaces
recruitments
college
admissions
news
and
social
media
rankings
can
also
be
implicit
when
systems
only
screen
or
rate
people
products
places
as
well
as
the
social
and
economic
exchange
of
goods
money
information
while
downstream
application
uses
their
scores
or
ratings
for
ranking
credit
ratings
standardized
test
scores
health
risk
assessment
scores
are
some
common
examples
of
the
above
information
presented
through
ranked
lists
influences
our
worldview
par11
tav20
rankings
not
only
influence
the
users
who
consume
them
but
also
act
as
vehicles
of
opportunities
for
the
items
being
ranked
biased
ranking
of
news
people
products
raises
ethical
concerns
and
can
potentially
cause
long-term
economic
and
societal
harm
to
demographics
and
businesses
nob18
many
state-of-the-art
rankings
that
maximize
utility
or
relevance
reflect
existing
societal
biases
and
are
often
oblivious
to
the
societal
harm
they
may
cause
by
amplifying
such
biases
when
these
systems
amplify
societal
biases
observed
in
their
training
data
they
worsen
social
and
economic
inequalities
polarize
opinions
and
reinforce
stereotypes
n16
in
addition
to
ethical
concerns
there
are
also
legal
obligations
to
remove
bias
disparate
impact
laws
prohibit
even
unintentional
but
biased
outcomes
in
employment
housing
and
many
other
areas
if
one
group
of
people
belonging
to
protected
group
is
adversely
affected
compared
to
another
bs16
protected
groups
could
vary
for
specific
statutes
and
include
race
gender
age
religion
national
origin
etc
group-fairness
in
machine
learning
literature
has
focused
on
outcome-based
or
proportion-based
definitions
of
fairness
demographic
parity
equality
of
opportunity
in
classification
ranking
and
selection
problems
hps16
bhn19
another
notion
of
fairness
studied
in
fairness
literature
is
individual
fairness
achieving
individual
fairness
in
classification
often
means
similar
predictions
for
two
similar
individuals
or
two
similar
data
points
in
terms
of
their
features
or
risks
dhp
12
cdpf
17
group-fairness
is
desirable
goal
but
arbitrary
corrections
to
achieve
group-fairness
can
cause
further
harm
if
they
are
perceived
as
individually
unfair
cro04
in
classification
as
well
as
ranking
if
we
consider
individual
fairness
on
average
then
it
is
closely
tied
to
the
overall
accuracy
or
relevance
however
if
we
remove
the
aggregation
or
averaging
to
focus
on
the
parts
where
individual-fairness
really
matters
it
is
not
always
the
same
as
accuracy
or
relevance
recent
work
has
pointed
out
these
subtleties
between
group-fairness
and
individual-fairness
in
both
classification
and
ranking
bgw18
bin20
krw17
fairness
in
ranking
fairness
in
ranking
has
three
broad
requirements
sufficient
presence
of
items
belonging
to
different
groups
consistent
treatment
of
similar
individuals
or
individual
fairness
and
proper
representation
to
avoid
representational
harm
to
members
of
protected
groups
cas19a
the
first
and
the
third
requirements
are
about
group-fairness
whereas
the
second
requirement
is
about
individual-fairness
for
example
diversity
alone
in
top
ranks
satisfies
sufficient
presence
for
the
user
who
consumes
the
ranking
but
need
not
provide
consistent
treatment
and
proper
representation
in
the
way
items
are
ranked
fair
ranking
algorithms
can
be
divided
into
two
categories
first
re-ranking
algorithms
that
modify
given
ranking
of
high
utility
to
incorporate
fairness
constraints
while
trying
to
preserve
the
utility
second
learning-to-rank
algorithms
that
incorporate
fairness
and
utility
objectives
into
learning
ranker
from
training
data
re-ranking
can
be
used
to
post-process
the
prediction
of
any
given
ranker
as
well
as
pre-process
the
training
data
of
any
given
ranker
we
survey
previous
work
on
fair
ranking
with
the
above
distinction
in
mind
fair
ranking
can
be
framed
as
an
integer
optimization
problem
csv18
given
set
of
items
along
with
their
group
memberships
where
single
item
can
belong
to
multiple
groups
and
matrix
whose
entries
wij
indicate
the
utility
of
assigning
rank
to
item
the
objective
is
to
maximize
the
total
utility
of
rank
assignments
while
satisfying
the
given
group-fairness
constraints
for
the
top
positions
they
consider
group-fairness
constraints
as
lower
and
upper
bounds
on
the
group-wise
utilities
in
the
top
positions
and
allow
such
constraints
for
all
values
of
for
matrices
corresponding
to
most
practical
utility
metrics
discounted
cumulative
gain
dcg
greedy
assignment
of
the
highest
valued
item
available
at
each
rank
maximizes
the
total
utility
if
is
the
maximum
number
of
groups
an
item
belongs
to
then
fair
and
greedy
re-ranking
gives
approximation
to
the
group-fair
ranking
of
maximum
utility
csv18
the
fair
top-k
selection
problem
gives
another
formulation
of
fair
re-ranking
zbc
17
for
given
and
list
of
items
with
numerical
quality
value
assigned
to
each
item
the
objective
of
fair
top-k
selection
problem
is
to
select
items
to
maximize
utility
while
ensuring
minimum
proportion
from
protected
group
in
the
topl
ranks
for
all
the
authors
divide
utility
into
two
objectives
selection
and
ordering
selection
utility
quantifies
if
every
candidate
in
the
top-k
is
more
qualified
than
the
rest
and
ordering
utility
quantifies
if
every
pair
in
the
top-k
is
ranked
according
to
their
numerical
quality
values
they
give
an
efficient
algorithm
called
fa
ir
to
solve
the
fair
top-k
selection
problem
by
re-ranking
given
true
or
color-blind
ranking
which
orders
all
the
items
by
their
numerical
quality
values
fair
ranking
problem
can
also
be
defined
in
the
learning-to-rank
ltr
setting
where
model
is
trained
to
maxi2
mize
utility
subject
to
fairness
constraints
in
ltr
setting
the
ranking
is
probabilistic
and
the
fairness
guarantees
are
often
on
average
given
query-document
pair
the
probability
of
each
document
being
ranked
at
top-1
is
called
its
exposure
listnet
is
neural
network
model
trained
to
rank
list
of
documents
by
minimizing
loss
function
based
on
their
true
and
predicted
exposure
cql
07
building
upon
this
deltr
zc20
learns
fair
ranking
via
multi-objective
optimization
that
maximizes
utility
and
minimizes
disparate
exposure
for
different
groups
of
items
for
group-fairness
or
different
items
for
individual-fairness
this
general
learning-to-rank
framework
facilitates
optimizing
multiple
utility
metrics
while
satisfying
equal
exposure
and
fair-pg-ltr
sj19
learns
ranking
that
satisfies
fairness
of
exposure
experimental
results
on
the
above
fair
ltr
algorithms
give
better
fairness
and
utility
both
when
compared
to
post-processing
algorithms
on
real-world
datasets
yahoo
ltr
and
germancredit
data
there
is
related
work
on
defining
and
maximizing
various
group-fairness
metrics
overall
top-l
prefixes
of
the
top-k
ranks
ys17
for
given
using
an
optimization
algorithm
to
learn
fair
representations
zws
13
there
are
also
other
measures
of
group-fairness
in
ranking
based
on
pairwise
comparisons
ncgw20
bcd
19
recent
work
has
also
studied
fairness-aware
ranking
in
search
and
recommendations
for
real-world
recruitment
tools
using
fairness
metrics
based
on
skew
in
the
top-k
and
normalized
discounted
kl-divergence
ndkl
divergence
gak19
intersectional
fairness
where
the
items
belong
to
more
than
one
group
and
counterfactually
fair
ranking
measured
for
their
group
fairness
demographic
parity
at
top-k
equal
opportunity
at
top-k
and
ranking
utility
utility
loss
at
top-k
average
precision
at
top-k
are
studied
in
yls20
to
the
best
of
our
knowledge
all
existing
fair
ranking
algorithms
guarantee
group
fairness
but
can
provide
only
an
aggregate
guarantee
for
individual
fairness
they
study
trade-offs
between
group
fairness
and
utility
of
fair
rankings
but
do
not
provide
guarantees
for
the
worst-case
individual
fairness
in
this
work
we
address
this
gap
in
the
fair
ranking
literature
our
group-fairness
definition
ensures
sufficient
presence
of
all
groups
similar
to
previous
work
but
we
give
new
natural
definition
of
individual-fairness
our
main
contributions
can
be
summarized
as
follows
we
define
individual-fairness
based
on
the
worst-case
deviation
of
re-ranking
from
the
true
merit-based
or
color-blind
ranking
for
the
top-k
items
for
any
this
directly
captures
the
loss
of
visibility
suffered
by
items
of
high
merit
that
may
get
ranked
lower
in
order
to
achieve
high
group-fairness
we
prove
lower
bound
on
the
trade-off
achievable
between
individual-fairness
and
group-fairness
simultaneously
we
propose
fair
individual
and
group-fair
ranking
figr
algorithm
that
takes
given
merit-based
or
color-blind
ranking
and
outputs
another
ranking
with
simultaneous
individual
and
group-fairness
guarantees
comparable
to
the
lower
bound
mentioned
above
our
algorithm
can
be
used
to
both
pre-process
the
training
data
as
well
as
post-process
the
output
of
the
existing
ranking
algorithms
we
do
extensive
experiments
to
show
that
our
algorithm
performs
better
than
the
state-of-the-art
fair
ltr
and
fair
post-processing
baselines
on
standard
real-world
datasets
such
as
compas
recidivism
german
credit
risk
and
chilesat
used
in
fair
ranking
literature
individual
and
group
fair
rankings
in
the
rest
of
this
paper
we
say
that
rank
is
lower
than
rank
if
and
we
say
that
rank
is
higher
than
rank
if
we
now
formally
define
the
notion
of
group
fairness
this
definition
is
similar
to
the
notions
studied
in
the
literature
cas19a
csv18
definition
2.1
group
fairness
ranking
is
said
to
satisfy
group
fairness
if
any
consecutive
ranks
have
at
most
αk
items
from
any
group
this
notion
of
group
fairness
has
the
desirable
property
that
even
if
few
low
ranked
items
are
removed
from
the
ranking
the
remaining
ranking
still
satisfies
the
group
fairness
conditions
in
case
the
given
true
ranking
of
the
items
doesn
already
satisfy
group
fairness
conditions
the
re-ranking
algorithms
rearrange
the
items
in
the
true
ranking
such
that
the
group
fairness
conditions
are
satisfied
using
the
notion
of
individual
fairness
we
would
like
to
capture
how
much
an
item
has
been
displaced
from
its
true
rank
during
re-ranking
for
group
fairness
definition
2.2
individual
fairness
ranking
is
said
to
satisfy
individual
fairness
if
the
rank
of
each
item
is
at
most
times
its
true
rank
we
remark
that
unless
the
true
ranking
satisfies
the
group
fairness
conditions
some
items
with
high
merit
must
suffer
loss
of
visibility
during
the
process
of
re-ranking
for
group
fairness
that
is
the
output
group
fair
ranking
has
strictly
less
than
individual
fairness
this
manifests
the
trade-off
between
the
group
fairness
and
the
individual
fairness
in
ranking
we
also
note
that
true
ranking
is
not
always
available
for
the
real-world
datasets
in
our
experiments
we
use
some
natural
substitutes
for
the
true
ranking
see
section
for
details
closely
related
to
individual
fairness
is
the
well
studied
notion
of
precision
of
ranking
jk00
mrs08
zc20
for
given
ranking
precision
is
defined
as
the
number
of
items
in
the
top
ranks
of
the
true
ranking
which
also
appear
in
the
top
ranks
of
the
given
ranking
we
get
the
following
relation
between
individual
fairness
and
precision
corollary
2.3
ranking
satisfying
individual
fairness
also
has
precision
at
least
αk
proof
fix
ranking
having
individual
fairness
by
definition
the
top
αk
items
in
the
true
ranking
get
displaced
at
most
to
the
rank
αk
hence
at
least
the
top
αk
items
in
the
true
ranking
are
also
in
the
top
ranks
in
an
individually
fair
ranking
therefore
precision
is
at
least
αk
our
first
main
result
is
lower
bound
on
the
trade-off
achievable
for
simultaneous
individual
and
group
fairness
in
ranking
theorem
2.4
fix
and
for
every
n0
there
exists
an
such
that
n0
and
there
exists
true
ranking
of
2n
items
grouped
into
two
groups
of
items
each
such
that
the
following
holds
any
ranking
satisfying
individual
fairness
the
true
ranking
and
group
fairness
in
the
first
ranks
must
have
proof
let
where
set
to
be
any
integer
multiple
ak
such
that
n0
consider
true
ranking
where
all
the
items
from
group
are
placed
in
first
ranks
followed
by
the
items
from
group
now
consider
any
ranking
of
these
items
satisfying
individual
fairness
and
group
fairness
in
the
first
ranks
observe
that
by
our
choice
of
parameters
is
an
integer
by
the
definition
of
individual
fairness
we
get
that
the
first
ranks
must
contain
all
the
items
from
group
since
the
ranking
satisfies
group
fairness
any
consecutive
ranks
have
at
most
βk
items
from
group
this
implies
that
for
any
any
consecutive
ck
ranks
have
at
most
βck
items
from
group
by
our
choice
of
parameters
αk
is
an
integer
let
algorithm
figr
algorithm
input
ranking
of
the
items
and
parameters
satisfying
the
conditions
in
theorem
2.5
set
for
ǫk
down
to
do
for
to
min
ǫk
ǫk
do
move
item
at
rank
ǫk
to
rank
ǫk
end
end
for
to
ǫk
do
for
ǫk
to
ǫk
do
if
rank
is
unoccupied
then
10
move
the
first
item
at
rank
higher
than
that
can
be
moved
to
rank
without
violating
the
ǫk
group
fairness
constraints
among
the
items
in
ranks
ǫk
to
ǫk
if
any
such
element
is
available
11
end
12
end
13
end
14
for
to
do
15
if
rank
is
unoccupied
then
16
move
to
rank
the
first
item
at
rank
higher
than
17
end
18
end
19
output
final
ranking
αk
therefore
the
first
ck
ranks
contain
at
most
βck
βn
items
from
group
if
then
first
ranks
contain
strictly
less
than
elements
from
group
which
is
contradiction
therefore
we
must
have
our
next
main
result
is
fair
ranking
algorithm
that
takes
any
given
ranking
and
outputs
another
ranking
with
individual
and
group
fairness
guarantees
comparable
to
that
of
theorem
2.4
theorem
2.5
given
true
ranking
of
items
grouped
into
disjoint
groups
with
each
group
having
at
least
items
and
fairness
parameters
and
there
exists
polynomial
time
algorithm
to
compute
ranking
satisfying
individual
fairness
k2
group
fairness
in
the
first
ranks
for
the
rest
of
the
section
let
k2
let
the
ith
block
of
ranks
refer
to
the
ranks
ǫk
to
ǫk
lemma
2.6
the
ranking
output
by
algorithm
satisfies
ǫk
individual
fairness
proof
fix
an
item
having
true
rank
at
the
end
of
step
its
rank
is
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
subsequent
steps
do
not
increase
the
ranking
of
any
item
lemma
2.7
at
the
end
of
step
13
no
positions
in
the
first
ǫk
ranks
will
be
empty
proof
consider
step
10
of
the
algorithm
rank
will
be
left
unoccupied
if
either
each
group
already
has
ǫk
elements
in
the
block
containing
rank
or
ii
the
groups
which
have
less
than
ǫk
elements
in
the
block
containing
rank
do
not
have
any
elements
in
ranks
higher
than
by
our
choice
of
parameters
we
have
ǫk
1ℓ
and
1ℓ
ǫk
ǫk
1ℓ
ǫk
ǫk
ǫk
ǫk
therefore
if
each
group
has
ǫk
elements
in
the
block
containing
rank
then
every
rank
in
this
block
has
to
be
occupied
therefore
case
can
not
happen
for
any
block
the
first
ǫk
elements
in
any
intermediate
ranking
including
the
final
ranking
contain
at
most
ǫk
items
from
each
group
since
there
are
at
least
elements
from
group
we
have
that
as
long
as
satisfies
ǫk
there
will
be
at
least
one
element
available
from
each
group
to
move
into
an
empty
spot
in
the
first
blocks
without
violating
ǫk
group
fairness
constraints
for
any
of
the
first
blocks
thus
the
first
blocks
will
be
filled
at
the
end
of
step
13
therefore
the
number
of
ranks
filled
is
at
least
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
ǫk
therefore
case
ii
will
not
happen
for
the
first
ǫk
ranks
thus
at
the
end
of
step
13
no
positions
in
the
first
ǫk
ranks
will
be
empty
lemma
2.8
at
the
end
of
step
13
each
block
has
at
most
ǫk
items
from
any
particular
group
proof
for
any
block
we
observe
that
at
the
end
of
step
block
of
size
ǫk
has
at
most
ǫk
nonempty
positions
and
therefore
has
at
most
ǫk
items
from
any
particular
group
step
10
ensures
that
when
the
algorithm
terminates
each
block
has
at
most
ǫk
items
from
any
particular
group
lemma
2.9
the
ranking
output
by
algorithm
satisfies
group
fairness
in
the
first
ǫk
ranks
proof
lemma
2.7
shows
that
none
of
the
first
ǫk
ranks
will
be
empty
at
the
end
of
step
13
therefore
these
ranks
will
remain
unchanged
in
the
steps
after
step
13
def
def
consider
any
consecutive
ranks
let
i1
ǫk
and
i2
ǫk
by
construction
the
blocks
i1
i2
are
fully
contained
in
the
ranks
for
any
the
number
of
items
from
group
in
ranks
to
is
at
most
the
number
of
items
from
group
in
blocks
i1
to
i2
using
lemma
2.8
we
get
that
this
is
at
most
αk
ǫk
we
note
that
this
bound
also
holds
for
cases
when
i2
i1
or
i2
i1
proof
of
theorem
2.5
follows
from
the
choice
of
and
from
lemma
2.6
lemma
2.7
and
lemma
2.9
we
also
obtain
slightly
stronger
guarantees
if
we
only
need
group
fairness
in
blocks
of
size
instead
of
group
fairness
guarantees
for
any
consecutive
ranks
theorem
2.10
given
true
ranking
of
items
grouped
into
disjoint
groups
with
each
group
having
at
least
items
and
fairness
parameters
and
such
that
there
exists
polynomial
time
algorithm
to
compute
ranking
satisfying
k1
individual
fairness
group
fairness
in
each
of
the
first
αk
blocks
of
size
proof
we
use
algorithm
with
now
the
ith
block
is
of
size
ǫk
fix
an
item
in
the
true
ranking
the
proof
of
lemma
2.6
shows
that
its
final
rank
will
be
at
most
ǫk
k1
here
the
equality
follows
from
our
choice
of
hence
the
ranking
output
by
algorithm
with
satisfies
k1
individual
fairness
lemma
2.8
shows
that
at
the
end
of
step
13
each
block
has
at
most
αk
items
from
any
particular
group
for
we
have
αℓ
αk
αk
1ℓ
therefore
if
block
contains
αk
items
from
each
of
the
groups
it
can
not
have
any
empty
ranks
consequently
as
long
as
αk
items
fom
each
group
are
available
blocks
to
will
not
contain
empty
ranks
since
there
are
at
least
items
from
each
group
no
rank
in
the
first
blocks
will
be
empty
where
satisfies
αk
hence
for
blocks
to
each
of
size
contain
at
most
αk
items
from
each
group
that
is
the
first
such
that
αk
αk
blocks
each
of
size
satisfy
group
fairness
experimental
validation
in
this
section
we
study
the
trade-off
between
individual
and
group
fairness
achieved
by
figr
on
various
realworld
datasets
we
also
compare
our
results
with
the
fair
post-processing
and
fair
ltr
baselines
3.1
datasets
we
consider
two
types
of
datasets
in
our
experiments
the
first
type
has
global
ranking
on
the
entire
dataset
calculated
using
subset
of
attributes
bias
in
one
or
more
of
these
attributes
might
introduce
bias
in
the
ranking
output
hence
fair
post-processing
algorithm
can
be
used
to
correct
these
biases
in
the
global
ranking
however
listwise
ltr
model
such
as
listnet
cql
07
requires
that
each
training
sample
consists
of
list
of
items
on
which
ranking
is
available
hence
the
experiments
based
on
ltr
model
are
performed
on
the
second
type
of
datasets
these
datasets
have
query-document
format
and
support
training
of
listwise
ltr
model
german
credit
risk
dataset
this
is
dataset
of
credit
risk
scoring
of
adult
german
residents
dg17
it
consists
of
set
of
1000
candidates
from
various
demographics
applying
for
loan
features
of
the
candidate
include
demographic
information
such
as
personal
status
gender
age
etc
as
well
as
financial
status
such
as
credit
history
property
housing
job
etc
schufa
scores
of
these
individuals
is
used
to
get
global
ranking
on
the
dataset
similar
to
zbc
17
and
ys17
cas19b
observed
that
schufa
scoring
is
biased
against
young
adults
hence
we
divide
the
dataset
into
protected
and
non-protected
groups
based
on
age
we
consider
two
such
cases
age
25
as
protected
group
and
ii
age
35
as
protected
group
similar
to
zbc
17
compas
recidivism
dataset
this
dataset
consists
of
violent
recidivism
assessment
of
nearly
7000
criminal
defendants
by
the
correctional
offender
management
profiling
for
alternative
sanctions
compas
tool
based
on
answers
to
questionnaire
consisting
of
137
questions
this
dataset
is
curated
by
almk16
to
analyze
the
biases
in
the
tool
their
study
points
out
racial
as
well
as
gender
biases
in
the
predictions
of
compas
in
our
experiments
we
consider
ranking
based
on
the
recidivism
score
individual
with
highest
recidivism
is
ranked
at
top-1
the
protected
groups
are
gender
female
and
ii
race
african
american
similar
to
zbc
17
chilesat
dataset
this
dataset
consists
of
chilean
university
admission
test
scores
of
the
students
admitted
their
highschool
grades
and
score
based
on
their
academic
performance
after
one
year
at
the
university
given
these
test
scores
and
their
highschool
grades
an
ltr
model
has
to
predict
in
advance
ranking
over
these
students
about
their
performance
at
the
end
of
the
first
academic
year
this
dataset
has
query-document
format
since
each
academic
year
can
be
considered
as
query
and
students
in
that
academic
year
are
the
list
of
candidates
to
be
ranked
there
are
around
450
to
600
student
records
in
each
of
the
academic
years
in
the
dataset
in
the
experiments
we
perform
fold
cross
validation
with
four
academic
years
for
training
and
one
for
validating
the
non-binary
genders
were
not
annotated
in
any
of
the
datasets
used
in
this
paper
ltr
models
we
consider
two
protected
groups
in
the
dataset
gender
female
and
high-school
type
public
to
study
the
behaviour
of
figr
when
two
different
types
of
bias
exist
these
biases
are
also
studied
by
zc20
in
our
experiments
we
use
the
processed
subsets
of
chilesat
engineering
students
german
credit
risk
and
compas
recidivism
datasets3
3.2
experiments
we
compare
our
results
with
baselines
such
as
listnet
cql
07
listwise
ltr
model
that
ranks
list
of
documents
based
on
their
relevance
scores
with
respect
to
the
query
ii
deltr
zc20
an
in-processing
ltr
model
that
is
trained
with
listnet
objective
along
with
the
group
fairness
constraint
fairness
of
exposure
and
iii
fa
ir
zbc
17
post-processing
algorithm
that
re-ranks
given
ranking
to
maintain
significant
proportions
of
the
protected
group
in
every
prefix
of
the
ranking
for
brevity
we
omit
detailing
the
intricacies
of
these
baseline
algorithms
following
is
brief
explanation
of
our
experimental
setup4
color-blind
ltr
listnet
trained
in
colorblind
fashion
no
access
to
protected
or
sensitive
attributes
color-aware
ltr
listnet
trained
on
all
the
attributes
deltr
trained
with
100k
refer
zc20
for
more
details
fa
ir
pre
fa
ir
with
parameter
used
to
pre-process
the
training
data
listnet
is
trained
on
this
data
fa
ir
post
listnet
predictions
on
test
data
post-processed
using
fa
ir
with
parameter
fa
ir
fa
ir
with
parameter
used
to
re-rank
the
global
ranking
in
the
compas
recidivism
and
german
credit
risk
datasets
figr
pre
figr
with
parameter
max
1ℓ
used
to
pre-process
the
training
data
we
add
small
number
to
since
figr
requires
that
is
strictly
greater
than
listnet
is
trained
on
this
data
figr
post
listnet
predictions
on
test
data
post-processed
using
figr
with
parameter
max
1ℓ
figr
figr
with
parameter
max
1ℓ
used
to
re-rank
the
global
ranking
in
the
compas
recidivism
and
german
credit
risk
datasets
training
details
for
fa
ir
the
parameter
is
chosen
from
where
is
the
proportion
of
the
protected
group
0.1
and
0.1
this
parameter
setting
is
adopted
from
zc20
in
all
the
experiments
the
number
of
groups
are
protected
and
non-protected
hence
we
use
and
0.01
in
figr
for
the
values
of
and
the
parameter
in
figr
will
be
max
0.51
for
all
the
datasets
in
the
experiments
we
set
the
value
of
to
100
this
is
reasonable
choice
since
the
number
of
ranked
items
is
at
least
500
in
all
the
datasets
for
the
ltr
models
listnet
and
deltr
we
use
the
same
parameter
and
hyper-parameter
settings
as
zc20
and
report
average
results
across
folds
of
the
chilesat
dataset
further
to
account
for
variability
in
learning
the
parameters
for
ltr
we
run
all
the
ltr
based
expreiments
times
and
report
the
final
average5
https://github.com/milkalichtblau/deltr-experiments/tree/master/data/engineeringstudents
https://github.com/dataresponsibly/fairrank/tree/master/datasets
code
for
figr
and
baselines
is
available
at
https://github.com/sruthigorantla/figr
our
runs
showed
minor
variations
in
the
learned
ltr
parameters
with
almost
same
ranking
predictions
protected
group
age
35
proportion
of
the
protected
group
proportion
of
the
protected
group
protected
group
age
25
0.6
0.5
0.4
0.3
0.2
0.1
0.6
0.5
0.4
0.3
0.2
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
0.1
figr
fa
ir
100
200
300
400
500
600
700
800
900
1000
figr
fa
ir
fa
ir
true
1.0
min
min
1.0
min
min
top-k
figr
0.8
0.8
0.6
0.6
item
binsofofsize
size100
100
blocks
10
blocks
of
size
100
10
figure
trade-offs
between
individual
and
group
fairness
in
the
german
credit
risk
dataset
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
reading
the
plots
for
every
combination
of
dataset
and
protected
group
we
show
pair
of
plots
to
understand
better
the
trade-off
between
group
and
individual
fairness
in
the
real-world
datasets
for
example
figure
and
figure
show
group
and
individual
fairness
respectively
on
the
german
credit
risk
dataset
with
age
25
as
protected
group
in
figure
x-axis
represents
the
rank
and
y-axis
shows
the
proportion
of
protected
group
items
in
the
top-k
ranks
in
this
plot
the
line
shows
the
proportion
of
the
protected
group
in
the
entire
training
data
whereas
the
line
true
represents
the
proportion
of
the
protected
group
in
the
top-k
ranks
of
the
true
ranking
these
two
lines
serve
as
guidelines
to
understand
the
behavior
of
various
algorithms
and
pick
the
best
performing
algorithm
in
figure
x-axis
represents
blocks
of
size
100
ith
block
consists
of
items
100
to
100i
of
the
true
ranking
for
any
item
in
the
block
αj
pred
where
pred
is
the
rank
of
item
in
the
output
ranking
y-axis
shows
the
minimum
αj
among
the
items
in
the
block
we
call
this
min
higher
the
value
of
min
higher
the
individual
fairness
according
to
the
definition
2.2
since
we
measure
individual
fairness
with
respect
to
the
true
ranking
the
line
true
has
individual
fairness
we
say
that
there
is
trade-off
between
group
and
individual
fairness
when
the
algorithms
placing
higher
proportions
of
protected
groups
in
the
top-k
ranks
consistently
suffer
from
lesser
individual
fairness
in
these
ranks
in
the
following
section
we
study
the
trade-offs
achieved
by
figr
and
the
baselines
on
the
real-world
datasets
10
protected
group
african
american
proportion
of
the
protected
group
proportion
of
the
protected
group
protected
group
female
0.6
0.5
0.4
0.3
0.2
0.1
0.6
0.5
0.4
0.3
0.2
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
figr
fa
ir
0.1
100
200
300
400
500
600
700
800
900
1000
top-k
figr
figr
fa
ir
fa
ir
1.0
min
min
1.0
min
min
true
0.8
0.8
0.6
0.6
item
binsofofsize
size
100
100
blocks
10
item
binsofofsize
size
100
100
blocks
10
figure
trade-offs
between
individual
and
group
fairness
in
the
compas
recidivism
dataset
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
we
show
the
reults
of
the
top
1000
ranks
3.3
figr
vs
fair
post-processing
methods
on
german
credit
risk
and
compas
datasets
figures
show
the
experimental
results
on
the
german
credit
risk
dataset
the
protected
group
age
25
younger
adults
is
significantly
underrepresented
in
the
dataset
with
0.15
see
figure
moreover
their
true
proportions
given
by
the
schufa
score
based
ranking
in
the
top-400
ranks
is
even
less
this
indicates
bias
against
younger
adults
in
the
ranking
all
three
variants
of
figr
allocate
almost
1.5
times
the
true
proportion
and
increase
the
representation
of
younger
adults
in
the
top-400
ranks
figr
also
achieves
approximately
70individual
fairness
which
is
reasonable
trade-off
for
group
fairness
in
this
case
fa
ir
on
the
other
hand
fails
to
correct
biases
with
and
even
with
it
fails
to
achieve
significant
improvement
in
the
representation
of
the
younger
adults
in
case
of
the
protected
group
age
35
young
adults
although
their
representation
in
the
entire
dataset
is
almost
11
1.0
0.45
0.8
min
min
proportion
of
the
protected
group
pre-processing
vs
deltr
vs
ltr
0.50
0.40
0.6
0.35
0.4
0.30
0.2
100
0.45
0.45
0.40
200
300
400
500
color-blind
ltr
color-aware
ltr
deltr
top-k
figr
pre
figr
pre
figr
pre
blocks
of
size
100
0.50
fa
ir
pre
fa
ir
pre
0.45
fa
ir
pre
true
figure
trade-offs
between
individual
and
group
fairness
in
the
chilesat
dataset
with
public
school
as
the
protected
group
comparision
of
pre-processig
in-processing
and
standard
ltr
methods
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
see
table
in
appendix
for
tabulated
results
half
0.55
they
are
all
ranked
in
the
tail
of
the
true
ranking
this
shows
strong
bias
against
young
adults
see
figure
fa
ir
stays
too
close
to
the
true
proportions
on
the
other
hand
with
slightly
larger
values
of
fa
ir
and
clearly
overcompensate
for
the
lack
of
representation
of
the
protected
group
at
the
cost
of
very
low
individual
fairness
0.50
0.65
this
also
reduces
the
proportion
of
the
non-protected
group
to
much
lower
than
it
true
proportion
leading
to
inversion
of
bias
figr
by
design
avoids
this
problem
for
values
of
higher
than
is
set
to
1ℓ
where
is
small
value
0.01
in
our
experiments
hence
the
representation
of
both
protected
and
non-protected
groups
is
bounded
see
theorem
2.5
this
is
also
evident
in
our
results
with
and
figr
achieves
same
results
because
in
both
the
cases
is
set
to
0.51
figr
with
each
of
these
parameter
settings
finds
the
best
trade-off
between
group
fairness
45
in
the
top
100
positions
and
individual
fairness
0.75
in
the
top
100
positions
figures
show
experimental
results
on
the
compas
recidivism
dataset
females
in
the
dataset
0.19
face
bias
in
the
top-200
ranks
due
to
the
biases
introduced
by
the
compas
tool
see
figure
once
again
fa
ir
with
and
stays
close
to
the
true
proportions
with
it
improves
the
female
representation
in
the
top-400
ranks
figr
also
shows
same
trends
as
fa
ir
both
figr
and
trade
off
individual
fairness
for
group
fairness
nevertheless
this
doesn
cause
an
inversion
of
bias
in
the
output
ranking
the
protected
group
race
african
american
is
substantially
underrepresented
in
the
dataset
in
the
top-500
positions
even
though
their
representation
in
the
entire
data
is
high
0.51
see
figure
this
shows
bias
against
the
protected
group
in
the
true
ranking
based
on
the
recidivism
score
since
this
case
of
bias
is
similar
to
the
bias
towards
young
adults
in
the
german
credit
risk
dataset
we
see
similar
results
figr
improves
their
representation
in
the
top-500
ranks
while
achieving
reasonable
individual
fairness
fa
ir
is
close
competitor
to
figr
for
the
best
choice
of
algorithm
however
fa
ir
and
once
again
overcompensate
for
the
lack
of
representation
of
african-americans
in
the
top-1000
positions
since
they
actually
are
the
majority
group
in
the
data
12
1.0
0.45
0.8
min
min
proportion
of
the
protected
group
post-processing
vs
deltr
vs
ltr
0.50
0.40
0.6
0.4
0.35
0.2
0.30
100
200
300
400
top-k
0.45
0.45
color-blind
ltr
color-aware
ltr
deltr
0.40
500
figrpost
pre
figr
figr
figrpost
pre
figr
figrpost
pre
blocks
of
size
100
0.50
fa
ir
pre
postp
fa
ir
fa
ir
post
pre
pp
0.45
fa
ir
post
pre
pp
true
figure
trade-offs
between
individual
and
group
fairness
in
the
chilesat
dataset
with
public
school
as
the
protected
group
comparision
of
post-processig
in-processing
and
standard
ltr
methods
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
see
table
in
appendix
for
tabulated
results
3.4
figr
vs
fair
ltr
methods
on
the
chilesat
dataset
figure
and
figure
show
the
group
and
individual
fairness
trade-offs
achieved
by
the
ltr
models
deltr
and
pre-processing
training
data
with
figr
and
fa
ir
whereas
figure
and
figure
show
these
results
for
same
ltr
and
deltr
models
compared
with
post-processing
methods
applied
on
the
listnet
predictions
representation
of
the
students
from
public
school
in
the
top-300
ranks
is
higher
than
their
total
representation
0.34
see
figure
this
means
that
the
students
from
public
schools
having
same
test
scores
as
private
school
students
indeed
have
caliber
to
perform
better
at
the
university
hence
it
is
evident
that
the
test
scores
are
biased
against
the
students
from
the
public
school
such
biases
are
naturally
adjusted
by
color-aware
ltr
whereas
color-blind
ltr
re-inforces
these
biases
see
figure
deltr
does
not
further
adjust
the
ranks
interestingly
pre-processing
with
all
three
variants
of
figr
and
fa
ir
with
rank
higher
proportions
of
the
public
school
students
in
the
top-200
ranks
and
at
the
same
time
achieve
high
individual
fairness
compared
to
the
ltr
models
see
figure
although
color-aware
ltr
corrects
biases
its
impact
is
limited
deltr
on
the
other
hand
limits
itself
from
achieving
higher
group
fairness
as
well
as
individual
fairness
since
fairness
of
exposure
is
already
satisfied
in
the
case
of
post-processing
the
listnet
predictions
only
figr
with
achieves
similar
results
see
figure
and
still
has
as
much
individual
fairness
as
others
female
students
are
substantially
under-represented
in
the
dataset
0.20
the
color-blind
ltr
ranks
almost
the
same
proportion
of
the
females
in
all
top-k
ranks
as
that
of
true
proportions
see
figure
this
means
that
the
academic
performance
of
females
both
before
and
after
the
first
year
at
the
university
is
bad
in
this
case
color-aware
ltr
will
further
learn
to
discriminate
against
the
females
because
in
this
case
placing
females
in
the
top
few
ranks
would
hurt
the
utility
as
expected
the
color-aware
ltr
stays
below
the
color-blind
ltr
deltr
acts
on
the
difference
in
the
exposure
of
the
groups
hence
it
places
more
number
of
females
in
the
top-200
ranks
purely
to
achieve
fairness
of
exposure
but
deltr
is
oblivious
to
its
impact
on
the
individual
fairness
although
13
0.35
1.0
0.30
0.8
min
min
proportion
of
the
protected
group
pre-processing
vs
deltr
vs
ltr
0.25
0.20
0.6
0.4
0.15
0.2
0.10
100
200
300
400
top-k
0.45
0.45
color-blind
ltr
color-aware
ltr
deltr
0.40
500
figr
pre
figr
pre
figr
pre
blocks
of
size
100
0.50
fa
ir
pre
fa
ir
pre
0.45
fa
ir
pre
true
figure
trade-offs
between
individual
and
group
fairness
in
the
chilesat
dataset
with
female
as
the
protected
group
comparision
of
pre-processig
in-processing
and
standard
ltr
methods
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
see
table
in
appendix
for
tabulated
results
pre-processing
with
figr
achieves
less
individual
fairness
than
deltr
post-processing
with
figr
places
the
highest
number
of
females
in
the
top-200
while
achieving
as
much
individual
fairness
as
deltr
see
figure
in
both
the
cases
figr
with
most
of
the
parameter
settings
achieves
the
best
group
fairness
as
well
as
individual
fairness
even
in
cases
where
it
loses
some
individual
fairness
compared
to
the
baselines
the
trade-off
is
minimal
in
contrast
the
fair
ltr
and
fair
post-processing
baselines
do
not
achieve
these
trade-offs
conclusion
fair
ranking
is
crucial
to
search
and
recommendations
and
has
been
matter
of
global
concern
in
the
quest
towards
responsible
ai
we
studied
group
and
individual
fairness
notions
in
ranking
we
defined
individual
fairness
based
on
how
close
the
predicted
rank
of
each
item
is
to
its
true
rank
and
proved
lower
bound
on
the
trade-off
achievable
for
simultaneous
individual
and
group
fairness
in
ranking
while
other
works
csv18
etc
have
studied
aggregate
forms
of
individual
fairness
to
the
best
of
our
knowledge
our
work
is
the
first
to
give
provable
guarantees
on
the
worst-case
displacement
of
an
item
in
the
output
ranking
with
respect
to
its
true
rank
we
presented
the
first
to
the
best
of
our
knowledge
algorithm
that
takes
any
given
ranking
and
outputs
another
ranking
with
simultaneous
individual
and
group
fairness
guarantees
comparable
to
the
lower
bound
we
proved
our
algorithm
performed
better
than
the
state-of-the-art
fair
learning
to
rank
and
fair
post-processing
baselines
one
limitation
of
our
work
and
other
re-ranking
algorithms
is
that
it
requires
the
true
ranking
as
input
all
our
theoretical
guarantees
are
with
respect
to
this
true
ranking
in
practice
true
merit-based
ranking
may
be
debatable
or
unavailable
due
to
incomplete
data
unobserved
features
legal
and
ethical
considerations
behind
the
downstream
application
of
these
rankings
etc
14
0.35
1.0
0.30
0.8
min
min
proportion
of
the
protected
group
post-processing
vs
deltr
vs
ltr
0.25
0.20
0.6
0.4
0.15
0.2
0.10
100
200
300
400
500
0.45
0.45
0.40
color-blind
ltr
color-aware
ltr
deltr
top-k
figrpost
pre
figr
figr
figrpost
pre
figr
figrpost
pre
blocks
of
size
100
0.50
fa
ir
pre
postp
fa
ir
fa
ir
post
pre
pp
0.45
fa
ir
post
pre
pp
true
figure
trade-offs
between
individual
and
group
fairness
in
the
chilesat
dataset
with
female
as
the
protected
group
comparision
of
post-processig
in-processing
and
standard
ltr
methods
in
all
the
experiments
with
figr
the
parameter
settings
are
0.01
and
100
then
max
0.51
see
table
in
appendix
for
tabulated
results
acknowledgements
al
was
supported
in
part
by
serb
award
ecr
2017
003296
and
pratiksha
trust
young
investigator
award
al
is
also
grateful
to
microsoft
research
for
supporting
this
collaboration
references
almk16
julia
angwin
jeff
larson
surya
mattu
and
lauren
kirchner
machine
bias
2016
at05
adomavicius
and
tuzhilin
toward
the
next
generation
of
recommender
systems
survey
of
the
state-of-the-art
and
possible
extensions
ieee
transactions
on
knowledge
and
data
engineering
17
734
749
2005
bcd
19
alex
beutel
chen
doshi
qian
wei
wu
heldt
zhe
zhao
hong
ed
huai
hsin
chi
and
cristos
goodrow
fairness
in
recommendation
ranking
through
pairwise
comparisons
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
2019
bgw18
asia
biega
krishna
gummadi
and
gerhard
weikum
equity
of
attention
amortizing
individual
fairness
in
rankings
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
sigir
18
page
405414
new
york
ny
usa
2018
association
for
computing
machinery
bhn19
solon
barocas
moritz
hardt
and
arvind
narayanan
fairness
and
machine
learning
fairmlbook
org
2019
http://www.fairmlbook.org.
15
bin20
reuben
binns
on
the
apparent
conflict
between
individual
and
group
fairness
in
fat
20
conference
on
fairness
accountability
and
transparency
barcelona
spain
january
27
30
2020
pages
514
524
acm
2020
bp98
sergey
brin
and
lawrence
page
the
anatomy
of
large-scale
hypertextual
web
search
engine
in
proceedings
of
the
seventh
international
conference
on
world
wide
web
www7
page
107117
nld
1998
elsevier
science
publishers
bs16
solon
barocas
and
andrew
selbst
104
671
732
2016
cas19a
carlos
castillo
fairness
and
transparency
in
ranking
sigir
forum
52
6471
january
2019
cas19b
carlos
castillo
fairness
and
transparency
in
ranking
volume
52
page
6471
new
york
ny
usa
january
2019
association
for
computing
machinery
big
data
disparate
impact
california
law
review
cdpf
17
sam
corbett-davies
emma
pierson
avi
feller
sharad
goel
and
aziz
huq
algorithmic
decision
making
and
the
cost
of
fairness
in
proceedings
of
the
23rd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
kdd
17
page
797806
new
york
ny
usa
2017
association
for
computing
machinery
cql
07
zhe
cao
tao
qin
tie-yan
liu
ming-feng
tsai
and
hang
li
learning
to
rank
from
pairwise
approach
to
listwise
approach
in
proceedings
of
the
24th
international
conference
on
machine
learning
icml
07
page
129136
new
york
ny
usa
2007
association
for
computing
machinery
cro04
crosby
affirmative
action
is
dead
long
live
affirmative
action
current
perspectives
in
psychology
yale
university
press
2004
csv18
elisa
celis
damian
straszak
and
nisheeth
vishnoi
ranking
with
fairness
constraints
in
ioannis
chatzigiannakis
christos
kaklamanis
da
niel
marx
and
donald
sannella
editors
45th
international
colloquium
on
automata
languages
and
programming
icalp
2018
july
13
2018
prague
czech
republic
volume
107
of
lipics
pages
28
28
15
schloss
dagstuhl
leibnizzentrum
fu
informatik
2018
dg17
dheeru
dua
and
casey
graff
uci
machine
learning
repository
2017
dhp
12
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
and
richard
zemel
fairness
through
awareness
in
proceedings
of
the
3rd
innovations
in
theoretical
computer
science
conference
itcs
12
page
214226
new
york
ny
usa
2012
association
for
computing
machinery
gak19
sahin
cem
geyik
stuart
ambler
and
krishnaram
kenthapadi
fairness-aware
ranking
in
search
recommendation
systems
with
application
to
linkedin
talent
search
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
kdd
19
page
22212231
new
york
ny
usa
2019
association
for
computing
machinery
hps16
hardt
price
and
nathan
srebro
equality
of
opportunity
in
supervised
learning
in
nips
2016
jk00
kalervo
ja
rvelin
and
jaana
keka
la
inen
ir
evaluation
methods
for
retrieving
highly
relevant
documents
in
proceedings
of
the
23rd
annual
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
00
page
4148
new
york
ny
usa
2000
association
for
computing
machinery
16
klh16
christoph
kofler
martha
larson
and
alan
hanjalic
user
intent
in
multimedia
search
survey
of
the
state
of
the
art
and
future
challenges
acm
comput
surv
49
august
2016
krw17
michael
kearns
aaron
roth
and
zhiwei
steven
wu
meritocratic
fairness
for
cross-population
selection
volume
70
of
proceedings
of
machine
learning
research
pages
1828
1836
international
convention
centre
sydney
australia
06
11
aug
2017
pmlr
mrs08
christopher
manning
prabhakar
raghavan
and
hinrich
schu
tze
introduction
to
information
retrieval
cambridge
university
press
usa
2008
ncgw20
harikrishna
narasimhan
andy
cotter
maya
gupta
and
serena
lutong
wang
pairwise
fairness
for
ranking
and
regression
in
33rd
aaai
conference
on
artificial
intelligence
2020
nob18
safiya
umoja
noble
algorithms
of
oppression
how
search
engines
reinforce
racism
nyu
press
2018
n16
cathy
neil
weapons
of
math
destruction
how
big
data
increases
inequality
and
threatens
democracy
crown
publishing
group
usa
2016
par11
eli
pariser
the
filter
bubble
what
the
internet
is
hiding
from
you
penguin
group
the
2011
pzz
19
changhua
pei
yi
zhang
yongfeng
zhang
fei
sun
xiao
lin
hanxiao
sun
jian
wu
peng
jiang
junfeng
ge
wenwu
ou
and
dan
pei
personalized
re-ranking
for
recommendation
in
proceedings
of
the
13th
acm
conference
on
recommender
systems
recsys
19
page
311
new
york
ny
usa
2019
association
for
computing
machinery
sj19
ashudeep
singh
and
thorsten
joachims
policy
learning
for
fairness
in
ranking
in
neurips
2019
tav20
herman
tavani
search
engines
and
ethics
in
edward
zalta
editor
the
stanford
encyclopedia
of
philosophy
metaphysics
research
lab
stanford
university
fall
2020
edition
2020
yls20
ke
yang
joshua
loftus
and
julia
stoyanovich
causal
intersectionality
for
fair
ranking
arxiv
abs
2006.08688
2020
ys17
ke
yang
and
julia
stoyanovich
measuring
fairness
in
ranked
outputs
in
proceedings
of
the
29th
international
conference
on
scientific
and
statistical
database
management
ssdbm
17
new
york
ny
usa
2017
association
for
computing
machinery
zbc
17
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
and
ricardo
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
in
proceedings
of
the
2017
acm
on
conference
on
information
and
knowledge
management
cikm
17
page
15691578
new
york
ny
usa
2017
association
for
computing
machinery
zc20
meike
zehlike
and
carlos
castillo
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
proceedings
of
the
web
conference
2020
2020
zws
13
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
and
cynthia
dwork
learning
fair
representations
in
sanjoy
dasgupta
and
david
mcallester
editors
proceedings
of
the
30th
international
conference
on
machine
learning
volume
28
of
proceedings
of
machine
learning
research
pages
325
333
atlanta
georgia
usa
17
19
jun
2013
pmlr
appendix
for
experimental
results
17
top-k
ranks
method
true
color-blind
ltr
color-aware
ltr
deltr
fa
ir
pre
fa
ir
pre
fa
ir
pre
figr
pre
figr
pre
figr
pre
fa
ir
post
fa
ir
post
fa
ir
post
figr
post
figr
post
figr
post
100
grp
ind
0.42
0.32
0.17
0.38
0.18
0.41
0.19
0.39
0.18
0.45
0.2
0.39
0.18
0.45
0.2
0.49
0.21
0.43
0.19
0.38
0.18
0.4
0.17
0.38
0.18
0.41
0.19
0.45
0.18
0.41
0.18
200
grp
ind
0.38
0.32
0.46
0.38
0.4
0.41
0.42
0.39
0.4
0.45
0.54
0.39
0.4
0.45
0.54
0.49
0.52
0.44
0.54
0.38
0.4
0.41
0.46
0.38
0.4
0.41
0.47
0.44
0.46
0.41
0.47
300
grp
ind
0.36
0.32
0.63
0.37
0.72
0.39
0.76
0.38
0.64
0.43
0.65
0.38
0.61
0.43
0.63
0.45
0.65
0.41
0.62
0.37
0.72
0.41
0.7
0.37
0.72
0.41
0.67
0.45
0.71
0.4
0.65
400
grp
ind
0.35
0.33
0.88
0.36
0.87
0.36
0.77
0.36
0.87
0.38
0.82
0.36
0.87
0.38
0.82
0.39
0.82
0.37
0.82
0.35
0.87
0.41
0.84
0.35
0.87
0.39
0.83
0.41
0.87
0.39
0.83
500
grp
ind
0.31
0.3
0.31
0.31
0.31
0.32
0.31
0.32
0.32
0.32
0.31
0.34
0.31
0.33
0.34
0.33
table
individual
and
group
fairness
results
for
the
chilesat
dataset
with
public
school
as
the
protected
group
the
plots
for
these
results
are
as
shown
in
figure
and
figure
the
column
grp
shows
the
proportion
of
the
protected
group
items
in
the
top-k
ranks
and
the
column
ind
shows
min
in
the
ranks
99
to
note
that
99
to
is
the
100
th
block
of
size
100
shown
in
the
plots
18
top-k
ranks
method
true
color-blind
ltr
color-aware
ltr
deltr
fa
ir
pre
fa
ir
pre
fa
ir
pre
figr
pre
figr
pre
figr
pre
fa
ir
post
fa
ir
post
fa
ir
post
figr
post
figr
post
figr
post
100
grp
ind
0.1
0.13
0.17
0.1
0.17
0.18
0.16
0.12
0.16
0.23
0.14
0.11
0.17
0.27
0.14
0.35
0.13
0.27
0.14
0.15
0.16
0.25
0.15
0.11
0.17
0.3
0.13
0.35
0.13
0.31
0.13
200
grp
ind
0.14
0.14
0.46
0.12
0.46
0.22
0.49
0.14
0.47
0.27
0.39
0.12
0.45
0.3
0.38
0.35
0.35
0.31
0.36
0.17
0.42
0.26
0.45
0.12
0.46
0.3
0.46
0.35
0.44
0.31
0.52
300
grp
ind
0.17
0.17
0.63
0.15
0.59
0.22
0.6
0.17
0.72
0.26
0.71
0.15
0.62
0.27
0.62
0.3
0.63
0.29
0.65
0.17
0.75
0.27
0.6
0.15
0.6
0.27
0.61
0.3
0.6
0.28
0.6
400
grp
ind
0.19
0.18
0.88
0.16
0.86
0.21
0.85
0.18
0.9
0.23
0.96
0.17
0.88
0.23
0.95
0.24
0.92
0.23
0.91
0.18
0.92
0.24
0.88
0.16
0.86
0.24
0.87
0.24
0.89
0.22
0.85
500
grp
ind
0.17
0.17
0.16
0.18
0.16
0.19
0.16
0.19
0.19
0.19
0.16
0.2
0.16
0.2
0.2
0.18
table
individual
and
group
fairness
results
for
the
chilesat
dataset
with
female
as
the
protected
group
the
plots
for
these
results
are
as
shown
in
figure
and
figure
the
column
grp
shows
the
proportion
of
the
protected
group
items
in
the
top-k
ranks
and
the
column
ind
shows
min
in
the
ranks
99
to
note
that
th
block
of
size
100
shown
in
the
plots
99
to
is
the
100
19