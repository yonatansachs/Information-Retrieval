A Study of the Relative Bias of Web Search
Engines Toward News Media Providers
by Ciaran Owens
submitted in part fulfilment of the requirements of
the Degree of Master in Science of Computing Science
at the University of Glasgow
April 24th 2009

Abstract
As search engines are used more and more to filter and process the ever growing
web they become the guardians of information and knowledge. Web users themselves have elevated the search engines to this role as the engines have eased their
use of the web. Thus search engines affect what information users access. With such
influence, a major concern is search engine bias, i.e. do search engines lead users to
particular sites over other sites unfairly? We address the issues of measuring and
reporting bias. We measured the relative news bias of 3 search engines. We did this
as users should be given the most relevant results, unaltered by any negative motivation on part of the designers and owners of search engines. We report the relative
bias amongst search engines in various forms such as political bias and predilection for specific sites. We showed these results to users to ascertain if knowledge
of biases affected a user’s viewpoint. To uncover these biases we performed an experiment over 9 weeks. In this experiment we posed a large number of realistic and
currently topical queries to the news section of the 3 search engines and stored the
resulting list of URLs. We had posited several hypotheses, which were questions
arising from a pilot study. We performed a survey using these results. From this
experiment we see that there are significant biases in all of the engines, but the bias
seems quite extreme for one particular engine. When we ran the survey, most users
rated one engine very highly but after seeing how biased it was they lowered their
opinion of it and raised their opinion of the other 2 engines, which were less clearly
biased. From the survey we can see that news bias is important to users. These
results are only a snapshot of 9 weeks of querying 3 search engines on 5 topics and
are not immediately generalisable. However, our work provides a methodology for
uncovering and displaying this bias. We propose some future work to expand on
different topics, news sources, engines, locations and display devices (e.g. PDAs)
and on methods to inform users of the known biases of search engines.

Contents
1 Introduction
1.1 Introduction . . . . . . . . . . . . . . .
1.2 Research Questions . . . . . . . . . . .
1.3 Contributions . . . . . . . . . . . . . .
1.4 Publications Arising From This Work .
1.5 Structure and Outline . . . . . . . . . .
2 Search Engine Bias
2.1 What is Bias? . . . . . . . . .
2.2 Problems with Defining Bias
2.3 Relative Bias . . . . . . . . .
2.4 Categories of Bias . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.

6
6
7
8
8
9

.
.
.
.

10
10
11
11
12

3 Background and Related Work
3.1 General Work in Media Bias . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Newspapers and the News . . . . . . . . . . . . . . . . . . . .
3.2 Detecting and Measuring Bias . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Accessibility of Information on the Web . . . . . . . . . . . . .
3.2.2 Computing Web Page Reputations . . . . . . . . . . . . . . . .
3.2.3 Bias on the Web . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.4 Search Engine Coverage Bias: Evidence and Possible Causes .
3.2.5 Bias and Controversy: Beyond the Statistical Deviation . . . .
3.2.6 Accessibility and Retrievability . . . . . . . . . . . . . . . . . .
3.3 PageRank Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 Impact of Search Engines on Page Popularity . . . . . . . . . .
3.3.2 Page Quality: In Search of an Unbiased Web Ranking . . . . .
3.3.3 The Case for Partially Randomized Ranking of Search Engine
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Summary of Related Work . . . . . . . . . . . . . . . . . . . . . . . . .

27
28

4 Empirical Study
4.1 Experimental Methodology . . . . . . . . . . . . .
4.2 Research Questions . . . . . . . . . . . . . . . . . .
4.2.1 Hypotheses Derived From the Pilot Study
4.2.2 Subjects of the News Bias Experiment . . .
4.2.3 Variables of the Experiment . . . . . . . . .
4.2.4 Behaviour of the Experiment . . . . . . . .
4.2.5 Statistical Tests Used in the Experiment . .

29
29
30
31
31
33
34
36

2

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

16
16
16
19
19
20
21
22
22
23
26
26
26

4.3

Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Experimental Results
5.1 Overview of the Top Result . . . . . . . . . . . . . . . . . . . .
5.2 Overview of the Top 20 Results by Gravity Measure . . . . . .
5.3 H1: Comparison of the Consistency of Results . . . . . . . . .
5.3.1 Explaining the Variance . . . . . . . . . . . . . . . . . .
5.4 H2: Search Engine Preferences of Their Own News Service . .
5.4.1 Explaining the Results of Self-Preference . . . . . . . .
5.5 H3: Less Results Over Time . . . . . . . . . . . . . . . . . . . .
5.5.1 Explaining the Matter of Less Results Over Time . . . .
5.6 H4: Differences in Results Over Time . . . . . . . . . . . . . . .
5.6.1 Explaining Differences in Results Over Time . . . . . .
5.7 H5: Predominance of USA Results Over UK Results . . . . . .
5.7.1 Explaining the UK vs. USA Results . . . . . . . . . . .
5.8 Exploration 1: Political Bias . . . . . . . . . . . . . . . . . . . .
5.8.1 Using www.skewz.com to Show Political Bias . . . . .
5.8.2 Problems with www.skewz.com . . . . . . . . . . . . .
5.8.3 The Results of Political Bias by Using www.skewz.com
5.8.4 Explaining Political Bias . . . . . . . . . . . . . . . . . .
5.9 Exploration 2: Topical Accuracy . . . . . . . . . . . . . . . . . .
5.9.1 Defining Topical Accuracy . . . . . . . . . . . . . . . . .
5.10 Bias Profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.10.1 Defining the Bias Profile of a Web Search Engine . . . .
5.10.2 Bias Profiles . . . . . . . . . . . . . . . . . . . . . . . . .

36

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

39
39
40
43
46
46
47
47
49
50
50
51
52
52
52
52
52
53
56
56
56
56
57

.
.
.
.
.
.

58
58
58
58
58
59
60

7 User Survey
7.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 H1: Engine A is the Preferred Engine . . . . . . . . . . . . . . . . . . .
7.3 H2: Knowledge of Bias Will Change User Perceptions . . . . . . . . .

61
61
62
62

8 Discussion and Conclusion
8.1 Questions Arising from the Study . . . . . . . . . .
8.2 Interesting Results . . . . . . . . . . . . . . . . . .
8.3 Limitations of the Study . . . . . . . . . . . . . . .
8.3.1 Practical Limitations . . . . . . . . . . . . .
8.3.2 Experimental Limitations . . . . . . . . . .
8.3.3 Scope and Context Limitations . . . . . . .
8.3.4 Limitations of the Generality of the Results
8.4 Future Work . . . . . . . . . . . . . . . . . . . . . .

64
64
64
64
65
65
65
66
66

6 Methodology of the User Survey
6.1 Derivation of the Survey from the Pilot Study . . .
6.1.1 Hypotheses of the Outcomes of the Survey
6.1.2 Subjects of the Survey . . . . . . . . . . . .
6.1.3 Variables of the Survey . . . . . . . . . . . .
6.1.4 Behaviour of the Survey . . . . . . . . . . .
6.1.5 Statistical Tests Used in the Survey . . . . .

3

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

8.5

Conclusion of the Contributions

. . . . . . . . . . . . . . . . . . . . .

Bibliography

66
68

A Pilot Study of News Bias
A.1 Experiments in Detecting and Measuring Relative Bias in News from
Web Search Engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.1.1 Overview of the Experiments . . . . . . . . . . . . . . . . . . .
A.1.2 The Method for Generating Appropriate News Queries . . . .
A.1.3 The Framework for the Experiments . . . . . . . . . . . . . . .
A.1.4 The Immediate Effect Experiment . . . . . . . . . . . . . . . .
A.1.5 The Short Delayed Effect Experiment . . . . . . . . . . . . . .
A.1.6 The Long Delayed Effect Experiment . . . . . . . . . . . . . .
A.2 A Survey Regarding the Effects of Known Relative Bias in News from
Web Search Engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 Tools Created for the Pilot Experiments . . . . . . . . . . . . . . . . .

74
75

B Table of Sources Appearing in the Results

77

C Tables of Results
C.1 Top 20: Engine A . . . . . .
C.1.1 Topic: Business . . .
C.1.2 Topic: Entertainment
C.1.3 Topic: Science . . . .
C.1.4 Topic: Sport . . . . .
C.1.5 Topic: World . . . . .
C.2 Top 20: Engine B . . . . . . .
C.2.1 Topic: Business . . .
C.2.2 Topic: Entertainment
C.2.3 Topic: Science . . . .
C.2.4 Topic: Sport . . . . .
C.2.5 Topic: World . . . . .
C.3 Top 20: Engine C . . . . . .
C.3.1 Topic: Business . . .
C.3.2 Topic: Entertainment
C.3.3 Topic: Science . . . .
C.3.4 Topic: Sport . . . . .
C.3.5 Topic: World . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

81
82
82
83
84
85
86
87
87
88
89
90
91
92
92
93
94
95
96

.
.
.
.
.
.
.
.

97
97
97
97
98
98
98
99
99

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

D Topical Accuracy Results
D.1 The Topical Accuracy of Engine A
D.1.1 Topic: Business . . . . . . .
D.1.2 Topic: Entertainment . . . .
D.1.3 Topic: Science . . . . . . . .
D.1.4 Topic: Sport . . . . . . . . .
D.1.5 Topic: World . . . . . . . . .
D.2 The Topical Accuracy of Engine B .
D.2.1 Topic: Business . . . . . . .
4

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

71
71
71
72
72
73
74
74

D.2.2 Topic: Entertainment . . . .
D.2.3 Topic: Science . . . . . . . .
D.2.4 Topic: Sport . . . . . . . . .
D.2.5 Topic: World . . . . . . . . .
D.3 The Topical Accuracy of Engine C
D.3.1 Topic: Business . . . . . . .
D.3.2 Topic: Entertainment . . . .
D.3.3 Topic: Science . . . . . . . .
D.3.4 Topic: Sport . . . . . . . . .
D.3.5 Topic: World . . . . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

99
99
100
100
100
100
101
101
101
102

E Survey Questionnaire

103

F Acknowledgements

107

5

Chapter 1
Introduction
1.1 Introduction
Search engines, such as Google, Cuil and AllTheWeb, are the primary means for
millions of users to access online content and as such, they influence much of the
information that is consumed. Search engines are used predominantly for focused
retrieval of a quite specific set of information elements per user session [10] i.e. a
user has a specific goal to achieve per session and to do so they use a search engine.
Additionally, according to Beauvisage [5] (second-hand translation from French to
English by Van Couvering [10]) search engine queries account for 1.3% of all web
traffic and are used in approximately 20% of all user sessions with a web browser.
The results from [28] show that direct search engine page requests and following
the links from these results make up 13.6% of the web traffic at the Department of
Computing Science at the University of California. From this same study, Qui, Lui
and Cho estimate that a web session starts directly from a search engine 6.8% of the
time. The amount of total traffic is small compared to the percentage of traffic that
is directly related to search engine results and the amount of user sessions that involve a search engine. This could imply that users take the top results at face value
and assume that they are relevant, or at least good enough. These facts could also
imply that each user uses only one or two search engines per session that invloves
searching. Given this evidence we can say that search engines play a vital part in
navigating the complexity of the web. Therefore if search engines do not provide
relevant content then the search experience of the millions of web users is negatively impacted.
A recognised problem is the inherent bias of search engines [13]. Firstly, search
engines try to discriminate relevant from non-relevant information given a user’s
query; this invariably leads to some documents being favoured over other documents. This is a necessary search bias to help the user. However, there are a number
of other biases that are possible and that may occur within a search engine retrieval
mechanism such that a long term bias exists, which means that documents from one
source are favoured over another source. For example, if a search engine only returned favourable results for a particular political party then this bias would have a
positive effect on that party that it may not deserve. As a search engine controls its
search index then it can direct users toward what it prefers and away from what it
6

does not like. It can moralise to hide away parts of the web that it disagrees with.
As another example, consider a search engine that dislike results from outside its
country of origin, In this case users would be at a disadvantage when searching for
anything but a narrow range of topics.
Since search engines are inherently biased (as demonstrated in the various pieces
of work discussed in Chapter 3), the objective of this study was not to determine
whether they are biased, but to show how much a search engine favours particular sites over other sites, compared with other search engines. Since search engines
have to supply results and in most cases assign each element in an order then web
search engines must be biased in some manner. Thus, the aim of our study was
to quantify the relative bias of search engines for different sites, and in particular
to compare the relative bias of search engines toward news media providers. This
study was performed using news sources due to the widely held belief that topics
such as worldwide news and technology news should be reported on by reputable
news outlets so that the method of this study should not have influenced the results.
News sites will report the news with their own bias and our method was used to
determine the scale of search engine bias toward and against news providers.
In this report, methods for measuring the relative bias between a collection of search
engines across a number of different news media sites is outlined. These results have
been used to discover if knowledge of the biases of a search engine will alter users‘
perceptions of the search engine.

1.2 Research Questions
In this project we have devised a methodology for measuring the relative bias between search engines across different news media providers (i.e. does a search retrieve relatively more news stories from one news media provider than another),
where we aimed to answer the following research questions:
• How can we measure bias in search engines? Section 3.2 discusses the existing
measurements of bias, some of which we use. Chapter 4 is where we select
and refine the bias measures.
• What is the relative bias towards the different news media providers across the
search engines? This question is discussed in Section 5.1, Section 5.2, Section
5.3, Section 5.4, Section 5.7 and Subsection 5.8.3.
• How does the relative bias change over time? This question is discussed in
Section 5.5 and Section 5.6.
• How does the relative bias change over topic? This question is discussed in
Section 5.2 and Section 5.9.
• Are some news media providers preferred over other media providers across
search engines? And not preferred? This question is discussed in Section 5.2.
7

• Does any of this actually matter to users? Or will they continue using their
favourite search engine despite its noted biases? This question is answered in
Chapter 7.
Simply put, we are trying to uncover bias in search engines and show it to users to
discover if it actually matters to them.

1.3 Contributions
The novel contributions of this project are:
• An experimental methodology for measuring the relative bias of search engines. See Chapter 4.
• Knowledge that different engines exhibit various degrees of bias on different
facets of the overall bias. One engine was systematically relatively more biased
overall than the other 2 engines.
• Confirmation that the gravity measure using the cost function with the β value
of 0.5 is a suitable measure as it correlates better with the other measures (c=1,
c=3, c=10, c=25, c=50) than these measures did with each other. See Section
4.3.
• The relative news biases amongst 3 search engines, which are the results of the
experiments outlined in Chapter 4. See Chapter 5 and Appendix C for these
results.
• A set of measures to profile the predilections of search engines. See Section
5.10.
• A user survey of knowledge of biases in search engines to discover if biases
actually matter to users. The method is in Chapter 6, the results are in Chapter
7 and the questionnaire is in Appendix E.
• As far as we can tell from the literature, a study of news bias in web search
engines has never been conducted on a scale similar or larger than this study
nor with the same scientific rigour.

1.4 Publications Arising From This Work
Publication submitted with Azzopardi, L., Search engine predilections towards news
media providers. To appear in the Proceedings of the 32nd Annual International
ACM Conference in Information Retrieval, July 2009, Boston, USA.

8

1.5 Structure and Outline
The structure of this report is as follows:
• Chapter 2 describes what exactly bias is, the different kinds of bias and why
bias is a problem in search engines.
• Chapter 3 explains the findings of work in the field of bias in search engines
over the categories of general work in news, detecting and measuring bias, the
effects of bias and how to overcome bias.
• Chapter 4 outlines the approach taken in this project, detailing the experiments
that were conducted and the research questions that were studied.
• Chapter 5 gives an overview of the results of the news bias experiment, a detailed discussion of the outcomes with respect to each hypothesis and two
exploratory studies of the bias results.
• Chapter 6 details a survey of users on the effects of bias in search engines.
• In Chapter 7 we discuss the outcomes of the survey that was conducted to ascertain whether knowledge of search engine bias would affect a user’s viewpoint.
• Finally the concluding remarks regarding this project are discussed in Chapter
8, including limitations and directions for future work.

9

Chapter 2
Search Engine Bias
In this chapter we explore the meaning of bias. In Section 2.1 we provide our strict
mathematical and unattainable definition of bias. We arrive at the definition we use
in the rest of this dissertation in Section 2.2. This definition is easier to work with.
To make this definition suitable without an unbiased statistical norm to compare
the results to, we describe how we have restricted the comparison to be relative
amongst our search engines in Section 2.3. Section 2.4 provides an overview of the
categories of bias.

2.1 What is Bias?
Bias in web search engines is the tendency to favour one set of results over another
possible set. This bias can be intentional or unintentional. Such bias can occur in a
large number of forms, which are detailed in Section 2.4. Bias may also be from the
user or from the search engine.
The most fundamental formulation of bias, in our view, is comparing the results
to the ‘ground truth’ of a perfectly unbiased system. Therefore we say that a web
search engine is unbiased (in the most fundamental sense) if for every formulated
query possible it returns the canonical objective injective correctly ordered sequence
of results. By this we mean that the simplest ranking of results must not have duplicates and must be in the correct order, according to the actual needs of all users.
Defined mathematically:
Where the set of all queries Q = {q0 , q1 , . . . , qn }
Where the result of a query qx is an injective sequence R = (r0 , r1 , . . . , rn ).
Where R is an ordered subset of all web documents W = {w0 , w1 , . . . , wn }.
F(qx ) 7→ R ∀qx ∈ Q

10

2.2 Problems with Defining Bias
Given the above characterisation, defining bias in this manner solves no problems,
as it shows all web search engines as biased as we cannot define the unbiased norm.
A gold standard to review the bias of search engines against cannot reasonably be
achieved without knowledge of the contents, and therefore usefulness, of all pages
in the web. Therefore for this project we propose that the relative bias amongst
search engines is studied instead of each engine’s biases versus the unbiased norm.
Defining biased versus unbiased web search engines as given in Section 2.1 is difficult to put into practice in the full web. There are problems with obtaining the
results for each and every query due to:
• The breadth of all possible queries. However each element from the set of all
correct languages phrases may not have a non-empty result set. There exist
a huge number of realistic posable queries which should have a result set of
web pages with some relevance with regards to a query.
• The objectiveness and perceived correctness of each result in the ordered sequence. Individuals may have differing viewpoints as to the correct elements
and their ordering in a result set. There may be a fully objectively correct order
of results for each posable query which returns a non-empty set of documents.
This is reliant on the point below.
• Finding all web documents. If every document on the web is not known by a
search engine then such a search engine cannot be claimed to be fully unbiased
as they cannot know how well this document relates to each posable query
simply beacuse the search engine does not know it exists. This knowledge of
how well a search engine covers the web is discussed in detail in [18].
Martin in [22] makes the observation that many social science researchers have
avoided the term ‘bias’ as bias is a controversial subject with various meanings and
interpretations. An alternate term that is suggested is ‘predilection’ that has the
same meaning. We use these terms interchangeably. We choose to use the words
bias and predilection freely with a very specific meaning given below:

Bias in web search engines is the tendency of a search engine to favour one site over another, as given by the frequency and rankingof these sites.
2.3 Relative Bias
The biases under study in this project are the relative biases between the search engines in the study. This is due to the problems associated with solely defining bias
against some norm, as given in Section 2.2. However using a relative bias can show
traits such as Engine A being 10 times more biased toward a certain political party

11

when compared to Engine B, which is 5 times more favoured than this same political party than Engine C.
As a more concrete example, assuming we have a measure that quantifies how likely
a site is to be retrieved by a given search engine, then assume Engine A gives the
BBC a score of 10 using this metric, Engine B gives the BBC a score of 5 and Engine C
gives the BBC a score of 2.5. From this we can say that, relatively, Engine A favours
the BBC 2 times as much as Engine B does, and 4 times as much as Engine C does.
Therefore for an example query, assuming that the maximum score in this metric
was 100, then 1 in 10 results from a query will be from the BBC when using Engine
A, 1 in 20 will be from the BBC when using Engine B and 1 in 40 will be from the
BBC when using Engine C. So we can say that Engine A is twice as likely to return
the BBC when compared to Engine B, which in turn is twice as likely to return the
BBCA
BBC when compared to Engine C. So we can have ratios such as: BBC
= 2 with
B
this clearly showing that the BBC site is twice as likely to be retrieved using Engine
A than it is with Engine B. A summary of this example is in Table 2.1.
Engine vs Engine
A vs B
B vs C
C vs A

Ratio
BBCA
BBCB
BBCB
BBCC
BBCC
BBCA

=
=
=

10
=2
5
5
=2
2.5
2.5
= 0.25
10

Chance of Retrieval for 1st Engine (%)
10%
5%
2.5%

Table 2.1: Example of relative bias using ratios
Relative bias also works amongst the results of a single search engine. As an example, assuming Engine A gives a score of 10 to the ABC, 5 to the BBC and 2.5 to
CNN, then for Engine A, the probability of the ABC news source appearing will be
2 times as likely as the BBC to appear, which in turn will be 2 times as likely as CNN
to be retrieved. Now we can combine both relative bias amongst search engines and
amongst the documents they return. Continuing from this example, assume Engine
B rates ABC = 5, BBC = 10, CNN = 2.5 and Engine C scores ABC = 2.5, BBC = 5,
CNN = 10. We provide a summary of this example in Table 2.2.
Engine
A
B
C

ABC
10
5
2.5

BBC
5
10
5

CNN
2.5
2.5
10

Table 2.2: Example of relative bias amongst engines and sources

2.4 Categories of Bias
There are many kinds of bias in existence. However only biases relevant to search
engines will be discussed. A summary of these biases and at what part in the search
hierarchy they can occur at is given in Figure 2.1.
1. Confirmation bias [19] occurs when a process to find a solution weighs evidence that supports an existing conclusion higher than evidence which negates
12

this conclusion. This could occur with either a user (such as the common case
[15], the ‘Googlearchy’, where a user will only search the web using the Google
search engine under the assumption that it is the only way to search the web)
or as the search engine process if it is weighing up evidence to return a set of
results.
2. Commercial bias [6, 12] is a situation such as a search engine using paid-for
placement strategies and advertising to increase revenue and make operating
a search engine a profitable venture. This places results higher in the ranking
based on monetary gain for the search engine and not on the actual quality of
the result. If users know that paid-for placement is being used then they may
feel that the results are not reliable and may bias their judgement of the search
engine and how or whether they should use it.
3. Media/News (and to an extent political) bias may occur in a search engine.
This kind of bias favours a specific view of the world over other viewpoints
that are not based on impartiality. These two types of bias may manifest themselves due to the search engine designers’ own affiliations or funding gained
from a particular media outlet or political organisation. This is the category of
bias that we are studying in this dissertation. This bias is an important issue
as discussed in Section 1.1 and we have created a method for measuring such
biases relatively between search engines in Section 4.
4. Inductive bias [21] may occur when a student (the search engine or user)
learns imperfectly from some teacher. Therefore the search engine may accidentally favour one kind of result over another. Inductive bias can also manifest itself when a user does not know how to use the engine correctly or how to
search the web effectively and possibly reinforces the ‘Googlearchy’ in their assumption that Google (or the only other search engine which they have used)
is the only way to access the web.
5. Sensationalist bias is the phenomenon of preferring an outlier result over
what the searcher actually desired. An example is searching for the word
‘earthquake’ in early June 2008 returned many results for the Great Sichuan
Earthquake which had happened in May 2008 over results which would be
typical.
6. Systematic bias is a bias embedded in the search engine. This is likely to be
true for all search engines but the degree of bias may be different between
them and the bias may be attributed to separate causes. For example [3] shows
that length normalisation is a common systematic bias in information retrieval
systems. PageRank is also a notable systematic bias that has caused many
problems in the web [8, 9, 27] and further discussed in Section 3.3.
7. Sampling bias [4, 2] where the statistical sample set is not truely representative of the whole. This can manifest itself in web search engines when a web
crawler does not cover the whole web, thus leaving some web pages inaccessible through the search engine.

13

8. Pre-existing cognitive and cultural biases [11] are biases that are influenced by
the culture and mindset of the search engine designers or the users.
9. A language bias [7] may exist, for instance due to the difficulty of expressing a query in a particular language or in relation to search engine designers
optimising their engines for one language or a small group of languages.
10. Security and anti-spam biases [12, 30], such as not indexing a web site which
is known or suspected of having a computer security threat, may exist in engines in an attempt to stop users from getting to the web page. This kind of
bias could be positive if it has a very high accuracy and does not discriminate
against legitimate web pages. However there are concerns over how search engines classify pages and domains as security threats and spam. There are further concerns about how webmasters can be delisted from security/spam lists
at a search engine. ‘Deceptive practices’ (e.g. inserting irrelevant keywords)
and ‘inconvenient practices’ (e.g. competition) also fall under this kind of bias
as the search engines will likely list security or anti-spam as the root reason of
being delisted, if any reason is given at all.
11. Web content bias is a bias of a search engine toward or against particular types
of content. For instance, most search engines are more able to index textual
data more thouroughly. The semantic contents of photographs, animations,
videos, audio, scripts, games etc are more difficult to extract and manipulate.
12. Personalisation bias [12, 13] occurs when search engines employ some method
of personalisation to suit a user. This may be based on previous uses that are
recorded as preferences, location or IP address range. Such a bias may be seen
as beneficial but this means that different users who issue the same query at
the same time may see different results.

14

Figure 2.1: Diagramatical representation of categories of bias

15

Chapter 3
Background and Related Work
This chapter is formatted as follows: Section 3.1 contains work relating to media
bias. We discuss such work to show that it is an existing problem in traditional
media outlets such as newspapers. This section gives a broad overview of news
and how people consume it. We also discuss how we can apply the lessons from
measuring bias in printed media to the web. Finally in this section we show how
our work sits at a meta-level compared to the work in this book. Section 3.2 contains
related work that pertains to the detection and subsequent measurement of bias. We
need to look to this work to find an appropriate measure to use in our experiments
in Chapter 4. We compare the measures on certain characteristics such as if they
work on a relative basis amongst search engines. Section 3.3 discusses work on
how bias affects users and the web. This is under dicsussion to show the very real,
quantifiable impact of search engine bias. In Section 3.4 we conclude the related
work, including the ideas, measures and methods that we have adopted from these
pieces of literature under survey.

3.1 General Work in Media Bias
3.1.1 Newspapers and the News
Kingsbury et al. [16] is a compliation of early work in the field of news media bias
by social scientists. The work gives examples of bias in printed news media and the
methods used to uncover and measure bias. Our objective is slightly different.
There have been complaints of bias within the domain of news publications since at
least 1799, with the most notable being from John Ward Fenno, editor of the Gazette
of the United States. He was remarked as saying:
The American newspapers are the most base, false, servile and venal
publications, that ever polluted the fountains of society - their editors the
most ignorant, mercenary, and vulgar automatons that ever were moved
by continually rusting wires of sordid mercantile avarice.
The authors use a tool called Spectrum of News Interests that can be used to discover
from what base article type a news story derives. The authors describe three basic
categories of news stories:
16

• Consumer interests that resonate with the personal interests of the reader.
• Socialised interests that relate to a community (whether large or small) that the
reader belongs to or is interested in.
• Sensationalist interests that are effectively thrilling stories for light reading.
According to this book there are four main styles of journalism:
• Sensationalists who ignore actual news.
• Special interest groups with selective topics.
• General newspapers with heavy biases.
• General newspapers with hastily put together news in an attempt to get news
out there on time so it may not have all the facts but the story is fair and honest.
Table 3.1 is a reproduction of a table from this book which details how much front
page headline space is devoted to specific ‘lines of interest’, which generally means
the kind of story the headline is about, for example business. This table is from
a representative study of American newspapers over a short period in 1921. The
Socialisation-Sensationalism Index (Social-vs-Sensation column) measures how much
a Line of Interest appears to be weighted toward socialised interests or sensationaslist
interests (a positive number being socialised and a negative number being sensational). The intent of socialised journalism is to give the public a service of current
knowledge in comparison to sensationalist tabloids which are merely for amusement. A more reasonable Socialisation-Sensationalism Index of a newspaper implies
that it has a wide and deep coverage of issues so is likely to be a good newspaper.

Mitigating Bias by Dispersal of Information
The authors are of the opinion that:
If research workers in journalism and in sociology will persistently calculate and publish the indexes of bias of leading newspapers, one of the
three results will ultimately be achieved: either the editors and owners of the papers guilty of the worst distortion will take the steps necessary to achieve higher standards, or they will cease to broadcast the
high-sounding assertions of integrity which now help to cover up their
perversions of the news, or else that minority of the public which keeps
informed on public questions will be able to identify with increasing certainty the oustanding hypocrites of journalism.
This could potentially be an avenue for reversing or negating bias naturally so that
the users of services can have an informed opinion of which service to use, to more
suit their viewpoints. In the time that this book was published, each newspaper had
a known slant, and news tended to reflect the opinions of the readers rather than

17

Social-vs-Sensation
41
39
35
34
29
27
26
26
22
15
9
4
-2
-6
-11
-12
-13
-28
-28
-33
-39
-46
-53

Lines of Interests
business
foreign problems
us-international
citizenship
prohibition(general)
intellectual
consumer, labour
foreign sensational
foreign aviation
personalities
domestic aviation
public crime and morals
horror-danger
rescue-hero
public violence
mystery
murder and misc crime
money sensational
horror-danger-sex
sob stories
weddings and society
sex
money-sex

Headline Space(%)
3
3
7
7
2
2
2
10
1
3
11
9
13
1
6
2
3
4
2
3
1
4
1

Table 3.1: Socialisation versus sensationalist index scores for 23 lines of interest,
reproduced from the insert in [16]

18

influence it. However in this age where search engines dominate the selection of information this may no longer be fact, the bias of the search engines is more likely to
be transferred to the user as they have less choice in what they view, especially with
the ‘Googlearchy’ phenomenon. If users were more informed of the bias of search
engines then perhaps their views on those search engines might change.
This paper has given insight into how newspaper bias affects readers. We are assuming an analogue in the web news domain. The ideas presented here show that
news media bias is a serious topic. The methods used in this book are measures
such as headline and column inches, which are applied manually and will not work
well in the web. These methods are not suitable in our case as we are not measuring
the bias of individual articles or overall news outlets, we are considering the bias
of search engines recommending which news media providers the users should be
viewing. Therefore this is a meta level bias, the bias is transmitted from an intermediary - the search engine. We have taken the idea that giving readers the knowledge
of biases may allow them to make better judgements. We performed a survey to
check if this is true, the results are in Chapter 7.

3.2 Detecting and Measuring Bias
There have been a number of proposed ways to measure search engine bias in the
last decade. Here we give an overview of the principal papers in this area in chronological order, including what restrictions and advantages each of their measures has.
For each paper, we introduce it, describe the method and any experiments (including a summary of the results) and describe how and why their work was useful to
us (or conversely why it was not useful). At the end of the section we summarise all
these measures (plus the measure from [16], discussed in Subsection 3.1.1) in Table
3.2.

3.2.1 Accessibility of Information on the Web
Lawrence and Giles [18] performed an experiment to discover the coverage of search
engine indexes of the web, how up to date the indexes were and how large the whole
web was. There were 6 engines used in the study as they were thought to have a
sufficient and similar coverage as indexing the same documents was needed in their
methodology. A query log from a scientific organisation was used as the basis for
comparing the coverage of the search engines. The queries that were chosen needed
to have returned the same set of pages for all 6 engines, with duplicate URLs or
pages removed. Additionally, each page was downloaded and only kept if it contained the query term.
The size of the indexable web, i.e. documents that are not behind search forms or are
excluded by RFC 1945, was esimated by the overlap of the 6 search engines results
by pairing engines and measuring their overlap. Their work shows that no single
search engine covers more than 57.5% of the estimated full web. Some large search
engines only cover less than 5% of the web, which is an alarming figure. However
19

this data is over a decade old. Their experiment gave a figure of 320 million documents as the most likely figure for the size of the web at the end of 1997. The
website www.worldwidewebsize.com, which uses a technique similar to Lawrence
and Giles, gives the number of documents in the web for 17/04/2009 as at least 23.1
billion.
An interesting result was that the engine with the best coverage had the largest percentage of its results as invalid links whereas the engine with the smallest coverage
had the lowest percentage of its results as invalid links.
The authors’ solution to the problem of search engines not indexing the whole web
are to use metasearch engines or to define goal-driven search engines that have a
specific focus e.g. sports or scientific literature.

3.2.2 Computing Web Page Reputations
The work by Rafiei and Mendelzon [29] provides the scientific community with an
analogue of page quality, namely a formalisation of a web page’s reputation on a
particular topic.
Finding the reputation of a page involves reversing the hubs and authorities associated with the HITS algorithm or to a lesser extent generalising the PageRank
algorithm.
Their experimental work provides for a case when there is not a web crawl and
doing so gives a good approximation of the true reputation score of a page on a particular topic using both the HITS style and PageRank style calculations.
The authors do admit some drawbacks of their model that were discovered during
their experiments. The authors mention the problems associated with a page that is
linked to by many pages with a very wide ranging set of topics so that the reputation
of the page being linked to cannot be discerned adequately. The other extreme of
this problem is that a page with only a few links, despite being of high quality, will
not receive the reputation it is deserving of because all incoming links are weighted
equally. There are also probable confirmation biases in agreeing on the authorities
in the HITS algorithm.
Using this page quality measure it may be possible to discover if there is a correlation between a page’s reputation (potential quality) and page’s rank in a search
engine (popularity derived from the PageRank or HITS (Hyperlink-Induced Topic
Search) metric). We suggest that if there is a discrepancy then there may be a case
for calculating the distance between the reputation and the rank and using this as a
bias measure. We do not use this measure, but it is an insight that may be useful for
other researchers in this area.
From this work we implemented the TOPIC program to define the topical accuracy
of the search engines under study. Topical accuracy is a major component of the bias
20

profile defined in Section 5.10. We have used TOPIC despite our concerns noted
about its utility due to the constantly changing nature of news. We discovered that
topical accuracy for news sites is problematic, as shown in Section 5.9 and we would
not recommend this measure to other researchers in news bias.

3.2.3 Bias on the Web
The study by Mowshowitz and Kawaguchi [23] was undertaken to discover if there
was any detectable bias in major search engines. Their results using their own bias
measurements confirmed their hypothesis that there was some bias in all search engines.
The bias quantification that the authors proposed involved the number of unique
domains as a ranked array based on the combination of all web search results. This
was their statistical norm. However their bias measurement was not based on all
possible results from the web but only the combination of the web pages returned
for every search engine used in the study. This is an attempt to create a perfectly
unbiased ranking for each query as discussed in Section 2.1. However, this measure
itself could have concievably introduced bias into the experiment as the authors
were assuming that all of the search engines’ combined web coverage was sufficient
to model the set of all possible results. This measure disregarded poorly accessible
but potentially high quality resources.
Fifteen commercial search engines were included in the study, which used the ACM
Computing Classification System for query terms and the top 30 results for each
engine were recorded. The results are compared against the compound array for
each search engine to obtain their own array. Then the bias magnitude is computed
as 1 minus the dot product of the statistical norm array and the search engine array
divided by the square root of their lengths. This formula is as follows using Google
as an example:
Resultsall ⊙ResultsGoogle
bias = 1 − √

|Resultsall |x|ResultsGoogle|

Their measure cannot show if there is a bias against particular results if all of the included search engines are biased against the same results. In an attempt to counter
this the experiments were performed over a large number of search engines. There
was a small set of queries that were used and the results from this compounded
with the aforementioned flaw in the bias measurement may not be enough to prove
or quantify the bias in these search engines.
Additionally, the bias measure is only the distance from their supposedly unbiased
norm but it does not provide any indication of any form of direction of the bias
other than away from the norm. It is possible that using their measure a search engine may be categorised as being biased if it provided high quality pages which only
appeared a small number of times in the set of all returned pages. Such a potentially
misleading measure must be avoided in future work on bias in web search engines.

21

As of now there is no suitable ground truth that we can compare the results of search
engines to as their methodology is flawed due to accessibility issues such as those
raised in [2]. We therefore have to concern ourselves only with the relative bias
amongst search engines. This paper highlights the issue that we will also have to run
any experiments over a long term to discover any useful bias, as their experiment
was a short pilot. The query set used was suitable for their purpose and we have
created a method for obtaining useful and realistic news queries, see Subsection
4.2.4.

3.2.4 Search Engine Coverage Bias: Evidence and Possible Causes
Vaughan and Thelwall performed a study on the coverage of documents from 42
countries to discover the index bias of 3 search engines. Their measure was the site
coverage ratio to detect the amount of bias. This was computed as the number of
pages covered by the search engine divided by the number of pages covered by
their research crawler. To estimate the site coverage ratio, they crawled domains in
42 countries. For each domain crawled, they submitted queries to each search engine that retrieved all the documents from each of the domains, if the domain was
indexed.
While the method is quite useful for discovering an index bias it is subject to manipulation since search engines may report that they have indexed more pages than
they actually have. However they are under the assumption that their crawler indexes every page on the web, which is unlikely. Additionally it is subject to the
constantly changing nature of the web as their crawl may lag behind the indexes of
search engines as researcher do not have quite the same resources available as major
corporations such as large search engines.

3.2.5 Bias and Controversy: Beyond the Statistical Deviation
Lauw et al. [17] provide several measures of discovering and measuring bias using a
generic reviewer-object model. In the case of web searching, the reviewers can be regarded as web search engines and the objects that they are reviewing are web pages.
In their model a reviewer ri gives an object oj a score eij in [0,1] (in our case for a
query qk ). For each eij there is a dij which is the deviation from the consensus of the
score of oj which also has a controversy value cj .
Their work supports the probable case that bias can arise in both a reviewer (the
search engine) or in the controversy (the subjectiveness of a quality measure). The
proposed measures of deviation (and some measure of bias in a vein similar to Subsection 3.2.3 as the returned collection is their statistical norm) by the authors is the
deviation from co-reviewers using overlap, Kendall or Spearman measurements.
The problem with this solution is defining the controversy value accurately to then
measure the bias.

22

This paper details the use of a reviewer of a set of objects. A reviewer would have
to be a human who selects how biased a search engine is for or against a particular
news source. This is too resource intensive for our project. It would also introduce
potential errors from the subjectivity of the reviewers. To mitigate such errors would
require a large number of reviewers, again, something that this project cannot afford.
We have taken none of the measures or methods from this paper to use in our work
but it serves to detail that bias can be from search engines or the topic under study,
which is why we have chosen to observe the bias of search engines over a range of
topics.

3.2.6 Accessibility and Retrievability
The accessibility paper [2] introduces the concept of accessibility in information retrieval systems, which is an idea ported from transportation planning. In the context
of transportation planning, accessibility is the idea that opportunities for a particular activity (e.g. shopping at a supermarket) is subject to some cost function related
to the distance and opportunites to travel there. In the context of information retrieval, accessibility is the idea that searching for information is subject to some cost
function related to how easy it is for a user to ask the system what they want and
how the system displays the results to the user.
Core to this paper, beyond the introduction of the concept, is the application of the
two primary measures used in transport planning: cumulative opportunity measures and gravity based measures. Cumulative measures give a count of opportunities in an area given the distance a user is willing to travel. In information retrieval
terms this can be related to how far down a ranked list a user is willing to go before stopping looking. Gravity based measures include some form of cost function,
generally a negative exponential function. Gravity based measures usually take in
to account the non-linear effort it takes to go further. We adopt these measures for
reporting bias, particularly the cost function in the gravity measure.
The authors give the results of an experiment to ascertain whether accessibility
can show bias in an information retrieval system. The experiment consisted of the
AQUAINT TREC collection with the Associated Press, New York Times and Xinhua
News documents from 1996 to 2000. The systems under study were three standard retrieval models in the Lemur information retrieval toolkit with the default
settings. Their results showed that different models favoured different features of
documents, e.g. that the TDIDF model favoured longer documents.
From this paper we have adopted their adapted measures of accessibility in information retrieval systems. See Subsection 4.2.4.
Continuing on from the theme of accessibility, the retrievability paper [3] examines
the concept of document retrievability i.e. how easy an information retrieval system
can return a document. The authors propose a measure which could be used for
search engine bias as their work demonstrates the use of the measure in a lab study
on newspaper articles and web documents. Their main conclusions were:
23

• Current retrieval mechanisms exhibit such a retrievability bias, which cannot
be well measured using traditional relevance evaluation techniques.
• The least retrievable documents are more difficult to find in a system.
• Whether this is a problem depends on the bias of the system.
The retrievability of a document d in the document collection C should be high
if there are many queries, q, in the universe of queries, Q, that relate to d. Their
measure of retrievability is the following formula:
r(d) =

X

oq  f (kdq , c)

qεQ

Where f (kdq , c) is the utility versus cost function of rank d when q is applied to a
system, and c is the cut-off rank that the user is willing to read to. The retrievability
of a document is a summed score of all the times the document can be retrieved
within c in all the queries Q. This r(d) measure is an intuitive way to show the relative bias between both: sets of information retrieval systems and sets of documents.
For example, if system A returns rA (d1 ) = 10 and system B returns rB (d1 ) = 1 then
relatively, system A will retrieve document d1 10 times more than system B. Alternatively if for system A rA (d1 ) = 10 and rA (d2 ) = 1 then system A favours and
retrieves d1 10 times more than it does d2 . These can be written as ratios such that
rA (d1 )
= 10 which gives a more concise view of the relation of system A to system B
rB (d1 )
with respect to document d1 .
The authors use an approximation of Q due to the large size of the set of all queries
and difficulty in generating them, so that r(d) was also an approximation. Their
experiments used the .GOV (a set of USA government data) and AQUAINT TREC
(the same newspaper article sets from the accessibility paper) collections and four
standard retrieval models from the Lemur toolkit as per the accesibility paper. Thus
these are not web search engines and they are not using real news web sites. However the measure looks promising for use in a real setting.
We have adapted the r(document) measure from this paper to an r(website) measure to account for the bias of search engines toward or against specific news sources
and not just a single page. We formulate this measure as:
r(w) =

X

r(d)

dεW

A summary of the measures for measuring bias is given in Table 3.2. ‘Ground truth?’
means - does this measure require a ground truth style set of results (see in Section
2.1)? ‘Relative to other systems?’ means - can this measure be compared relatively
to other systems i.e. can we compare a set of search engines side-by-side for their
biases? ‘Relative to other documents?’ means - can the bias toward a specific document be compared to another document within one system? ‘Query set?’ means does this measure require some queries to run to obtain useful scores? ‘Used in our
24

Year
1937
1999
2000
2002
2004
2006
2008a
2008b

Measure
Socialisation-Sensationalisation
Index Coverage
TOPIC
Norm Divergence
Index Bias
Controversy
Accessibility
Retrievability

Ground truth?
No
No
No
Yes
No
No
No
No

Relative to other systems?
N/A
Yes
Yes
No
Yes
Yes
Yes
Yes

Relative to other documents?
Yes
No
Yes
No
No
Yes
Yes
Yes

Query set?
No
Yes
No
Yes
Yes
No
Yes
Yes

Used in our work?
No
No
Yes
No
No
No
Yes
Yes

work’ means - is this measure used at any point in our work?

Table 3.2: Summary of measures of bias in the related work

25

Author
Kingsbury et al.
Lawrence, Giles
Rafiei, Mendelzon
Mowshowitz, Kawaguchi
Vaughan, Thelwall
Lauw et al.
Azzopardi, Vinay
Azzopardi, Vinay

3.3 PageRank Bias
3.3.1 Impact of Search Engines on Page Popularity
The Cho and Roy experiments [8] from 2004 uncovered a bias in the PageRank algorithm [26] used by many search engines [31]. Since the PageRank algorithm equates
a page’s popularity with the page’s quality it leaves out high quality pages which
are not popular perhaps because these pages are new or just not well respected in
the web community.
Cho and Roy have shown through a 7 month study that the use of the PageRank
algorithm is a cause of the ‘rich get richer’ phenomenon on the web as it has been
discovered previously that users generally are only interested in the top few results
of a web search [20] so these results will be used more often than lower ranked results which makes sense from both a user and search engine point of view. However
the negative effect of this is that a very small amount of probably high quality results
are used when there are possibly a larger amount of higher quality results available.
The use of PageRank thus helps ensure that the ‘rich get richer’ while the ‘poor get
poorer’.
Due to the limited availability of resources in their work they freely admit that their
study was conducted over a small section of the web and therefore may not be totally representative of the wider web.
Their study confirmed their hypothesis that popular pages get even more popular
which negatively impacts on the popularity of lower ranked results since popularity is normalised due to there only being a static amount of popularity available in
the web community. Their work used a random-surfer model versus a search engine user model to show that due to the PageRank algorithm a newly created page
under the search engine user model takes 66 times longer than the random-surfer
model to obtain its rightful rank due only to the quality of the page.
This work shows that PageRank is a large cause of bias in the web, however we
do not use any of their work in our work as our study is not focussed specifically
on the causes of bias but rather on how much bias exists relatively between search
engines. This paper highlights the severity of bias in web search engines and why it
is a problem.

3.3.2 Page Quality: In Search of an Unbiased Web Ranking
The Cho et al. paper [9] builds on the Cho and Roy paper from 2004 [8] that discovered the bias effect of the PageRank algorithm on web search results. In this study
the authors proposed an alternative ranking algorithm to PageRank which takes
into account the probable intention of the original PageRank algorithm. This could
be viewed as a solution to the bias in web search engines.
This new algorithm uses a formal definition of page quality which is the conditional
26

probability that any user will like the page when the user first finds the page. This
improvement over the reasonable assumptions of PageRank (that creating a link is
an indication that a user likes a page and that a page of high quality will be liked in
such a manner by users who visit the page) negates the ‘rich get richer’ phenomenon
by taking into account the likely future popularity and by association the quality of
a page based on the page’s popularity change over time. Therefore once every user
visits the page its popularity should match its actual quality.
The problem with this metric is convincing search engines to re-implement their
PageRank algorithms as a more correct predictive PageRank. However any search
engine that does so will have results which will more accurately reflect page quality
and therefore users may be more inclined to use that search engine, thus increasing
its market share. In relation to this we have untertaken a study amongst a group
of search engine users to discover if knowledge of bias altered their perceptions of
such search engines. See Chapter 7 for the results of this survey.

3.3.3 The Case for Partially Randomized Ranking of Search Engine Results
In this paper Pandey et al. [27] discussed the effect of promoting some of the lower
ranked results of a search engine which uses a PageRank style algorithm to counter
bias. This is an effort to remove the ‘rich get richer’ phenomenon by studying the effect of altering the ranking returned by a search engine. If a result is of high quality
then this method will give it a chance to be visited and therefore a chance to increase
its popularity (and therefore rank) naturally.
Their work required the manual creation and use of thousands of web pages by
nearly a thousand users over a period of more than six weeks. Their results showed
a significant increase in perceived quality by their test users.
This solution could be implemented as a filter over an existing search engine to
promote the returned results in the manner specified in the research. The problem
with this solution is the scale of it would mean that pages would only achieve their
rightful rank once enough web users had discovered the page, randomly boosting
lower ranked result for a subset of the users will likely change the ranking of the
page to the correct level but this will take a significant amount of time. However it
will probably be less than the 66 fold time increase which was proven to exist in the
PageRank algorithm by Cho and Roy [8], discussed in Subsection 3.3.1.
We cannot use any of their results in our work but this seems to be one of a few
measures that have been designed in an attempt to counter bias. However all of
these measures require the search engines themselves to implement the changes.

27

3.4 Summary of Related Work
From ‘Newspapers and the News’ we have taken the idea of giving users the knowledge of the biases of their news filters (newspapers and magazines in their case and
search engines in our case). This book also illustrates that news media bias is not a
new subject and at least dates back to 1799. It also gives a very thorough view of the
news process. We feel that the measures in ‘Bias on the Web’ are not suitable as we
do not agree with their assumption that the combination of a set of search engines
will provide a fair set of ground truth results as doing so ignores the problem of
accessibility described in [2]. From the ‘Retrievability’ paper we have adapted the
measure of retrievability, r(d), to cover a domain and not just a single document.
‘Impact of Search Engines on Page Popularity’ details the severity of bias in search
engines and the follow-up paper ‘Page Quality’ provides a solution to this specific
bias, however implementing this solution is up to individual search engines. Another solution, detailed in ‘Shuffling a Stacked Deck’ has the same problem in that it
requires individual search engines to implement the solution. The Cho et al. paper
[9] discussed in Subsection 3.3.2 gave a page quality measure which was designed
to supercede PageRank as it was based on the growth of a page’s popularity rather
than the strict correspondence of in-links to a page as a measure of popularity. Unfortunately due to the timescale and the ephemeral aspect of news this measure is
not suitable for providing a page quality measure that could be used to show the
page quality versus the search engine results for news results.

28

Chapter 4
Empirical Study
In this chapter we detail the methodology of the experiments. Many of these hypotheses are continuations of work carried out in the pilot study in Appendix A.
Overall, the methodology consisted of collecting realistic news-based queries and
submitting these queries to 3 search engines. The results were then stored for later
analysis.
Many people are eschewing the traditional daily newspaper and are consuming
news using the web instead. A comparison of the year-on-year circulation change
for 2008 on 10 UK national newspapers [24] showed a drop of slightly over 5% overall. For the USA [25] a similar trend occured with a year-on-year drop of 4.7% overall. An article by The Guardian newspaper [14] shows that The Sun’s website has
seen a 118% year-on-year increase in unique visitors, while The Telegraph’s website’s number of unique visitors was up by 112% year-on-year and the reporting
newspaper gained a 30% increase in unique web traffic over a year. Clearly people
are reading physical copies of newspapers less and relying more on news websites.
It is also interesting to note that the overall increase in web traffic to news sites is
much larger than the drop in newspaper sales suggesting that people supplement
their physical copy with the website and that potentially people who do not read
newspapers are beginning to read news websites.

4.1 Experimental Methodology
This project is tackling news bias in web search engines and so requires an experimental set-up which can distinguish the relative news bias amongst web search
engines. We had elected to use the titles from news items from a variety of news
sources as the queries. This has not been done before in the literature and finding appropriate queries is a difficult problem in this area, but this has solved it in
this case. See Section A.3 for a discussion of the tools created for the pilot study,
which were also used in this experiment. The biases under study are relative biases
amongst the search engines, as given in Section 2.3. We use the following formula,
which is adapted from [2]:
XX
r(w) =
f(d, q)
dεW qεQ

29

We use 5 different cut-offs for the cumulative measure and one exponential based
function for the gravity measures, as described in [2]. The cumulative measure
means that there is a sum of all the times a dεW appears before the rank cut-off
= c ∀qεQ. The cumulative cut-off used were: 1, 3, 10, 25, 50. For the gravity measure
we set function f as:
f(cdq , β) =

1
(cdq )β

and used a β value of 0.5.
The facets analysed are:
• Political bias with a simple left-right leaning.
• Relative predilection for specific sources, in relation to news topics.
• How the results vary from query to query.
• How the results vary from time to time.
• The search engine’s preference for its own news service.
• The search engine’s preference for UK-based or USA-based results.
• How a search engine’s results reflect their relation to the news topic under
study.

4.2 Research Questions
Below are the research questions from Section 1.2.
• How can we measure bias in search engines? Section 3.2 discusses the existing
measurements of bias, some of which we use. Chapter 4 is where we select
and refine the bias measures.
• What is the relative bias towards the different news media providers across the
search engines? This question is discussed in Section 5.1, Section 5.2, Section
5.3, Section 5.4, Section 5.7 and Subsection 5.8.3.
• How does the relative bias change over time? This question is discussed in
Section 5.5 and Section 5.6.
• How does the relative bias change over topic? This question is discussed in
Section 5.2 and Section 5.9.
• Are some news media providers preferred over other media providers across
search engines? And not preferred? This question is discussed in Section 5.2.
• Does any of this actually matter to users? Are users concerned? This question
is answered in Chapter 7.
30

4.2.1 Hypotheses Derived From the Pilot Study
H1: Engine A consistently returns the same news results when compared to the variation in engines B and C. This is a two-tailed hypothesis as the measure is checking
the difference between two subjects, one at a time.
H2: Engine A consistently prefers its own news service when compared to engine
B’s preference for its own news source and approximately equally for engine C’s
preference for its own news service. This is a one-tailed hypothesis as two subjects
at a time are being measured for overall counts of an occurrence.
H3: When the same news stories are searched for after a significant period of time
there will be less results returned for all engines. This is a one-tailed hypothesis as
it only needs to find out if one number is larger than the other.
H4: When the same news stories are searched for a day later significant differences
in the returned results will be evident for all search engines. This is a two-tailed
hypothesis as the check is on whether two result sets diverge significantly.
H5: The international version of each search engine will favour USA based news
sources.
The reasoning behind H1 is that from the pilot study in Appendix A engine A is
shown as returning results consistently whereas engines B and C are viewed as less
consistent. This will have implications for bias toward certain sources which will
have a negative effect on genuinely informative news stories from a non-preferred
service. This effect is shown by Cho and Roy in [8]. The reasoning behind H2 is that
from the pilot study we notice that engine A rates its own news service highly, which
may be removing good articles from non-preferred sources. On the other hand engines B and C do not seem to have a significant preference for their respective own
news service. H3 is based on news being ephemeral in society yet it still should be
indexed for retrieval later by search engines. H4 is based on the thought that as new
facts emerge about news then additional stories are written which will be of higher
quality. H5 is based on the belief that the web is US-centric in the english speaking
world as there are more web pages based there.

4.2.2 Subjects of the News Bias Experiment
Subjects: The titles from a large variety of UK-based and USA-based news sources
were used to approximate a user searching for a news story. These sequences of
queries were used as the samples. For each topic there was 22 news sources - 11 UKbased and 11 USA-based. Each topic had different news sources as some sources did
not have an RSS feed for every topic. During the first week of the experiment it was
observed that 3 of every 4 requests to engine A were being refused, as were a minority of the requests to engines B and C. As it was the case that the refusals were
uneven between search engines, the number of news sources was cut to 3 UK and 3
USA. The particular news sources were changed weekly to allieviate bias from the
31

selection of sources, the original point in chosing 22 sources. The refusals from engine A came in batches so that every 3 of 4 files were empty, therefore to even up the
amount of data collected between all search engines, any file that matched the time
of a zero length file (and the zero length file itself) was deleted. After university
started back up from the winter holidays, there were even more connection refusals
so it was decided that the set of sources would be further reduced by 1 source from
each UK and USA set. The following news sources were used in the experiment.
The 17 UK news sources that were used:
• The Guardian (Centre-left UK Berliner-form newspaper - www.guardian.co.uk)
• The Independent (Centre-left UK compact-form newspaper www.independent.co.uk)
• Sky News (Right-wing UK based satellite TV service - news.sky.com)
• The BBC (Government funded editorially independent UK TV service news.bbc.co.uk)
• Reuters (International but UK founded news service - uk.reuters.com)
• The Telegraph (Centre-right UK broadsheet newspaper - www.telegraph.co.uk)
• The Times (Centre-right UK compact-form newspaper - www.timesonline.co.uk)
• The Sun (Right-wing UK tabloid newspaper - www.thesun.co.uk)
• Daily Record (Centrist UK tabloid newspaper - www.dailyrecord.co.uk)
• The Metro (UK compact-form newspaper distributed on public transport www.metro.co.uk)
• The Daily Mirror (Centre-left UK tabloid newspaper - www.mirror.co.uk)
• The Herald (Centrist Scottish-based broadsheet newspaper - www.theherald.co.uk)
• The Daily Mail (Right-wing UK tabloid - www.dailymail.co.uk)
• The Scotsman (Centre-right UK compact-form newspaper - www.scotsman.com)
• The Belfast Telegraph (Centre-right Northern Irish compact-form and
broadsheet newspaper - www.belfasttelegraph.co.uk)
• New Scientist (UK based science magazine - www.newscientist.com)
• Digital Spy (UK entertainment news website - www.digitalspy.co.uk)
The 17 US news sources that were used:
• ABC (Centrist US TV network - abc.go.com)
• CNN (Centre-right US TV network - www.cnn.com)
32

• Fox News (Right-wing US-based satellite TV news service - www.foxnews.com)
• The New York Times (Centre-left US broadsheet newspaper - www.nyt.com)
• The Washington Post (Centre-left US broadsheet newspaper www.washingtonpost.com)
• The Los Angeles Times (Liberal US broadsheet newspaper - www.latimes.com)
• The Boston Globe (Centre-left US broadhseet newspaper www.boston.com/bostonglobe)
• The Seattle Times (Centre-right US broadsheet newspaper seattletimes.nwsource.com)
• The Philly Inquirer (Liberal US tabloid newspaper - www.philly.com/inquirer)
• The New York Daily (Centrist US tabloid newspaper - www.nydailynews.com)
• The Financial Times (US financial broadsheet newspaper - www.ft.com)
• Detroit News (Conservative US broadsheet newspaper - www.detnews.com)
• The Seattle Post-Intelligencer (Centrist US broadsheet newspaper seattlepi.nwsource.com)
• TechWeb (US science and technology website - www.techweb.com)
• The Star Tribune (Centrist US broadsheet newspaper - www.startribune.com)
• St. Petersburg Times (Centre-left US broadsheet newspaper www.tampabay.com)
• Journal Sentinel (Centrist US broadsheet newspaper - www.jsonline.com)
These appellations of political motivation were taken solely from opinion but we
believed there is enough of a mix of affiliations and viewpoints that the sample is
representative of all mainstream and minority viewpoints.

4.2.3 Variables of the Experiment
Independent Variables: The set of search engines denoted by A, B and C was the independent variable. The secondary independent variable, time, was only relevant to
hypotheses 3 and 4 and it did not interfere with the other independent variable.
Dependant Variables: The result set generated from each query was used as the dependant variable as we hypothesised that the results should vary between search
engines.
Extraneous Variables: A narrow selection of news stories or biases from the news
sources would have influenced the outcome of the experiment. Inheriting such a statistical bias from the sample was mitigated by using a large number of news sources
33

which provided a broad spectrum of perceived bias that cancelled out the overall
bias. Major news stories may have skewed results by overshadowing smaller news
stories. This is sensationalist bias, which is detailed in Section 2.4. This could not
reasonably be alleviated and should not have been anyway be as it is part of the
news information process.

4.2.4 Behaviour of the Experiment
The queries were taken from a large and varied set of news sources. They were
constructed from the titles from RSS feeds that were checked and stored every 3 hr.
These were conceivable queries that reflected the terms that would be submitted to
a search engine when looking for a specific news story. There is further reasoning
behind just using titles in this manner as given in [16]. News story titles are easy
and consistentent to use. Additionally, headlines are used to attract attention and
are therefore representative of the story content. The titles were stripped of punctuation, set to lowercase and the words were put in to alphabetical order. These alterations from the title to a query were done to alleviate any bias in selecting queries
that matched exactly an existing news item title from only one news source. Stop
words were not removed as these may have been dependant on the search engine
used and retaining stop words in the query allowed the query to continue to sound
as if it was posed by a human, albeit in alphabetical order. The list of queries was
treated as an injective sequence i.e. exact duplicate queries were not added again
and the queries were performed in the order in which they were added. As each
query was stored it was submitted to all three search engines, negating any time
lapse or order bias. The domain and the rank of each result from a query were
stored for analysis. Thus the relation of a news story to the news sources a search
engine directs a user to was shown. The international version of each search engine
was used. We predicted through H1 that this behaviour was constant for search engine A when engines B and C are used as a comparison of the variability of news
sources. After the experiment had run for 168 hr (1 week, 56 iterations) the second
phase was supposed to be started. This involved resubmitting the same queries
that were generated exactly one week prior. However, due to the large number of
connection refusals, it was decided that the first 2 experiments should run simultaneously and that the last 2 weeks of the experiment would be run again with a 20
day delay to discover how a large time gap affects the results returned from search
engines. This was to show how the results varied over time. Network maintenance
and an accidental overwrite of the data cancelled this entire section of the experiment.This was to be repeated over the course of 9 weeks to gain sufficient data to
provide a reliable outcome but as stated, was reduced to 2 weeks and has no outcome.
However this experiment could not distinguish if a subject is learning through repeated querying of different news titles as a search engine may have then biased its
results. This could have been mitigated by distributing the query over many machines but this was not feasible due to resource limitations. It is also an unlikely
effect and if it existed it was part of the bias inherent in the search engines (which is
34

the effect under study) and not within the experiment.
The dependant variable that is being measured is the result set (including rank) obtained from each search engine when each query is posed to them. This variable
was also measured 24 hr later.
The results of all hypotheses of this experiment and two exploratory studies are
given in Chapter 5.
To ascertain whether H1 stood, these measurements were checked to discover if engine A’s results varied significantly less than engines B and C’s results.
H2 was checked by summing up the occurrences of each engine’s news service in
their respective results. The main measures of bias used in these experiments were
the cumulative and gravity measures used in the pilot study in Appendix A. The
cumulative score is a positive integer given to each provider which is a sum of the
number of times this provider appears as a result from a query to a search engine
within a specified rank cut-off. The cumulative scores were recorded for a cut-off of
1, 3, 10, 25 and 50.
The gravity score used a β (dampening factor) value of 0.5. The gravity measure
was chosen as it accurately reflects the actual user experience - that the users value
the top most results with greater weighting [20]. Additionally we learned that this
score correlated the best with the other scores compared to their correlations with
each other and the gravity score in Section 4.3. This score was essentially 1 divided
by the square root rank of a result from a query. The result of these measures was
that for the news providers that the search engine will supply as a result were given
a series of numbers that reflected how much the search engine favoured the source
on the current subject with reference and being relative to all other news sources
that were listed by the search engine.
H3 was to be checked by the seven day delay in re-running the experiment with the
same queries. As this part of the experiment failed we used the 24 hr delay results
compared to the no delay results. The total number of returned results for all queries
was compared for both runs of the experiment.
H4 was checked by comparing the results of the immediate submission of the queries
that were collected every 3 hr and by the results of the 24 hr delayed submission of
all queries taken in the prior 24 hr period. If they were significantly different by
having far more results in the delayed submission this may imply that a form of
sampling bias had occured in that the speed of indexing of fresh pages by the search
engine meant that many news sites were not taken in to consideration by the search
engine in the immediate submission of results.
To check H5 required a simple measure of the number of UK-based sites versus
the number of USA-based sites that were returned in the results. This summation
took account of the ranking as an engine could have an equal number of USA and
35

UK results but the UK results could have been at the bottom of the ranking. This
measure had to assume an equal page quality between the UK and the USA.

4.2.5 Statistical Tests Used in the Experiment
From a quick check there was approximately an average of 15 news titles per RSS
feed and for the 22 sources this gave a sample size of 2,640 queries per day per topic,
of which there were 5. This was a total of about 92,000 queries per week. Given the
probable small differences between the results of each engine for each query, a large
number of queries was required, which meant running the experiment over several
weeks. In reality only 6 of the 22 sources were used so there were about 25,000
queries per week. We had chosen 9 weeks as a practical limit given the amount of
time which we had to finish the project, so we estimated that there were 225,000
queries in total. The actual number of queries was 592,623, however this was inflated due to the first 2 weeks.
The statistical test for H1 measured the variance between the ranked results of each
search engine. The measure that was used for this was the Kolmogorov-Smirnov
test which is a non-parametric test.
To validate H2 we checked the statistical significance of the difference in the averages of scores for each search engine for the cumulative and gravity scores using
ANOVA at each score point. The average over the average of distinct domains was
also checked for each engine-score point combination using ANOVA.
If the differences in the sum of returned results for all three search engines was
statistically significantly less after one day then there would be some evidence supporting H3.
The statistical test for H4 also used the Kolmogorov-Smirnov tests in the test for H1,
as the variance between ranked results was the desired measurement.
Each search engine has a score for the USA and the UK-based on the rank and number of results from each country. Thus this number was compared between the three
search engines using a simple statistical significance test to discover if the data supports H5. A one-way ANOVA test over the three scores was used.

4.3 Calibration
To discover which measure we should organise the results by we used conducted
a pairwise Pearson correlation between each of the c values and the single g value.
The results for each engine are given in Tables 4.1, 4.2 and 4.3. The average of each
correlation score over the 3 engines is given in Table 4.4. We averaged the score
out for each search engine at each cumulative or gravity value, then averaged this
score over the three engines. We can see that the gravity measure with β at 0.5 has
the highest overall Pearson correlation scores with the other measures. The β value
36

of 0.5 seems like an apt choice as it correlates well with all the cumulative cut-off
values, but especially with c=10 where it looks as if the 2 measures are practically
synonymous. The distance between the cumulative cut-off is indicative of how correlated they are i.e. c=1 and c=50 are loosely correlated, whereas c=10 and c=25 are
quite correlated. The largest correlation per row has been set to bold and the highest
correlation in each column has been italicised.
c1vsc3
0.8323

c1vsc10
0.7112
c3vsc10
0.9383

c1vsc25
0.5957
c3vsc25
0.8520
c10vsc25
0.9706

c1vsc50
0.4831
c3vsc50
0.7474
c10vsc50
0.8975
c25vsc50
0.9722

c1vsg
0.7359
c3vsg
0.9288
c10vsg
0.9865
c25vsg
0.9775
c50vsg
0.9294

Table 4.1: Engine A correlations between different measures
c1vsc3
0.9061

c1vsc10
0.5823
c3vsc10
0.8134

c1vsc25
0.331
c3vsc25
0.5684
c10vsc25
0.911

c1vsc50
0.1935
c3vsc50
0.4205
c10vsc50
0.8027
c25vsc50
0.9691

c1vsg
0.6248
c3vsg
0.8029
c10vsg
0.9644
c25vsg
0.9277
c50vsg
0.848

Table 4.2: Engine B correlations between different measures
c1vsc3
0.9723

c1vsc10
0.8973
c3vsc10
0.9675

c1vsc25
0.8234
c3vsc25
0.9106
c10vsc25
0.9801

c1vsc50
0.7536
c3vsc50
0.8474
c10vsc50
0.9307
c25vsc50
0.9801

c1vsg
0.9255
c3vsg
0.9779
c10vsg
0.9929
c25vsg
0.9745
c50vsg
0.936164562

Table 4.3: Engine C correlations between different measures

37

c1vsc3
0.9036

c1vsc10
0.7303
c3vsc10
0.9064

c1vsc25
0.5834
c3vsc25
0.777
c10vsc25
0.9539

c1vsc50
0.4767
c3vsc50
0.6717
c10vsc50
0.8769
c25vsc50
0.9738

c1vsg
0.7621
c3vsg
0.9032
c10vsg
0.9812
c25vsg
0.9599
c50vsg
0.9045

Table 4.4: Correlations between different measures over all engines

38

Chapter 5
Experimental Results
For this chapter all reported results are ordered by the gravity measure with a β
value of 0.5 unless otherwise stated. In Section 5.1 the most favoured result overall for each engine and topic combination is given. Section 5.2 expands upon the
previous section by detailing the top 20 results for each search engine and topic
combination. This section also shows how much each engine has a relative bias for
or against the news sources that the queries in the experiment were taken from. Section 5.3 shows the result of the investigation of H1, which is concerned with how
consistent the results from each engine were over time and topic. Section 5.4 is
the results of the investigation into H2, that Engine A clearly prefers its own news
serivce when compared to the preference of Engine’s B and C for their respective
own news services. In Section 5.5 we discuss the outcome of comparing the amount
of results from the immediate experiment with the amount of results from the 24
hr delay experiment. Section 5.6 contains the results of the investigation of the differences between the results of the immediate experiment and the results of the 24
hr delay experiment. Section 5.7 contains the results of the work concerning H5,
that USA-based results will have a larger score than UK-based results. The next two
sections do not concern any specific hypothesis but are rather exploratory studies of
an aspect of bias. Section 5.8 discusses the political bias of each search engine over
various political topics such as civil liberties and economic policy. In Section 5.9 the
concept of topical accuracy is explained and the results from applying an algorithm
to find what the website is most well known for. Finally in Section 5.10 we combine
some of these aspects of bias in to an easily readable and comparable form for each
engine.

5.1 Overview of the Top Result
In this section we show the news media provider that was favoured the most for
all topics and search engines in Table 5.1. In this table we can see that Engine A
ranks itself and the Telegraph (UK broadsheet newspaper) first. Engine B ranks the
BBC (UK television station) first most of all with the UK business website ino for
business news and the Daily Mail (UK tabloid newspaper) for sports news. Engine
C ranks the BBC and the Guardian (UK broadsheet newspaper) as first. These results, even just as the top result, give the impression that all of these search engines
greatly favour UK results. This is not the full picture as Section 5.7 shows. The fact
39

that Engine A is top 3 times out of 5 and that neither Engine B nor Engine C chooses
itself as top matches well with the results in Section 5.4.
The full list of the names of news sources that appear in this document can be found
in Appendix B.
Topic
business
entertainment
science
sport
world

Engine A
ENGINE A
ENGINE A
telegraph
telegraph
ENGINE A

Engine B
ino
bbc
bbc
dailymail
bbc

Engine C
bbc
guardian
guardian
bbc
guardian

Table 5.1: The most preferred news source for each engine-topic combination

5.2 Overview of the Top 20 Results by Gravity Measure
In this section an overall view of the predilections of each search engine (denoted
by A, B and C) for specific news sources is given for each of the topics that were included in the experiment (business, entertainment, science, sport and world news).
The top 20 for each engine-topic combination were taken by ordering the results
from each set by the gravity measure with the β value set at 0.5 from the 24 hr delay
experiment. The full tables of the top 20 results can be found in Appendix C. The
full list of the names of news sources that appear in this document can be found in
Appendix B.
Figure 5.2 has the top 20 sources of the 24 hr delay experiment as the x axis values
over all engines. The y axis values are the gravity scores. The legend for Figures 5.2
and 5.3 is shown in Figure 5.3.
Some interesting observations from Figure 5.2 are (more comments can be found in
Appendix C:
• BBC is heavily favoured by all engines over all topics.
• Engine A has a massive preference for its own news source over most topics.
• Engine A and Engine C have a large preference for the Guardian over all topics.
• For sports news, Sky was greatly favoured by Engine A and Engine C but did
not get any score from Engine B.

40

Figure 5.1: Top 20 sources by gravity measure from the 24 hr delay experiment in
the following order by topic: business, entertainment, science, sport and world

41

Figure 5.2: 24 hr delay results on the gravity measure showing each engine’s preference for the news sources that queries were generated from in the following order
by topic: business, entertainment, science, sport and world

42

Figure 5.3: Legend for figures 5.2 and 5.2
Figure 5.2 plots the gravity scores with the β value set at 0.5 from the 24 hr delay
experiment (y axis) against the 21 or 22 RSS feed sources from the experiment. From
this we can see each engine’s relative bias toward or against the news sources that
formed the queries for use in the experiment.
Some interesting features of Figure 5.2 are:
• BBC is favoured by all engines quite highly for all topics.
• Daily Mail is similarly favoured quite highly and equally amongst all engines
over all topics.
• All engines are severely biased against Seattle Times in the 4 topics that it
appears in, in fact it recieved no ranking within the first 50 results from any
engine on any query.
• Telegraph ranks highly amongst all engines, but especially Engine A.
• The Guardian generates a similar shape of bar graphs for every topics, being
ranked reasonably highly for all engines.
• The bias toward the Daily Mail varies for each engien over each topic, but is
generally quite high.
• Engines A and C are very highly biased toward Reuters but Engine B is biased
against it.
• All engines are severely biased against the Metro.

5.3 H1: Comparison of the Consistency of Results
Recall that H1 states that engine A will have a lower variance in the returned results
on a per topic basis. This variance will be similar over time. This low variance (i.e.
high consistency) is in comparison to the variance of engines B and C.
The Kolmogorov-Smirnov statistical test was used to compare the normalised gravity score of the combined top 5 sources for each engine-topic-time triple, against
each other triple. The Kolmogorov-Smirnov test is a non-parametric test over, in
this instance, 2 one-dimensional data sets to assess whether they appear to be drawn
from the same distribution.
Tables 5.2, 5.3 and 5.4 show the p-value obtained from comparing each set against
every other set using the Kolmogorov-Smirnov test, then the mean and standard
deviation of these scores between all three engines. The prefix of ‘i’ in the ‘Versus’
43

column means the immediate delay experiment results and the prefix of ‘s’ in the
‘Versus’ column means the short (24 hr.) delay experiment results.
Versus
ibus vs ient
ibus vs isci
ibus vs isport
ibus vs iworld
ient vs isci
ient vs isport
ient vs iworld
isci vs isport
isci vs iworld
isport vs iworld

ENGINE A ENGINE B ENGINE C
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0.9994
0.9748
0.995
1
1
0.8281
1
1
0.995
1
1

Table 5.2: Consistency measures of the results of the immediate experiment using
the Kolmogorov-Smirnov test

44

Versus
ibus vs sbus
ibus vs sent
ibus vs ssci
ibus vs ssport
ibus vs sworld
ient vs sbus
ient vs sent
ient vs ssci
ient vs ssport
ient vs sworld
isci vs sbus
isci vs sent
isci vs ssci
isci vs ssport
isci vs sworld
isport vs sbus
isport vs sent
isport vs ssci
isport vs ssport
isport vs sworld
iworld vs sbus
iworld vs sent
iworld vs ssci
iworld vs ssport
iworld vs sworld

ENGINE A
1
1
1
1
1
0.995
0.995
0.995
0.995
0.995
0.8281
0.8281
0.995
0.995
0.995
0.995
0.8281
0.8281
0.8281
0.995
0.995
0.995
0.8281
0.8281
0.8281

ENGINE B ENGINE C
1
1
1
1
1
1
1
1
1
1
1
0.9748
0.9994
0.9748
0.9994
0.9748
0.9994
0.9748
0.9994
0.9748
0.9994
0.9748
0.9994
0.6751
0.9994
0.9748
0.9994
0.9748
0.9994
0.6751
1
0.9748
0.9994
0.9748
0.9303
0.6751
0.6725
0.9748
0.9303
0.9748
0.9994
0.9748
0.6725
0.6751
0.9303
0.9748
0.6725
0.6751
0.3874
0.6751

Table 5.3: Consistency measures of the results of the immediate experiment against
the 24 hr delay experiment using the Kolmogorov-Smirnov test
Versus
sbus vs sent
sbus vs ssci
sbus vs ssport
sbus vs sworld
sent vs ssci
sent vs ssport
sent vs sworld
ssci vs ssport
ssci vs sworld
ssport vs sworld

ENGINE A
0.995
0.995
0.8281
0.028
0.8281
0.4889
0.995
0.087
0.2264
0.8281

ENGINE B ENGINE C
0.9994
0.9748
0.9303
0.3129
0.3874
0.3129
0.3874
0.6751
0.9303
0.9748
0.6725
0.9748
0.0813
0.9748
0.6725
0.9748
0.9303
0.9748
0.9994
0.9748

Table 5.4: Consistency measures of the results of the short experiment using the
Kolmogorov-Smirnov test

45

5.3.1 Explaining the Variance
The p-values show that there is not too much variance amongst the results of each
search engine. However, the results go against H1 which stated that Engine A’s variance would be lower than Engine B’s and Engine C’s. The reverse was true for the
mean value, but only slightly. Engine C seemed to be the most consistent on average
and with the lowest standard deviation. Engine A contained the only comparison
which produced a p-value below the traditional 0.05 threshold, in the short business
versus short world comparison. The other 2 engines also had low scores (at least in
comparison to all the other scores) in this category. Engine A’s short science versus
short world comparison was also quite low. Engine A’s short science versus short
sport was also quite low, meaning they were quite different. So Engine A is clearly
using topic specific news sites in most cases. Engine B’s immediate world versus
short world comparison yields a low number meaning that Engine B switched to a
different set of news sources after 24 hr of delay. Short entertainment versus short
world in Engine B has an almost zero score showing that for these two categories the
results are quite different. Short entertainment versus short sport and short science
versus short sport have lower than average scores showing that they have mildly
different sources. Engine C’s immediate science versus short entertainment, immediate science versus short world, immediate sport versus short science, immediate
world versus short entertainment, immediate sport versus short science, immediate
world versus short entertainment and short business versus short world comparisons all have a medium p-value showing that they are slightly different. Most of
these are different times and topics which makes sense, although short business and
short world are the same times, which means that they have different sources at the
same time. Short business versus short science and short business versus short sport
have quite different p-values so they have different sources, probably due to having
quite topical news sources. For all engines, the immediate versus other immediate
comparisons are very similar so it seems the immediate experiment meant that the
search engines had very similar results amongst the same topics (not that the search
engines had the same results, but that for each search engine it returned similar
results for all topics).

5.4 H2: Search Engine Preferences of Their Own News
Service
Recall that H2 of the outcome of the news bias experiment is that engine A will
greatly prefer news sourced from its own news service in comparison to the preference of engines B and C to their respective own news sources.
Table 5.5 details how many times the respective search engine appeared at the top
ranking (c=1), within the top 3 (c=3), within the top 10 (c=10), within the top 25
(c=25), within the top 50 (c=50) and using the gravity score with a dampening factor
of 0.5 (g=0.5). The table shows each engine’s self-preference for each topic at each
score point. The columns labelled ‘Num in A—B—C’ give the total number of distinct domains that were given in the results by that search engine. The row labelled
46

‘Mean’ contains, for each engine, the mean of the self-preference scores of the total
over all topics for each score point. Then in the cell matched up with the ‘Mean’
row and the ‘Num in A—B—C’ column is this mean divided by the mean of the
number of distinct domains taken from the total over all topics for this particular
score point. Thus each engine, at each score point and for each topic and all, has a
number corresponding to how much it prefers its own news source, in comparison
and relative to the other 2 engines. For clarity, the largets values per row are in bold.
c=1
ENGINE A
Bus
419
Ent
82
Sci
134
Sport 251
World 625
Mean 302.2
c=10
ENGINE A
Bus
2015
Ent
556
Sci
1885
Sport 2093
World 5511
Mean 2412
g=0.5
ENGINE A
Bus
1745.9981
Ent
424.3828
Sci
1388.5124
Sport 1558.1403
World 4319.4579
Mean 1887.2982

ENGINE B
3
5
53
17
16
18.8

ENGINE C
38
7
45
46
47
36.6

Num in A
4794
3478
6685
3875
5459
0.2318

Num in B
2161
1979
2674
1958
2214
0.0106

Num in C
2014
1909
2631
1778
2153
0.0159

ENGINE B
121
80
643
131
309
256.8

ENGINE C
407
88
459
247
596
359.4

Num in A
4794
3478
6685
3875
5459
1.2234

Num in B
2161
1979
2674
1958
2214
0.1339

Num in C
2014
1909
2631
1778
2153
0.1511

ENGINE B
119.716
70.6965
575.3095
127.5590
359.0764
250.4715

ENGINE C
316.0969
79.2864
396.9811
195.7003
436.6211
284.9372

Num in A
4794
3478
6685
3875
5459
1.0368

Num in B
2161
1979
2674
1958
2214
0.132

Num in C
2014
1909
2631
1778
2153
0.1208

Table 5.5: The self-preference scores for all engines at c=1, c=10 and g=0.5

5.4.1 Explaining the Results of Self-Preference
Engine A has an outright bias toward its own news service on all topics whereas for
Engine B and Engine C the preference is less clear cut.

5.5 H3: Less Results Over Time
Recall that H3 of the outcome of the news bias experiment is that for all search engines there will be less results over time overall.

47

topic/time
Bus
Ent
Sci
Sport
World
Mean

A
immed.
520658
323692
1341232
500846
741077
685501

short
86126
38952
183021
85034
133255
98283.25

diff.
0.165417606
0.1203
0.1365
0.1698
0.1798
0.1434

B
immed.
376282
243416
602189
359044
593349
434856

short
73793
34413
98667
69790
109970
69165.75

diff.
0.1961
0.1414
0.1638
0.1944
0.1853
0.1591

C
immed.
289371
210972
818390
234637
419619
394597.8

short
56693
29673
134830
48003
81638
70167.4

diff.
0.196
0.1406
0.1648
0.2046
0.1946
0.1778

Table 5.6: The differences between the number of total results from the immediate
experiment and the 24 hr delay experiment
Figures 5.4, 5.5 and 5.6 show the results of plotting the top 5 results (as determined
in the consistency measure in 5.3) with the x-axis as the gravity scores from the immediate delay and the y-axis as the gravity scores from the short delay experiment.

Figure 5.4: ENGINE A: immediate vs. short. We can see the linear measurement
in science is quite shallow when compared to the other topics, which are grouped
together. The exception to the close grouping the far outlier for world news. This
shallow linearity shows science loses more results over time compared to the other
topics

48

Figure 5.5: ENGINE B: immediate vs. short. Entertainment has a shallow but consistent linearity. The other topics are very widely dispersed

Figure 5.6: ENGINE C: immediate vs. short. Entertainment has a shallow but more
consistent linearity. The other topics are slightly more dispersed but have a very
similar gradient

5.5.1 Explaining the Matter of Less Results Over Time
There is a large drop in the number of results from the immediate to the 24 hr delay
experiment for all engines and topics, giving evidence for H3. For some reason the
science topic has the most results for every search engine at the immediate time, but
this shifts to the world topic after 24 hr for Engine B and stays the same for Engine
49

A and C. Engine A had more results for every topic than the other 2 engines for
both times. Engine B had more results than Engine C for every topic at both times,
except for science. Engine A has a larger drop in the number of results for all topics
after 24 hr compared to the other 2 engines. For all engines the largest drop after
24 hr was in the entertainment topic, perhaps indicating how vacuous the news in
the entertainment topic really is. Engine C had the smallest drop of results after 24
hr of all 3 engines for all topics. For Engine A, world news is the topic that had the
smallest drop after 24 hr The topic with the smallest drop after 24 hr for Engine B
was business news. Engine C’s sports results had the smallest drop after 24 hr

5.6 H4: Differences in Results Over Time
Recall that H4 of the outcome of the news bias experiment is that for all search engines there will be significant differences from the results of a query when posed at
different times.
The Kolmogorov-Smirnov statistical test was used to compare the normalised gravity score of the combined top 5 sources for an engine-topic-time triple, against the
other triple with the same engine and topic but a different time delay.
Table 5.7 shows the p-value obtained from comparing each triple’s results set against
the associated results set triple with a different time using the Kolmogorov-Smirnov
test. The last row contains the mean over all of the p-values for each engine. The
prefix of ‘i’ in the ‘Versus’ column means the immediate delay experiment results
and the prefix of ‘s’ in the ‘Versus’ column means the short (24 hr.) delay experiment results.
Versus
ibus-vs-sbus
ient-vs-sent
isci-vs-ssci
isport-vs-ssport
iworld-vs-sworld
Mean

ENGINE A ENGINE B ENGINE C
0.8281
0.9303
0.9748
0.087
0.9994
0.9748
0.8281
0.9994
0.9748
0.8281
0.9994
0.9748
0.087
0.9303
1
0.5317
0.9718
0.9798

Table 5.7: p-values from Kolmogorov-Smirnov test comparing immediate experiment results per topic with 24 hr delay results per topic, for all engines

5.6.1 Explaining Differences in Results Over Time
Overall Engine A had the largest difference over time with a mean p-value of 0.53166.
The other 2 engines did not differ very much over time. Engine A had very large
differences in the entertainment and world news topics over the 24 hr delay. The
other 3 topics had some differences for Engine A as well. This may be due to the
changing nature of news, especially entertainment and world news.
50

5.7 H5: Predominance of USA Results Over UK Results
Recall that H5 of the outcome of the news bias experiment is that for all search
engines, the results that they return will primarily be from the USA.
Table 5.8 shows the sum of the gravity score of the top 20 (as ranked by gravity
measure) for each engine-topic combination. For the purposes of summation, a UK
source was considered positive whereas a USA source was considered negative.
These were changed back to UK for positive number and USA for negative numbers once the calculations had taken place. The last row shows the mean over all
topics for this engine.

Bus
Ent
Sci
Sport
World
Overall

ENGINE A
110.69 USA
44.57 USA
80.56 UK
38.46 UK
128.14 USA
32.88 USA

ENGINE B
295.01 USA
21 USA
18.35 USA
49.05 USA
180.58 USA
112.8 USA

ENGINE C
162.88 UK
61.168 UK
154.77 UK
97.53 UK
261.46 UK
147.56 UK

Table 5.8: The UK or USA preferences for all 3 search engines over all 5 topics
Figure 5.7 shows the box plots of the UK versus USA scores of all engines.

Figure 5.7: UK vs USA using g=0.5 scores for all topics, all engines

51

5.7.1 Explaining the UK vs. USA Results
Engine A has a slightly UK-centric view of science and sport, a slightly USA-based
entertainment bias and more heavily USA-based biased world and business news.
Engine B is all USA-based but it is very small in the entertainment, science and
sports news topics, whereas it is heavily biased toward the USA in business and
world news, just like Engine A. Engine C is completely UK biased, with only a
slight bias in entertainment news but a large bias for all other topics, especially
world. Overall, Engine A is the most neutral whereas Engine C is very heavily
biased toward the UK.

5.8 Exploration 1: Political Bias
5.8.1 Using www.skewz.com to Show Political Bias
Political bias, as discussed in Section 2.4, is a major problem for the web. As evidenced by the last USA presidential election (2008), the web can play a major role
as a medium for communication of news and opinion on large issues. Using the results from the user voted political bias website www.skewz.com we have tabulated
the political biases of the three search engines using the gravity measure of the top
5 results from the 24 hr delay experiment.

5.8.2 Problems with www.skewz.com
Skewz is a USA-centric website so the bias of the search engines may not be applicable to other countries or cultures as their definitions of left-right may differ. The one
dimensional left-right scale is also a problem as it is limited in expressive power,
for example, variations of Marxist communism (very far left, just before anarchocommunism) have resulted in great suffering of millions of innocent people and
are intolerant of many viewpoints, which are traits generally considered very rightwing. However, it is simple to measure and use.

5.8.3 The Results of Political Bias by Using www.skewz.com
Table 5.9 (continued on in Table 5.10) details the results of using the skewz website to assign each engine a series of left-right bias scores. The top 5 sources from
each engine-topic combination ranked by the gravity measure on the 24 hr delay
experiment were taken to provide the political bias score. Under each skewz section
(e.g. Civil Liberties) there is a score from 5 left to 5 right, including a zero score.
These scores are assigned to individual news sources (e.g. cnn.com) and by this we
multiplied the gravity measure of this source by the score assigned to each section
by skewz reviewers. If a score was left then it was regarded as a positive number
and if it was right then it was regarded as a negative number. This was purely as a
transitional stage when working out the sum of scores. Positive scores are suffixed
with a L and negative scores are suffixed with a R.

52

Engine
A
B
C
A
B
C
A
B
C
A
B
C
A
B
C

Topic
Business
Business
Business
Entertainment
Entertainment
Entertainment
Science
Science
Science
Sport
Sport
Sport
World
World
World

Civil Liberties
188.87 L
167.94 L
232.26 L
106.14 L
97.46 L
138.87 L
350.78 L
164.37 L
736.34 L
136.7 L
293.26 L
195.97 L
421.21 L
28.87 L
437.55 L

Domestic Policy
319.8 L
74.19 L
199.74 L
82.07 L
11.61 R
88 L
182.47 L
157.61 L
136.91L
66.31 L
144.52 R
30.32 L
282.67 L
108.37 R
383.52 L

Economic Policy
345.96 L
192 L
423.57 L
133.71 L
2.93 L
121.54 L
537.22 L
337.23 L
714.98 L
347.34 L
232.41 L
252.13 L
690.05 L
201.64 L
548.98 L

Elections 2008
40 L
3.38 R
180.15 L
65.03 L
65.22 L
109.5 L
349.04 L
77.85 L
453.11 L
76.63 L
179.06 L
206.06 L
147.08 L
187.8 R
372 L

Energy & Tech
471 L
357.06 L
252.79 L
108.63 L
24.78 L
85.14 L
588.49 L
415.08 L
207.183 L
393.93 L
396.89 L
122.01 L
950.92 L
358.99 L
362.12 L

Table 5.9: Political bias of each search engine over the skewz topics and the experiment topics pt. 1
Engine
A
B
C
A
B
C
A
B
C
A
B
C
A
B
C

Topic
Business
Business
Business
Entertainment
Entertainment
Entertainment
Science
Science
Science
Sport
Sport
Sport
World
World
World

Foreign Policy
428.65 L
225.36 L
222.83 L
103.92 L
58.12 L
68.13 L
444.42 L
183.44 L
387.38 L
241.65 L
228.67 L
105.27 L
809.61 L
42.18 L
273.67 L

Health
62.31 L
1.66 R
44.6 L
34.79 L
65.53 L
36.89 L
207.29 L
214.74 L
417.66 L
52.09 L
273.64 L
65.49 R
17.81 L
10.4 L
11.95 L

Other
57.09 R
96.85 L
195.03 R
1.91 L
16.76 R
39.41 R
52.083 R
158.32 L
293.96 R
198.83 R
90.51R
203.9 R
238.88 R
33.04 L
189.76 R

Overall by Engine-Topic
224.94 L
138.55 L
170.11 L
79.53 L
35.71 L
76.08 L
325.95 L
213.58 L
344.95 L
139.48 L
171.11 L
80.3 L
385.06 L
47.37 L
275 L

Overall by Engine
230.99 L
121.26 L
189.29 L
230.99 L
121.26 L
189.29 L
230.99 L
121.26 L
189.29 L
230.99 L
121.26 L
189.29 L
230.99 L
121.26 L
189.29 L

Table 5.10: Political bias of each search engine over the skewz topics and the experiment topics pt. 2
Figure 5.8 shows the spread of each engine’s political bias on skewz. The score was
recorded for each sub-topic in the skewz measure that matched to a source in the
top 20 of each engine. So for each of the distinct scores in the range 5 to -5 the results
were summed.

5.8.4 Explaining Political Bias
There is very little right-wing bias from any of the engines. The most conservative
column is ‘other’.
53

Figure 5.8: skewz scores for all engines over all topics

Civil Liberties
All 3 engines have a moderate liberal preference in the business and entertainment
topics. However Engine B has a slightly more neutral tone. In science and world
news, Engines A and C have a large preference for the left, while Engine B’s preference is much less well defined. In sport however, the reverse is true, it is Engine B
which has the larger bias for the left.
Domestic Policy
Engine B is slightly conservative in entertainment news with the other 2 engines
being even more slightly left. Engine A and Engine C are heavily liberal when it
comes to business news, with Engine B being significantly less liberal. All 3 engines
are moderately left biased in science. Engine A and Engine C are so slighly left
in sports with Engine B being heavily conservative in comparison. World news
is reported with a very liberal slant by Engine A and Engine C, and a moderate
conservative bias by Engine B.
Economic Policy
Economic policy should be very important for business news. The same pattern of
Engine A and Engine C being quite liberal, with Engine B being less liberal holds
for all topics. Engine B is very close to neutral for entertainment news, however the
pattern holds still as Engine A and Engine C are quite low in their bias for liberal
entertainment news. Science was the most liberal topic for Engines B and C, with
world being the most liberal topic for Engine A.

54

Elections 2008
The presidential election of 2008 was a massive event given the current global crisis
and the outcome of the election. For business news, Engine B was very slightly rightwing with the other engines being slightly left-wing, with it being more pronounced
for Engine C. The same pattern of Engine A and C being heavily liberal with Engine
B being less liberal or slightly conservative also held in this category. Engine B was
quite right-wing in world news, being more right-wing than Engine A was liberal
in world news, with Engine C being strongly liberal. Engine C was very liberal in
this category.
Energy & Technology
This category is very science-based, so the science topic should be quite important.
The familiar pattern also hold for world and entertainment news in this category.
Engine A is very heavily liberal in this category. Entertainment is quite low in bias
for all engines. All other topic and engine combinations are quite heavily biased
toward liberal views.
Foreign Policy
Foreign policy is important to world news. The familiar pattern applies to entertainment, science and world news. Again, Engine A is heavily left biased, having
the highest bias of all engines, being quite extreme with world news. Engine B has
an almost neutral, slightly left bias for world news. Engine C’s world news bias is a
lot more moderate than Engine A’s, but more pronounced than Engine B’s.
Health
All engines are almost neutral, but still slightly left biased on world news. The same
is true for entertainment news, but slightly more left biased for all engines. Business
news is also quite low on liberal bias for Engine A and Engine C, with Engine B
being slightly right biased, just over neutral. Engine C is moderately right biased on
sport with Engine A being moderately liberal biased. Engine B is heavily left biased
on sport.
Other
Engine A and C are quite right-wing in this category, whereas Engine B is quite
liberal in comparison, but for entertainment and sport, Engine B is still right, just
less so than Engine A and Engine C.
Overall by Engine-Topic
Overall every engine tends to liberal news soucres. The general pattern for Engine A
and Engine C having a moderate liberal bias in comparison with Engine B’s less liberal bias also holds in the overall category, except for sport where Engine B actually
has a heavier liberal bias than the other engines.

55

5.9 Exploration 2: Topical Accuracy
5.9.1 Defining Topical Accuracy
Topical accuracy of a search engine in the context of news results can be defined as:
given a topic (e.g. sports) how well do the results of a query related to such a topic
match this topic. We propose that the measures defined by Rafiei and Mendelzon
in [29] and discussed in Subsection 3.2.2, which ascribe a reputation on a topic set
to a web page by crawling in-links to the page, is a suitable measure for relative
topical accuracy between the search engines. Using the algorithm from Rafiei and
Mendelzon’s TOPIC program we computed how well each domain from the top 5
results ranked by gravity measure from each engine-topic combination was related
to that domain. We listed only the top 10 results from TOPIC for each news source.
See Appendix D.
As we can see from the results, the use of TOPIC on news websites is not very
useful. It does not return useful terms for the majority of news sites. We would not
recommend this metric for further use in bias measurements. However we do note
the results of the topical accuracy scores used in the bias profiles in Section 5.10.

5.10 Bias Profiles
5.10.1 Defining the Bias Profile of a Web Search Engine
Given the knowledge gained throughout this report on the bias within each of the
search engines under study we can collate and display a profile view of each search
engine. These measures are taken from the results of the short delay (24 hr.) experiment. The profile attributes will be as follows:
• Normalised mean overall political leaning by gravity measure, taken from
Subsection 5.8.3 and normalised by the largest value among the 3 engines.
The closer score to 0 (either left or right) means that this engine is neutral with
respect to politics.
• Topical accuracy score which is the sum of the reciprocal rank of the top 10
reputation terms that match or are semantically similar to the topic (e.g. technology counted as a match in the science topic), taken from the top 5 sources.
These are the results from running the TOPIC program as described in Section
5.9. These scores were normalised by the largest score. A high topical accuracy
score means that over all set topics this engine returns topically relevant news
sources.
• Mean UK vs USA preference by gravity measure, normalised by the largest
value. The closer score to 0 (whether USA or UK) means that this engine selects
its results without regard for where they are based.
• Mean overall self-preference rating by gravity measure, normalised by the
largest value. The closer score to 0 means that this engine returns its own
news service as a results infrequently and at very low rank.
56

The number to normalise political bias is 230.9906604. The number to normalise
topical accuracy by is 35.80. The number to normalise UK vs USA is 147.5588815.
The number to normalise the self-preference measure is 1887.2982.
The bias profiles of the 3 engines under study is given in Table 5.11.

5.10.2 Bias Profiles
ENGINE A
Political Bias
1L
Topical Accuracy
0.832
UK vs USA Preference 0.223 USA
Self-preference
1

ENGINE B
0.525 L
1
0.7644 USA
0.1327

ENGINE C
0.8195 L
0.88
1 UK
0.151

Table 5.11: The relative bias profiles of the 3 engines under study
We can see that Engine A is known for its severe liberal bias, close topical accuracy,
mild USA preference and massive preference for its own news source. Engine B is a
moderately left leaning search engine, which provides news sources appropriate to
the topic being searched for, it has a large USA preference and a minimal preference
for its own news source. Finally, Engine C is quite heavily left aligned, is quite good
at returning topically accurate results, has a massive preference for UK sources and
a marginal preference for its own news service.

57

Chapter 6
Methodology of the User Survey
6.1 Derivation of the Survey from the Pilot Study
6.1.1 Hypotheses of the Outcomes of the Survey
H1: It was hypothesised that Engine A will be the most popular search engine over
all the survey participants. This hypothesis was based on the previous work in the
pilot study as described in Appendix A.
H2: It was thought that the knowledge of biases in a set of popular search engines
and the extent of these biases would change the perception of users of these search
engines. This hypothesis was based on the previous work in the pilot study.
The reasoning behind H1 was that this particular search engine is very well known
as the most popular at the time the survey was to be performed. Confirming this
helped with H2.
H2 was given as we suspect that knowledge of biases within a search engine would
change a user’s perception of that search engine. This belief was contrary to the
results of the confirmation bias experiment conducted in [19] by Lehner et al. We
had taken the opposite view from Lehner et al. as their experiment was conducted
with respect to a very complex task by field experts. The task of using search engines
is not as complex.

6.1.2 Subjects of the Survey
The subjects in this experiment were regular web search engine users who had
agreed to take part in a survey questionnaire to gauge the amount of confirmation
bias with regard to search engine usage.

6.1.3 Variables of the Survey
Independent Variables: The results of the news bias experiment was the independant
variable. These results were used as pieces of evidence in a survey to find the reac58

tions of the subjects, which could show confirmation bias if they did not follow the
evidence provided.
Dependant Variables: The reactions of each subject were the entities under study in
this experiment. These reactions were the effects of giving the subjects evidence that
showed how each search engine was biased with regard to news.
Extraneous Variables: The way in which the results were presented may have influenced the way in which the subjects interpreted the evidence. Therefore the display
of the results was made clear, self-evident, without complexity and understandable
by all. This was achieved by dedicating a significant amount of time to the formatting of the results and the design of the survey questionnaire. The outcome of the
news bias experiment was an extraneous variable as this experiment was not able
to provide evidence for anything if there was no evident search engine bias. This
was mitigated by running the news bias experiment for 9 weeks to uncover even
miniscule amounts of news bias.

6.1.4 Behaviour of the Survey
Each subject gave their gender and age range (18-22, 23-27, 28-32, 32-36, 37+). Then
they gave their primary use of the web (Business, Entertainment, Information, News,
Shopping, Work). Each subject listed, in order, up to their 5 preferred search engines.
This allows us to confirm or disprove H1. This knowledge also allowed us to contextualise the results of this experiment. The subjects were also asked how often
they use search engines (Daily, Weekly, Monthly) and if they answered ‘Daily’ then
they were asked how often per day (1-5, 6-10, 11+). The subjects were asked how
they rate the search results of the three search engines under study (very unreliable,
unreliable, somewhat, reliable, reliable, very reliable). The subjects were then given
knowledge of the news bias of the three search engines by showing them the top
20 preferred sources from the 24 hr delay experiment on the world news topic. The
bias profiles (see 5.10) of each search engine were also shown to the subjects, below
the top 20 sources. These results revealed the names of the search engines under
study, as this is required for the experiment to work. These questions were used
to discover if H2 held (the subjects changed their mind) or if there was a confirmation bias (the subjects did not change their mind when presented with evidence that
should have changed their minds).
The experimental subjects were not participating in the study again in any form and
so the learning effect did not need to be taken in to account in this respect. The
subjects may have inferred what would have came next in the questionnaire from
the current question. This did not pose a problem as they were given every question.
The aspects that were measured were the overall user preference of each search engine before and after they were shown results.
To ascertain whether H1 stood we checked the ranked list of each the preferred
59

search engine of each survey participant. We summed each search engine using the
formula 1/rank e.g. ranking Yahoo! as 1, Google as 2 and Ask as 3 will get Yahoo!
as score of 1, Google a score of 0.5 and Ask a score of 0.333, and these numbers were
added to their respective search engines for every other participant.
H2 was checked by calculating the change in how users view the search engines.

6.1.5 Statistical Tests Used in the Survey
The test for H1 measured the statistical differences between the total preference
score for each search engine.
To validate H2 we checked the responses of subjects before and after providing them
with evidence of bias.

60

Chapter 7
User Survey
7.1 Results
The results for the favourite search engine have been omitted as it would give hints
to the identity of the three search engines under study. The number of survey replies
was 70, but only 59 were used as several of the web based questionnaires were not
filled in fully. Of these 59, 40 were male and 19 were female. 41 subjects were in the
age range of 18 to 22 and the final 18 were in the age range of 23 to 27. The primary
use was information (31), followed by entertainment (24) with business and news
have 2 each. Every subject used a search engine daily with 17 using using them
between 1 and 5 times per day, 23 using them between 6 and 10 times per day and
19 using them more than 11 times per day. The overall rating from 1 to 5 before
the subjects saw the evidence for biases for Engine A was 4.3898, Engine B was
2.9491 and Engine C was 2.1525. The overall difference in score from 1 to 5 after the
subjects saw the evidence for biases for Engine A was -0.9588, Engine B was 0.0508
and Engine C was 0.4337.

Figure 7.1: before vs. after - differences in perception of reliability of results

61

7.2 H1: Engine A is the Preferred Engine
Recall that H1 of the survey is that the overall preferred search engine from all survey participants is Engine A.
Engine A was the top engine for every participant so gained a normalised (by number of subjects) sum of inverse rank score of 1. Engine B was the next most favoured
engine and scored 0.209. Engine C was favoured directly after Engine B and scored
0.09. 14 more engines appeared in the survey but with very insignificant scores.
Therefore our hypothesis has serious evidence that Engine A is the most favoured
engine, at least amongst 18 to 27 year olds (the only age ranges that took part in the
survey).

7.3 H2: Knowledge of Bias Will Change User Perceptions
Recall that H2 of the confirmation bias experiment is that when confronted with evidence of bias in a search engine a user will not have a confirmation bias, i.e. the
user’s perception of the search engine will change.
The user’s perception of all three engines changed but rather marginally for the
better in the case of Engines B and C. The average user perception for Engine A
dropped quite a lot yet it was still regarded as the engine that gave the most reliable
results of the 3.
Unfortunately the questionnaire did not ask the participant an important question:
which measures were useful when deciding on their opinion on the search engines?
Therefore to somewhat negate this deficiency we asked several survey particpants
for their opinion about how their views were changed. These participants were
asked this question approximately 2 weeks after they took part in the survey.
The following responses have had the engine names changed to keep them confidential.
A history of art student said: “I felt that even though Engine A favoured their own
news so much I still prefer them. However I did see that the other search engines
were better than I thought. I thought that since Engine A is so well used that the
others were not as trustworthy.”
A telecomunications worker said “It was very insightful. I liked the top 20 section as
I could tell for myself what engine I would use from now on based on my favourite
news sites.”
An electronics engineer said: “I liked the survey, especially the political bias results
and the UK versus the USA results in the profiles. I still use Engine A as my main
search engine but I am a bit more wary of the results.”
62

A computing science student said: “The survey was interesting; I had never thought
of the political bias that could occur in search engines, or that they would favour one
country over another. I still use Engine A as my only search engine but I now take
into consideration the bias towards the US.”
To summarise, knowledge of biases of web search engines changed user opinion
of all of the engines. However it is unlikely to make the user change from their
favourite search engine, even though it may make them more wary of the results.

63

Chapter 8
Discussion and Conclusion
8.1 Questions Arising from the Study
Will changing the device change the bias? Are the results delivered to a desktop or
laptop machine the same as those given to a mobile phone or PDA?
Will changing the country that the experiment is carried out in change the results?
Will the results stay approximately the same? Or will running this experiment again
provide different results for the search engines?

8.2 Interesting Results
All of the results are interesting in some way and provide some insight into what
each engine prefers. The most interesting results from Engine A are that it so heavily favours itself despite having the largest number of distinct sources, that it is so
liberal and that it also puts youtube.com so high in its rankings. The most interesting results from Engine B are that it is so USA biased, has such a low preference
for its own news and even puts Engine C higher than itself. The most interesting
results from Engine C are its huge bias for UK-based results and that it did not drop
as many results with a 24 hr time delay.

8.3 Limitations of the Study
There are several kinds of limitations of this study:
• Practical
• Experimental
• Scope and context
• Can results be generalised?

64

8.3.1 Practical Limitations
An obvious limitation of the study was the limited amount of time that could be
dedicated to the project. Another limitation was the number of survey participants
that could be used, however we believe the 59 participants was a large enough pool
of people.

8.3.2 Experimental Limitations
There were some problems with flooding search engines with requests therefore the
number of queries in the experiments were reduced by:
• Reducing the number of RSS feeds to approximately one quarter of the original
size and cycling the sources every week.
• Reducing the number of time delays to 2, keeping immediate and 24 hr delay
and dropping the 168 hr delay.
An attempt was made to re-start the 168 hr delay experiment as a 432 hr delay and
do 2 weeks of a study over it. However this attempt failed due to server maintenance and overwritting data and as the experiment is very dependant on time, this
part of the study did not take place. This means that we cannot know what long
term delay biases there are, only a 24 hr delay bias.
If we had more web proxies then we could have mitigated more bias from the selection of a larger number of RSS feeds. Additionally this would have allowed the
study to cover more topics and different search engines, given additional time to
program the extra search engine interfaces and select the RSS feeds.
The political bias is from www.skewz.com which is user generated scores, but we
are assuming that these scores are objective and that any biases are averaged out
by a user with the opposite view. Additionally, if a source or column for a source
did not exist (it was quite rare) then a score of 0 was recorded. It is not perfect but
skewz is the only source for political bias and it has served its purpose well and
small pertubations from a user’s bias or lack of data for a small number of sources
is something that we can accept.
The TOPIC program is a partial implementation of algorithm 3 from [29] but the
authors state that it has worked well for them in finding what topics a web page is
known for. A larger problem is that TOPIC may not be applicable to news sites as
their content is constantly changing. We ran TOPIC on the sites at the same time so
that if there was any bias based on content then it would be static for that time. So
we hoped that the news sources would have the same kinds of keywords as they
would hopefully be reporting the same news.

8.3.3 Scope and Context Limitations
The experiments were conducted in the UK and on a desktop machine. There may
be different biases on mobile devices and in other countries. This could be a focus
65

for future work.
The experiment only covers predilections for specific sites with respect to news not
general information. We chose news as we had a ready supply of realistic topic
seperated news queries (titles of news articles from RSS feeds) which would work
in a real setting. News bias is also a very real problem which has not been studied
in this context before by scientists. This could be a focus for future work.

8.3.4 Limitations of the Generality of the Results
These results are not generalisable as the exact values are a snapshot of the biases of
the 3 search engines over 9 weeks. The bias has potential to change over time but as
far as we know, from late November 2008 to the end of January 2009, these results
are an accurate representation of the biases of the 3 search engines under study. We
have defined a methodology for measuring bias that could be used to monitor bias
on a long term basis. This could be a focus for future work.

8.4 Future Work
Further work in this vein could be to discover how the bias changes in different
countries or on different devices (e.g. mobile phones).
More work needs to be done in the field of preventing or removing the bias in
search engines. Examples of this would be to publish the results in a way similar to skewz.com or perhaps filtering and re-ordering results in a web browser or
web browser extension.
A useful area of work stemming from the survey would be to find out what bias
measures users understand.

8.5 Conclusion of the Contributions
In this report we have shown that long term biases do exist within web search engines. In particular we have shown that individual search engines have predilections for certain news sources, which in turn have their own biases. We have displayed these preferences for specific sources in the form of graphs of the top 20
sources (Section 5.2), ordered by the gravity measure. Additionally we show the
graphs of the search engines’ preferences for the news sources that were used as
sources for queries in these experiments. These graphs are listed by topic so that
the reader can determine how relevant the results are to the topic. Additionally we
show (in Section 5.9) the topical accuracy of the top 5 sources for each search engine
on each topic, however this measure was not particularly useful. In Section 5.3 we
show that for all of the engines under examination, on average they produce consistent results across the 5 topics. We show the extreme preference of Engine A for
its own news service in Section 5.4, in comparison to Engines B and C. We showed
that over 24 hr (Section 5.6) Engine A changed the most while Engines B and C
66

stayed roughly the same. This implies that Engine A had resher results. There also
exists a UK versus USA bias in search engines. We predicted it would be a severe
USA-based bias yet Engine C had a large UK-based bias in comparison to the small
USA-based bias of Engine A and the medium USA-based bias of Engine B. Overall
the UK versus USA bias was even when taking all three engines in to account. The
political bias of these search engines was also mapped out. There is a heavy liberal
bias in all 3 of the search engines. The major bias features were displayed in a simple
bias profile for each engine allowing their biases to be shown easily in comparison
to the other 2 engines. Chapter 7 shows that there is no real confirmation bias as
knowledge of biases of the 3 search engines made participants only drop their respect for one search engine but allowed it to retain its top place, even though the
other 2 engines (B and C) gained a slight reputation increase.

67

Bibliography
[1] Leif Azzopardi and Ciaran Owens. Search engine predilection towards news
media providers. In To appear in the Proceedings of the 32nd Annual International
ACM Conference in Information Retrieval, July 2009.
[2] Leif Azzopardi and Vishwa Vinay. Document accessibility: evaluating the access afforded to a document by the retrieval system. In ECIR 2008: Workshop on
Novel Methodologies for Evaluation in Information Retrieval in the Proceedings of the
30th European Conference on Information Retrieval, pages 52 – 59, March 2008.
[3] Leif Azzopardi and Vishwa Vinay. Retrievability: An evaluation measure for
higher order information access tasks. In Proceedings of the ACM International
Conference in Information and Knowledge Management, 2008.
[4] Ziv Bar-Yossef and Maxim Gurevich. Random sampling from a search engine’s
index. In WWW ’06: Proceedings of the 15th international conference on World Wide
Web, pages 367 – 376, New York, NY, USA, 2006. ACM.
[5] T Beauvisage. Smantique des parcours des utilisateurs sur le web. PhD thesis,
Universit de Paris X, 200 Avenue de la Rpublique, 92001, Nanterre, Paris, 2004.
[6] Hemant K. Bhargava and Juan Feng. Paid placement strategies for internet
search engines. In WWW ’02: Proceedings of the 11th international conference on
World Wide Web, pages 117–123, New York, NY, USA, 2002. ACM.
[7] Michael Chau, Xiao Fang, and Christopher C. Yang. Web searching in chinese:
A study of a search engine in hong kong. Journal of the American Society for
Information Science and Technology, 58(7):1044 1054, June 2007.
[8] Junghoo Cho and Sourashis Roy. Impact of search engines on page popularity.
In WWW ’04: Proceedings of the 13th international conference on World Wide Web,
pages 20–29, New York, NY, USA, 2004. ACM.
[9] Junghoo Cho, Sourashis Roy, and Robert E. Adams. Page quality: in search
of an unbiased web ranking. In SIGMOD ’05: Proceedings of the 2005 ACM
SIGMOD international conference on Management of data, pages 551–562, New
York, NY, USA, 2005. ACM.
[10] Elizabeth Van Couvering. Web behaviour: Search engines in context. Technical report, Department of media and communications, London school of economics and political science, February 2005.

68

[11] Batya Friedman and Helen Nissenbaum. Minimizing bias in computer systems. In CHI ’95: Conference companion on Human factors in computing systems,
page 444, New York, NY, USA, 1995. ACM.
[12] Rob Garner.
17/02/09.

Deconstructing search engine bias, Febuary 2008.

Retrieved

[13] Eric Goldman. Search engine bias and the demise of search engine utopianism.
Technical Report 06-08 and 06-20, Marqette University Law School and Santa
Clara University Legal Studies Department, 2006.
[14] The Guardian. Sun online overtakes four rivals to become most popular newspaper site. http://www.guardian.co.uk/media/2009/mar/26/sunonline-most-popular-newspaper-website, March 2009.
[15] Matthew Hindman, Kostas Tsioutsiouliklis, and Judy A. Johnson.
“googlearchy”: How a few heavily-linked sites dominate politics on the
web. In In Annual Meeting of the Midwest Political Science Association, pages 3 –
6, 2003.
[16] Susan Kingsbury, Hornell Hart, Romayne Rowe, Jessie Bloodworth, Anna
Holbrook-Clark, Lois Galbraith, Julia Ann Bishop, and Ella Hart. Newspapers
and the News. G. P. Putnam’s Sons, 1937.
[17] Hady W. Lauw, Ee-Peng Lim, and Ke Wang. Bias and controversy: beyond the
statistical deviation. In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 625–630, New
York, NY, USA, 2006. ACM.
[18] Steve Lawrence and C Lee Giles. Accessibility of information on the web. Nature, 400, 1999.
[19] P.E. Lehner, L. Adelman, B.A. Cheikes, and M.J. Brown. Confirmation bias in
complex analyses. Systems, Man and Cybernetics, Part A, IEEE Transactions on,
38(3):584–592, May 2008.
[20] Ronny Lempel and Shlomo Moran. Predictive caching and prefetching of query
results in search engines. In WWW ’03: Proceedings of the 12th international conference on World Wide Web, pages 19–28, New York, NY, USA, 2003. ACM.
[21] Yan Liu, Yiming Yang, and Jaime Carbonell. Boosting to correct inductive bias
in text classification. In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management, pages 348–355, New York, NY,
USA, 2002. ACM.
[22] Vivian B. Martin. Going beyond fair and balanced. Scientific American, 299(5),
November 2008.
[23] Abbe Mowshowitz and Akira Kawaguchi. Bias on the web. Communications of
the ACM, 45(9):56–60, 2002.

69

[24] Audit Bureau of Circulations. National daily newspaper circulation figures
(uk). http://www.guardian.co.uk/media/table/2008/sep/05/abcs.pressandpublishing,
September 2008.
[25] Audit Bureau of Circulations. National daily newspaper circulation figures
(usa). http://www.inquisitr.com/6447/us-newspaper-circulation-down-47/,
September 2008.
[26] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford
Digital Library Technologies Project, 1998.
[27] Sandeep Pandey, Sourashis Roy, Christopher Olston, Junghoo Cho, and
Soumen Chakrabarti. Shuffling a stacked deck: the case for partially randomized ranking of search engine results. In VLDB ’05: Proceedings of the 31st international conference on Very large data bases, pages 781–792. VLDB Endowment,
2005.
[28] Feng Qui, Zhenyu Liu, and Junghoo Cho. Analysis of user web traffic with
a focus on search activities. In webdb ’05: Proceedings of the 8th international
workshop on the web and databases. ACM, June 2005.
[29] Davood Rafiei and Alberto O. Mendelzon. What is this page known for? computing web page reputations. Comput. Netw., 33(1-6):823 – 835, 2000.
[30] searchenginehonesty. Search engine censoring (“banning” and “penalization”).
Retrieved 17/02/09.
[31] John A. Tomlin. A new paradigm for ranking pages on the world wide web.
In WWW ’03: Proceedings of the 12th international conference on World Wide Web,
pages 350–355, New York, NY, USA, 2003. ACM.

70

Appendix A
Pilot Study of News Bias
In this chapter we present the work done during the Summer of 2008 when the author was a student researcher under the supervision of Dr. Leif Azzopardi. The
project aim was to investigate search engine bias and develop an apparatus to facilitate the measurement of some forms of relative bias of search engines. An overall
view of the research is provided, with details of the methodology used.
We performed this pilot study for several important reasons:
• To find and refine measures for news bias in search engines.
• To see if the experiments produced sensible and interesting results in the short
term.
• To create the tools necessary for a larger scale study.
• To calibrate these tools for later use.

A.1 Experiments in Detecting and Measuring Relative
Bias in News from Web Search Engines
A.1.1 Overview of the Experiments
A pilot study of the effects of search engine bias with respect to news was conducted
in the Summer of 2008.
Aim:
The overall aim of the experiments detailed in A.1.4 was to investigate the relative
bias of web search engines toward news media providers. Three popular news topics and three popular search engines were selected for these experiments. To form
realistic queries news headlines were used as these contain the terms a user would
concievably pose. In addition as the subject is news such queries had to be from
the correct time period therefore obtaining query logs from search engines would
not be useful. These experiments gave evidence for the bias of one engine to a
news provider relative to other news providers and to other engines’ bias toward
(or against) the same news provider. These experiments used the same query set
71

from the three topics, submitted to the three search engines three times. Doing so
allowed an analysis of any changes in bias over time. The experiments covered:
• Topic: is there a difference between a search engine’s results on different topics?
• Time: does a search engine’s preference for different providers change over
time?
• Search engine to news provider: what are the relative biases between search
engines?

A.1.2 The Method for Generating Appropriate News Queries
The basis for the experiments in detecting and measuring the bias of search engines
in relation to news media providers (for example Reuters) was to obtain a suitable
base of current news queries in a selection of topics (for example technology news)
and run these queries in a set of search engines that found out which news sources
are favoured by each search engine for a set type of news. Finding suitable queries
was achieved by crawling a set of news source web sites for RSS (Really Simple
Syndication) feeds on a particular news topic and taking the titles from each news
item as a query. The titles were stripped of punctuation, set to lowercase and the
words were put into alphabetical order. These alterations from the title to a query
were done to alleviate any bias in selecting queries which matched exactly an existing news item title from only one news source. Stopwords were not removed as
these may be dependant on the search engine used and retaining stopwords in the
query allows the query to continue to sound as if it was posed by a human, albeit
in alphabetical order. The list of queries was treated as an injective sequence i.e. exact duplicate queries were not added again and the queries were performed in the
order in which they were added.

A.1.3 The Framework for the Experiments
Three popular search engines were chosen for these experiments. These search engines used the US/international version of the news search. In the experiments
these three search engines were known as A, B and C in an attempt to disguise the
actual identity of each search engine.
The topics covered in the experiment were world news, business news and technology news. The news providers were drawn from both the USA and the UK using
English as the source language.
The sources from the USA were:
• American Broadcasting Company (ABC)
• Cable News Network (CNN)
• Fox (News Corporation)
72

• New York Times (NYT)
The sources from the UK were:
• British Broadcasting Corporation (BBC)
• British Sky Broadcasting Group (Sky, BSkyB, BSB)
• The Guardian
• Reuters (UK version)
• The Times
These news providers were selected as they all had RSS feeds for the three topics
(world, business and technology) and despite any perceived bias that they may in
fact have, they were also generally known as providers of relevant, fresh and accurate news.
The titles from the RSS feed items were retrieved every three hr for three days spread
over a seven day period.
The main measures of bias used in these experiments were the cumulative and gravity measures used by Azzopardi and Vinay [2] whose report deals with the accessability of documents within information retrieval systems. The cumulative score is
a positive integer given to each provider which is a sum of the number of times this
provider appears as a result from a query to a search engine within a specified rank
cutoff. The cumulative score was recorded for a cutoff of 1, 3, 10, 25 and 50. The
gravity score used the formula:
f(cdq , β) =

1
(cdq )β

The gravity score used a β (dampening factor) value of 0.5. The gravity measure was
chosen as it accurately reflects the actual user experience [20] - that the users value
the top most results with greater weighting. This score was essentially 1 divided by
the square root rank of a result from a query. The result of these measures were that
for each topic on each search engine each news provider which the search engine
gave as a result was given a series of numbers which reflected how much the search
engine favoured the source on the current subject with reference and being relative
to all other news sources which were listed by the search engine.

A.1.4 The Immediate Effect Experiment
One of the experiments was to find the results of running these titles as queries
immediately. The hypothesis of this experiment was that unbiased search engines
returned results based on the news sources which are quickest to add a high quality
news story on the exact subject of the query. There was possible bias in the order
which search engines index the news websites as each iteration of this experiment
on each topic was run as soon as all nine RSS feeds were retrieved and parsed,
therefore it was possible that the results from some news providers, regardless of
the page quality, had not been indexed by a search engine as news is just that - new.
73

A.1.5 The Short Delayed Effect Experiment
The next experiment was run after eight iterations of the three hr breaks between
obtaining queries from the set news providers’ RSS feeds so that there was a large
injective sequence of queries to send to the search engines. These queries were sent
over the next 24 hr to attempt to negate any delay in search engine indexing strategies as discussed in A.1.4. The hypothesis of this experiment was to show that when
there was a lack of systematic bias due to delayed indexing, there are further underlying biases which existed and were measureable. In the time lapse there may be
more facts about a news item discovered or there is time to write a better article
than a news source which haphazardly posted the known details at the time to ensure that the information was given. It is natural that one news source will have an
average objectively higher quality page that another news source and so the better
news source should be ranked higher. If this ranking is not enforced then there is the
case for naming a search engine as biased in some manner, regardless of the intent
of the search engine or the designers.

A.1.6 The Long Delayed Effect Experiment
This experiment was run one week after the end of the immediate effect and short
delayed effect experiments. This experiment used the same queries in the same order as the prior two experiments and was conducted over one week to ensure that
the submission of queries to search engines was approximately (to the coarseness of
a few hr) one week after the original query related to a particular news story. The
intention of this experiment was to discover if the rankings are the same or very
similar to the previous experiments given in A.1.4 despite a one week gap in the
search and the inherent variablilty of the meaning of queries relating to news due to
the constantly changing environment.
The results of these 3 experiments can be found in [1].

A.2 A Survey Regarding the Effects of Known Relative
Bias in News from Web Search Engines
A survey was conducted to determine if knowing the news biases of a set of search
engines would affect a user’s attitude to the effects of bias. This survey was performed as a questionnaire amongst 11 student and staff participants from the University of Glasgow and Strathclyde University. The participant was asked to give
up to their favourite 5 news providers and up to their favourite 5 search engines
as ranked lists in order of preference. The participant was given the names of the
three search engines and shown bar graphs of the first 5 results ordered by using the
gravity measure with a β value of 1. The participant was also shown the number of
sources that were returned. The participant was then asked to rate A, B and C using
a Likert scale with relation to how much they believed the results of each search
engine. The user was then shown the results of the short term experiment on the
effects of bias and was asked, with the addition of the knowledge of the biases of
74

each search engine, to redo the previous question.

A.3 Tools Created for the Pilot Experiments
The tools created in the pilot study were used with slight modifications in the full
study. The main tool consisted of a java program with several ancillary classes. A
class to retrieve the text from a HTTP connection, with a method to return it as a
string object, was created as this is a common task. A base class for all of the engines was defined. All 3 engine classes are subclasses of this class. These individual
classes fetch news results from the international version of the respective search engine on a specific query. A maximum of 50 results are returned. The source URL of
each result is stripped from the page and stored in a file in the format:
query : URL : rank
The file is specific to the time delay of the experiment, the time it started and the
specific engine. The queries that are used in these classes are taken from the titles
of news stories from RSS feeds. RSS feeds were fetched using the HTTP class and
the titles were stripped from the text. These titles were stored one per line on a file
with a name which was specified by the iteration of the driver program and the
RSS source. Every 3 hr the driver program ran, writing the queries to disk and also
holding them in memory. When this section of the program was finished, the immediate experiment was ran. A thread was created for each search engine. Each
engine then fetched the results from the queries in turn and wrote them to disk as
described previously. After 24 hr the short delay experiment started. Each engine
then fetched the results from the queries in turn and wrote them to disk as for the
immediate experiment, however to distinguish the files a suffix of .short was added.
After 168 hr (1 week) the long delay experiment was ran. This had the same result
as the previous 2 experiments but added a .long suffix to the files.
Overleaf is a simple data flow diagram of the program.

75

Figure A.1: Program data flow diagram

76

Appendix B
Table of Sources Appearing in the
Results
The following table (continued over the page on p. 76 - 77) contains all the news
sources from the top 20 results from all 3 engines over all topics in the experiment,
including the full domain name and whether the source is based in the United Kingdom (UK) or the United States of America (USA). The ’Source Name‘ column is the
short version which is used throughout the experiment as a shorthand for the news
source from the column ’Website‘.

77

Source Name
411mania
4thegame
abc
alertnet
ananova
baynews9
bbc
bleacherreport.com
bloomberg
blueridgenow
boston
centredaily
channel4
charlotteobserver

Website
www.411mania.com
www.4thegame.com
abc.go.com
www.alertnet.org
www.ananova.com
www.baynews9.com
news.bbc.co.uk
bleacherreport.com
www.bloomberg.com
www.blueridgenow.com
www.boston.com
www.centredaily.com
www.channel4.com
www.charlotteobserver.com

78

UK/USA
USA
UK
USA
UK
UK
USA
UK
USA
USA
USA
USA
USA
UK
USA

Source Name
cnet
cnn
dailymail
dailyrecord
economist
ENGINE A
ENGINE B
ENGINE C
eonline
espn
ew
examiner
football365
forbes
fox
fxstreet
globeinvestor
goal
guardian
iii
independent
informationweek
ino
inrich
kansascity
lasvegassun
latimes
marketwatch
miamiherald
mirror
newscientist
newsweek
nj
nwsource
nydailynews
nytimes
pantagraph
philly
reuters
salon
scotsman
scout

Website
www.cnet.co.uk
www.cnn.com
www.dailymail.co.uk
www.dailyrecord.co.uk
www.economist.com
BLANK
BLANK
BLANK
www.eonline.com
espn.go.com
www.ew.com
www.examiner.com
www.football365.com
www.forbes.com
www.foxnews.com
www.fxstreet.com
www.globeinvestor.com
www.goal.com
www.guardian.co.uk
www.iii.co.uk
www.independent.co.uk
www.infromationweek.com
www.ino.com
www.inrich.com
www.kansascity.com
www.lasvegassun.com
www.latimes.com
www.marketwatch.com
www.miamiherland.com
www.dailymirror.co.uk
www.newscientist.com
www.newsweek.com
www.nj.com
www.nwsource.com
www.nydailynews.com
www.nytimes.com
www.pantagraph.com
www.philly.com
www.reuters.com
www.salon.com
www.scotsman.com
www.scout.com

79

UK/USA
UK
USA
UK
UK
UK
USA
USA
USA
USA
USA
USA
USA
UK
USA
USA
USA
USA
UK
UK
UK
USA
USA
USA
USA
USA
USA
USA
USA
UK
UK
USA
USA
USA
USA
USA
USA
USA
UK
USA
UK
USA

Source Name
seekingalpha
sfgate
sky
sportinglife
star-telegram
startribune
syracuse
teamtalk
telegraph
theatlantic
theglobeandmail
theherald
thestate
thesun
thewest
timesonline
tiscali
usatoday
washingtonpost
welt
wired
wsj
youtube
zdnet

Website
seekingalpha.com
www.sfgate.com
news.sky.com
www.sportinglife.com
www.star-telegram.com
www.startribune.com
www.syracuse.com
www.teamtalk.com
www.telegraph.co.uk
www.theatlantic.com
www.theglobeandmail.com
www.theherald.co.uk
www.thestate.com
www.thesun.co.uk
www.thewest.com.au
www.timesonline.co.uk
www.tiscali.com
www.usatoday.com
www.washingtonpost.com
www.welt.de
www.wired.com
wsj.com
www.youtube.com
www.zdnet.com

80

UK/USA
USA
USA
UK
UK
USA
USA
USA
UK
UK
USA
UK
USA
UK
UK
USA
UK
USA
USA
USA
USA

Appendix C
Tables of Results
This Appendix takes on the following format:
• Each Section (C.1, C.2, C.3) is specific to a search engine.
• Within each of these sections a short overview of the results from the engine
over all topics is given.
• Within each of these sections there is a section for each of the topics so that
there is a section for each of the engine-topic combinations.
• Within each section is a table with the top 20 for the associated engine-topic
combination ranked by the gravity measure obtained from the 24 hr delay
experiment.
• Within each section is a discussion of the results.

81

C.1 Top 20: Engine A
C.1.1 Topic: Business
ENGINE A
ENGINE A
telegraph
reuters
bloomberg
bbc
nytimes
guardian
timesonline
washingtonpost
youtube
forbes
independent
marketwatch
cnn
wsj
latimes
boston
dailymail
abc
seekingalpha
Total # Sources

c=1
419
492
164
31
307
281
128
225
197
0
18
163
80
46
19
104
90
49
114
6
4794

c=3
1026
737
437
286
669
556
268
350
372
304
113
243
227
164
119
215
135
120
171
37

c=10
2015
1054
1049
894
918
753
654
647
583
549
379
390
420
403
400
333
290
297
261
165

c=25
3261
1418
1750
1785
1256
1038
1165
1043
786
919
851
597
713
718
739
461
496
528
373
494

c=50
4576
1817
2522
2903
1552
1345
1809
1470
1016
1314
1445
924
1035
1069
1078
631
798
707
488
897

g=0.5
1746
939.79
886.67
836.81
785.50
664.91
601.31
591.95
485.60
429.51
398
381.24
381.13
355.73
333.73
285.63
279.32
252.9
233.7
223.15

The most striking feature is the predominance of Engine A’s own news service.
Overall, business-specific news sites such as the Wall Street Journal and marketwatch appear in the top 20. The only real surprise for business news is youtube,
which is a video sharing website and not a legitimate news source. It was listed in
10th yet it never had a single top spot which seems very strange in comparison to
the other results, every result from 11 to 20 had some top spot rankings.

82

C.1.2 Topic: Entertainment
ENGINE A
ENGINE A
bbc
latimes
guardian
nytimes
timesonline
washingtonpost
ENGINE C
youtube
telegraph
dailymail
reuters
nwsource
411mania
eonline
scotsman
boston
fox
abc
examiner
Total # Sources

c=1
82
124
109
107
34
46
58
0
0
37
32
28
49
1
36
48
19
38
42
1
3478

c=3
285
270
223
189
87
68
86
4
117
60
86
73
78
29
67
85
38
76
57
17

c=10
556
411
351
286
213
161
201
46
198
142
162
157
123
112
131
123
108
120
105
79

c=25
801
544
539
431
371
373
306
305
314
258
214
241
213
236
214
162
209
155
166
166

c=50
1052
671
773
609
587
519
440
828
464
419
317
328
316
435
280
197
318
207
238
372

g=0.5
424.38
335.14
321.33
264.42
192.58
175.29
168.4
165.07
150.51
139.12
128.81
128.22
126.14
115.25
112.31
103.47
101.7
98.86
98.23
91.19

Again, Engine A’s own news service appears as the top ranking. But it is not by such
a large margin. Surprises include youtube again (at rank 9), but in the entertainment
topic this may seem more plausible. Yet again it never gained top ranking for the
whole experiment. Engine C at rank 8 is also a surprise, especially as Engine C
sometimes does not even rank itself that high.

83

C.1.3 Topic: Science
ENGINE A
telegraph
guardian
bbc
ENGINE A
nytimes
independent
timesonline
dailymail
washingtonpost
marketwatch
reuters
cnet
latimes
informationweek
abc
newscientist
youtube
fox
cnn
sky
Total # Sources

c=1
945
598
542
134
373
449
361
349
299
98
218
119
208
354
298
475
0
216
74
175
6685

c=3
1907
1185
1250
619
819
783
667
769
571
271
586
296
446
529
439
548
436
342
229
335

c=10
2621
1785
1798
1885
1269
1169
1137
1070
956
825
1002
864
791
647
641
586
711
543
515
528

c=25
3242
2308
2326
2990
1723
1543
1640
1351
1434
1712
1348
1491
1106
778
872
609
1174
688
837
697

c=50
3826
2911
2766
3994
2228
1995
2261
1609
1939
2825
1647
2183
1438
944
1118
651
1535
816
1172
792

g=0.5
2133.93
1472.08
1444.62
1388.51
1055.44
999.27
974.46
871.94
831.74
828.19
768.77
724.49
640.25
580.56
570.68
548.74
532.47
438.12
426.54
414.45

Engine A’s own news service takes rank 4, which is surprising compared to business
and entertainment news. Yet it is still ranked very highly. Youtube is still in the top
20, at rank 17, but the other sources seem very reputable, including some science
specific news sources such as newscientist and cnet. The general news sites are also
very reputable. The number of distinct sources also increases significantly compared
to business and entertainment news.

84

C.1.4 Topic: Sport
ENGINE A
telegraph
ENGINE A
sky
dailymail
timesonline
bbc
nytimes
washingtonpost
guardian
goal
usatoday
sportinglife
latimes
independent
nwsource
dailyrecord
bleacherreport.com
espn
scout
philly
Total # Sources

c=1
720
251
538
289
297
192
309
256
194
71
228
37
128
90
109
198
31
45
12
122
3875

c=3
1495
1073
1175
675
505
451
577
365
317
197
353
134
322
202
198
242
80
126
58
194

c=10
1969
2093
1793
970
907
819
747
588
588
514
532
459
488
449
374
324
336
361
245
286

c=25
2494
2926
2504
1362
1361
1228
954
870
895
899
662
940
704
758
573
443
678
627
621
426

c=50
3019
3821
3299
1712
1830
1608
1141
1164
1152
1328
819
1350
865
1049
760
589
1053
924
1068
565

g=0.5
1662.19
1558.14
1526.23
826.86
783.15
675.78
640.89
532.68
500.62
444.74
440.5
410.38
404.1
385.02
314.61
311.27
308.71
303.85
280.08
263.26

Engine A’s own news service appears 2nd but with a lot more lower scored results
so that most of the results seem to be coming from Engine A. A large number of
sources in the top 20 (7) are heavily sport oriented news sources. However they
appear in the bottom half of the top 20.

85

C.1.5 Topic: World
ENGINE A
ENGINE A
bbc
youtube
timesonline
guardian
telegraph
reuters
dailymail
nytimes
washingtonpost
abc
welt
latimes
independent
fox
bloomberg
sky
cnn
ENGINE C
nwsource
Total # Sources

c=1
625
724
0
587
487
424
137
355
199
284
408
384
107
133
177
14
141
29
1
131
5459

c=3
2366
1352
1847
915
913
850
486
781
511
511
492
411
344
274
318
132
249
181
13
206

c=10
5511
2090
2454
1436
1476
1318
1194
996
905
806
733
577
649
514
540
474
450
425
147
346

c=25
8899
2748
3512
2012
2004
1744
1835
1231
1301
1139
1038
846
876
745
703
904
607
611
725
497

c=50
11589
3327
4340
2534
2503
2182
2524
1485
1756
1468
1337
1097
1187
1005
855
1300
726
796
1348
667

g=0.5
4319.46
1703.64
1690.53
1247.95
1217.84
1079.96
940.2
838.76
743.75
694.66
681.69
570.22
498.01
427.07
426.94
403.93
355.41
308.3
304.26
302.19

The predominance of Engine A’s own news service is overwhelming. With a gravity
measure at 0.5 of 2.5 times the 2nd ranking source, this is a clear indication of its
preference for itself. However it does not have the highest number of top spots.
The remainder of the results seem respectable general news sources, again with the
exception of the video sharing website youtube at rank 3, yet inexplicably again
without a single top spot. Engine C’s news source appears at number 19, primarily
from a large number of lower down ranked results.

86

C.2 Top 20: Engine B
C.2.1 Topic: Business
ENGINE B
ino
iii
bloomberg
bbc
telegraph
miamiherald
sfgate
nytimes
washingtonpost
nwsource
independent
fox
globeinvestor
baynews9
tiscali
fxstreet
syracuse
inrich
abc
centredaily
Total # Sources

c=1
40
84
28
327
483
68
243
241
220
177
307
185
13
3
97
24
10
14
101
2
2161

c=3
190
411
186
574
566
248
510
419
404
328
377
239
63
22
165
94
58
111
156
11

c=10
1185
1198
760
852
714
807
757
638
622
598
505
331
269
255
307
297
289
317
245
219

c=25
3143
2173
1776
1083
844
1285
927
773
778
830
651
463
655
740
494
541
586
489
387
593

c=50
5648
3170
3259
1264
950
1694
1093
867
959
1010
790
651
1259
1112
709
812
795
639
510
841

g=0.5
1408.26
1015.28
845.63
689.4
647.78
595.84
590.6
499
497.64
477.41
465.4
320.4216152
317.18
285.27
282.51
254.68
239.26
227.81
226.27
219.44

There is a noticable lack of Engine B’s own service which is a stark contrast to Engine
A’s business news. Many of the news services (6) are business-specific news sites
and they are mostly in the top half of the top 20.

87

C.2.2 Topic: Entertainment
ENGINE B
bbc
nwsource
usatoday
abc
dailymail
scotsman
miamiherald
mirror
latimes
independent
startribune
pantagraph
channel4
guardian
fox
kansascity
thestate
nydailynews
inrich
thewest
Total # Sources

c=1
146
126
39
51
36
83
10
13
95
65
24
18
58
37
63
9
18
51
10
5
1979

c=3
266
163
90
90
80
149
31
51
121
93
64
40
91
81
83
26
66
78
36
53

c=10
399
239
181
196
178
203
158
176
158
175
123
96
128
132
118
93
136
105
108
132

c=25
590
379
326
286
303
231
386
338
202
243
251
235
177
189
161
242
179
146
196
188

c=50
814
488
585
432
472
252
572
484
257
305
368
416
230
260
209
385
221
178
314
256

g=0.5
365.02
231.9
185.22
165.45
162.68
161.51
160.21
151.11
146.5
143.21
123.53
117.8
115.57
112.63
107.11
106.85
96.79
95.76
94.65
92.17

There is a smaller spread of news sources but still a smaller number of distinct news
sources, especially in comparison to Engine A. The news sources seem reputable
and do not include any surprises or even Engine B’s own news service.

88

C.2.3 Topic: Science
ENGINE B
bbc
telegraph
washingtonpost
guardian
nytimes
fox
newscientist
informationweek
dailymail
ENGINE B
independent
abc
reuters
latimes
iii
cnn
timesonline
bloomberg
blueridgenow
usatoday
Total # Sources

c=1
521
851
400
627
276
478
477
460
283
53
261
330
150
216
43
173
132
32
71
41
2674

c=3
969
1071
809
791
561
588
726
586
437
191
413
415
308
382
232
231
235
115
155
130

c=10
1456
1278
1350
970
994
720
769
652
690
643
659
536
626
498
576
358
395
316
405
308

c=25
1845
1432
1793
1108
1267
948
784
682
949
1250
848
672
794
588
775
483
496
588
530
511

c=50
2194
1569
2147
1296
1494
1169
807
727
1169
1869
1064
845
898
698
979
623
591
928
584
764

g=0.5
1165.8
1143.9
1059.94
872.86
748.28
696.11
672.17
587.48
586.811
575.31
542.95
497.45
441.83
413.88
387.49
317.63
307.97
287.23
267.21
263.17

Engine B’s own news service makes an appearance at rank 10. Some sources are
science-specific sources such as newscientist but they are not numerous even though
they are in the top 10. The remaining sources seem quite respectable news sources.

89

C.2.4 Topic: Sport
ENGINE B
dailymail
bbc
usatoday
telegraph
sportinglife
nytimes
cnn
timesonline
independent
4thegame
abc
washingtonpost
miamiherald
startribune
football365
nwsource
latimes
espn
tiscali
star-telegram
Total # Sources

c=1
549
363
481
558
120
288
171
194
162
21
141
235
10
132
9
131
147
45
85
22
1958

c=3
1096
820
703
699
347
493
348
264
251
95
267
288
54
259
68
207
268
157
162
176

c=10
1594
1476
1018
916
767
628
557
418
400
403
432
358
308
332
308
318
341
327
351
385

c=25
1953
2087
1316
1080
1249
703
827
563
581
845
533
419
723
459
705
498
392
517
477
510

c=50
2194
2446
1572
1204
1590
757
1089
662
701
1058
603
455
1101
608
1071
652
461
712
600
605

g=0.5
1249.23
1141.56
872.5
800.96
614.54
508.73
482.72
353.98
344.86
335.3
328.0599356
319.67
303.99
303.56
297.69
294.28
283.71
265.84
264.58
256.26

The top 20 includes 5 sport-specific news sites. The other sources seem like reasonable sources for news. Again Engine B’s own news service does not appear.

90

C.2.5 Topic: World
ENGINE B
bbc
lasvegassun
charlotteobserver
telegraph
dailymail
fox
thestate
boston
abc
miamiherald
kansascity
independent
timesonline
tiscali
syracuse
nytimes
theherald
bloomberg
washingtonpost
ENGINE B
Total # Sources

c=1
440
31
210
633
472
366
102
224
205
38
50
259
326
71
36
148
260
25
162
16
2214

c=3
926
313
746
804
703
607
594
458
303
145
154
412
420
220
100
301
345
100
317
89

c=10
1472
1693
1416
1055
951
919
1305
848
681
682
644
620
562
579
515
570
483
369
489
309

c=25
2001
2296
1752
1252
1298
1445
1623
1262
1087
1389
1278
845
712
1016
1076
743
644
835
615
727

c=50
2435
2641
1957
1443
1582
1926
1820
1557
1505
1921
1756
1023
844
1361
1472
933
714
1414
744
1381

g=0.5
1183.91
995.27
960.56
931.67
858.13
857.41
832.12
682.33
586.19
576.46
543.69
529.28
503.46
475.47
440.99
432.84
424.23
390.02
385.29
359.08

The majority of the news sources seem like reputable general news sources. A few
seem to be USA city or county news sources such as the Charlotte Observer, which
also have news from further afield. Engine B’s own news service makes it at rank
20. The differences in gravity score from top 1 to top 20 are not that large.

91

C.3 Top 20: Engine C
C.3.1 Topic: Business
ENGINE C
bbc
guardian
reuters
timesonline
independent
telegraph
startribune
economist
ENGINE C
wsj
abc
nj
theatlantic
dailymail
bloomberg
scotsman
washingtonpost
theglobeandmail
wired
nwsource
Total # Sources

c=1
508
493
349
384
208
197
2
181
38
35
48
54
18
41
7
40
75
4
22
55
2014

c=3
1054
1025
731
629
415
316
294
353
157
112
135
136
70
120
57
138
143
29
91
104

c=10
1686
1498
1233
1100
724
566
647
539
407
322
302
311
302
325
248
332
226
163
220
201

c=25
1970
1619
1577
1363
897
820
858
594
679
620
563
511
541
513
519
420
344
442
403
356

c=50
2007
1636
1853
1390
976
855
1002
594
864
968
762
658
696
553
738
440
444
758
547
501

g=0.5
1209.85
1074.27
938.94
807.41
530.04
445.45
415.78
384.68
316.1
297.25
267.6
250.96
229.9
227.81
216.48
208.51
199.64
194.26
188.05
187.54

There are some (3) business-specific news sources. Engine C’s own news service
appears at rank 9. The other news sources also seem quite reputable too. The upper
few sources seem to contain the largest majority of all the results.

92

C.3.2 Topic: Entertainment
ENGINE C
guardian
reuters
bbc
timesonline
telegraph
latimes
dailymail
independent
thesun
startribune
fox
ENGINE B
nj
abc
ew
scotsman
nwsource
ENGINE C
nydailynews
ananova
Total # Sources

c=1
200
190
171
71
38
61
32
28
45
15
44
6
16
24
11
20
39
7
39
43
1909

c=3
359
312
308
163
105
113
89
75
86
55
80
47
43
54
36
57
49
37
56
70

c=10
625
494
521
364
310
199
266
189
162
161
123
126
94
102
115
118
89
88
79
97

c=25
727
611
655
489
424
340
395
310
214
241
165
218
198
180
209
197
153
162
110
106

c=50
751
703
665
510
451
484
429
363
217
293
234
329
318
277
289
218
223
250
176
128

g=0.5
444.55
392.51
383.76
245.62
196.18
186.4
179.3
144.26
116.66
112.93
107.77
104.63
99.44
98.31
94.65
90.94
86.87
79.29
78.48
78.1

The most odd fact about this table is that Engine B’s news source appears at rank
12 whereas Engine C’s own news service appears at rank 18. There is a very thin
spread of the scores so that top 1 and top 20 are not that far away score-wise. The
results seem like reputable news sources.

93

C.3.3 Topic: Science
ENGINE C
guardian
bbc
cnet
wired
telegraph
timesonline
dailymail
independent
latimes
abc
reuters
scotsman
fox
newscientist
ENGINE C
zdnet
wsj
theatlantic
nj
washingtonpost
Total # Sources

c=1
1647
952
252
210
551
476
550
340
203
220
457
161
269
370
65
89
39
44
64
170
2631

c=3
2742
1991
908
759
912
824
840
614
402
435
706
332
525
535
235
217
173
206
216
267

c=10
3634
3485
2287
2153
1532
1357
1216
1088
901
893
917
730
652
611
634
559
508
568
522
458

c=25
3985
4263
3490
3475
1957
1766
1475
1449
1566
1460
1083
1097
815
656
1051
1035
994
970
855
698

c=50
4102
4561
4205
4334
2132
1931
1558
1642
2320
2043
1210
1275
967
691
1489
1362
1536
1231
1122
938

g=0.5
2857.56
2494.61
1660.69
1612.31
1177.62
1047.2
971.7
830.84
818.64
777.04
769.94
559.13
557.24
530.29
510.74
479
470.16
434.92
413.7419308
408.97

Again science seemed to have quite a lot more distinct news sources than the others.
Engine C’s own service appears at rank 15, which is quite a bit down especially
compared to Engine A’s self-preference. There are some (4) science-specific news
sites such as newscientist. The difference between top 1 and top 20 is quite large but
not over-the-top.

94

C.3.4 Topic: Sport
ENGINE C
bbc
sky
telegraph
guardian
espn
timesonline
startribune
football365
philly
independent
dailymail
nwsource
usatoday
reuters
bleacherreport.com
teamtalk
sportinglife
latimes
cnn
ENGINE C
Total # Sources

c=1
410
628
439
247
108
247
132
88
220
90
125
92
147
132
21
63
48
87
74
46
1778

c=3
815
823
704
516
285
426
323
264
279
214
198
187
210
253
85
177
160
140
122
128

c=10
1339
1041
1078
875
667
649
552
521
352
411
416
337
313
322
237
321
320
228
220
247

c=25
1588
1151
1303
1050
1103
766
702
624
437
576
584
485
403
359
515
380
405
328
343
370

c=50
1614
1170
1338
1067
1341
789
788
637
531
664
609
585
482
391
743
383
419
437
479
482

g=0.5
961.19
878.3
830.7
620.96
527.36
489.36
406.67
338.86
324.87
306.64
305.77
267.77
266.92
259.08
231.86
213.82
208.86
198.61
197.91
195.7

There are (7) sport-specific news sites which is similar to the other 2 engines. The
other news sources seem like reputable sources. Again, Engine C is in the upper 20
at rank 20.

95

C.3.5 Topic: World
ENGINE C
guardian
bbc
reuters
timesonline
telegraph
startribune
dailymail
independent
fox
ENGINE C
abc
alertnet
nwsource
ENGINE B
newsweek
sky
salon
sfgate
economist
cnn
Total # Sources

c=1
1591
773
494
488
319
33
253
238
189
47
77
101
83
7
47
96
6
20
92
39
2153

c=3
2391
1312
1139
788
582
330
460
446
357
206
222
328
183
127
179
162
99
108
217
89

c=10
2965
1860
1755
1212
1090
987
827
845
676
596
550
563
375
356
354
340
356
332
346
231

c=25
3129
2072
2084
1346
1432
1453
1096
1066
999
930
889
603
584
672
500
488
582
546
379
477

c=50
3144
2083
2331
1352
1466
1666
1131
1143
1239
1209
1152
603
744
930
607
544
753
733
381
761

g=0.5
2410.76
1414.4
1301.37
893.85
790.73
642.7
614.4
607.21
547.09
436.62
431.59
361.34
303.33
297.47
259.71
258.21
255.24
249.52
236.57
236.41

The top result has a large difference between the other results (1.7 times the second
ranked result). Engine C’s news source appears at rank 10 and Engine B’s news
source appears at 14. The results look like reputable news sources.

96

Appendix D
Topical Accuracy Results
D.1 The Topical Accuracy of Engine A
D.1.1 Topic: Business
• ENGINE A: campaigns, news, authorities, antibiotics, opensource, congressional, comfort, parliament, national, finance.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• bloomberg: mitsubishi, financings, scoldings, sarin, nationalisation, weakens,
tumbles, punctual, worsens, caution.
• reuters: trumping, sweetens, capsizes, dampens, bondholders, recapitalisation, revamps, enablement, slumps, nationalisation.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.

D.1.2 Topic: Entertainment
• ENGINE A: campaigns, news, authorities, antibiotics, opensource, congressional, comfort, parliament, national, finance.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• latimes: missive, mishmash, outposts, prelate, homicides, baldy, barbies, dispensary, sharia, caltech.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• nytimes: wackiness, fudging, jazzmen, morphosis, instrangent, venezuelans,
squalor, optimists, capitualtion, unease.

97

D.1.3 Topic: Science
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• ENGINE A: campaigns, news, authorities, antibiotics, opensource, congressional, comfort, parliament, national, finance.
• nytimes: wackiness, fudging, jazzmen, morphosis, instrangent, venezuelans,
squalor, optimists, capitualtion, unease.

D.1.4 Topic: Sport
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• ENGINE A: campaigns, news, authorities, antibiotics, opensource, congressional, comfort, parliament, national, finance.
• sky: fanciable, serialisation, crapland, waxworks, arsonists, baftas, nationalisation, personalise, philanthropist, axed.
• dailymail: snoozily, bloodfest, idiotically, shambolic, watermills, vertiginous,
deporting, officious, capsizes, oligarch.
• timesonline: ministers, technology, politics, local, above, greenie, causes, intended, questions, personal.

D.1.5 Topic: World
• ENGINE A: campaigns, news, authorities, antibiotics, opensource, congressional, comfort, parliament, national, finance.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• youtube: oxfam, aardman, auditioning, eastenders, apes, defends, gracious,
topgear, comedian, gambling.
• timesonline: ministers, technology, politics, local, above, greenie, causes, intended, questions, personal.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.

98

D.2 The Topical Accuracy of Engine B
D.2.1 Topic: Business
• ino: markets, downtrends, uptrends, oil, soybeans, rallying, confines, mercantile, usdgbp, fertiliser.
• iii: quotes, cattle, reassurances, nationalisation, shares, isas, barclays, aviva,
lloyds, intercept.
• bloomberg: mitsubishi, financings, scoldings, sarin, nationalisation, weakens,
tumbles, punctual, worsens, caution.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.

D.2.2 Topic: Entertainment
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• nwsource: ageless, fleas, haircuts, rejuvination, consignment, reacted, clementines, sleepwear, tweed, horseshoes.
• usatoday: esquenazi, altercations, regretful, bigwigs, tatters, peacemaker, overshadow, soaks, mistooks.
• abc: incessant, motherhood, stems, uncover, friction, oscars, makeover, housewives, nominations, premieres.
• dailymail: snoozily, bloodfest, idiotically, shambolic, watermills, vertiginous,
deporting, officious, capsizes, oligarch.

D.2.3 Topic: Science
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• washingtonpost: completed, academic, orbital, research, excited, employees,
execution, prehistoric, issues, latest.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• nytimes: wackiness, fudging, jazzmen, morphosis, instrangent, venezuelans,
squalor, optimists, capitualtion, unease.
99

D.2.4 Topic: Sport
• dailymail: snoozily, bloodfest, idiotically, shambolic, watermills, vertiginous,
deporting, officious, capsizes, oligarch.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• usatoday: esquenazi, altercations, regretful, bigwigs, tatters, peacemaker, overshadow, soaks, mistooks.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• sportinglife: tipster, punters, shareholding, sceptical, humiliating, greyhounds,
billionaire, boosted, scorecard, scouting.

D.2.5 Topic: World
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• lasvegassun: scrumpdelicious, dingers, jeopordises, misconceptions, titleholder,
bondholders, blanketed, welterweight, wranglers, hoisted.
• charlotteobserver: uncc, homophobes, mudslides, blinders, presbytarians, rebuilds, decaf, stuns, sprained, crutches.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• dailymail: snoozily, bloodfest, idiotically, shambolic, watermills, vertiginous,
deporting, officious, capsizes, oligarch.

D.3 The Topical Accuracy of Engine C
D.3.1 Topic: Business
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• timesonline: ministers, technology, politics, local, above, greenie, causes, intended, questions, personal.
• reuters: trumping, sweetens, capsizes, dampens, bondholders, recapitalisation, revamps, enablement, slumps, nationalisation.
• independant: scrutinises, cyberchondria, misjudgement, basks, woodworm,
somerfield, deportations, uncultivated, linesman, expletive.
100

D.3.2 Topic: Entertainment
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• reuters: trumping, sweetens, capsizes, dampens, bondholders, recapitalisation, revamps, enablement, slumps, nationalisation.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• timesonline: ministers, technology, politics, local, above, greenie, causes, intended, questions, personal.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.

D.3.3 Topic: Science
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• cnet: reshuffles, giggles, devouring, lightest, compusa, lumps, backlit, humps,
cellphones, realises.
• wired: geekmates, herpicide, geekteen, zombification, philematology, funcrusher,
remixable, flyswat, contextualises, pluripotency.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.

D.3.4 Topic: Sport
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• sky: fanciable, serialisation, crapland, waxworks, arsonists, baftas, nationalisation, personalise, philanthropist, axed.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• espn: gamecast, sportsnation, jaunt, likeable, intimidated, scripted, behaving,
dunks, scrum, wnba.

101

D.3.5 Topic: World
• guardian: reprocessor, astrobiologist, disbands, colonising, dampens, chinks,
liveliest, opportunism, catwalks, commoner.
• bbc: television, radio, channels, cultures, languages, sport, football, learning,
english, british.
• timesonline: ministers, technology, politics, local, above, greenie, causes, intended, questions, personal.
• telegraph: deportations, dodecanese, rfu, peleponnese, usurp, nationalise, dolomites,
cropper, carnivals, ionian.
• reuters: trumping, sweetens, capsizes, dampens, bondholders, recapitalisation, revamps, enablement, slumps, nationalisation.

102

Appendix E
Survey Questionnaire
The following questionnaire is a copy of the web questionnaire that was used to provide evidence against the confirmation bias of users of search engines. In page 4 the
full tables and bias profiles are removed as they are duplicates of the work detailed
in the specific sections shown at the appropriate part of the questionnaire.
Page 0 - Declaration of Acceptance

The purpose of this questionnaire is to find out how reputable people find the results
of a set of three web search engines. You will be first asked some simple questions
to give us your demographic group. You will be asked to list up to your favourite
five search engines. These should be in order of first being your most liked search
engine and fifth (or last) being your least favourite of your provided search engines.
After this you will be asked some questions on your usage of the web. Then you will
be asked to rate how reliable/reputable you view each of the three search engines
under study with regards to their results. You will then be shown the results of a
recent experiment, which tracked the preferences of the three search engines under
study. You will again be asked to rate all three engines using the same scale but with
new knowledge of how the search engines act.
• No names, addresses or contact details will be requested.
• If you wish to contact myself (Ciaran) you may do so using my email address
owensc AT dcs.gla.ac.uk or alternatively you may contact my supervisor (Leif)
through his departmental email address leif AT dcs.gla.ac.uk.
• Only continue on if you agree to your anonymous views being used in the
outcome of this survey. Do so by following the hyperlink below.
To Page 1 - Demographic Questions
NEW PAGE
Page 1 - Demographics
Please select your gender:
103

Male
Female

Please select your age range:
18 - 22
23 - 27
28 - 32
32 - 36
37+

Next Page 2 - Web Usage Questions
NEW PAGE
Page 2 - Web Usage Questions
Primarily (i.e. answer one) what do you use the web for?
Business
Entertainment
Information
News
Shopping
Work
Please give up to five of your favourite web search engines in the order which you
prefer them:

How often do you use search engines?
Daily
Weekly
Monthly

If you answered daily to the previous question, how many times on average do you
use any search engine per day?
N/A
1-5
6 - 10
11+
104

Please rank how you view each search engine’s reputation for reliable results using
the scale provided:
Google (www.google.com)
very unreliable unreliable somewhat reliable reliable very reliable
Yahoo (www.yahoo.com)
very unreliable unreliable somewhat reliable reliable very reliable
MSN Live (www.live.com)
very unreliable unreliable somewhat reliable reliable very reliable

Next Page 3 - Search Engine Bias
NEW PAGE
Page 3 - Search Engine Bias
Below are tables showing the top 20 news sources for Google, Yahoo! and MSN
Live based on their results for world news. The numbers in the columns labelled
c=1 to c=50 mean that for c (the cut-off) then from all of the results, this news source
appeared within the top c number of results this many times. The g column takes
account of the top results being much much more influentual than lower down results in tests. An example is for Google, the BBC news source appears 3327 times in
the top 50 results over all of the queries that were sent in this experiment.

What you are asked to do is look over these tables and use your own judgement as
to whether this influences your view of the results of each search engine.
WORLD NEWS TOP 20
The next part is called the bias profile, which contains 4 numbers for each search
engine. These numbers correspond to:
• The political bias of the search engine. A L suffix means left/liberal bias and a
R suffix means conservative/right bias. The closer the number is to 0 the more
neutral the engine is.
• Topical accuracy of the search engine. This means how close the results of the
search engine are well regarded in relation to the topics that are being searched
for. The closer to 1, the better the topical accuracy.
• How much a search engine prefers UK over USA news sources. A UK suffix
means that the engine prefers UK sources and a USA suffix means an engine
prefers USA sources. The closer to 0 means more neutral.
105

• How much a search engine prefers its own news service. The closer to 1, the
higher preference for its own service.
BIAS PROFILES
Please rank how you view each search engine’s reputation for reliable results using
the scale provided:
Google (www.google.com)
very unreliable unreliable somewhat reliable reliable very reliable
Yahoo (www.yahoo.com)
very unreliable unreliable somewhat reliable reliable very reliable
MSN Live (www.live.com)
very unreliable unreliable somewhat reliable reliable very reliable

Next Page 4 - Thank you for participating
NEXT PAGE
Page 4 - Thank you for your participation

Thank you for participating in this survey. If you have any questions please contact
myself (Ciaran) using my email address owensc AT dcs.gla.ac.uk or alternatively
you may contact my supervisor (Leif) through his departmental email address leif
AT dcs.gla.ac.uk.
Google
Yahoo!
MSN Live

106

Appendix F
Acknowledgements
Thank you to my supervisor Dr. Leif Azzopardi for convincing me to do this project
and for supporting me throughout it.
Thank you to all survey participants, your 15 minutes of effort was very much appreciated.
Thank you to www.subcity.org, the University of Glasgow radio station, for hosting
the questionnaire.
Thank you to my fellow computing students, whether MSci, MSc, MRes or BSc(Hons),
who helped me through the whole course.
Thank you to all of my friends who made me do this and always supported me,
especially Gordon, Chris, Caggy, Roo, Fiona, Aaron, Theo and Hannah.

107

