journal
of
artificial
intelligence
research
submitted
submitted
11
21
published
framework
for
fairness
systematic
review
of
existing
fair
ai
solutions
brianna
richardson
juan
gilbert
richardsonb@ufl.edu
juan@ufl.edu
arxiv
2112
05700v1
cs
ai
10
dec
2021
university
of
florida
gainesville
fl
32601
usa
abstract
in
world
of
daily
emerging
scientific
inquisition
and
discovery
the
prolific
launch
of
machine
learning
across
industries
comes
to
little
surprise
for
those
familiar
with
the
potential
of
ml
neither
so
should
the
congruent
expansion
of
ethics-focused
research
that
emerged
as
response
to
issues
of
bias
and
unfairness
that
stemmed
from
those
very
same
applications
fairness
research
which
focuses
on
techniques
to
combat
algorithmic
bias
is
now
more
supported
than
ever
before
large
portion
of
fairness
research
has
gone
to
producing
tools
that
machine
learning
practitioners
can
use
to
audit
for
bias
while
designing
their
algorithms
nonetheless
there
is
lack
of
application
of
these
fairness
solutions
in
practice
this
systematic
review
provides
an
in-depth
summary
of
the
algorithmic
bias
issues
that
have
been
defined
and
the
fairness
solution
space
that
has
been
proposed
moreover
this
review
provides
an
in-depth
breakdown
of
the
caveats
to
the
solution
space
that
have
arisen
since
their
release
and
taxonomy
of
needs
that
have
been
proposed
by
machine
learning
practitioners
fairness
researchers
and
institutional
stakeholders
these
needs
have
been
organized
and
addressed
to
the
parties
most
influential
to
their
implementation
which
includes
fairness
researchers
organizations
that
produce
ml
algorithms
and
the
machine
learning
practitioners
themselves
these
findings
can
be
used
in
the
future
to
bridge
the
gap
between
practitioners
and
fairness
experts
and
inform
the
creation
of
usable
fair
ml
toolkits
introduction
today
applications
of
machine
learning
ml
and
artificial
intelligence
ai
can
be
found
in
nearly
every
domain
jordan
mitchell
2015
from
medicine
and
healthcare
goldenberg
et
al
2019
chang
et
al
2018
lin
et
al
2020
munavalli
et
al
2020
to
banking
and
finance
sunikka
et
al
2011
choi
lee
2018
moysan
zeitoun
2019
at
rate
that
has
grown
exponentially
over
the
last
decade
companies
are
seizing
the
opportunity
to
automate
and
perfect
procedures
in
the
field
of
healthcare
machine
learning
is
being
used
to
diagnose
and
treat
prostate
cancer
goldenberg
et
al
2019
perform
robotic
surgeries
chang
et
al
2018
organize
and
schedule
patients
munavalli
et
al
2020
and
digitize
electronic
health
record
data
lin
et
al
2020
within
banking
and
finance
machine
learning
is
being
used
to
personalize
recommendations
for
consumers
sunikka
et
al
2011
detect
and
prevent
instances
of
fraud
choi
lee
2018
and
provide
faster
and
personalized
services
through
the
use
of
chatbots
moysan
zeitoun
2019
while
machine
learning
is
praised
for
its
ability
to
speed
up
time-consuming
processes
automate
mundane
procedures
and
improve
accuracy
and
performance
of
tasks
it
is
also
praised
for
its
ability
to
remain
neutral
and
void
of
human
bias
this
is
what
sandvig
submitted
ai
access
foundation
all
rights
reserved
richardson
gilbert
2014
calls
the
neutrality
fallacy
which
is
the
common
misconception
that
ai
does
not
perpetuate
the
trends
of
its
data
which
is
often
clouded
with
human
biases
for
this
reason
machine
learning
continues
to
undergo
heavy
scrutiny
for
the
role
it
plays
in
furthering
social
inequities
over
the
past
few
years
major
companies
have
been
headlined
for
their
ai
technologies
that
cause
harms
against
consumers
buolamwini
and
gebru
2018
assessed
commercially-used
facial
recognition
systems
and
found
that
darker-skinned
women
were
the
most
misclassified
demographic
group
noble
2018
found
that
search
engines
perpetuated
stereotypes
and
contributed
substantially
to
representational
harms
and
angwin
et
al
2016
found
biases
in
automated
recidivism
scores
given
to
criminal
offenders
circumstances
such
as
these
have
led
to
surplus
of
contributions
to
the
solution
space
for
algorithmic
bias
by
stakeholders
and
researchers
alike
zhong
2018
institutions
have
been
quick
to
formulate
their
own
ethics
codes
centered
around
concepts
like
fairness
transparency
accountability
and
trust
jobin
et
al
2019
new
conferences
like
acm
fairness
accountability
and
transparency
facct
conference1
and
new
workshops
have
emerged
centered
on
these
concepts
furthermore
in
most
machine
learning
conferences
new
tracks
have
been
added
that
focus
on
algorithmic
bias
research
this
surplus
of
recognition
by
stakeholders
and
computer
science
researchers
has
encouraged
commensurate
rise
in
related
contributions
to
improve
the
ethical
concerns
surrounding
ai
one
major
objective
for
responsible
ai
researchers
is
the
creation
of
fairness
tools
that
ml
practitioners
across
domains
can
use
in
their
application
of
responsible
and
ethical
ai
these
tools
translate
top-tier
research
from
the
responsible
ai
space
into
actionable
procedures
or
functions
that
practitioners
can
implement
into
their
pipelines
despite
the
number
of
currently
existing
tools
there
is
still
lack
of
application
of
ethical
ai
found
in
industry
recent
research
suggests
disconnect
between
the
fairness
ai
researchers
creating
these
tools
and
the
ml
practitioners
who
are
meant
to
apply
them
holstein
et
al
2019
law
et
al
2020b
madaio
et
al
2020
law
et
al
2020a
veale
binns
2017
greene
et
al
2019
emphasizes
that
the
solution
to
this
problem
lies
in
the
intersection
of
technical
and
design
expertise
responsible
and
fair
ai
must
undergo
deliberate
design
procedures
to
match
the
needs
of
practitioners
and
satisfy
the
technical
dilemmas
found
in
bias
research
this
survey
paper
will
highlight
users
expectations
when
engaging
with
fair
ai
tooling
and
it
will
summarize
both
fairness
concerns
and
design
flaws
discussed
in
literature
first
section
focuses
on
algorithmic
bias
and
the
major
entry
points
for
bias
that
perpetuate
the
need
for
fairness
research
and
fairness
tools
section
focuses
on
the
solutions
that
have
been
posed
thus
far
by
fairness
researchers
it
will
highlight
major
contributions
and
the
features
offered
by
them
section
will
focus
on
how
these
solutions
are
working
in
practice
it
will
highlight
many
of
the
drawbacks
and
provide
recommendations
for
fairness
experts
organizations
and
ml
practitioners
this
paper
is
the
first
of
its
kind
to
provide
comprehensive
review
of
the
solution
space
of
fair
ai
the
problem
space
algorithmic
bias
the
first
recorded
case
of
algorithmic
bias
was
the
discrimination
suit
that
was
filed
against
st
george
hospital
medical
school
in
the
1980
lowry
macpherson
1988
leaders
of
the
admission
program
had
decided
to
create
an
algorithm
that
could
mimic
the
admission
acm
facct
conference
https://facctconference.org/
framework
for
fairness
figure
taxonomy
of
the
currently
existing
forms
of
bias
process
by
completing
the
first
screening
of
applicants
upon
completion
of
the
algorithm
they
validated
its
performance
by
comparing
its
results
to
manually
generated
decisions
and
found
90
95
similarity
after
few
years
in
circulation
staff
began
to
notice
trends
in
admission
and
brought
their
concerns
to
the
attention
of
the
school
internal
review
board
irb
when
the
irb
agreed
that
the
correlation
between
machine
scores
and
human
scores
delegitimized
claims
of
bias
those
claims
were
taken
to
the
u.k
commission
for
racial
equity
after
thorough
analysis
of
the
algorithm
the
commission
confirmed
these
claims
were
true
the
algorithm
was
placing
value
on
applicant
names
and
place
of
birth
penalizing
individual
with
non-caucasian
sounding
names
lowry
macpherson
1988
the
ruling
of
this
committee
was
pivotal
to
the
start
of
algorithmic
bias
research
since
it
set
precedent
that
the
inclusion
of
bias
in
system
even
for
the
sake
of
accuracy
was
impermissible
nonetheless
for
the
past
40
years
this
issue
has
been
recurring
biases
continue
to
emerge
in
the
creation
and
use
of
machine
learning
legitimizing
unfair
and
biased
practices
across
multitude
of
industries
lum
isaac
2016
in
the
often-cited
2017
presentation
at
neurips
crawford
2017
discusses
two
potential
harms
from
algorithmic
bias
representational
and
allocative
harms
representational
harms
are
problems
that
might
arise
from
the
troublesome
ways
certain
populations
are
represented
in
the
feature
space
and
allocative
harms
are
problems
that
arise
from
how
decisions
are
allocated
to
certain
populations
crawford
2013
literature
has
demonstrated
that
bias
can
arise
in
all
shapes
and
forms
and
the
task
for
fairness
experts
is
to
organize
and
confront
those
biases
richardson
gilbert
friedman
and
nissenbaum
1996
separates
types
of
biases
into
three
categories
preexisting
technical
and
emergent
biases
they
define
pre-existing
biases
as
those
that
are
rooted
in
institutions
practices
and
attitudes
technical
biases
are
those
that
stem
from
technical
constraints
or
issues
stemming
from
the
technical
design
of
the
algorithm
lastly
emergent
biases
arise
in
the
context
of
real
use
by
the
user
friedman
nissenbaum
1996
this
section
will
adopt
this
categorization
to
classify
influences
of
bias
found
in
literature
visual
depiction
of
the
forms
of
biases
that
will
be
explained
can
be
seen
in
figure
2.0
pre-existing
bias
large
portion
of
literature
attention
has
gone
to
pre-existing
biases
these
are
biases
that
are
associated
with
an
individual
or
institutions
when
human
preferences
or
societal
stereotypes
influence
the
data
and
or
the
model
that
is
said
to
be
the
effect
of
pre-existing
bias
the
saying
garbage
in
garbage
out
is
well-known
in
the
data
science
community
as
euphemism
for
the
quality
of
data
you
give
system
will
be
the
quality
of
your
results
fairness
researchers
translate
this
to
mean
that
machine
learning
models
serve
as
feedback
loop
reflecting
the
biases
it
has
been
fed
barocas
et
al
2019
bias
can
come
from
an
assortium
of
stages
in
the
machine
learning
pipeline
the
most
prominently
discussed
form
of
bias
is
historical
bias
or
biases
that
are
perpetuated
in
data
and
are
the
result
of
issues
that
existed
at
the
time
veale
binns
2017
calders
liobaite
2013
olteanu
et
al
2019
barocas
et
al
2019
rovatsos
et
al
2019
suresh
guttag
2019
guszcza
2018
hellstro
et
al
2020
suresh
and
guttag
2019
defines
historical
biases
as
those
that
arise
out
of
the
misalignment
between
the
world
and
its
values
and
what
the
model
perpetuates
the
impact
of
historical
bias
can
be
reflected
in
what
data
was
collected
how
the
data
was
collected
the
quality
of
the
data
and
even
the
labels
given
to
the
samples
suresh
guttag
2019
rovatsos
et
al
2019
barocas
selbst
2018
calders
liobaite
2013
hellstro
et
al
2020
the
subjectivity
of
labels
is
major
concern
because
it
reproduces
and
normalizes
the
intentional
or
unintentional
biases
of
the
original
labeler
barocas
selbst
2018
kasy
abebe
2021
from
analyzing
large
collection
of
ml
publications
geiger
et
al
2019
found
dire
need
for
more
normalized
and
rigorous
standards
for
evaluating
datasets
and
data
collection
strategies
prior
to
any
machine
learning
processing
aligning
with
the
recommendations
from
crawford
2013
besides
the
historical
biases
there
also
exists
data
collection
biases
or
biases
that
stem
from
the
selection
methods
used
for
data
sources
olteanu
et
al
2019
data
collection
bias
umbrellas
variety
of
different
biases
recognized
in
literature
including
sampling
bias
and
representation
bias
sampling
bias
consists
of
issues
that
arise
from
the
non-random
sampling
of
subgroups
that
might
lead
to
the
under
or
over-sampling
of
certain
populations
mehrabi
et
al
2021
hellstro
et
al
2020
historically
vulnerable
populations
have
often
been
undersampled
veale
binns
2017
mester
2017
discusses
two
types
of
sampling
bias
selection
bias
and
self-selection
bias
while
selection
bias
focuses
on
the
biases
of
the
data
collector
self-selection
bias
stems
from
which
individuals
were
available
and
willing
to
participate
in
the
data
collection
process
mester
2017
concerns
about
sampling
bias
stem
from
the
fact
that
inferences
made
from
mis-balanced
dataset
will
inevitably
lead
to
the
disadvantage
of
populations
who
were
missampled
asaro
2019
framework
for
fairness
representation
bias
also
called
data
bias
olteanu
et
al
2019
is
another
form
of
data
collection
bias
and
consists
of
bias
that
arises
from
the
insufficient
representation
of
subgroups
within
the
dataset
crawford
2017
suresh
guttag
2019
this
misrepresentation
can
stem
from
the
subjective
selection
of
unfitting
attributes
or
the
incomplete
collection
of
data
calders
liobaite
2013
veale
and
binns
2017
describes
this
issue
in
detail
stating
that
subgroups
can
contain
nuanced
patterns
that
are
difficult
or
impossible
to
quantify
in
dataset
resulting
in
models
that
misrepresent
those
populations
another
prominent
form
of
pre-existing
bias
is
data
processing
bias
or
bias
that
is
introduced
by
data
processing
procedures
olteanu
et
al
2019
the
selection
of
methods
across
the
ml
pipeline
can
be
incredibly
subjective
and
many
of
these
steps
can
introduce
unintended
biases
into
the
model
for
example
the
use
of
sensitive
attributes
within
models
is
well-known
taboo
in
data
science
government
regulation
has
even
gone
as
far
as
prohibiting
organizations
access
to
protected
data
veale
binns
2017
nonetheless
the
use
of
proxies
or
attributes
that
encode
sensitive
information
is
prolific
calders
liobaite
2013
corbett-davies
goel
2018
besides
the
use
of
sensitive
data
other
processing
steps
could
incorporate
bias
including
the
categorization
of
data
veale
binns
2017
feature
selection
and
feature
engineering
veale
binns
2017
barocas
selbst
2018
or
even
how
data
is
evaluated
suresh
guttag
2019
evaluation
bias
can
occur
from
the
use
of
performance
metrics
that
are
ill-fitting
for
the
given
model
suresh
guttag
2019
the
final
category
of
pre-existing
bias
is
systematic
bias
or
bias
that
stems
from
institutional
or
governmental
regulations
and
procedures
while
regulations
limiting
access
to
sensitive
data
can
be
beneficial
for
limiting
use
of
sensitive
data
they
can
also
prevent
the
identification
and
de-biasing
of
proxy
attributes
leading
to
problematic
models
veale
binns
2017
funding
can
also
produce
bias
by
manipulating
the
priorities
of
institutions
and
therefore
practitioners
mester
2017
2.0
technical
bias
while
pre-existing
biases
consider
what
the
model
is
adopting
from
the
data
technical
biases
consider
the
limitations
of
computers
and
how
technology
insufficiencies
might
create
bias
friedman
nissenbaum
1996
baeza-yates
2018
considers
this
algorithmic
bias
and
defines
it
as
bias
that
was
not
present
in
the
data
but
was
added
by
the
algorithm
overlapping
on
pre-existing
bias
technical
bias
can
also
stem
from
processing
procedures
often
the
selection
of
features
models
or
training
procedures
can
introduce
bias
that
is
not
affiliated
with
the
practitioner
but
with
the
insufficiency
of
those
procedures
to
characterize
the
data
for
example
hyperparameter
tuning
may
in
an
attempt
to
reduce
the
sparsity
of
the
model
end
up
removing
distinct
patterns
found
in
sub-populations
veale
binns
2017
hellstro
et
al
2020
furthermore
certain
models
such
as
regression
fail
to
capture
the
correlation
between
attributes
and
subgroups
veale
binns
2017
skitka
et
al
1999
computer
tools
bias
is
another
form
of
technical
bias
that
originates
from
the
limitations
of
statistics
and
technology
friedman
nissenbaum
1996
simpson
paradox
is
statistical
issue
that
arises
from
the
aggregation
of
distinct
subgroup
data
that
produces
misperformance
for
one
or
all
subgroups
blyth
1972
suresh
guttag
2019
richardson
gilbert
lastly
friedman
and
nissenbaum
1996
defines
formulation
of
human
constructs
bias
as
that
which
stems
from
the
inability
of
technology
to
properly
grasp
human
constructs
in
situations
such
as
this
fairness
experts
often
insist
that
practitioners
or
organizations
reconsider
the
use
of
machine
learning
rakova
et
al
2020
selbst
et
al
2019
describes
the
effects
of
this
bias
as
the
ripple
effect
trap
where
they
describe
how
the
use
of
technology
in
areas
where
human
constructs
bias
exists
can
potentially
change
human
values
2.0
emerging
bias
this
final
category
of
bias
referred
to
by
suresh
and
guttag
2019
as
deployment
bias
is
defined
as
biases
that
result
from
the
deployment
of
model
emerging
bias
can
emerge
in
two
different
forms
via
population
bias
and
use
bias
population
bias
stems
from
the
insufficiency
of
model
to
represent
its
population
or
society
after
deployment
olteanu
et
al
2019
this
can
stem
from
new
knowledge
that
has
emerged
and
made
model
irrelevant
friedman
nissenbaum
1996
or
mismatch
between
the
sample
population
used
to
train
the
model
and
the
population
who
is
impacted
by
its
deployment
friedman
nissenbaum
1996
calders
liobaite
2013
rovatsos
et
al
2019
selbst
et
al
2019
refers
to
this
failure
to
consider
how
model
works
in
context
as
the
portability
trap
use
bias
umbrellas
variety
of
different
ways
in
which
the
use
of
models
can
create
biases
in
study
conducted
by
skitka
et
al
1999
participants
interacted
with
an
autonomous
aid
which
provided
recommendations
for
simulated
flight
task
the
results
depicted
trends
of
commission
where
user
agreed
with
recommendation
even
when
it
contradicted
their
training
skitka
et
al
1999
the
neutrality
fallacy
introduced
by
sandvig
2014
is
major
pitfall
to
ai
application
because
it
forces
users
to
believe
the
tool
is
correct
even
when
it
should
be
obvious
that
it
is
not
sandvig
2014
skitka
et
al
1999
suggests
that
users
would
be
less
willing
to
challenge
decision
if
that
decision
was
made
with
an
algorithm
related
form
of
use
bias
is
presentation
bias
or
bias
that
stems
from
how
model
is
deployed
baeza-yates
2018
in
these
situations
users
make
assumptions
about
the
presentation
format
of
algorithms
and
may
come
to
incorrect
conclusions
for
example
in
search
algorithm
many
users
attribute
the
rank
of
the
result
to
the
relevancy
which
may
not
be
accurate
in
all
cases
baeza-yates
2018
belief
bias
is
also
form
of
use
bias
that
occurs
when
someone
is
so
sure
of
themselves
they
ignore
the
results
mester
2017
this
can
also
impact
when
they
choose
to
use
the
results
of
model
and
when
they
choose
to
ignore
them
veale
et
al
2018
particularly
when
the
model
decision
counteracts
their
own
beliefs
veale
et
al
2018
discusses
decision-support
design
that
can
be
used
to
prevent
this
selectional
belief
bias
gamification
is
another
use
bias
in
which
person
learns
how
to
manipulate
an
algorithm
veale
et
al
2018
ciampaglia
et
al
2017
introduces
the
concept
of
popularity
bias
or
preferences
that
strongly
correlate
to
popularity
and
discusses
how
these
forms
of
bias
are
often
gamified
on
the
internet
by
bots
and
fake
feedback
the
final
case
of
use
bias
is
the
curse
of
knowledge
bias
when
you
assume
someone
has
the
background
you
do
and
can
use
the
model
appropriately
mester
2017
this
concern
appears
in
various
literature
surrounding
predictive
policing
technologies
asaro
2019
framework
for
fairness
bennett
moses
chan
2018
ridgeway
2013
where
critics
voice
concerns
about
whether
police
understand
the
limitations
of
these
technologies
and
are
using
them
appropriately
guszcza
2018
details
the
importance
of
understanding
the
environment
where
these
machine
learning
tools
will
be
used
and
creating
with
those
environments
in
mind
the
solution
space
fairness
technologies
the
solution
space
to
algorithmic
bias
consists
of
diverse
array
of
solutions
that
contribute
to
the
responsible
ai
space
including
explainability
transparency
interpretability
and
accountability
cheng
et
al
2021
while
each
solution
has
diverged
from
different
ethical
dilemma
that
emerged
from
the
machine
learning
production
pipeline
the
objective
remains
the
same
to
produce
fair
ai
or
ai
that
is
free
from
unintentional
algorithmic
bias
fairness
as
defined
by
mehrabi
et
al
2021
is
he
absence
of
any
prejudice
or
favoritism
toward
an
individual
or
group
based
on
their
inherent
or
acquired
characteristics
fair
ai
for
the
purpose
of
this
paper
consists
of
solutions
to
combat
algorithmic
bias
which
is
often
inclusive
of
top-tier
solutions
from
explainability
transparency
interpretability
and
accountability
research
since
the
problem
space
of
algorithmic
bias
is
so
large
and
diverse
concerted
effort
has
gone
into
making
fairness
tools
that
practitioners
can
use
to
employ
state-of-the
art
fairness
techniques
within
their
ml
pipelines
these
solutions
mainly
come
in
two
forms
software
toolkits
and
checklists
toolkits
serve
as
functions
accessible
via
programming
languages
that
can
be
used
to
detect
or
mitigate
biases
checklists
are
extensive
guides
created
by
fairness
experts
that
ml
ai
practitioners
can
use
to
ensure
the
inclusion
of
ethical
thought
throughout
their
pipelines
while
this
section
won
provide
complete
description
of
the
solution
space
it
will
highlight
the
diversity
of
tools
provided
by
both
organizations
and
academic
institutions
3.1
software
toolkits
variety
of
software
toolkits
currently
exist
each
with
overlapping
and
distinct
characteristics
to
identify
them
while
some
are
accessible
via
website
portals
saleiro
et
al
2018
many
exist
as
installable
packages
that
can
be
imported
into
python
programs
bird
et
al
2020
srinivasan
2020
johnson
et
al
2020
wexler
et
al
2019
or
other
languages
used
by
ml
practitioners
bellamy
et
al
2018
vasudevan
kenthapadi
2020
the
following
selection
of
toolkits
provides
diverse
look
of
available
solutions
3.1
google
toolkits
google
provides
two
relevant
toolkits
fairness
indicators
and
the
what-if
toolkit
wit
wexler
et
al
2019
both
toolkits
function
as
interactive
widgets
where
users
can
provide
their
model
their
performance
and
fairness
metrics
of
choice
and
the
attributes
on
which
slicing
and
evaluating
will
take
place
richardson
et
al
2021
these
toolkits
are
embedded
tensorflow
fairness
evaluation
and
visualization
toolkit
https://github.com/tensorflow/fairnessindicators
richardson
gilbert
within
the
tensorflow
package
in
python
but
users
can
build
their
own
custom
prediction
functions
if
their
model
exists
outside
of
tensorflow
unlike
other
toolkits
google
toolkits
require
that
users
provide
model
seng
et
al
2021
additionally
both
toolkits
allow
model
comparison
between
at
most
models
while
fairness
indicators
works
with
binary
and
multiclass
problems
and
allows
intersectional
analysis
wit
works
with
binary
or
regression
problems
within
fairness
indicators
users
can
create
their
own
visualizations
to
compare
models
across
different
performance
and
fairness
metrics
users
can
select
and
deselect
performance
metrics
to
focus
on
the
wit
includes
features
overview
performance
chart
and
data
point
editor
the
features
overview
provides
visualizations
to
depict
the
distribution
of
each
feature
with
some
summary
statistics
the
performance
chart
provides
simulated
results
depicting
the
outcome
of
select
fairness
transformations
including
an
option
to
customize
fairness
solution
lastly
the
data
point
editor
provides
custom
visualizations
of
data
points
from
the
data
and
allows
users
to
compare
counterfactual
points
make
modifications
to
points
to
see
how
the
results
change
and
plot
partial
dependence
plots
to
determine
model
sensitivity
to
feature
3.1
uchicago
aequitas
aequitas
is
an
audit
report
toolkit
that
can
be
accessed
via
command
line
as
python
package
and
web
application
saleiro
et
al
2018
aequitas
is
tool
built
for
classification
models
and
allows
for
the
comparison
of
models
while
aequitas
does
not
provide
fairness
mitigation
techniques
it
does
evaluate
models
based
on
commonly
used
fairness
criteria
users
can
label
sensitive
attributes
and
aequitas
will
calculate
group
metrics
between
sensitive
subgroups
and
provide
disparity
scores
that
reflect
the
degree
of
unfairness
aequitas
also
produces
plots
that
record
these
fairness
disparities
and
group
metrics
within
these
visualizations
users
can
choose
to
have
groups
colored
based
on
whether
or
not
subgroup
disparities
pass
set
threshold
for
fair
or
unfair
saleiro
et
al
2018
this
tool
was
built
with
two
types
of
users
in
mind
data
scientists
ai
researchers
who
are
building
these
ai
tools
and
policymakers
who
are
approving
their
use
in
practice
saleiro
et
al
2018
3.1
ibm
ai
fairness
360
ai
fairness
360
aif360
is
toolkit
that
provides
both
fairness
detection
and
mitigation
strategies
bellamy
et
al
2018
it
can
be
used
in
both
python
and
the
fairness
metrics
provided
include
general
performance
metrics
group
fairness
metrics
and
individual
fairness
metrics
users
also
have
an
assortment
of
mitigation
strategies
that
they
can
apply
to
their
models
these
mitigation
strategies
can
be
applied
across
variety
of
stages
in
the
pipeline
including
pre-processing
while
processing
and
post-processing
in
order
to
apply
these
strategies
users
can
distinguish
between
privileged
and
unprivileged
subgroups
on
which
to
complete
analysis
to
assist
users
with
learning
their
website
includes
wide
selection
of
tutorials
and
web
demos
bellamy
et
al
2018
while
this
toolkit
does
not
provide
its
own
visualization
functions
it
does
provide
guidance
for
how
its
functions
can
be
used
in
conjunction
with
the
local
interpretable
model-agnostic
explanations
toolkit
aka
lime
ribeiro
et
al
2016
similar
to
aequitas
aif360
was
built
with
business
users
and
ml
developers
in
mind
bellamy
et
al
2018
framework
for
fairness
3.1
linkedin
fairness
toolkit
lift
lift
is
recently
released
toolkit
that
provides
fairness
detection
strategies
for
measuring
fairness
across
variety
of
metrics
vasudevan
kenthapadi
2020
they
provide
an
assortment
of
metrics
segregated
based
on
whether
they
are
for
the
data
or
the
model
outputs
similar
to
ai
fairness
360
they
do
not
produce
their
own
visualizations
unlike
previous
toolkits
they
are
built
to
be
applied
in
scala
spark
programs
furthermore
this
toolkit
is
unique
in
its
flexibility
and
scalability
overcoming
previous
issues
with
measuring
fairness
in
large
datasets
vasudevan
kenthapadi
2020
3.1
other
solutions
an
assortment
of
other
toolkits
exist
including
microsoft
fairlearn
bird
et
al
2020
ml
fairness
gym
srinivasan
2020
scikit
fairness
tool
johnson
et
al
2020
pymetrics
audit-ai
and
new
ones
arise
often
while
these
toolkits
may
differ
in
the
fairness
metrics
they
consider
and
the
fairness
mitigation
strategies
they
may
employ
the
highlights
of
their
features
can
be
seen
in
the
toolkits
above
some
are
interactive
some
provide
visual
support
some
allow
intersectional
analysis
some
focus
on
detection
or
mitigation
alone
etc
3.2
checklist
unlike
toolkits
checklists
mostly
exist
as
documentation
meant
to
guide
readers
through
incorporating
ethical
and
responsible
ai
throughout
the
lifecycle
of
their
projects
while
some
are
specifically
meant
for
data
scientists
or
machine
learning
engineers
building
the
tools
other
checklists
are
generally
written
with
all
relevant
parties
in
mind
while
toolkits
provide
more
statistical
support
when
it
comes
to
strategies
to
detect
and
mitigate
checklists
engage
developers
with
questions
and
tasks
to
effectively
ensure
that
ethical
thought
occurs
from
the
idea
formulation
to
the
auditing
after
deployment
patil
et
al
2018
3.2
deon
deon
is
fairness
checklist
created
by
drivendata
data
scientist-led
company
that
utilizes
crowdsourcing
and
cutting-edge
technology
to
tackle
predictive
problems
with
societal
impact
the
checklist
is
described
as
default
toolkit
but
customizations
are
made
for
practitioners
with
domain-specific
concerns
deon
is
split
into
five
areas
data
collection
data
storage
analysis
modeling
and
deployment
to
assist
users
with
utilizing
the
checklist
each
checklist
item
is
accompanied
with
use
cases
to
exemplify
relevant
concerns
one
of
deon
most
unique
features
is
its
command
line
interface
that
practitioners
can
use
to
interact
with
the
checklist
3.2
microsoft
ai
fairness
checklist
madaio
et
al
2020
produced
the
ai
fairness
checklist
unlike
most
checklists
this
checklist
was
co-designed
with
the
iterative
feedback
of
practitioners
the
checklist
is
split
into
different
parts
envision
define
prototype
build
launch
and
evolve
envision
define
and
prototype
provide
ethical
considerations
that
would
fit
best
in
the
initial
planning
and
pymetric
audit-ai
https://github.com/pymetrics/audit-ai
driven
data
deon
tool
https://deon.drivendata.org/
richardson
gilbert
designing
stages
of
project
build
provides
guidance
for
when
ai
is
in
the
creation
phases
and
launch
and
evolve
provide
guidance
for
the
deployment
stages
of
the
product
it
also
comes
with
comprehensive
preamble
that
introduces
the
complexity
of
fairness
provides
instructions
for
how
the
checklist
should
be
used
and
encourages
practitioners
and
teams
to
personalize
and
customize
the
checklist
to
best
fit
their
environment
madaio
et
al
2020
3.2
legal
and
ethics
checklist
lifshitz
and
mcmaster
2020
provides
unique
ethics
checklist
that
focuses
on
legal
considerations
that
should
be
noted
in
the
creation
of
ai
unlike
related
checklists
this
checklist
is
not
sectioned
by
stages
in
the
ai
lifecycle
but
by
legal
priorities
including
human
agency
oversight
security
safety
privacy
data
governance
transparency
accessibility
etc
this
checklist
provides
unique
viewpoint
from
lawyer
for
institutions
and
practitioners
to
use
to
avoid
running
into
legal
issues
lifshitz
mcmaster
2020
3.2
ibm
ai
factsheets
ai
factsheets
is
unique
guide
that
provides
methodology
for
incorporating
responsibility
and
transparency
into
the
ai
pipeline
via
documentation
while
ai
factsheets
is
not
described
as
checklist
it
does
provide
guidance
for
how
to
create
documentation
that
depicts
responsible
ai
practices
the
creation
of
factsheet
template
is
very
domain-dependent
and
arnold
et
al
2019
provides
detailed
methodology
for
how
factsheet
can
be
customized
to
the
project
in
satisfying
the
components
of
the
factsheets
institutions
and
teams
can
engender
trust
with
consumers
by
increasing
transparency
and
ensuring
their
products
satisfy
necessary
ethical
considerations
arnold
et
al
2019
these
checklists
provide
an
overview
of
the
differing
features
that
can
be
found
across
the
checklist
landscape
some
focus
on
legality
some
are
domain
specific
and
others
have
been
built
with
the
ai
pipeline
in
mind
other
checklists
and
guides
have
been
produced
as
well
bradley
et
al
2020
manders-huits
zimmer
2009
gebru
et
al
2018
mitchell
et
al
2019
similarly
structured
and
intended
to
assist
practitioners
with
implementing
fair
or
responsible
practices
in
their
pipelines
shortcomings
of
solutions
recommendations
for
the
future
while
the
solutions
posed
above
have
been
pivotal
starting
point
for
fairness
research
much
work
has
been
done
addressing
the
pitfalls
of
these
contributions
as
well
this
section
aims
to
discuss
the
prominent
issues
that
have
arisen
in
the
literature
and
the
corresponding
recommendations
that
have
been
made
for
future
work
while
many
of
these
recommendations
were
made
to
fairness
researchers
and
designers
there
also
are
recommendations
for
organizations
and
ml
practitioners
4.1
recommendations
for
fairness
experts
towards
the
goal
of
universally
ethical
ai
much
of
the
responsibility
lies
in
the
hands
of
fair
ai
researchers
to
define
design
and
translate
fairness
into
palatable
procedure
that
can
be
easily
applied
by
practitioners
and
institutions
this
requires
that
the
process
of
producing
fair
ai
undergo
human-centered
research
and
design
procedure
nonetheless
10
framework
for
fairness
previous
research
has
found
that
there
exist
major
gaps
between
practitioners
and
fairness
experts
indicating
lack
of
communication
between
those
producing
fair
ai
and
those
intended
to
apply
it
holstein
et
al
2019
law
et
al
2020b
madaio
et
al
2020
law
et
al
2020a
veale
binns
2017
the
results
from
these
works
have
been
thematically
categorized
accompanied
by
suggestions
for
improvement
4.1
conflicting
fairness
metrics
along
with
identifying
23
different
types
of
biases
and
different
categories
for
discrimination
mehrabi
et
al
2021
identified
10
different
types
of
fairness
metrics
due
to
the
vast
quantity
and
similarity
between
these
metrics
many
authors
have
tried
to
categorize
them
into
related
bins
mehrabi
et
al
2021
identified
three
different
types
of
fairness
metrics
individual
group
and
subgroup
individual
fairness
gives
similar
predictions
to
similar
individuals
group
fairness
treats
different
groups
equally
and
subgroup
fairness
attempts
to
achieve
balance
of
both
group
and
individual
fairness
mehrabi
et
al
2021
barocas
et
al
2019
also
identified
19
proposed
fairness
metrics
and
also
categorized
them
into
three
categories
independence
separation
and
sufficiency
barocas
et
al
2019
defines
these
categories
through
properties
of
the
joint
distribution
of
sensitive
attribute
the
target
variable
and
the
classifier
or
score
random
variable
satisfies
independence
if
random
variable
satisfies
separation
if
and
random
variable
satisfies
sufficiency
if
barocas
et
al
2019
lastly
in
comprehensive
review
and
explanation
of
fairness
metrics
verma
and
rubin
2018
isolated
20
different
fairness
metrics
and
categorized
them
into
different
categories
statistical
measures
similarity
measures
and
causal
reasoning
statistical
measures
are
those
that
depend
on
true
positive
false
positive
false
negative
and
true
negative
similarity
based
measures
attempt
to
address
issues
that
are
ignored
by
statistical
measures
by
focusing
on
insensitive
attributes
as
well
and
causal
reasoning
uses
causal
graphs
to
draw
relations
between
attributes
to
determine
their
influence
on
the
outcome
and
allow
users
to
understand
where
exactly
bias
is
coming
from
and
whether
it
is
permissible
verma
rubin
2018
while
each
author
recognized
legitimate
patterns
within
and
between
metrics
these
works
highlight
major
issue
that
exists
there
are
too
many
metrics
for
measuring
unfairness
with
too
few
differences
to
delineate
them
mehrabi
et
al
2021
furthermore
major
trade-offs
exist
between
fairness
metrics
making
the
selection
of
metrics
highly
situationdependent
binns
2018
verma
and
rubin
2018
concludes
that
many
of
these
metrics
are
advanced
and
require
expert-level
input
which
in
itself
produces
implicit
biases
furthermore
many
works
have
identified
conflicts
between
metrics
that
make
them
incompatible
with
each
other
which
forces
individuals
to
choose
berk
et
al
2021
friedler
et
al
2021
kleinberg
et
al
2017
mittelstadt
2019
rovatsos
et
al
2019
different
metrics
may
emphasize
different
aspects
of
performance
japkowicz
shah
2011
and
much
work
has
been
done
comparing
and
contrasting
these
metrics
garcia-gathright
et
al
2018
verma
rubin
2018
chouldechova
2017
corbett-davies
goel
2018
binns
2018
fish
et
al
2016
kilbertus
et
al
2017
simoiu
et
al
2017
cramer
et
al
2019
zliobaite
2015
this
decision
is
not
one
that
should
be
taken
lightly
because
the
act
of
choosing
fairness
metric
is
very
political
in
that
it
valorizes
one
point
of
view
while
silencing
another
bowker
11
richardson
gilbert
star
1999
friedler
et
al
2021
according
to
rovatsos
et
al
2019
the
task
of
choosing
the
right
metric
currently
lies
in
the
hands
of
practitioners
most
of
whom
are
unfamiliar
with
fairness
research
and
that
is
heavy
expectation
considering
the
fact
that
society
as
whole
has
not
decided
which
ethical
standards
to
prioritize
to
support
this
issue
verma
and
rubin
2018
suggests
that
more
work
is
needed
to
clarify
which
definitions
are
appropriate
for
which
situations
friedler
et
al
2021
states
that
fairness
experts
must
explicitly
state
the
priorities
of
each
fairness
metric
to
ensure
practitioners
are
making
informed
choices
furthermore
friedler
et
al
2018
emphasizes
that
new
measures
of
fairness
should
only
be
introduced
if
that
metric
behaves
fundamentally
differently
from
those
already
proposed
4.1
other
metric-specific
pitfalls
in
addition
to
the
conflicting
nature
of
fairness
metrics
there
exist
several
other
pitfalls
that
have
been
recognized
by
researchers
kilbertus
et
al
2017
discussed
the
insufficiency
of
observational
criteria
which
are
the
criteria
often
used
as
sensitive
attributes
within
toolkits
they
emphasize
that
such
methodologies
are
unable
to
confirm
that
protected
attributes
have
direct
causal
influence
on
results
and
instead
they
propose
two
new
metrics
of
causal
reasoning
proxy
discrimination
and
unresolved
discrimination
kilbertus
et
al
2017
proxy
discrimination
can
be
potentially
inferred
from
causal
graph
when
there
exists
path
from
protected
attribute
to
predicted
attribute
that
includes
proxy
variable
and
unresolved
discrimination
can
be
inferred
from
causal
graph
if
the
only
path
from
protected
attribute
to
the
predicted
attribute
includes
resolving
variable
kilbertus
et
al
2017
furthermore
friedler
et
al
2018
found
that
many
fairness
metrics
lack
robustness
by
simply
modifying
dataset
composition
and
changing
train-test
splits
the
results
of
their
study
depicted
that
many
fairness
criteria
lacked
stability
they
proposed
that
only
measures
that
depict
stability
and
success
should
be
used
to
report
fairness
hoffmann
2019
discusses
the
lack
of
intersectional
analysis
in
fairness
criteria
many
metrics
provide
analysis
for
sensitive
attributes
but
only
one-dimensionally
hoffmann
2019
emphasizes
that
proposed
metrics
should
strategically
identify
multi-dimensional
correlations
between
attributes
and
outcomes
wagstaff
2012
emphasizes
that
the
use
of
abstract
metrics
distract
from
the
problems
specific
to
datasets
and
applications
furthermore
the
impact
of
the
metrics
cannot
be
inferred
from
the
scores
themselves
furthermore
kasy
and
abebe
2021
demonstrates
that
many
of
the
standard
metrics
legitimize
the
focus
on
merit
deterring
individuals
from
questioning
the
legitimacy
of
the
status
quo
additionally
there
is
concern
about
the
assumptions
made
by
fairness
experts
when
producing
fairness
metrics
many
metrics
and
available
fairness
tooling
depend
on
access
to
sensitive
attributes
which
many
practitioners
do
not
have
holstein
et
al
2019
law
et
al
2020a
rovatsos
et
al
2019
additionally
there
are
legal
restrictions
against
some
fairness
definitions
xiang
raji
2019
for
those
with
legal
access
to
sensitive
data
some
practitioners
are
concerned
with
how
the
public
would
scrutinize
the
use
of
sensitive
attributes
even
for
the
detection
of
bias
rovatsos
et
al
2019
furthermore
when
studying
political
philosophy
and
how
it
connects
to
fairness
in
machine
learning
binns
2018
notes
that
12
framework
for
fairness
risks
of
incomplete
fairness
analysis
arise
when
users
are
forced
to
adhere
to
static
set
of
prescribed
protected
classes
instead
of
doing
thorough
analysis
to
identify
discrimination
to
combat
this
issue
law
et
al
2020a
suggests
that
coarse-grained
demographic
info
be
utilized
in
fairness
tools
and
proposed
to
practitioners
furthermore
fairness
experts
should
utilize
and
promote
fairness
metrics
that
do
not
rely
on
sensitive
attributes
holstein
et
al
2019
law
et
al
2020a
rovatsos
et
al
2019
4.1
oversimplification
of
fairness
major
concern
in
literature
is
the
emphasis
on
technical
solutions
to
algorithmic
bias
which
is
socio-technical
problem
many
authors
emphasize
the
need
to
supplement
statistical
definitions
with
social
practices
veale
et
al
2018
madaio
et
al
2020
verma
rubin
2018
fazelpour
lipton
2020
jacobs
wallach
2021
birhane
2021
madaio
et
al
2020
called
the
sole
use
of
technical
solutions
ethics
washing
and
selbst
et
al
2019
describes
the
failure
to
account
for
the
fact
that
fairness
cannot
be
solely
achieved
through
mathematical
formulation
as
the
formalism
trap
fazelpour
and
lipton
2020
details
that
the
practice
of
ethics
washing
can
lead
to
misguided
strategies
for
mitigating
bias
and
harcourt
2007
emphasizes
that
the
perceived
success
of
these
technical
solutions
stalls
pursuits
to
achieve
actual
fairness
with
the
aid
of
social
practices
birhane
2021
calls
for
fundamental
shift
towards
considering
the
relational
factors
and
impact
of
machine
learning
from
societal
standpoint
fazelpour
and
lipton
2020
recommends
that
mathematical
assessment
be
supplemented
with
social
assessment
tools
like
thorough
analysis
of
data
collection
strategies
and
understanding
the
assumptions
made
by
different
ml
algorithms
furthermore
toolkits
and
checklists
for
fair
evaluation
should
avoid
solely
providing
mathematical
solutions
4.1
misguided
fairness
objectives
in
addition
to
the
issue
of
ethics
washing
other
concerns
have
arisen
concerning
the
trajectory
of
fairness
research
hoffmann
2019
critiques
fairness
research
for
its
focus
on
avoiding
disadvantage
instead
of
understanding
where
advantage
stems
from
in
support
of
better
understanding
where
biases
exist
in
the
data
veale
and
binns
2017
proposes
that
more
energy
should
be
placed
in
data
exploration
through
the
use
of
unsupervised
learning
as
strategy
to
identify
hidden
patterns
futhermore
many
fairness
metrics
and
strategies
rely
on
assumptions
about
the
world
that
may
be
problematic
in
themselves
for
example
hu
and
kohler-hausmann
2020
discusses
how
the
use
of
the
social
concept
of
sex
can
perpetuate
problematic
and
harmful
sex
discrimination
4.1
making
fair
ai
applicable
works
such
as
those
done
by
holstein
et
al
2019
veale
et
al
2018
rakova
et
al
2020
richardson
et
al
2021
are
novel
in
their
inclusion
of
practitioner
feedback
into
fairness
literature
common
theme
that
emerged
from
these
papers
was
the
lack
of
applicability
that
most
practitioners
held
toward
fairness
tools
and
support
practitioners
interviewed
by
holstein
et
al
2019
emphasized
the
need
for
domain-specific
procedures
and
metrics
these
participants
requested
that
fairness
experts
pool
knowledge
and
resources
by
domain
for
easy
access
additionally
practitioners
in
this
study
and
veale
et
al
2018
had
concerns
13
richardson
gilbert
with
the
scalability
of
fairness
analysis
which
has
been
backed
by
data
scientists
in
verma
and
rubin
2018
practitioners
interviewed
by
rakova
et
al
2020
felt
that
metrics
in
academic-based
research
had
vastly
different
objectives
than
metrics
in
industry
which
required
that
industry
researchers
do
the
additional
work
to
translate
their
work
using
insufficient
metrics
to
supplement
this
workload
practitioners
felt
that
fairness
research
by
academia
did
not
fit
smoothly
into
industry
pipelines
and
required
significant
amount
of
energy
to
fully
implement
rakova
et
al
2020
in
study
by
richardson
et
al
2021
practitioners
had
the
opportunity
to
interact
with
fairness
tools
and
provide
comments
and
feedback
authors
summarized
key
themes
from
practitioners
regarding
features
and
design
considerations
that
would
make
these
tools
more
applicable
the
following
lists
contains
some
of
the
features
requested
for
fairness
toolkits
applicable
to
diverse
range
of
predictive
tasks
model
types
and
data
types
holstein
et
al
2019
can
detect
mitigate
bias
holstein
et
al
2019
olteanu
et
al
2019
mehrabi
et
al
2021
can
intervene
at
different
stages
of
the
ml
ai
life
cycle
bellamy
et
al
2018
holstein
et
al
2019
veale
binns
2017
fairness
and
performance
criteria
agnostic
corbett-davies
goel
2018
barocas
et
al
2019
verma
rubin
2018
diverse
explanation
types
ribeiro
et
al
2016
dodge
et
al
2019
arya
et
al
2019
binns
et
al
2018
provides
recommendations
for
next
steps
holstein
et
al
2019
well-supported
with
demos
and
tutorials
holstein
et
al
2019
the
results
from
these
studies
collecting
practitioner
feedback
supplement
what
many
data
scientists
and
social
scientists
have
confirmed
in
study
conducted
by
corbett-davies
et
al
2017
when
utilizing
fairness
metrics
in
recidivism
use
case
results
depicted
that
some
metrics
had
significant
trade-offs
with
public
safety
this
exemplified
the
importance
of
domain-specific
guides
and
details
when
it
comes
to
fairness
procedures
in
similar
landscape
summaries
of
algorithmic
bias
rovatsos
et
al
2019
reisman
et
al
2018
also
discuss
how
concerns
of
algorithmic
bias
differ
substantially
by
domain
application
and
yet
most
domains
lack
thorough
and
specific
algorithmic
bias
guidance
lastly
when
studying
the
use
of
fair
ai
checklists
madaio
et
al
2020
cramer
et
al
2018
emphasized
the
importance
of
aligning
these
checklists
with
team
workflows
4.1
designing
usable
fair
ai
concerns
with
applicability
emphasize
an
over-arching
importance
of
human-centered
design
procedures
in
the
creation
of
fair
ai
tools
efforts
should
be
made
to
collect
practitioner
feedback
and
incorporate
this
feedback
into
the
creation
of
fair
ai
richardson
et
al
2021
holstein
et
al
2019
rakova
et
al
2020
14
framework
for
fairness
major
component
of
usability
when
it
comes
to
fair
ai
tools
is
the
integration
of
affordances
that
support
ai
fairness
according
to
robert
et
al
2020
these
affordances
include
transparency
explainability
voice
and
visualization
in
this
work
they
define
transparency
as
making
the
underlying
ai
mechanics
visible
and
known
to
the
employee
explainability
as
describing
the
ai
decision
actions
to
the
employee
in
human
terms
voice
as
providing
employees
with
an
opportunity
to
communicate
and
provide
feedback
to
the
ai
and
visualization
as
representing
information
to
employees
via
images
diagrams
or
animations
robert
et
al
2020
26
this
work
is
not
the
only
one
of
its
kind
to
discuss
the
importance
of
such
features
in
strengthening
fairness
tools
results
from
studies
that
interviewed
or
studied
practitioners
emphasized
the
importance
of
having
the
ability
to
interact
with
fairness
toolkits
richardson
et
al
2021
holstein
et
al
2019
cramer
et
al
2019
using
the
tool
to
compare
models
and
methods
richardson
et
al
2021
providing
understandable
visualizations
richardson
et
al
2021
veale
et
al
2018
law
et
al
2020a
ribeiro
et
al
2016
dodge
et
al
2019
arya
et
al
2019
law
et
al
2020b
veale
binns
2017
and
receiving
feedback
from
these
tools
ribeiro
et
al
2016
richardson
et
al
2021
solutions
to
aid
users
in
understanding
bias
issues
reside
in
interpretability
and
explainability
research
hutchinson
and
mitchell
2018
emphasizes
the
importance
of
utilizing
explanations
in
fairness
pursuits
in
study
by
ribeiro
et
al
2016
participants
had
the
opportunity
to
interact
with
lime
the
local
interpretable
model-agnostic
explanations
toolkit
results
from
this
study
depict
the
importance
of
explanations
as
tool
for
users
to
decide
whether
or
not
they
should
trust
classifier
or
determine
where
and
how
to
fix
classifier
ribeiro
et
al
2016
without
concrete
explanations
users
were
either
willfully
ignorant
or
unable
to
rely
on
the
classifier
ribeiro
et
al
2016
furthermore
study
by
dodge
et
al
2019
found
that
different
fairness
problems
were
better
explained
with
different
types
of
explanations
when
users
were
exposed
to
different
types
of
fairness
explanations
they
exhibited
very
different
opinions
on
the
model
while
some
explanations
were
considered
inherently
less
fair
meaning
users
tended
to
view
their
models
as
biased
others
enhanced
participants
confidence
in
the
classifier
dodge
et
al
2019
authors
in
arya
et
al
2019
did
similar
study
when
introducing
the
ai
explainability
360
toolkit
they
compared
and
contrasted
different
explainer
types
including
data
directly
interpretable
local
post-hoc
and
global
post-hoc
explainers
they
provide
taxonomy
of
explanations
that
provides
guidance
for
which
explainers
are
best
for
which
users
and
future
work
that
needs
to
be
done
arya
et
al
2019
law
et
al
2020b
ran
simulation
with
practitioners
to
depict
how
plethora
of
visualizations
could
be
used
without
overloading
the
participants
depending
on
the
use
case
they
proposed
different
techniques
for
organizing
visualizations
one
technique
which
they
referred
to
as
recommendation
list
provides
summary
of
fairness
results
and
was
best
for
situations
where
there
were
many
performance
metrics
the
other
technique
visual
cues
provided
high
comprehensiveness
for
few
metrics
law
et
al
2020b
despite
the
indubitable
strengths
of
fairness
toolkits
lakkaraju
and
bastani
2019
and
kaur
et
al
2020
also
gave
some
risks
that
came
with
the
use
of
visualizations
and
explanations
lakkaraju
and
bastani
2019
found
that
explanations
for
black-box
model
could
still
be
manipulated
to
hide
issues
of
unfairness
by
simply
omitting
features
that
users
considered
problematic
by
doing
this
generated
explanations
increased
user
trust
15
richardson
gilbert
by
nearly
10
fold
lakkaraju
bastani
2019
furthermore
kaur
et
al
2020
depicted
the
importance
of
training
users
before
providing
them
with
toolkits
and
visualizations
that
they
are
unfamiliar
with
when
users
were
not
internalizing
tutorials
they
made
incorrect
assumptions
about
the
data
and
the
model
they
could
not
successfully
uncover
issues
with
the
dataset
and
had
low
confidence
when
interacting
with
the
tool
nonetheless
they
reported
high
trust
in
the
fairness
tools
because
it
provided
visualizations
and
it
was
publicly
available
kaur
et
al
2020
these
findings
should
be
considered
in
the
creation
and
the
deployment
of
these
toolkits
4.1
avoid
the
over-reliance
on
fair
ai
as
was
depicted
in
the
work
of
kaur
et
al
2020
many
practitioners
overly
trusted
toolkits
despite
not
completely
understanding
the
results
of
the
toolkit
of
particular
concern
when
users
were
asked
to
select
from
dropdown
list
the
functionalities
of
the
toolkit
most
of
them
grossly
overestimated
what
the
toolkit
could
do
these
results
depicted
misalignment
between
practitioners
understanding
of
the
toolkits
and
the
toolkits
intended
use
kaur
et
al
2020
practitioners
interviewed
in
veale
et
al
2018
depicted
similar
hesitations
about
interacting
with
fairness
tooling
they
suspected
they
would
over
or
under-rely
on
them
4.1
communicating
fairness
to
stakeholders
across
several
interview
studies
where
researchers
collected
practitioner
feedback
practitioners
requested
help
communicating
fairness
concerns
to
stakeholders
holstein
et
al
2019
veale
et
al
2018
law
et
al
2020a
some
practitioners
discussed
how
the
objectives
of
stakeholders
were
different
from
their
own
and
requested
that
resources
be
provided
for
communicating
the
fairness
trade-off
as
it
relates
to
those
objectives
veale
et
al
2018
law
et
al
2020a
they
also
requested
that
these
resources
be
understandable
by
laymen
or
business-oriented
individuals
so
that
they
can
understand
the
cause
of
bias
and
the
importance
of
fairness
considerations
law
et
al
2020a
garcia-gathright
et
al
2018
one
practitioner
said
that
fairness
toolkits
should
incorporate
the
assumption
that
there
will
be
tech
resistance
and
many
stakeholders
and
practitioners
will
reject
the
output
from
fairness
toolkits
if
its
not
effectively
communicated
veale
et
al
2018
while
prior
discussions
on
different
explanations
for
differing
users
could
most
definitely
be
useful
in
explaining
fairness
issues
arya
et
al
2019
fairness
experts
should
consider
creating
guides
for
communicating
fairness
results
4.1
the
burden
of
fairness
common
sentiment
when
it
came
to
discussing
or
utilizing
fairness
tools
was
practitioners
feeling
overwhelmed
whether
users
were
interacting
with
toolkit
law
et
al
2020b
richardson
et
al
2021
or
checklist
cramer
et
al
2019
many
felt
that
either
they
were
not
qualified
holstein
et
al
2019
ethics
was
too
difficult
to
operationalize
madaio
et
al
2020
ethics
presented
more
questions
than
answers
binns
2018
or
the
checklists
and
tools
were
too
overwhelming
cramer
et
al
2019
richardson
et
al
2021
law
et
al
2020b
while
some
of
these
concerns
require
support
from
institutions
many
works
discussed
what
fairness
experts
could
do
to
relieve
this
burden
16
framework
for
fairness
since
there
exists
so
many
avenues
for
bias
and
so
many
feasible
solutions
mehrabi
et
al
2021
olteanu
et
al
2019
cramer
et
al
2018
suggests
that
fairness
experts
create
taxonomies
of
existing
biases
that
can
help
practitioners
recognize
and
counter
these
biases
to
overcome
the
information
overload
that
might
accompany
visual
toolkit
law
et
al
2020b
law
et
al
2020a
richardson
et
al
2021
suggests
that
toolkits
filter
out
biases
on
the
users
behalf
so
that
alarming
outcomes
can
stand
out
but
this
solution
presents
selection
biases
of
the
fairness
developers
and
important
outcomes
may
differ
in
different
contexts
results
from
cramer
et
al
2019
suggests
that
checklist
be
avoided
as
self-serve
tool
since
they
were
often
found
to
be
overwhelming
for
users
instead
an
interactive
interface
might
be
beneficial
cramer
et
al
2019
the
goal
for
fairness
practitioners
should
be
to
incorporate
fairness
into
pre-existing
workflows
in
such
way
that
disruption
and
chaos
are
minimized
with
this
objective
in
mind
fairness
would
be
considered
substantially
less
overwhelming
for
practitioners
4.1
10
supplemental
resources
for
practitioners
the
major
challenge
presented
to
fairness
experts
is
translating
principles
and
ethics
codes
into
actionable
items
that
practitioners
and
institutions
can
implement
mittelstadt
2019
this
includes
providing
guidance
for
users
to
detect
and
foresee
bias
madaio
et
al
2020
law
et
al
2020a
garcia-gathright
et
al
2018
mehrabi
et
al
2021
cramer
et
al
2018
determine
where
bias
might
stem
from
holstein
et
al
2019
garcia-gathright
et
al
2018
determine
how
to
respond
to
biases
holstein
et
al
2019
veale
et
al
2018
law
et
al
2020a
garcia-gathright
et
al
2018
and
how
to
maintain
fairness
after
deployment
veale
et
al
2018
practitioners
are
requesting
taxonomies
of
potential
harms
and
biases
madaio
et
al
2020
cramer
et
al
2018
easy-to-digest
summaries
explaining
biases
garcia-gathright
et
al
2018
cramer
et
al
2018
and
guidelines
for
best
practices
throughout
the
ml
pipeline
holstein
et
al
2019
users
are
also
requesting
plethora
of
additional
resources
like
community
forums
for
fairness
domain-specific
guides
and
plenty
of
tutorials
exemplifying
how
to
incorporate
fairness
holstein
et
al
2019
madaio
et
al
2020
richardson
et
al
2021
gray
chivukula
2019
fairness
experts
should
do
as
much
as
possible
to
provide
these
resources
for
practitioners
to
support
their
use
of
fairness
tools
4.2
recommendations
for
institutions
while
considerable
portion
of
recommendations
were
given
to
fairness
researchers
organizations
also
have
large
portion
of
responsibility
for
ensuring
the
success
of
fairness
implementation
several
works
provide
critique
and
actionable
steps
for
institutions
and
organizations
to
use
to
assist
their
practitioners
in
the
implementation
of
fairness
toolkits
4.2
prioritize
fairness
with
the
plethora
of
institutions
who
have
created
ethical
principles
or
guidelines
centered
on
fairness
accountability
and
transparency
in
ai
there
is
no
doubt
that
organizations
have
recognized
the
importance
in
prioritizing
ethics
jobin
et
al
2019
nonetheless
the
application
of
fairness
in
practice
is
rare
and
most
ethical
codes
are
not
put
to
action
by
engineers
and
practitioners
frankel
1989
emphasizes
that
ethics
codes
that
fail
to
put
val17
richardson
gilbert
ues
into
practice
are
political
tools
meant
to
manipulate
the
public
one
of
the
easiest
ways
to
promote
culture
of
bias
awareness
is
to
at
an
organizational
level
prioritize
fairness
as
global
objective
garcia-gathright
et
al
2018
stark
hoffmann
2019
organizations
can
do
this
by
considering
fairness
in
their
own
hiring
practices
garcia-gathright
et
al
2018
prioritizing
bias
correction
as
they
prioritize
privacy
and
accessibility
madaio
et
al
2020
garcia-gathright
et
al
2018
frankel
1989
providing
practitioners
with
resources
or
teams
that
can
provide
them
with
actionable
steps
madaio
et
al
2020
mittelstadt
2019
and
changing
organizational
structure
to
embrace
the
inclusion
of
fairness
considerations
cramer
et
al
2019
practitioners
interviewed
in
rakova
et
al
2020
provided
plethora
of
recommendations
for
organizations
intending
to
prioritize
fairness
organizations
could
provide
reward
systems
to
incentivize
practitioners
to
continue
educating
themselves
on
ethical
ai
furthermore
they
could
allow
practitioners
to
work
more
closely
with
marginalized
communities
to
ensure
their
data
is
representative
and
their
products
are
not
harmful
to
them
lastly
practitioners
emphasized
the
importance
of
rejecting
an
ai
system
if
it
is
found
to
perpetuate
harms
to
individuals
rakova
et
al
2020
reisman
et
al
2018
recommended
that
organizations
practice
transparency
with
the
public
as
method
for
ensuring
accountability
and
therefore
fairness
they
suggest
that
the
public
be
given
notice
of
any
technology
that
might
impact
their
lives
reisman
et
al
2018
fairness
research
also
emphasizes
the
importance
of
public
transparency
from
institutions
when
it
comes
to
the
existence
and
the
auditing
of
their
ai
richardson
et
al
2020
4.2
operationalize
fairness
one
method
to
exhibit
prioritization
of
fairness
is
the
operationalization
of
fairness
as
they
exist
now
most
published
ethics
codes
are
difficult
to
operationalize
due
to
their
abstract
nature
madaio
et
al
2020
stark
hoffmann
2019
by
operationalizing
organizations
force
themselves
to
consider
how
they
want
to
enact
fairness
robert
et
al
2020
frankel
1989
and
relieve
the
burden
of
ethical
responsibility
placed
on
the
practitioner
stark
hoffmann
2019
by
making
the
big
decision
on
things
like
what
type
of
demographics
biases
and
stakeholders
they
intend
to
consider
in
their
efforts
cramer
et
al
2018
this
will
require
discussions
by
top-players
within
organizations
in
deciding
how
the
prioritization
of
fairness
may
displace
other
priorities
and
it
requires
the
consultation
of
legal-consultants
who
can
ensure
new
policies
satisfy
government
regulations
veale
et
al
2018
mittelstadt
2019
furthermore
leaders
must
make
the
decision
of
how
much
bias
is
permissible
and
be
prepared
to
handle
the
consequences
of
these
decisions
barocas
selbst
2018
barocas
et
al
2019
4.2
avoid
dividing
practitioner
loyalty
according
to
recent
research
many
practitioners
feel
divided
by
the
desire
to
create
ethicallysound
products
and
the
obligation
to
their
role
as
employees
stark
hoffmann
2019
mittelstadt
2019
some
felt
as
though
conversations
of
ethics
were
taboo
and
could
impact
their
goals
of
career
advancement
madaio
et
al
2020
rakova
et
al
2020
many
18
framework
for
fairness
felt
like
it
was
an
individual
battle
where
they
were
the
sole
advocate
battling
for
fairness
holstein
et
al
2019
madaio
et
al
2020
when
studying
ethics
in
engineering
it
becomes
obvious
that
these
concerns
are
nothing
new
to
the
field
of
engineering
works
by
frankel
1989
and
davis
1991
discuss
the
ethical
dilemma
that
has
impacted
engineers
for
decades
when
studying
the
code
of
ethics
taught
to
engineers
it
is
clear
that
engineers
should
prioritize
ethical
obligation
over
institutional
obligations
and
call
out
those
who
do
not
davis
1991
nonetheless
this
rhetoric
is
rarely
implemented
and
those
who
attempt
to
follow
codes
are
often
referred
to
as
whistleblowers
institutions
can
prevent
this
by
creating
clear
guidelines
for
what
practitioners
should
do
when
they
encounter
these
issues
and
rewarding
individuals
who
recognize
biases
and
follow
the
appropriate
steps
to
fixing
them
frankel
1989
4.2
create
organizational
structure
around
ethics
when
interviewed
practitioners
stated
concerns
based
on
the
lack
of
fairness
infrastructure
within
organizations
many
practitioners
stated
that
most
fairness
mitigation
was
done
independently
by
practitioners
and
often
their
efforts
went
uncompensated
madaio
et
al
2020
rakova
et
al
2020
others
said
they
were
unsure
of
who
to
share
fairness
issues
with
or
if
it
was
even
their
responsibility
to
do
so
veale
et
al
2018
rakova
et
al
2020
stark
hoffmann
2019
lastly
some
practitioners
had
concerns
with
how
to
handle
fairness
issues
when
different
parts
of
an
ai
product
are
owned
and
or
handled
by
different
teams
cramer
et
al
2018
these
comments
suggest
dire
need
for
organizations
to
establish
clear
delegations
of
tasks
around
fairness
some
practitioners
even
recommended
that
fairness
teams
and
internal
review
boards
be
created
to
be
used
as
resource
for
practitioners
and
as
auditors
of
ai
technologies
rakova
et
al
2020
robert
et
al
2020
with
these
additional
support
teams
there
should
be
formal
path
for
communication
that
dictates
clearly
to
practitioners
the
hierarchy
for
communicating
relevant
issues
veale
et
al
2018
if
issues
arise
as
it
relates
to
fairness
organizations
should
have
prepared
plans
for
how
these
issues
will
be
dealt
with
rakova
et
al
2020
organizations
should
consider
the
use
of
internal
and
external
investigation
committees
especially
if
handling
sensitive
protected
attributes
rakova
et
al
2020
veale
binns
2017
it
is
critical
that
these
teams
be
given
open
access
to
the
data
the
models
and
the
procedures
in
order
to
optimize
transparency
and
reproducibility
haibe-kains
et
al
2020
additionally
education
infrastructure
focusing
on
educating
practitioners
on
recognizing
and
combating
algorithmic
bias
should
be
made
available
to
practitioners
and
product
managers
to
ensure
baseline
of
understanding
amongst
practitioners
rakova
et
al
2020
the
prioritization
of
fairness
by
organizations
is
critical
for
their
practitioners
their
global
image
and
the
consumers
of
their
products
and
services
organizations
can
prioritize
fairness
by
operationalizing
it
defining
an
explicit
infrastructure
for
it
and
ensuring
protection
for
employees
interested
in
implementing
it
4.3
recommendations
for
ml
practitioners
despite
the
predominant
portion
of
work
that
needs
to
be
done
by
fairness
experts
and
practitioners
there
are
still
few
recommendations
in
literature
for
ml
practitioners
in
19
richardson
gilbert
order
to
ensure
the
success
of
fairness
effort
must
also
be
made
by
practitioners
to
educate
themselves
and
properly
implement
fairness
when
it
is
made
available
to
them
this
section
briefly
provides
recommendations
for
how
practitioners
can
correctly
implement
fairness
into
their
pipelines
4.3
implement
fairness
throughout
the
pipeline
as
stated
by
stark
and
hoffmann
2019
ethics
cannot
be
considered
final
checkpoint
but
should
be
considered
from
the
very
start
of
project
and
should
be
reflected
within
every
stage
of
the
process
currently
most
practitioners
engage
with
fairness
ad-hocly
or
as
final
performance
check
holstein
et
al
2019
madaio
et
al
2020
rakova
et
al
2020
but
the
literature
suggests
that
fairness
would
be
much
easier
to
implement
if
it
is
considered
from
the
idea
formulation
stage
of
the
project
cramer
et
al
2018
2019
friedman
et
al
2002
the
task
becomes
much
more
difficult
when
model
has
been
deployed
and
running
for
while
because
those
unintended
biases
will
have
become
recursive
and
therefore
much
more
difficult
to
manage
cramer
et
al
2018
furthermore
practitioners
should
make
an
effort
in
including
wide
variety
of
experts
and
lay-persons
into
the
design
implementation
and
evaluation
of
ml
wagstaff
2012
practitioners
should
make
sure
that
assessing
and
addressing
bias
becomes
normal
procedure
throughout
the
pipeline
of
their
projects
4.3
iteratively
implement
fairness
while
fairness
should
be
considered
throughout
the
process
sometimes
it
may
be
overwhelming
to
immediately
counteract
every
instance
of
unfairness
found
when
cramer
et
al
2018
allowed
practitioners
to
engage
with
fairness
checklists
they
noticed
concerns
practitioners
had
with
how
these
checklists
might
conflict
with
the
rapid
delivery
development
procedures
that
existed
within
their
organizations
cramer
et
al
2018
concluded
that
addressing
algorithmic
bias
would
need
to
be
done
in
short-term
narrow
steps
with
plans
for
iterative
improvement
of
fairness
issues
cramer
et
al
2019
also
suggested
that
such
method
could
reduce
the
overload
felt
by
practitioners
therefore
practitioners
should
iteratively
implement
small
fixes
to
fairness
issues
between
deployments
and
not
try
to
detect
and
handle
every
issue
immediately
related
works
while
this
work
is
the
first
of
its
kind
to
provide
comprehensive
review
of
the
solution
space
of
fairness
ai
there
are
few
related
reviews
section
summative
breakdown
of
biases
that
exists
in
machine
learning
are
similar
to
that
of
friedman
nissenbaum
1996
mehrabi
et
al
2021
olteanu
et
al
2019
hutchinson
mitchell
2018
hellstro
et
al
2020
mehrabi
et
al
2021
provides
comprehensive
list
of
23
types
of
biases
which
are
not
organized
in
any
format
friedman
and
nissenbaum
1996
and
olteanu
et
al
2019
both
provided
hierarchy
of
biases
creating
their
own
structure
for
how
these
biases
should
be
organized
while
olteanu
et
al
2019
summarizes
the
different
biases
that
are
associated
with
social
data
friedman
and
nissenbaum
1996
defines
new
set
of
biases
around
data
in
machine
learning
this
work
adopts
part
of
friedman
and
nissenbaum
1996
hierarchical
structure
and
utilizes
thematic
analysis
to
categorize
20
framework
for
fairness
different
biases
introduced
across
comprehensive
list
of
works
hutchinson
and
mitchell
2018
discusses
the
history
of
fairness
including
the
differing
notions
of
fairness
and
gaps
in
fairness
work
lastly
hellstro
et
al
2020
creates
their
own
taxonomy
of
bias
discusses
the
relation
between
forms
of
bias
and
provides
examples
to
support
differing
definitions
their
taxonomy
however
is
depicted
based
on
chronological
influences
for
bias
while
to
our
knowledge
there
are
no
reviews
that
exists
that
are
similar
to
our
section
there
are
some
works
that
do
user-focused
comparative
analysis
of
different
toolkits
richardson
et
al
2021
is
paper
that
allows
practitioners
to
engage
with
two
toolkits
uchicago
aequitas
saleiro
et
al
2018
and
google
what-if
wexler
et
al
2019
and
collects
their
feedback
seng
et
al
2021
is
similar
work
that
compares
six
different
toolkits
scikit
fairness
tool
johnson
et
al
2020
ibm
fairness
360
bellamy
et
al
2018
uchicago
aequitas
saleiro
et
al
2018
google
what-if
wexler
et
al
2019
pymetrics
audit-ai
and
microsoft
fairlearn
bird
et
al
2020
to
our
knowledge
there
is
no
other
work
that
reviews
both
fairness
toolkits
and
checklists
furthermore
our
contributions
from
section
are
entirely
novel
while
richardson
et
al
2021
does
brief
literature
review
of
related
works
that
take
practitioner
feedback
into
consideration
this
paper
is
the
first
of
its
kind
to
combine
recommendations
from
practitioners
fairness
experts
and
institutions
alike
and
presents
comprehensive
review
of
necessary
work
for
fairness
research
conclusion
there
exists
seemingly
endless
list
of
potential
influences
of
bias
when
it
comes
to
machine
learning
algorithms
in
order
to
respond
to
the
prolific
nature
of
bias
fairness
researchers
have
produced
wide
variety
of
tools
that
practitioners
can
use
mainly
in
the
form
of
software
toolkits
and
checklists
companies
academic
institutions
and
independent
researchers
alike
have
created
their
own
versions
of
these
resources
each
with
unique
features
that
those
developers
thought
were
critical
nonetheless
there
still
exists
disconnect
between
the
fairness
developers
and
the
practitioners
that
prevents
the
use
of
these
resources
in
practice
the
literature
suggests
that
fairness
in
practice
can
be
normalized
at
the
effort
of
fairness
developers
institutions
who
create
ai
and
practitioners
who
build
ai
this
review
provides
summary
of
algorithmic
bias
issues
that
have
arisen
in
literature
highlights
some
unique
contributions
in
the
fairness
solution
space
and
summarizes
recommendations
for
the
normalization
of
fairness
practices
to
our
knowledge
there
exists
no
other
work
that
provides
such
comprehensive
review
of
the
problem
and
solution
space
of
fairness
research
the
literature
suggests
that
there
is
still
much
work
to
be
done
and
this
review
highlights
those
major
needs
acknowledgments
the
authors
wish
to
thank
hans-martin
adorf
don
rosenthal
richard
franier
peter
cheeseman
and
monte
zweben
for
their
assistance
and
advice
we
also
thank
ron
musick
and
our
anonymous
reviewers
for
their
comments
the
space
telescope
science
institute
is
operated
by
the
association
of
universities
for
research
in
astronomy
for
nasa
21
richardson
gilbert
references
angwin
larson
mattu
kirchner
2016
machine
bias
tech
rep
propublica
arnold
piorkowski
reimer
richards
tsay
varshney
bellamy
hind
houde
mehta
mojsilovic
nair
ramamurthy
olteanu
2019
factsheets
increasing
trust
in
ai
services
through
supplier
declarations
of
conformity
ibm
journal
of
research
and
development
63
arya
bellamy
chen
dhurandhar
hind
hoffman
houde
liao
luss
mojsilovic
mourad
pedemonte
raghavendra
richards
sattigeri
shanmugam
singh
varshney
wei
zhang
2019
one
explanation
does
not
fit
all
toolkit
and
taxonomy
of
ai
explainability
techniques
arxiv
abs
1909.03012
asaro
2019
ai
ethics
in
predictive
policing
from
models
of
threat
to
an
ethics
of
care
ieee
technology
and
society
magazine
38
40
53
baeza-yates
2018
bias
on
the
web
communications
of
the
acm
61
54
61
barocas
hardt
narayanan
2019
fairness
and
machine
learning
fairmlbook
org
barocas
selbst
2018
big
data
disparate
impact
ssrn
electronic
journal
104
671
732
bellamy
dey
hind
hoffman
houde
kannan
lohia
martino
mehta
mojsilovic
nagar
ramamurthy
richards
saha
sattigeri
singh
varshney
zhang
2018
ai
fairness
360
an
extensible
toolkit
for
detecting
understanding
and
mitigating
unwanted
algorithmic
bias
advances
in
neural
information
processing
systems
2017
decem
nips
5681
5690
bennett
moses
chan
2018
algorithmic
prediction
in
policing
assumptions
evaluation
and
accountability
policing
and
society
28
806
822
berk
heidari
jabbari
kearns
roth
2021
fairness
in
criminal
justice
risk
assessments
the
state
of
the
art
sociological
methods
research
50
44
binns
2018
fairness
in
machine
learning
lessons
from
political
philosophy
journal
of
machine
learning
research
81
11
binns
van
kleek
veale
lyngs
zhao
shadbolt
2018
it
reducing
human
being
to
percentage
perceptions
of
justice
in
algorithmic
decisions
conference
on
human
factors
in
computing
systems
proceedings
2018april
bird
dudı
edgar
horn
lutz
milan
sameki
wallach
walker
2020
fairlearn
toolkit
for
assessing
and
improving
fairness
in
ai
tech
rep
microsoft
birhane
2021
algorithmic
injustice
relational
ethics
approach
patterns
100205
22
framework
for
fairness
blyth
1972
on
simpson
paradox
and
the
sure-thing
principle
journal
of
the
american
statistical
association
67
338
366
bowker
star
1999
sorting
things
out
classification
and
its
consequences
mit
press
bradley
ambrose
bernstein
deloatch
dreisigmeyer
gonzales
grubb
haralampus
hawes
johnson
kopp
krebs
marsico
morgan
osatuke
vidrine
2020
data
ethics
framework
tech
rep
united
kingdom
department
of
digital
culture
media
and
sport
buolamwini
gebru
2018
gender
shades
intersectional
accuracy
disparities
in
commercial
gender
classification
in
proceedings
of
machine
learning
research
vol
81
pp
15
calders
liobaite
2013
why
unbiased
computational
processes
can
lead
to
discriminative
decision
procedures
in
studies
in
applied
philosophy
epistemology
and
rational
ethics
vol
pp
43
57
springer
international
publishing
chang
raheem
rha
2018
novel
robotic
systems
and
future
directions
cheng
varshney
liu
2021
socially
responsible
ai
algorithms
issues
purposes
and
challenges
journal
of
artificial
intelligence
research
71
1137
1181
choi
lee
2018
an
artificial
intelligence
approach
to
financial
fraud
detection
under
iot
environment
survey
and
implementation
chouldechova
2017
fair
prediction
with
disparate
impact
study
of
bias
in
recidivism
prediction
instruments
big
data
153
163
ciampaglia
nematzadeh
menczer
flammini
2017
how
algorithmic
popularity
bias
hinders
or
promotes
quality
scientific
reports
corbett-davies
goel
2018
the
measure
and
mismeasure
of
fairness
critical
review
of
fair
machine
learning
arxiv
abs
1808.00023
corbett-davies
pierson
feller
goel
huq
2017
algorithmic
decision
making
and
the
cost
of
fairness
in
proceedings
of
the
23rd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
new
york
ny
usa
acm
cramer
garcia-gathright
springer
reddy
2018
assessing
and
addressing
algorithmic
bias
in
practice
interactions
25
58
63
cramer
reddy
bouyer
garcia-gathright
springer
2019
translation
tracks
data
an
algorithmic
bias
effort
in
practice
in
conference
on
human
factors
in
computing
systems
proceedings
association
for
computing
machinery
crawford
2013
the
hidden
biases
in
big
data
harvard
business
review
crawford
2017
the
trouble
with
bias
davis
1991
thinking
like
an
engineer
the
place
of
code
of
ethics
in
the
practice
of
profession
philosophy
and
public
affairs
20
150
167
dodge
liao
zhang
bellamy
dugan
2019
explaining
models
an
empirical
study
of
how
explanations
impact
fairness
judgment
in
23
richardson
gilbert
international
conference
on
intelligent
user
interfaces
proceedings
iui
vol
part
f1476
pp
275
285
association
for
computing
machinery
fazelpour
lipton
2020
algorithmic
fairness
from
non-ideal
perspective
aies
2020
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
57
63
fish
kun
lelkes
2016
confidence-based
approach
for
balancing
fairness
and
accuracy
16th
siam
international
conference
on
data
mining
2016
sdm
2016
144
152
frankel
1989
professional
codes
why
how
and
with
what
impact
journal
of
business
ethics
109
115
friedler
scheidegger
venkatasubramanian
choudhary
hamilton
roth
2018
comparative
study
of
fairness-enhancing
interventions
in
machine
learning
fat
2019
proceedings
of
the
2019
conference
on
fairness
accountability
and
transparency
329
338
friedler
scheidegger
venkatasubramanian
2021
the
im
possibility
of
fairness
communications
of
the
acm
64
136
143
friedman
kahn
borning
2002
value
sensitive
design
theory
and
methods
university
of
washington
technical
report
12
friedman
nissenbaum
1996
bias
in
computer
systems
acm
transactions
on
information
systems
14
330
347
garcia-gathright
springer
cramer
2018
assessing
and
addressing
algorithmic
bias
but
before
we
get
there
in
proceedings
ofthe
aaai2018
spring
symposium
designing
the
user
experience
ofartificial
intelligence
gebru
morgenstern
vecchione
vaughan
wallach
daume
iii
crawford
2018
datasheets
for
datasets
arxiv
geiger
yu
yang
dai
qiu
tang
huang
2019
garbage
in
garbage
out
do
machine
learning
application
papers
in
social
computing
report
where
human-labeled
training
data
comes
from
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
pp
325
336
association
for
computing
machinery
inc
goldenberg
nir
salcudean
2019
new
era
artificial
intelligence
and
machine
learning
in
prostate
cancer
nature
reviews
urology
16
391
403
gray
chivukula
2019
ethical
mediation
in
ux
practice
in
proceedings
of
the
2019
chi
conference
on
human
factors
in
computing
systems
association
for
computing
machinery
greene
hoffman
stark
2019
better
nicer
clearer
fairer
critical
assessment
of
the
movement
for
ethical
artificial
intelligence
and
machine
learning
in
proceedings
of
the
52nd
hawaii
international
conference
on
system
sciences
guszcza
2018
smarter
together
why
artificial
intelligence
needs
human-centric
design
tech
rep
22
deloitte
insights
24
framework
for
fairness
haibe-kains
adam
hosny
khodakarami
shraddha
kusko
sansone
tong
wolfinger
mason
jones
dopazo
furlanello
waldron
wang
mcintosh
goldenberg
kundaje
greene
broderick
hoffman
leek
korthauer
huber
brazma
pineau
tibshirani
hastie
ioannidis
quackenbush
aerts
2020
transparency
and
reproducibility
in
artificial
intelligence
nature
586
7829
e14
e16
harcourt
2007
against
prediction
profiling
policing
and
punishing
in
an
actuarial
age
university
of
chicago
press
chicago
il
hellstro
dignum
bensch
2020
bias
in
machine
learning
what
is
it
good
for
in
ceur
workshop
proceedings
vol
2659
pp
10
ceur-ws
hoffmann
2019
where
fairness
fails
data
algorithms
and
the
limits
of
antidiscrimination
discourse
information
communication
and
society
22
900
915
holstein
wortman
vaughan
daume
iii
dudı
wallach
2019
improving
fairness
in
machine
learning
systems
what
do
industry
practitioners
need
in
chi
conference
on
human
factors
in
computing
systems
acm
hu
kohler-hausmann
2020
what
sex
got
to
do
with
machine
learning
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
fat
20
513
new
york
ny
usa
association
for
computing
machinery
hutchinson
mitchell
2018
50
years
of
test
un
fairness
lessons
for
machine
learning
in
proceedings
of
the
2019
conference
on
fairness
accountability
and
transparency
pp
49
58
association
for
computing
machinery
inc
jacobs
wallach
2021
measurement
and
fairness
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
facct
21
375
385
new
york
ny
usa
association
for
computing
machinery
japkowicz
shah
2011
evaluating
learning
algorithms
classification
perspective
cambridge
university
press
jobin
ienca
vayena
2019
the
global
landscape
of
ai
ethics
guidelines
nature
machine
intelligence
389
399
johnson
bartola
angell
keith
witty
giguere
brun
2020
fairkit
fairkit
on
the
wall
who
the
fairest
of
them
all
supporting
data
scientists
in
training
fair
models
arxiv
abs
2012.09951
jordan
mitchell
2015
machine
learning
trends
perspectives
and
prospects
kasy
abebe
2021
fairness
equality
and
power
in
algorithmic
decision-making
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
vol
11
pp
576
586
association
for
computing
machinery
inc
kaur
nori
jenkins
caruana
wallach
wortman
vaughan
2020
interpreting
interpretability
understanding
data
scientists
use
of
interpretability
tools
for
machine
learning
in
chi
conference
on
human
factors
in
computing
systems
25
richardson
gilbert
kilbertus
rojas-carulla
parascandolo
hardt
janzing
scho
lkopf
2017
avoiding
discrimination
through
causal
reasoning
in
proceedings
of
the
2017
advances
in
neural
information
processing
systems
vol
30
kleinberg
mullainathan
raghavan
2017
inherent
trade-offs
in
the
fair
determination
of
risk
scores
leibniz
international
proceedings
in
informatics
lipics
67
23
lakkaraju
bastani
2019
how
do
fool
you
manipulating
user
trust
via
misleading
black
box
explanations
aies
2020
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
79
85
law
malik
du
sinha
2020a
designing
tools
for
semi-automated
detection
of
machine
learning
biases
an
interview
study
in
proceedings
of
the
chi
2020
workshop
on
detection
and
design
for
cognitive
biases
in
people
and
computing
systems
law
malik
du
sinha
2020b
the
impact
of
presentation
style
on
human-in-the-loop
detection
of
algorithmic
bias
in
graphics
interface
lifshitz
mcmaster
2020
legal
and
ethics
checklist
for
ai
systems
scitech
lawyer
17
28
34
lin
chen
chiang
hribar
2020
applications
of
artificial
intelligence
to
electronic
health
record
data
in
ophthalmology
translational
vision
science
and
technology
lowry
macpherson
1988
blot
on
the
profession
british
medical
journal
clinical
research
ed
296
6623
657
658
lum
isaac
2016
to
predict
and
serve
significance
13
14
19
madaio
stark
wortman
vaughan
wallach
2020
co-designing
checklists
to
understand
organizational
challenges
and
opportunities
around
fairness
in
ai
in
chi
conference
on
human
factors
in
computing
systems
honolulu
acm
manders-huits
zimmer
2009
values
and
pragmatic
action
the
challenges
of
introducing
ethical
intelligence
in
technical
design
communities
international
review
of
information
ethics
10
37
44
mehrabi
morstatter
saxena
lerman
galstyan
2021
survey
on
bias
and
fairness
in
machine
learning
acm
comput
surv
54
mester
2017
statistical
bias
types
explained
with
examples
mitchell
wu
zaldivar
barnes
vasserman
hutchinson
spitzer
raji
gebru
2019
model
cards
for
model
reporting
in
proceedings
of
the
2019
conference
on
fairness
accountability
and
transparency
pp
220
229
association
for
computing
machinery
inc
mittelstadt
2019
ai
ethics
too
principled
to
fail
nature
machine
intelligence
moysan
zeitoun
2019
chatbots
as
lever
to
redefine
customer
experience
in
banking
journal
of
digital
banking
242
249
26
framework
for
fairness
munavalli
rao
srinivasan
van
merode
2020
an
intelligent
realtime
scheduler
for
out-patient
clinics
multi-agent
system
model
health
informatics
journal
26
2383
2406
noble
2018
algorithms
of
oppression
how
search
engines
reinforce
racism
first
edition
nyu
press
olteanu
castillo
diaz
kıcıman
2019
social
data
biases
methodological
pitfalls
and
ethical
boundaries
frontiers
in
big
data
13
patil
mason
loukides
2018
of
oaths
and
checklists
rakova
yang
cramer
chowdhury
2020
where
responsible
ai
meets
reality
practitioner
perspectives
on
enablers
for
shifting
organizational
practices
reisman
schultz
crawford
whittaker
2018
algorithmic
impact
assessments
practical
framework
for
public
agency
accountability
tech
rep
ai
now
ribeiro
singh
guestrin
2016
why
should
trust
you
explaining
the
predictions
of
any
classifier
in
proceedings
of
the
22nd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
pp
1135
1144
richardson
garcia-gathright
spotify
way
jennifer
thom
henriette
cramer
garcia-gathright
thom
cramer
2021
towards
fairness
in
practice
practitioner-oriented
rubric
for
evaluating
fair
ml
toolkits
towards
fairness
in
practice
practitioner-oriented
rubric
for
evaluating
fair
ml
toolkits
in
chi
conference
on
human
factors
in
computing
systems
chi
21
yokohama
japan
richardson
prioleau
alikhademi
gilbert
2020
public
accountability
understanding
sentiments
towards
artificial
intelligence
across
dispositional
identities
in
ieee
2020
international
symposium
on
technology
and
society
ridgeway
2013
the
pitfalls
of
preduction
tech
rep
271
nij
journal
robert
pierce
marquis
kim
alahmad
2020
designing
fair
ai
in
human-computer
interaction
rovatsos
mittelstadt
koene
2019
landscape
summary
bias
in
algorithmic
decision-making
what
is
bias
in
algorithmic
decision-making
how
can
we
identify
it
and
how
can
we
mitigate
it
tech
rep
centre
for
data
ethics
and
innovation
saleiro
kuester
stevens
anisfeld
hinkson
london
ghani
2018
aequitas
bias
and
fairness
audit
toolkit
arxiv
abs
1811.05577
sandvig
2014
seeing
the
sort
the
aesthetic
and
industrial
defense
of
the
algorithm
journal
of
the
new
media
caucus
10
selbst
boyd
friedler
venkatasubramanian
vertesi
2019
fairness
and
abstraction
in
sociotechnical
systems
in
fat
2019
proceedings
of
the
2019
conference
on
fairness
accountability
and
transparency
pp
59
68
new
york
ny
usa
association
for
computing
machinery
inc
27
richardson
gilbert
seng
lee
singh
2021
the
landscape
and
gaps
in
open
source
fairness
toolkits
in
proceedings
of
the
2021
chi
conference
on
human
factors
in
computing
systems
acm
simoiu
corbett-davies
goel
2017
the
problem
of
infra-marginality
in
outcome
tests
for
discrimination
annals
of
applied
statistics
11
1193
1216
skitka
mosier
burdick
1999
does
automation
bias
decision-making
international
journal
of
human
computer
studies
51
991
1006
srinivasan
2020
ml-fairness-gym
tool
for
exploring
long-term
impacts
of
machine
learning
systems
google
ai
blog
stark
hoffmann
2019
data
is
the
new
what
popular
metaphors
professional
ethics
in
emerging
data
culture
journal
of
cultural
analytics
sunikka
bragge
kallio
2011
the
effectiveness
of
personalized
marketing
in
online
banking
comparison
between
search
and
experience
offerings
journal
of
financial
services
marketing
16
183
194
suresh
guttag
2019
framework
for
understanding
unintended
consequences
of
machine
learning
arxiv
abs
1901.10002
vasudevan
kenthapadi
2020
lift
scalable
framework
for
measuring
fairness
in
ml
applications
in
proceedings
of
the
29th
acm
international
conference
on
information
and
knowledge
management
veale
binns
2017
fairer
machine
learning
in
the
real
world
mitigating
discrimination
without
collecting
sensitive
data
big
data
society
veale
van
kleek
binns
2018
fairness
and
accountability
design
needs
for
algorithmic
support
in
high-stakes
public
sector
decision-making
conference
on
human
factors
in
computing
systems
proceedings
2018
april
verma
rubin
2018
fairness
definitions
explained
ieee
acm
international
workshop
on
software
fairness
18
wagstaff
2012
machine
learning
that
matters
in
proceedings
of
the
29th
international
conference
on
machine
learning
vol
pp
529
534
wexler
pushkarna
bolukbasi
wattenberg
vı
wilson
2019
the
what-if
tool
interactive
probing
of
machine
learning
models
ieee
transactions
on
visualization
and
computer
graphics
26
56
65
xiang
raji
2019
on
the
legal
compatibility
of
fairness
definitions
arxiv
zhong
2018
tutorial
on
fairness
in
machine
learning
zliobaite
2015
on
the
relation
between
accuracy
and
fairness
in
binary
classification
in
the
2nd
workshop
on
fairness
accountability
and
transparency
in
machine
learning
28