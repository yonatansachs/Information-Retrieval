data
mining
and
knowledge
discovery
2022
36
2074
2152
https://doi.org/10.1007/s10618-022-00854-z
algorithmic
fairness
datasets
the
story
so
far
alessandro
fabris1
stefano
messina1
gianmaria
silvello1
gian
antonio
susto1
received
27
august
2021
accepted
july
2022
published
online
17
september
2022
the
author
2022
abstract
data-driven
algorithms
are
studied
and
deployed
in
diverse
domains
to
support
critical
decisions
directly
impacting
people
well-being
as
result
growing
community
of
researchers
has
been
investigating
the
equity
of
existing
algorithms
and
propos
ing
novel
ones
advancing
the
understanding
of
risks
and
opportunities
of
automated
decision-making
for
historically
disadvantaged
populations
progress
in
fair
machine
learning
and
equitable
algorithm
design
hinges
on
data
which
can
be
appropriately
used
only
if
adequately
documented
unfortunately
the
algorithmic
fairness
commu
nity
as
whole
suffers
from
collective
data
documentation
debt
caused
by
lack
of
information
on
specific
resources
opacity
and
scatteredness
of
available
infor
mation
sparsity
in
this
work
we
target
this
data
documentation
debt
by
surveying
over
two
hundred
datasets
employed
in
algorithmic
fairness
research
and
producing
standardized
and
searchable
documentation
for
each
of
them
moreover
we
rigor
ously
identify
the
three
most
popular
fairness
datasets
namely
adult
compas
and
german
credit
for
which
we
compile
in-depth
documentation
this
unifying
doc
umentation
effort
supports
multiple
contributions
firstly
we
summarize
the
merits
and
limitations
of
adult
compas
and
german
credit
adding
to
and
unifying
recent
scholarship
calling
into
question
their
suitability
as
general-purpose
fairness
bench
marks
secondly
we
document
hundreds
of
available
alternatives
annotating
their
domain
and
supported
fairness
tasks
along
with
additional
properties
of
interest
for
responsible
editor
toon
calders
this
is
the
extended
version
of
fabris
et
al
2022
alessandro
fabris
fabrisal@dei.unipd.it
gianmaria
silvello
silvello@dei.unipd.it
gian
antonio
susto
gianantonio.susto@unipd.it
dipartimento
di
ingegneria
dell
informazione
università
di
padova
via
giovanni
gradenigo
6b
35131
padua
italy
123
algorithmic
fairness
datasets
the
story
so
far
2075
fairness
practitioners
and
researchers
including
their
format
cardinality
and
the
sen
sitive
attributes
they
encode
we
summarize
this
information
zooming
in
on
the
tasks
domains
and
roles
of
these
resources
finally
we
analyze
these
datasets
from
the
per
spective
of
five
important
data
curation
topics
anonymization
consent
inclusivity
labeling
of
sensitive
attributes
and
transparency
we
discuss
different
approaches
and
levels
of
attention
to
these
topics
making
them
tangible
and
distill
them
into
set
of
best
practices
for
the
curation
of
novel
resources
keywords
algorithmic
fairness
datasets
documentation
debt
introduction
following
the
widespread
study
and
application
of
data-driven
algorithms
in
con
texts
that
are
central
to
people
well-being
large
community
of
researchers
has
coalesced
around
the
growing
field
of
algorithmic
fairness
investigating
algorithms
through
the
lens
of
justice
equity
bias
power
and
harms
line
of
work
gaining
trac
tion
in
the
field
intersecting
with
critical
data
studies
human
computer
interaction
and
computer-supported
cooperative
work
focuses
on
data
transparency
and
standard
ized
documentation
processes
to
describe
key
characteristics
of
datasets
gebru
et
al
2018
holland
et
al
2018
bender
and
friedman
2018
geiger
et
al
2020
jo
and
gebru
2020
miceli
et
al
2021
most
prominently
gebru
et
al
2018
and
holland
et
al
2018
proposed
two
complementary
documentation
frameworks
called
datasheets
for
datasets
and
dataset
nutrition
labels
to
improve
data
curation
practices
and
favour
more
informed
data
selection
and
utilization
for
dataset
users
overall
this
line
of
work
has
contributed
to
an
unprecedented
attention
to
dataset
documentation
in
machine
learning
ml
including
novel
track
focused
on
datasets
at
the
confer
ence
on
neural
information
processing
systems
neurips
an
initiative
to
support
dataset
tracking
in
repositories
for
scholarly
articles
and
dedicated
works
producing
retrospective
documentation
for
existing
datasets
bandy
and
vincent
2021
garbin
et
al
2021
auditing
their
properties
prabhu
and
birhane
2020
and
tracing
their
usage
peng
et
al
2021
in
recent
work
bender
et
al
2021
propose
the
notion
of
documentation
debt
in
relation
to
training
sets
that
are
undocumented
and
too
large
to
document
retrospec
tively
we
extend
this
definition
to
the
collection
of
datasets
employed
in
given
field
of
research
we
see
two
components
at
work
contributing
to
the
documentation
debt
of
research
community
on
one
hand
opacity
is
the
result
of
poor
documentation
affecting
single
datasets
contributing
to
misunderstandings
and
misuse
of
specific
resources
on
the
other
hand
when
relevant
information
exists
but
does
not
reach
interested
parties
there
is
problem
of
documentation
sparsity
one
example
that
is
particularly
relevant
for
the
algorithmic
fairness
community
is
represented
by
the
german
credit
dataset
uci
machine
learning
repository
1994
popular
resource
in
this
field
many
works
of
algorithmic
fairness
including
recent
ones
carry
out
experiments
on
this
dataset
using
sex
as
protected
attribute
he
et
al
2020b
yang
https://medium.com/paperswithcode/datasets-on-arxiv-1a5a8f7bd104.
123
2076
fabris
et
al
et
al
2020a
baharlouei
et
al
2020
lohaus
et
al
2020
martinez
et
al
2020
wang
et
al
2021
perrone
et
al
2021
sharma
et
al
2021
while
existing
yet
overlooked
documentation
shows
that
this
feature
cannot
be
reliably
retrieved
grömping
2019
moreover
the
mere
fact
that
dataset
exists
and
is
relevant
to
given
task
or
given
domain
may
be
unknown
the
bupt
faces
datasets
for
instance
were
presented
as
the
second
existing
resource
for
face
analysis
with
race
annotations
wang
and
deng
2020
however
several
resources
were
already
available
at
the
time
including
labeled
faces
in
the
wild
han
and
jain
2014
utk
face
zhang
et
al
2017b
racial
faces
in
the
wild
wang
et
al
2019e
and
diversity
in
faces
merler
et
al
2019
to
tackle
the
documentation
debt
of
the
algorithmic
fairness
community
we
survey
the
datasets
used
in
over
500
articles
on
fair
ml
and
equitable
algorithmic
design
presented
at
seven
major
conferences
considering
each
edition
in
the
period
2014
2021
and
more
than
twenty
domain-specific
workshops
in
the
same
period
we
find
over
200
datasets
employed
in
studies
of
algorithmic
fairness
for
which
we
produce
compact
and
standardized
documentation
called
data
briefs
data
briefs
are
intended
as
lightweight
format
to
document
fundamental
properties
of
data
artifacts
used
in
algorithmic
fairness
including
their
purpose
their
features
with
particular
attention
to
sensitive
ones
the
underlying
labeling
procedure
and
the
envisioned
ml
task
if
any
to
favor
domain-based
and
task-based
search
from
dataset
users
data
briefs
also
indicate
the
domain
of
the
processes
that
produced
the
data
radiology
and
list
the
fairness
tasks
studied
on
given
dataset
fair
ranking
for
this
endeavour
we
have
contacted
creators
and
knowledgeable
practitioners
identified
as
primary
points
of
contact
for
the
datasets
we
received
feedback
incorporated
into
the
final
version
of
the
data
briefs
from
79
curators
and
practitioners
whose
contribution
is
acknowledged
at
the
end
of
this
article
moreover
we
identify
and
carefully
analyze
the
three
datasets
most
often
utilized
in
the
surveyed
articles
adult
compas
and
german
credit
retrospectively
producing
datasheet
and
nutrition
label
for
each
of
them
from
these
documentation
efforts
we
extract
summary
of
the
merits
and
limitations
of
popular
algorithmic
fairness
benchmarks
categorization
of
domains
and
fairness
tasks
for
existing
datasets
and
set
of
best
practices
for
the
curation
of
novel
resources
overall
we
make
the
following
contributions
unified
analysis
of
popular
fairness
benchmarks
we
produce
datasheets
and
nutrition
labels
for
adult
compas
and
german
credit
from
which
we
extract
summary
of
their
merits
and
limitations
we
add
to
and
unify
recent
scholarship
on
these
datasets
calling
into
question
their
suitability
as
general-purpose
fairness
benchmarks
due
to
contrived
prediction
tasks
noisy
data
severe
coding
mistakes
and
age
survey
of
existing
alternatives
we
compile
standardized
and
compact
documen
tation
for
over
two
hundred
resources
used
in
fair
ml
research
annotating
their
domain
the
tasks
they
support
and
the
roles
they
play
in
works
of
algorithmic
fairness
by
assembling
sparse
information
on
hundreds
of
datasets
into
sin
gle
document
we
aim
to
support
multiple
goals
by
researchers
and
practitioners
hereafter
for
brevity
we
only
report
dataset
names
the
relevant
references
and
additional
information
can
be
found
in
appendix
123
algorithmic
fairness
datasets
the
story
so
far
2077
including
domain-oriented
and
task-oriented
search
by
dataset
users
contextually
we
provide
novel
categorization
of
tasks
and
domains
investigated
in
algorithmic
fairness
research
summarized
in
tables
and
best
practices
for
the
curation
of
novel
resources
we
analyze
different
approaches
to
anonymization
consent
inclusivity
labeling
and
transparency
across
these
datasets
by
comparing
existing
approaches
and
discussing
their
advantages
we
make
the
underlying
concerns
visible
and
practical
and
extract
best
practices
to
inform
the
curation
of
new
datasets
and
post-hoc
remedies
to
existing
ones
the
rest
of
this
work
is
organized
as
follows
section
introduces
related
works
section
presents
the
methodology
and
inclusion
criteria
of
this
survey
section
ana
lyzes
the
perks
and
limitations
of
the
most
popular
datasets
namely
adult
sect
4.1
compas
sect
4.2
and
german
credit
sect
4.3
and
provides
an
overall
sum
mary
of
their
merits
and
limitations
as
fairness
benchmarks
sect
4.4
section
discusses
alternative
fairness
resources
from
the
perspective
of
the
underlying
domains
sect
5.1
the
fair
ml
tasks
they
support
sect
5.2
and
the
roles
they
play
sect
5.3
section
presents
important
topics
in
data
curation
discussing
existing
approaches
and
best
practices
to
avoid
re-identification
sect
6.1
elicit
informed
consent
sect
6.2
consider
inclusivity
sect
6.3
collect
sensitive
attributes
sect
6.4
and
document
datasets
sect
6.5
section
summarizes
the
broader
benefits
of
our
documentation
effort
and
envisioned
uses
for
the
research
community
finally
sect
contains
con
cluding
remarks
and
recommendations
interested
readers
may
find
the
data
briefs
in
appendix
followed
by
the
detailed
documentation
produced
for
adult
com
pas
and
german
credit
related
work
2.1
algorithmic
fairness
surveys
multiple
surveys
about
algorithmic
fairness
have
been
published
in
the
literature
mehrabi
et
al
2021
caton
and
haas
2020
pessach
and
shmueli
2020
these
works
typically
focus
on
describing
and
classifying
important
measures
of
algorithmic
fair
ness
and
methods
to
enhance
it
some
articles
also
discuss
sources
of
bias
mehrabi
et
al
2021
software
packages
and
projects
which
address
fairness
in
ml
caton
and
haas
2020
or
describe
selected
sub-fields
of
algorithmic
fairness
pessach
and
shmueli
2020
datasets
are
typically
not
emphasized
in
these
works
which
is
also
true
of
domain-specific
surveys
on
algorithmic
fairness
focused
on
ranking
pitoura
et
al
2021
natural
language
processing
nlp
sun
et
al
2019
and
computational
medicine
sun
et
al
2019
as
an
exception
pessach
and
shmueli
2020
and
zehlike
et
al
2021
list
and
briefly
describe
12
popular
algorithmic
fairness
datasets
and
19
datasets
employed
in
fair
ranking
research
respectivey
123
2078
fabris
et
al
2.2
data
studies
the
work
most
closely
related
and
concurrently
carried
out
to
ours
is
le
quy
et
al
2022
the
authors
perform
detailed
analysis
of
15
tabular
datasets
used
in
works
of
algorithmic
fairness
listing
important
metadata
domain
protected
attributes
collection
period
and
location
and
carrying
out
an
exploratory
analysis
of
the
prob
abilistic
relationship
between
features
our
work
complements
it
by
placing
more
emphasis
on
rigorous
methodology
for
the
inclusion
of
resources
wider
selection
of
over
200
datasets
spanning
different
data
types
including
text
image
timeseries
and
tabular
data
fine-grained
evaluation
of
domains
and
tasks
asso
ciated
with
each
dataset
and
the
analysis
and
distillation
of
best
practices
for
data
curation
it
will
be
interesting
to
see
how
different
goals
of
the
research
community
such
as
selection
of
appropriate
resources
for
experimentation
and
data
studies
can
benefit
from
the
breadth
and
depth
of
both
works
other
works
analyzing
multiple
datasets
along
specific
lines
have
been
published
in
recent
years
crawford
and
paglen
2021
focus
on
resources
commonly
used
as
training
sets
in
computer
vision
with
attention
to
associated
labels
and
underlying
taxonomies
fabbrizzi
et
al
2021
also
consider
computer
vision
datasets
describing
types
of
bias
affecting
them
along
with
methods
for
discovering
and
measuring
bias
peng
et
al
2021
analyze
ethical
concerns
in
three
popular
face
and
person
recognition
datasets
stemming
from
derivative
datasets
and
models
lack
of
clarity
of
licenses
and
dataset
management
practices
geiger
et
al
2020
evaluate
transparency
in
the
documentation
of
labeling
practices
employed
in
over
100
datasets
about
twitter
leonelli
and
tempini
2020
study
practices
of
collection
cleaning
visualization
sharing
and
analysis
across
variety
of
research
domains
romei
and
ruggieri
2014
survey
techniques
and
data
for
discrimination
analysis
focused
on
measuring
rather
than
enforcing
equity
in
human
processes
different
yet
related
family
of
articles
provides
deeper
analyses
of
single
datasets
prabhu
and
birhane
2020
focus
on
imagenet
ilsvrc
2012
which
they
analyze
along
the
lines
of
consent
problematic
content
and
individual
re-identification
kizh
ner
et
al
2020
study
issues
of
representation
in
the
google
arts
and
culture
project
across
countries
cities
and
institutions
some
works
provide
datasheets
for
given
resource
such
as
chexpert
garbin
et
al
2021
and
the
bookcorpus
bandy
and
vincent
2021
among
popular
fairness
datasets
compas
has
drawn
scrutiny
from
multiple
works
analysing
its
numerical
idiosyncrasies
barenstein
2019
and
sources
of
bias
bao
et
al
2021
ding
et
al
2021
study
numerical
idiosyncrasies
in
the
adult
dataset
and
propose
novel
version
for
which
they
provide
datasheet
grömping
2019
discuss
issues
resulting
from
coding
mistakes
in
german
credit
our
work
combines
the
breadth
of
multi-dataset
and
the
depth
of
single-dataset
studies
on
one
hand
we
survey
numerous
resources
used
in
works
of
algorithmic
fairness
analyzing
them
across
multiple
dimensions
on
the
other
hand
we
identify
the
most
popular
resources
compiling
their
datasheet
and
nutrition
label
and
summarize
their
perks
and
limitations
moreover
by
making
our
data
briefs
available
we
hope
to
contribute
useful
tool
to
the
research
community
favouring
further
data
studies
and
analyses
as
outlined
in
sect
123
algorithmic
fairness
datasets
the
story
so
far
2079
2.3
documentation
frameworks
several
data
documentation
frameworks
have
been
proposed
in
the
literature
three
popular
ones
are
described
below
datasheets
for
datasets
gebru
et
al
2018
are
general-purpose
qualitative
framework
with
over
fifty
questions
covering
key
aspects
of
datasets
such
as
motivation
composition
collection
preprocessing
uses
dis
tribution
and
maintenance
another
qualitative
framework
is
represented
by
data
statements
bender
and
friedman
2018
which
is
tailored
for
nlp
requiring
domain
specific
information
on
language
variety
and
speaker
demographics
dataset
nutrition
labels
holland
et
al
2018
describe
complementary
quantitative
framework
focused
on
numerical
aspects
such
as
the
marginal
and
joint
distribution
of
variables
popular
datasets
require
close
scrutiny
for
this
reason
we
adopt
these
frame
works
producing
three
datasheets
and
nutrition
labels
for
adult
german
credit
and
compas
this
approach
however
does
not
scale
to
wider
documentation
effort
with
limited
resources
for
this
reason
we
propose
and
produce
data
briefs
lightweight
documentation
format
designed
for
algorithmic
fairness
datasets
data
briefs
described
in
appendix
include
fields
specific
to
fair
ml
such
sensitive
attributes
and
tasks
for
which
the
dataset
has
been
used
in
the
algorithmic
fairness
literature
methodology
in
this
work
we
consider
every
article
published
in
the
proceedings
of
domain
specific
conferences
such
as
the
acm
conference
on
fairness
accountability
and
transparency
facct
and
the
aaai
acm
conference
on
artificial
intelligence
ethics
and
society
aies
every
article
published
in
proceedings
of
well-known
machine
learning
and
data
mining
conferences
including
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
the
conference
on
neural
information
processing
systems
neurips
the
international
conference
on
machine
learning
icml
the
international
conference
on
learning
representations
iclr
the
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
min
ing
kdd
every
article
available
from
past
network
events
and
older
workshops
and
events
of
the
facct
network
we
consider
the
period
from
2014
the
year
of
the
first
workshop
on
fairness
accountability
and
transparency
in
machine
learning
to
june
2021
thus
including
works
presented
at
facct
iclr
aies
and
cvpr
in
2021.4
to
target
works
of
algorithmic
fairness
we
select
subsample
of
these
articles
whose
titles
contain
either
of
the
following
strings
where
the
star
symbol
represents
the
wildcard
character
fair
targeting
fairness
unfair
bias
biased
debiasing
discriminat
discrimination
discriminatory
equal
equality
unequal
equit
equity
equitable
disparate
disparate
impact
parit
par
https://facctconference.org/network/.
we
are
working
on
an
update
covering
more
recent
work
including
articles
presented
at
the
acm
conference
on
equity
and
access
in
algorithms
mechanisms
and
optimization
123
2080
fabris
et
al
ity
disparities
these
selection
criteria
are
centered
around
equity-based
notions
of
fairness
typically
operationalized
by
measuring
disparity
in
some
algorithmic
prop
erty
across
individuals
or
groups
of
individuals
through
manual
inspection
by
two
authors
we
discard
articles
where
these
keywords
are
used
with
different
mean
ing
discarded
works
for
instance
include
articles
on
handling
pose
distribution
bias
zhao
et
al
2021
compensating
selection
bias
to
improve
accuracy
without
attention
to
sensitive
attributes
kato
et
al
2019
enhancing
desirable
discriminating
properties
of
models
chen
et
al
2018a
or
generally
focused
on
model
performance
li
et
al
2018
zhong
et
al
2019
this
leaves
us
with
558
articles
from
the
articles
that
pass
this
initial
screening
we
select
datasets
treated
as
impor
tant
data
artifacts
either
being
used
to
train
test
an
algorithm
or
undergoing
data
audit
an
in-depth
analysis
of
different
properties
we
produce
data
brief
for
these
datasets
by
reading
the
information
provided
in
the
surveyed
articles
consulting
the
provided
references
and
reviewing
scholarly
articles
or
official
websites
found
by
querying
popular
search
engines
with
the
dataset
name
we
discard
the
following
word
embeddings
wes
we
only
consider
the
corpora
they
are
trained
on
pro
vided
wes
are
trained
as
part
of
given
work
and
not
taken
off
the
shelf
toy
datasets
simulations
with
no
connection
to
real-world
processes
unless
they
are
used
in
more
than
one
article
which
we
take
as
sign
of
importance
in
the
field
auxiliary
resources
that
are
only
used
as
minor
source
of
ancillary
information
such
as
the
percentage
of
us
residents
in
each
state
datasets
for
which
the
available
information
is
insufficient
this
happens
very
seldom
when
points
and
outlined
above
result
in
little
to
no
information
about
the
curators
purposes
features
and
format
of
dataset
for
popular
datasets
this
is
never
the
case
for
each
of
the
226
datasets
satisfying
the
above
criteria
we
produce
data
brief
available
in
appendix
with
description
of
the
underlying
coding
procedure
from
this
effort
we
rigorously
identify
the
three
most
popular
resources
whose
perks
and
limitations
are
summarized
in
the
next
section
most
popular
datasets
figure
depicts
the
number
of
articles
using
each
dataset
showing
that
dataset
uti
lization
in
surveyed
scholarly
works
follows
long
tail
distribution
over
100
datasets
are
only
used
once
also
because
some
of
these
resources
are
not
publicly
available
complementing
this
long
tail
is
short
head
of
nine
resources
used
in
ten
or
more
articles
these
datasets
are
adult
118
usages
compas
81
german
credit
35
communities
and
crime
26
bank
marketing
19
law
school
17
celeba
16
movielens
14
and
credit
card
default
11
the
tenth
most
used
resource
is
the
toy
dataset
from
zafar
et
al
2017c
used
in
articles
in
this
section
we
summarize
positive
and
negative
aspects
of
the
three
most
popular
datasets
namely
adult
com
123
algorithmic
fairness
datasets
the
story
so
far
2081
fig
utilization
of
datasets
in
fairness
research
follows
long
tail
distribution
pas
and
german
credit
informed
by
extensive
documentation
in
appendices
and
4.1
adult
the
adult
dataset
was
created
as
resource
to
benchmark
the
performance
of
machine
learning
algorithms
on
socially
relevant
data
each
instance
is
person
who
responded
to
the
march
1994
us
current
population
survey
represented
along
demographic
and
socio-economic
dimensions
with
features
describing
their
profession
education
age
sex
race
personal
and
financial
condition
the
dataset
was
extracted
from
the
census
database
preprocessed
and
donated
to
uci
machine
learning
repository
in
1996
by
ronny
kohavi
and
barry
becker
binary
variable
encoding
whether
respondents
income
is
above
50
000
was
chosen
as
the
target
of
the
prediction
task
associated
with
this
resource
adult
inherits
some
positive
sides
from
the
best
practices
employed
by
the
us
census
bureau
although
later
filtered
somewhat
arbitrarily
the
original
sample
was
designed
to
be
representative
of
the
us
population
trained
and
compensated
inter
viewers
collected
the
data
attributes
in
the
dataset
are
self-reported
and
provided
by
consensual
respondents
finally
the
original
data
from
the
us
census
bureau
is
well
documented
and
its
variables
can
be
mapped
to
adult
by
consulting
the
original
doc
umentation
us
dept
of
commerce
bureau
of
the
census
1995
except
for
variable
denominated
fnlwgt
whose
precise
meaning
is
unclear
negative
aspect
of
this
dataset
is
the
contrived
prediction
task
associated
with
it
income
prediction
from
socio-economic
factors
is
task
whose
social
utility
appears
rather
limited
even
discounting
this
aspect
the
arbitrary
50
000
threshold
for
the
binary
prediction
task
is
high
and
model
properties
such
as
accuracy
and
fairness
are
very
sensitive
to
it
ding
et
al
2021
furthermore
there
are
several
sources
of
noise
affecting
the
data
roughly
of
the
data
points
have
missing
values
plausibly
due
to
123
2082
fabris
et
al
issues
with
data
recording
and
coding
or
respondents
inability
to
recall
information
moreover
the
tendency
in
household
surveys
for
respondents
to
under-report
their
income
is
common
concern
of
the
census
bureau
moore
et
al
2000
another
source
of
noise
is
top-coding
of
the
variable
capital-gain
saturation
to
99
999
to
avoid
the
re-identification
of
certain
individuals
us
dept
of
commerce
bureau
of
the
census
1995
finally
the
dataset
is
rather
old
sensitive
attribute
race
contains
the
outdated
asian
pacific
islander
class
it
is
worth
noting
that
set
of
similar
resources
was
recently
made
available
allowing
more
current
socio-economic
studies
of
the
us
population
ding
et
al
2021
4.2
compas
this
dataset
was
created
for
an
external
audit
of
racial
biases
in
the
correctional
offender
management
profiling
for
alternative
sanctions
compas
risk
assessment
tool
developed
by
northpointe
now
equivant
which
estimates
the
likelihood
of
defendant
becoming
recidivist
instances
represent
defendants
scored
by
compas
in
broward
county
florida
between
2013
2014
reporting
their
demographics
crim
inal
record
custody
and
compas
scores
defendants
public
criminal
records
were
obtained
from
the
broward
county
clerk
office
website
matching
them
based
on
date
of
birth
first
and
last
names
the
dataset
was
augmented
with
jail
records
and
compas
scores
provided
by
the
broward
county
sheriff
office
finally
public
incarcera
tion
records
were
downloaded
from
the
florida
department
of
corrections
website
instances
are
associated
with
two
target
variables
is_recid
and
is_violent_recid
indi
cating
whether
defendants
were
booked
in
jail
for
criminal
offense
potentially
violent
that
occurred
after
their
compas
screening
but
within
two
years
on
the
upside
this
dataset
is
recent
and
captures
some
relevant
aspects
of
the
compas
risk
assessment
tool
and
the
criminal
justice
system
in
broward
county
on
the
downside
it
was
compiled
from
disparate
sources
hence
clerical
errors
and
mismatches
are
present
larson
et
al
2016
moreover
in
its
official
release
prop
ublica
2016
the
compas
dataset
features
redundant
variables
and
data
leakage
due
to
spuriously
time-dependent
recidivism
rates
barenstein
2019
for
these
reasons
researchers
must
perform
further
preprocessing
in
addition
to
the
standard
one
by
propublica
more
subjective
choices
are
required
of
researchers
interested
in
counter
factual
evaluation
of
risk-assessment
tools
due
to
the
absence
of
clear
indication
of
whether
defendants
were
detained
or
released
pre-trial
mishler
et
al
2021
the
lack
of
standard
preprocessing
protocol
beyond
the
one
by
propublica
propublica
2016
which
is
insufficient
to
handle
these
factors
may
cause
issues
of
reproducibility
and
difficulty
in
comparing
methods
moreover
according
to
northpointe
response
to
the
propublica
study
several
risk
factors
considered
by
the
compas
algorithm
are
absent
from
the
dataset
dieterich
et
al
2016
as
an
additional
concern
race
cate
gories
lack
native
hawaiian
or
other
pacific
islander
while
hispanic
is
redefined
as
race
instead
of
ethnicity
bao
et
al
2021
finally
defendants
personal
information
race
and
criminal
history
is
available
in
conjunction
with
obvious
identifiers
making
re-identification
of
defendants
trivial
123
table
limitations
of
popular
algorithmic
fairness
datasets
adult
compas
german
credit
age
old
1994
recent
2013
2016
very
old
1973
1975
algorithmic
fairness
datasets
the
story
so
far
prediction
task
contrived
income
50k
realistic
recidivism
realistic
creditworthiness
sensitive
attributes
outdated
racial
categories
outdated
racial
categories
sex
cannot
be
retrieved
sources
of
noise
top-coding
tendency
to
under-report
data
leakage
label
bias
clerical
incorrect
code
table
income
errors
sample
representativeness
us
working
population
convenience
sample
broward
artificial
sample
credit
granted
county
negative
class
oversampled
preprocessing
needed
handling
missing
values
handling
missing
values
80
none
removing
redundant
features
ground
truth
on
detainment
additional
concerns
accuracy
and
fairness
are
sensitive
to
potential
for
misguided
discussion
on
interpretability
and
exploratory
arbitrary
50k
threshold
criminal
justice
analyses
are
invalid
2083
123
2084
fabris
et
al
compas
also
represents
case
of
broad
phenomenon
which
can
be
termed
data
bias
with
terminology
from
friedler
et
al
2021
when
it
comes
to
datasets
encoding
complex
human
phenomena
there
is
often
disconnect
between
the
construct
space
what
we
aim
to
measure
and
the
observed
space
what
we
end
up
observing
this
may
be
especially
problematic
if
the
difference
between
construct
and
observation
is
uneven
across
individuals
or
groups
compas
for
example
is
dataset
about
criminal
offense
offense
is
central
to
the
prediction
target
aimed
at
encoding
recidivism
and
to
the
available
covariates
summarizing
criminal
history
however
the
compas
dataset
observed
space
is
an
imperfect
proxy
for
the
criminal
patterns
it
should
summarize
construct
space
the
prediction
labels
actually
encode
re
arrest
instead
of
re-offense
larson
et
al
2016
and
are
thus
clearly
influenced
by
spatially
differentiated
policing
practices
fogliato
et
al
2021
this
is
also
true
of
criminal
history
encoded
in
compas
covariates
again
mediated
by
arrest
and
policing
practices
which
may
be
racially
biased
bao
et
al
2021
mayson
2018
as
result
the
true
fairness
of
an
algorithm
just
like
its
accuracy
may
differ
significantly
from
what
is
reported
on
biased
data
for
example
algorithms
that
achieve
equality
of
true
positive
rates
across
sensitive
groups
on
compas
are
deemed
fair
under
the
equal
opportunity
measure
hardt
et
al
2016
however
if
both
the
training
set
on
which
this
objective
is
enforced
and
the
test
set
on
which
it
is
measured
are
affected
by
race-dependent
noise
described
above
those
algorithms
are
only
fair
in
an
abstract
observed
space
but
not
in
the
real
construct
space
we
ultimately
care
about
friedler
et
al
2021
overall
these
considerations
paint
mixed
picture
for
dataset
of
high
social
rel
evance
that
was
extremely
useful
to
catalyze
attention
on
algorithmic
fairness
issues
displaying
at
the
same
time
several
limitations
in
terms
of
its
continued
use
as
flexible
benchmark
for
fairness
studies
of
all
sorts
in
this
regard
bao
et
al
2021
suggest
avoiding
the
use
of
compas
to
demonstrate
novel
approaches
in
algorithmic
fairness
as
considering
the
data
without
proper
context
may
lead
to
misleading
con
clusions
which
could
misguidedly
enter
the
broader
debate
on
criminal
justice
and
risk
assessment
4.3
german
credit
the
german
credit
dataset
was
created
to
study
the
problem
of
computer-assisted
credit
decisions
at
regional
bank
in
southern
germany
instances
represent
loan
applicants
from
1973
to
1975
who
were
deemed
creditworthy
and
were
granted
loan
bringing
about
natural
selection
bias
within
this
sample
bad
credits
are
oversampled
to
favour
balance
in
target
classes
grömping
2019
the
data
summarizes
applicants
financial
situation
credit
history
and
personal
situation
including
housing
and
number
of
liable
people
binary
variable
encoding
whether
each
loan
recipient
punctually
payed
every
installment
is
the
target
of
classification
task
among
the
covariates
marital
status
and
sex
are
jointly
encoded
in
single
variable
many
documentation
mistakes
are
present
in
the
uci
entry
associated
with
this
resource
uci
machine
learning
repository
1994
revised
version
with
correct
variable
encodings
called
123
algorithmic
fairness
datasets
the
story
so
far
2085
south
german
credit
was
donated
to
uci
machine
learning
repository
2019
with
an
accompanying
report
grömping
2019
the
greatest
upside
of
this
dataset
is
the
fact
that
it
captures
real-world
application
of
credit
scoring
at
bank
on
the
downside
the
data
is
half
century
old
significantly
limiting
the
societally
useful
insights
that
can
be
gleaned
from
it
most
importantly
the
popular
release
of
this
dataset
uci
machine
learning
repository
1994
comes
with
highly
inaccurate
documentation
which
contains
wrong
variable
codings
for
example
the
variable
reporting
whether
loan
recipients
are
foreign
workers
has
its
coding
reversed
so
that
apparently
fewer
than
of
the
loan
recipients
in
the
dataset
would
be
german
luckily
this
error
has
no
impact
on
numerical
results
obtained
from
this
dataset
as
it
is
irrelevant
at
the
level
of
abstraction
afforded
by
raw
features
with
the
exception
of
potentially
counterintuitive
explanations
in
works
of
interpretability
and
exploratory
analysis
le
quy
et
al
2022
this
coding
error
along
with
others
discussed
in
grömping
2019
was
corrected
in
novel
release
of
the
dataset
uci
machine
learning
repository
2019
unfortunately
and
most
importantly
for
the
fair
ml
community
retrieving
the
sex
of
loan
applicants
is
simply
not
possible
unlike
the
original
documentation
suggested
this
is
due
to
the
fact
that
one
value
of
this
feature
was
used
to
indicate
both
women
who
are
divorced
separated
or
married
and
men
who
are
single
while
the
original
documentation
reported
each
feature
value
to
correspond
to
same-sex
applicants
either
male-only
or
female-only
this
particular
coding
error
ended
up
having
non-negligible
impact
on
the
fair
ml
community
where
many
works
studying
group
fairness
extract
sex
from
the
joint
variable
and
use
it
as
sensitive
attribute
even
years
after
the
redacted
documentation
was
published
wang
et
al
2021
le
quy
et
al
2022
these
coding
mistakes
are
part
of
documentation
debt
whose
influence
continues
to
affect
the
algorithmic
fairness
community
4.4
summary
adult
compas
and
german
credit
are
the
most
used
datasets
in
the
surveyed
algo
rithmic
fairness
literature
despite
the
limitations
summarized
in
table
their
status
as
de
facto
fairness
benchmarks
is
probably
due
to
their
use
in
seminal
works
pedreshi
et
al
2008
calders
et
al
2009
and
influential
articles
angwin
et
al
2016
on
algo
rithmic
fairness
once
this
fame
was
created
researchers
had
clear
incentives
to
study
novel
problems
and
approaches
on
these
datasets
which
as
result
have
become
even
more
established
benchmarks
in
the
algorithmic
fairness
literature
bao
et
al
2021
on
close
scrutiny
the
fundamental
merit
of
these
datasets
lies
in
originating
from
human
processes
encoding
protected
attributes
and
having
different
base
rates
for
the
target
variable
across
sensitive
groups
their
use
in
recent
works
on
algorithmic
fairness
can
be
interpreted
as
signal
that
the
authors
have
basic
awareness
of
default
data
practices
in
the
field
and
that
the
data
was
not
made
up
to
fit
the
algorithm
over
arching
claims
of
significance
in
real-world
scenarios
stemming
from
experiments
on
these
datasets
should
be
met
with
skepticism
experiments
that
claim
extracting
sex
variable
from
the
german
credit
dataset
should
be
considered
noisy
at
best
as
for
alternatives
bao
et
al
2021
suggest
employing
well-designed
simulations
com
plementary
avenue
is
to
seek
different
datasets
that
are
relevant
for
the
problem
at
hand
123
2086
fabris
et
al
we
hope
that
the
two
hundred
data
briefs
accompanying
this
work
will
prove
useful
in
this
regard
favouring
both
domain-oriented
and
task-oriented
searches
according
to
the
classification
discussed
in
the
next
section
existing
alternatives
in
this
section
we
discuss
existing
fairness
resources
from
three
different
perspectives
in
sect
5.1
we
describe
the
different
domains
spanned
by
fairness
datasets
in
sect
5.2
we
provide
categorization
of
fairness
tasks
supported
by
the
same
resources
in
sect
5.3
we
discuss
the
different
roles
played
by
these
datasets
in
fairness
research
such
as
supporting
training
and
benchmarking
5.1
domain
algorithmic
fairness
concerns
arise
in
any
domain
where
automated
decision
making
adm
systems
may
influence
human
well-being
unsurprisingly
the
datasets
in
our
survey
reflect
variety
of
areas
where
adm
systems
are
studied
or
deployed
including
criminal
justice
education
search
engines
online
marketplaces
emergency
response
social
media
medicine
and
hiring
in
fig
we
report
subdivision
of
the
surveyed
datasets
in
different
macrodomains
we
mostly
follow
the
area-category
taxonomy
by
scimago
departing
from
it
where
appropriate
for
example
we
consider
computer
vision
and
linguistics
macrodomains
of
their
own
for
the
purposes
of
algorithmic
fairness
as
much
fair
ml
work
has
been
published
in
both
disciplines
below
we
present
description
of
each
macrodomain
and
its
main
subdomains
summarized
in
detail
in
table
computer
science
datasets
from
this
macrodomain
are
very
well
represented
comprising
information
systems
social
media
library
and
information
sciences
com
puter
networks
and
signal
processing
information
systems
heavily
feature
datasets
on
search
engines
for
various
items
such
as
text
images
worker
profiles
and
real
estate
retrieved
in
response
to
queries
issued
by
users
occupations
in
google
images
scientist
painter
zillow
searches
barcelona
room
rental
burst
taskrab
bit
online
freelance
marketplaces
bing
us
queries
symptoms
in
queries
other
datasets
represent
problems
of
item
recommendation
covering
products
businesses
and
movies
amazon
recommendations
amazon
reviews
google
local
movie
lens
filmtrust
the
remaining
datasets
in
this
subdomain
represent
knowledge
bases
freebase15k-237
wikidata
and
automated
screening
systems
cvs
from
singapore
pymetrics
bias
group
datasets
from
social
media
that
are
not
focused
on
links
and
relationships
between
people
are
also
considered
part
of
computer
science
in
this
sur
vey
these
resources
are
often
focused
on
text
powering
tools
and
analyses
of
hate
speech
and
toxicity
civil
comments
twitter
abusive
behavior
twitter
offensive
language
twitter
hate
speech
detection
twitter
online
harassment
dialect
twit
the
total
exceeds
226
due
to
multiple
domains
being
applicable
to
some
dataset
see
the
subject
area
and
subject
category
drop
down
menus
from
https://www.scimagojr.com/
journalrank
php
accessed
on
march
15
2022
123
algorithmic
fairness
datasets
the
story
so
far
2087
fig
datasets
employed
in
fairness
research
span
diverse
domains
see
table
for
detailed
breakdown
teraae
and
political
leaning
twitter
presidential
politics
twitter
is
by
far
the
most
represented
platform
while
datasets
from
facebook
german
political
posts
steeemit
steemit
instagram
instagram
photos
reddit
rtgender
reddit
comments
fitoc
racy
rtgender
and
youtube
youtube
dialect
accuracy
are
also
present
datasets
from
library
and
information
sciences
are
mainly
focused
on
academic
collaboration
networks
cora
papers
citeseer
papers
pubmed
diabetes
papers
arnetminer
cita
tion
network
4area
academic
collaboration
networks
except
for
dataset
about
peer
review
of
scholarly
manuscripts
paper-reviewer
matching
social
sciences
datasets
from
social
sciences
are
also
plentiful
spanning
law
education
social
networks
demography
social
work
political
science
transporta
tion
sociology
and
urban
studies
law
datasets
are
mostly
focused
on
recidivism
crowd
judgement
compas
recidivism
of
felons
on
probation
state
court
pro
cessing
statistics
los
angeles
city
attorney
office
records
and
crime
prediction
strategic
subject
list
philadelphia
crime
incidents
stop
question
and
frisk
real
time
crime
forecasting
challenge
dallas
police
incidents
communities
and
crime
with
granularity
spanning
the
range
from
individuals
to
communities
in
the
area
of
education
we
find
datasets
that
encode
application
processes
nursery
iit-jee
student
performance
student
law
school
unige
ilea
us
student
performance
indian
student
performance
edgap
berkeley
students
including
attempts
at
auto
mated
grading
automated
student
assessment
prize
and
placement
information
after
school
campus
recruitment
some
datasets
on
student
performance
support
studies
of
differences
across
schools
and
educational
systems
for
which
they
report
useful
features
law
school
ilea
edgap
while
the
remaining
datasets
are
more
focused
on
differences
in
the
individual
condition
and
outcome
for
students
typically
within
the
same
institution
datasets
about
social
networks
mostly
concern
online
123
2088
fabris
et
al
social
networks
facebook
ego-networks
facebook
large
network
pokec
social
network
rice
facebook
network
twitch
social
networks
university
facebook
net
works
except
for
high
school
contact
and
friendship
network
also
featuring
offline
relations
demography
datasets
comprise
census
data
from
different
countries
dutch
census
indian
census
national
longitudinal
survey
of
youth
section
203
determi
nations
us
census
data
1990
datasets
from
social
work
cover
complex
personal
and
social
problems
including
child
maltreatment
prevention
allegheny
child
wel
fare
emergency
response
harvey
rescue
and
drug
abuse
prevention
homeless
youths
social
networks
drugnet
resources
from
political
science
describe
reg
istered
voters
north
carolina
voters
electoral
precincts
mggg
states
polling
2016
us
presidential
poll
and
sortition
climate
assembly
uk
transportation
data
summarizes
trips
and
fares
from
taxis
nyc
taxi
trips
shanghai
taxi
trajecto
ries
ride-hailing
chicago
ridesharing
ride-hailing
app
and
bike
sharing
services
seoul
bike
sharing
along
with
public
transport
coverage
equitable
school
access
in
chicago
sociology
resources
summarize
online
libimseti
and
offline
dating
columbia
university
speed
dating
finally
we
assign
safegraph
research
release
to
urban
studies
computer
vision
this
is
an
area
of
early
success
for
artificial
intelligence
where
fairness
typically
concerns
learned
representations
and
equality
of
performance
across
classes
the
surveyed
articles
feature
several
popular
datasets
on
image
clas
sification
imagenet
mnist
fashion
mnist
cifar
visual
question
answering
visual
question
answering
segmentation
and
captioning
ms-coco
open
images
dataset
we
find
over
ten
face
analysis
datasets
labeled
faces
in
the
wild
utk
face
adience
fairface
ijb-a
celeba
pilot
parliaments
benchmark
ms-celeb
1m
diversity
in
faces
multi-task
facial
landmark
racial
faces
in
the
wild
bupt
faces
including
one
from
experimental
psychology
faces
for
which
fairness
is
most
often
intended
as
the
robustness
of
classifiers
across
different
subpopulations
without
much
regard
for
downstream
benefits
or
harms
to
these
populations
syn
thetic
images
are
popular
to
study
the
relationship
between
fairness
and
disentangled
representations
dsprites
cars3d
shapes3d
similar
studies
can
be
conducted
on
datasets
with
spurious
correlations
between
subjects
and
backgrounds
waterbirds
benchmarking
attribution
methods
or
gender
and
occupation
athletes
and
health
professionals
finally
the
image
embedding
association
test
dataset
is
fairness
benchmark
to
study
biases
in
image
embeddings
across
religion
gender
age
race
sexual
orientation
disability
skin
tone
and
weight
it
is
worth
noting
that
this
sig
nificant
proportion
of
computer
vision
datasets
is
not
an
artifact
of
including
cvpr
in
the
list
of
candidate
conferences
which
contributed
just
five
additional
datasets
multi-task
facial
landmark
office31
racial
faces
in
the
wild
bupt
faces
visual
question
answering
health
this
macrodomain
comprising
medicine
psychology
and
pharmacol
ogy
displays
notable
diversity
of
subdomains
interested
by
fairness
concerns
specialties
represented
in
the
surveyed
datasets
are
mostly
medical
including
public
health
antelope
valley
networks
willingness-to-pay
for
vaccine
kidney
matching
kidney
exchange
program
cardiology
heart
disease
arrhythmia
fram
ingham
endocrinology
diabetes
130
us
hospitals
pima
indians
diabetes
dataset
health
policy
heritage
health
meps-hc
specialties
such
as
radiology
national
123
algorithmic
fairness
datasets
the
story
so
far
2089
lung
screening
trial
mimic-cxr-jpg
chexpert
and
dermatology
siim-isic
melanoma
classification
ham10000
feature
several
image
datasets
for
their
strong
connections
with
medical
imaging
other
specialties
include
critical
care
medicine
mimic-iii
neurology
epileptic
seizures
pediatrics
infant
health
and
develop
ment
program
sleep
medicine
apnea
nephrology
renal
failure
pharmacology
warfarin
and
psychology
drug
consumption
faces
these
datasets
are
often
extracted
from
care
data
of
multiple
medical
centers
to
study
problems
of
auto
mated
diagnosis
resources
derived
from
longitudinal
studies
including
framingham
and
infant
health
and
development
program
are
also
present
works
of
algorithmic
fairness
in
this
domain
are
typically
concerned
with
obtaining
models
with
similar
performance
for
patients
across
race
and
sex
linguistics
in
addition
to
the
textual
resources
we
already
described
such
as
the
ones
derived
from
social
media
several
datasets
employed
in
algorithmic
fairness
literature
can
be
assigned
to
the
domain
of
linguistics
and
natural
language
processing
nlp
there
are
many
examples
of
resources
curated
to
be
fairness
benchmarks
for
different
tasks
including
machine
translation
bias
in
translation
templates
sentiment
analysis
equity
evaluation
corpus
coreference
resolution
winogender
winobias
gap
coreference
named
entity
recognition
in-situ
language
models
bold
and
word
embeddings
weat
other
datasets
have
been
considered
for
their
size
and
importance
for
pretraining
text
representations
wikipedia
dumps
one
billion
word
benchmark
bookcorpus
webtext
or
their
utility
as
nlp
benchmarks
glue
business
entity
resolution
speech
recognition
resources
have
also
been
considered
timit
economics
and
business
this
macrodomain
comprises
datasets
from
economics
finance
marketing
and
management
information
systems
economics
datasets
mostly
consist
of
census
data
focused
on
wealth
adult
us
family
income
poverty
in
colom
bia
costarica
household
survey
and
other
resources
which
summarize
employment
anpe
tariffs
us
harmonized
tariff
schedules
insurance
italian
car
insurance
and
division
of
goods
spliddit
divide
goods
finance
resources
feature
data
on
microcredit
and
peer-to-peer
lending
mobile
money
loans
kiva
prosper
loans
net
work
mortgages
hmda
loans
german
credit
credit
elasticities
credit
scoring
fico
and
default
prediction
credit
card
default
marketing
datasets
describe
mar
keting
campaigns
bank
marketing
customer
data
wholesale
and
advertising
bids
yahoo
a1
search
marketing
finally
datasets
from
management
information
sys
tems
summarize
information
about
automated
hiring
cvs
from
singapore
pymetrics
bias
group
and
employee
retention
ibm
hr
analytics
miscellaneous
this
macrodomain
contains
several
datasets
originating
from
the
news
domain
yow
news
guardian
articles
latin
newspapers
adressa
reuters
50
50
new
york
times
annotated
corpus
trec
robust04
other
resources
include
datasets
on
food
sushi
sports
fantasy
football
fifa
20
players
olympic
athletes
and
toy
datasets
toy
dataset
arts
and
humanities
in
this
area
we
mostly
find
literature
datasets
which
contain
text
from
literary
works
shakespeare
curatr
british
library
digital
corpus
victorian
era
authorship
attribution
nominees
corpus
riddle
of
literary
quality
which
are
typically
studied
with
nlp
tools
other
datasets
in
this
domain
include
domain
123
2090
fabris
et
al
specific
information
systems
about
books
goodreads
reviews
movies
movielens
and
music
last
fm
million
song
dataset
million
playlist
dataset
natural
sciences
this
domain
is
represented
with
three
datasets
from
biology
inaturalist
biochemestry
pp-pathways
and
plant
science
with
the
classic
iris
dataset
as
whole
many
of
these
datasets
encode
fundamental
human
activities
where
algorithms
and
adm
systems
have
been
studied
and
deployed
alertness
and
attention
to
equity
seems
especially
important
in
specific
domains
including
social
sciences
computer
science
medicine
and
economics
here
the
potential
for
impact
may
result
in
large
benefits
but
also
great
harm
particularly
for
vulnerable
populations
and
minorities
more
likely
to
be
neglected
during
the
design
training
and
testing
of
an
adm
after
concentrating
on
domains
in
the
next
section
we
analyze
the
variety
of
tasks
studied
in
works
of
algorithmic
fairness
and
supported
by
these
datasets
5.2
task
and
setting
researchers
and
practitioners
are
showing
an
increasing
interest
in
algorithmic
fairness
proposing
solutions
for
many
different
tasks
including
fair
classification
regression
and
ranking
at
the
same
time
the
academic
community
is
developing
an
improved
understanding
of
important
challenges
that
run
across
different
tasks
in
the
algorithmic
fairness
space
chouldechova
and
roth
2020
also
thanks
to
practitioner
surveys
holstein
et
al
2019
and
studies
of
specific
legal
challenges
andrus
et
al
2021
to
exemplify
the
presence
of
noise
corrupting
labels
for
sensitive
attributes
represents
challenge
that
may
apply
across
different
tasks
including
fair
classifica
tion
regression
and
ranking
we
refer
to
these
challenges
as
settings
describing
them
in
the
second
part
of
this
section
while
our
work
focuses
on
fair
ml
datasets
it
is
cognizant
of
the
wide
variety
of
tasks
tackled
in
the
algorithmic
fairness
literature
which
are
captured
in
specific
field
of
our
data
briefs
in
this
section
we
provide
an
overview
of
common
tasks
and
settings
studied
on
these
datasets
showing
their
variety
and
diversity
table
summarizes
these
tasks
listing
the
three
most
used
datasets
for
each
task
when
describing
task
we
explicitly
highlight
datasets
that
are
particularly
relevant
to
it
even
when
outside
of
the
top
three
5.2
task
fair
classification
calders
and
verwer
2010
dwork
et
al
2012
is
the
most
common
task
by
far
typically
it
involves
equalizing
some
measure
of
interest
across
subpop
ulations
such
as
the
recall
precision
or
accuracy
for
different
racial
groups
on
the
other
hand
individually
fair
classification
focuses
on
the
idea
that
similar
individuals
low
distance
in
the
covariate
space
should
be
treated
similarly
low
distance
in
the
outcome
space
often
formalized
as
lipschitz
condition
unsurprisingly
the
most
common
datasets
for
fair
classification
are
the
most
popular
ones
overall
sect
adult
compas
and
german
credit
fair
regression
berk
et
al
2017
concentrates
on
models
that
predict
real-valued
target
requiring
the
average
loss
to
be
balanced
across
groups
individual
fairness
in
123
algorithmic
fairness
datasets
the
story
so
far
2091
table
selection
of
datasets
through
the
lens
of
the
domain
taxonomy
domain
sample
datasets
computer
science
social
media
toxicity
and
hate
speech
civil
comments
wikipedia
toxic
comments
twitter
offen
sive
language
political
leaning
twitter
presidential
politics
dialect
twitteraae
library
and
information
sciences
collaboration
networks
paper-reviewer
matching
4area
arnetminer
citation
net
work
peer
review
paper-reviewer
matching
information
systems
search
engines
online
freelance
marketplaces
bing
us
queries
symptoms
in
queries
recommender
systems
amazon
recommendations
amazon
reviews
movielens
knowledge
bases
freebase15k-237
wikidata
computer
networks
kdd
cup
99
pattern
recognition
internet
ads
signal
processing
vehicle
social
sciences
urban
studies
safegraph
research
release
social
networks
university
facebook
networks
pokec
social
network
rice
facebook
network
demography
us
census
data
1990
dutch
census
national
longitudi
nal
survey
of
youth
sociology
columbia
university
speed
dating
libimseti
law
recidivism
prediction
compas
recidivism
of
felons
on
probation
state
court
pro
cessing
statistics
crime
prediction
communities
and
crime
stop
question
and
frisk
strategic
subject
list
political
science
registered
voters
north
carolina
voters
electoral
precincts
mggg
states
polling
2016
us
presidential
poll
sortition
climate
assembly
uk
education
application
processes
nursery
iit-jee
student
performance
student
law
school
unige
post-education
placement
campus
recruitment
123
2092
fabris
et
al
table
continued
domain
sample
datasets
social
work
child
maltreatment
prevention
allegheny
child
welfare
emergency
response
harvey
rescue
drug
abuse
prevention
homeless
youths
social
networks
drugnet
transportation
taxi
trips
nyc
taxi
trips
shanghai
taxi
trajectories
ride
hailing
chicago
ridesharing
ride-hailing
app
bike
sharing
seoul
bike
sharing
public
transport
equitable
school
access
in
chicago
computer
vision
general
purpose
imagenet
mnist
cifar
face
analysis
celeba
pilot
parliaments
benchmar
fairface
synthetic
dsprites
cars3d
shapes3d
health
sleep
medicine
apnea
critical
care
medicine
mimic-iii
public
health
kidney
exchange
program
willingness-to-pay
for
vaccine
kidney
matching
cardiology
arrhythmia
heart
disease
framingham
neurology
epileptic
seizures
pediatrics
infant
health
and
development
program
ihdp
dermatology
ham10000
siim-isic
melanoma
classification
medicine
stanford
medicine
research
data
repository
pharmacology
warfarin
endocrinology
diabetes
130
us
hospitals
pima
indians
diabetes
dataset
pidd
nephrology
renal
failure
radiology
chexpert
mimic-cxr-jpg
national
lung
screening
trial
nlst
health
policy
heritage
health
meps-hc
applied
psychology
drug
consumption
experimental
psychology
faces
economics
and
business
economics
census
adult
us
family
income
poverty
in
colombia
employment
anpe
tariffs
us
harmonized
tariff
schedule
insurance
italian
car
insurance
division
of
goods
spliddit
divide
goods
123
algorithmic
fairness
datasets
the
story
so
far
2093
table
continued
domain
sample
datasets
finance
peer-to-peer
lending
mobile
money
loans
kiva
prosper
loans
network
mortgages
hmda
credit
scoring
fico
other
credit
german
credit
credit
card
default
credit
elasticities
marketing
marketing
campaigns
bank
marketing
advertising
bids
yahoo
a1
search
marketing
wholesale
management
information
systems
automated
hiring
pymetrics
bias
group
cvs
from
singapore
employee
retention
ibm
hr
analytics
linguistics
general
purpose
wikipedia
dumps
one
billion
word
benchmark
bookcorpus
fairness
benchmarks
bias
in
translation
templates
equity
evaluation
corpus
wino
gender
arts
and
humanities
music
million
playlist
dataset
mpd
million
song
dataset
msd
last
fm
literature
goodreads
reviews
riddle
of
literary
quality
nominees
cor
pus
movies
movielens
filmtrust
natural
sciences
biology
inaturalist
datasets
biochemestry
pp-pathways
plant
science
iris
miscellaneous
news
trec
robust04
new
york
times
annotated
corpus
reuters
50
50
sports
fantasy
football
fifa
20
players
olympic
athletes
food
sushi
this
context
may
require
losses
to
be
as
uniform
as
possible
across
all
individuals
fair
regression
is
less
popular
task
often
studied
on
the
communities
and
crime
dataset
where
the
task
is
predicting
the
rate
of
violent
crimes
in
different
communities
fair
ranking
yang
and
stoyanovich
2017
requires
ordering
candidate
items
based
on
their
relevance
to
current
need
fairness
in
this
context
may
concern
both
the
people
producing
the
items
that
are
being
ranked
artists
and
those
consuming
the
items
users
of
music
streaming
platform
it
is
typically
studied
in
applications
of
recommendation
movielens
amazon
recommendations
last
fm
million
song
dataset
adressa
and
search
engines
yahoo
c14b
learning
to
rank
microsoft
learning
to
rank
trec
robust04
123
2094
fabris
et
al
table
most
used
datasets
by
algorithmic
fairness
task
and
setting
task
datasets
fair
classification
adult
compas
german
credit
fair
regression
communities
and
crime
law
school
student
fair
ranking
movielens
german
credit
kiva
fair
matching
nyc
taxi
trips
libimseti
columbia
university
speed
dating
fair
risk
assessment
compas
allegheny
child
welfare
infant
health
and
development
program
ihdp
fair
representation
learning
adult
compas
dsprites
fair
clustering
adult
bank
marketing
diabetes
130
us
hospitals
fair
anomaly
detection
adult
mnist
credit
card
default
fair
districting
mggg
states
fair
task
assignment
crowd
judgement
compas
fair
spatio-temporal
process
learning
real-time
crime
forecasting
challenge
dallas
police
incidents
harvey
rescue
fair
graph
diffusion
augmentation
university
facebook
networks
antelope
valley
networks
rice
facebook
network
fair
resource
allocation
subset
selection
ml
fairness
gym
us
federal
judges
climate
assembly
uk
fair
data
summarization
adult
student
credit
card
default
fair
data
generation
celeba
movielens
shapes3d
fair
graph
mining
movielens
freebase15k-237
pp-pathways
fair
pricing
willingness-to-pay
for
vaccine
credit
elasticities
italian
car
insurance
fair
advertising
yahoo
a1
search
marketing
north
carolina
voters
instagram
photos
fair
routing
shanghai
taxi
trajectories
fair
entity
resolution
winogender
winobias
business
entity
resolution
fair
sentiment
analysis
popular
baby
names
equity
evaluation
corpus
eec
twitteraae
bias
in
word
embeddings
wikipedia
dumps
word
embedding
association
test
weat
popular
baby
names
bias
in
language
models
twitteraae
bold
glue
fair
machine
translation
bias
in
translation
templates
fair
speech
recognition
youtube
dialect
accuracy
timit
setting
datasets
rich-subgroup
fairness
adult
compas
communities
and
crime
fairness
under
unawareness
adult
compas
hmda
limited-label
fairness
adult
german
credit
compas
robust
fairness
compas
adult
meps-hc
dynamical
fairness
fico
ml
fairness
gym
compas
preference-based
fairness
adult
compas
toy
dataset
multi-stage
fairness
adult
heritage
health
twitter
offensive
language
123
algorithmic
fairness
datasets
the
story
so
far
2095
table
continued
setting
datasets
fair
few-shot
learning
communities
and
crime
toy
dataset
mobile
money
loans
fair
private
learning
utk
face
chexpert
fairface
fair
federated
learning
vehicle
sentiment140
shakespeare
fair
incremental
learning
imagenet
cifar
fair
active
learning
adult
german
credit
heart
disease
fair
selective
classification
chexpert
celeba
civil
comments
fair
matching
kobren
et
al
2019
is
similar
to
ranking
as
they
are
both
tasks
defined
on
two-sided
markets
this
task
however
is
focused
on
highlighting
and
matching
pairs
of
items
on
both
sides
of
the
market
without
emphasis
on
the
ranking
component
datasets
for
this
task
are
from
diverse
domains
including
dating
libim
seti
columbia
university
speed
dating
transportation
nyc
taxi
trips
ride-hailing
app
and
organ
donation
kidney
matching
kidney
exchange
program
fair
risk
assessment
coston
et
al
2020
studies
algorithms
that
score
instances
in
dataset
according
to
predefined
type
of
risk
relevant
domains
include
healthcare
and
criminal
justice
key
differences
with
respect
to
classification
are
an
empha
sis
on
real-valued
scores
rather
than
labels
and
awareness
that
the
risk
assessment
process
can
lead
to
interventions
impacting
the
target
variable
for
this
reason
fairness
concerns
are
often
defined
in
counterfactual
fashion
the
most
popular
dataset
for
this
task
is
compas
followed
by
datasets
from
medicine
ihdp
stan
ford
medicine
research
data
repository
social
work
allegheny
child
welfare
economics
anpe
and
education
edgap
fair
representation
learning
creager
et
al
2019
concerns
the
study
of
features
learnt
by
models
as
intermediate
representations
for
inference
tasks
popular
line
of
work
in
this
space
called
disentaglement
aims
to
learn
representations
where
single
factor
of
import
corresponds
to
single
feature
ideally
this
approach
should
select
representations
where
sensitive
attributes
cannot
be
used
as
proxies
for
target
variables
cars3d
and
dsprites
are
popular
datasets
for
this
task
consisting
of
synthetic
images
depicting
controlled
shape
types
under
controlled
set
of
rotations
post-processing
approaches
are
also
applicable
to
obtain
fair
representations
from
biased
ones
via
debiasing
fair
clustering
chierichetti
et
al
2017
is
an
unsupervised
task
concerned
with
the
division
of
sample
into
homogenous
groups
fairness
may
be
intended
as
an
equitable
representation
of
protected
subpopulations
in
each
cluster
or
in
terms
of
average
distance
from
the
cluster
center
while
adult
is
the
most
common
dataset
for
problems
of
fair
clustering
other
resources
often
used
for
this
task
include
bank
marketing
diabetes
130
us
hospitals
credit
card
default
and
us
census
data
1990
fair
anomaly
detection
zhang
and
davidson
2021
also
called
outlier
detection
davidson
and
ravi
2020
is
aimed
at
identifying
surprising
or
anomalous
points
in
dataset
fairness
requirements
involve
equalizing
salient
quantities
acceptance
123
2096
fabris
et
al
rate
recall
precision
distribution
of
anomaly
scores
across
populations
of
interest
this
problem
is
particularly
relevant
for
members
of
minority
groups
who
in
the
absence
of
specific
attention
to
dataset
inclusivity
are
less
likely
to
fit
the
norm
in
the
feature
space
fair
districting
schutzman
2020
is
the
division
of
territory
into
electoral
dis
tricts
for
political
elections
fairness
notions
brought
forth
in
this
space
are
either
outcome-based
requiring
that
seats
earned
by
party
roughly
match
their
share
of
the
popular
vote
or
procedure-based
ignoring
outcomes
and
requiring
that
counties
or
municipalities
are
split
as
little
as
possible
mggg
states
is
reference
resource
for
this
task
providing
precinct-level
aggregated
information
about
demographics
and
political
leaning
of
voters
in
us
districts
fair
task
assignment
and
truth
discovery
goel
and
faltings
2019
li
et
al
2020d
are
different
subproblems
in
the
same
area
focused
on
the
subdivision
of
work
and
the
aggregation
of
answers
in
crowdsourcing
here
fairness
may
be
intended
concerning
errors
in
the
aggregated
answer
requiring
errors
to
be
balanced
across
subpopulations
of
interest
or
in
terms
of
the
work
load
imposed
to
workers
dataset
suitable
for
this
task
is
crowd
judgement
containing
crowd-sourced
recidivism
predictions
fair
spatio-temporal
process
learning
shang
et
al
2020
focuses
on
the
estima
tion
of
models
for
processes
which
evolve
in
time
and
space
surveyed
applications
include
crime
forecasting
real-time
crime
forecasting
challenge
dallas
police
incidents
and
disaster
relief
harvey
rescue
with
fairness
requirements
focused
on
equalization
of
performance
across
different
neighbourhoods
and
special
attention
to
their
racial
composition
fair
graph
diffusion
farnad
et
al
2020
models
and
optimizes
the
propagation
of
information
and
influence
over
networks
and
its
probability
of
reaching
individ
uals
of
different
sensitive
groups
applications
include
obesity
prevention
antelope
valley
networks
and
drug-use
prevention
homeless
youths
social
networks
fair
graph
augmentation
ramachandran
et
al
2021
is
similar
task
defined
on
graphs
which
define
access
to
resources
based
on
existing
infrastructure
transportation
which
can
be
augmented
under
budget
to
increase
equity
this
task
has
been
pro
posed
to
improve
school
access
equitable
school
access
in
chicago
and
information
availability
in
social
networks
facebook100
fair
resource
allocation
subset
selection
babaioff
et
al
2019
huang
et
al
2020
can
often
be
formalized
as
classification
problem
with
constraints
on
the
number
of
positives
fairness
requirements
are
similar
to
those
of
classification
subset
selection
may
be
employed
to
choose
group
of
people
from
wider
set
for
given
task
us
federal
judges
climate
assembly
uk
resource
allocation
concerns
the
division
of
goods
spliddit
divide
goods
and
resources
ml
fairness
gym
german
credit
fair
data
summarization
celis
et
al
2018
refers
to
presenting
summary
of
datasets
that
is
equitable
to
subpopulations
of
interest
it
may
involve
finding
small
subset
representative
of
larger
dataset
strongly
linked
to
subset
selection
or
select
ing
the
most
important
features
dimensionality
reduction
approaches
for
this
task
have
been
applied
to
select
subset
of
images
scientist
painter
or
customers
bank
marketing
that
represent
the
underlying
population
across
sensitive
demographics
fair
data
generation
xu
et
al
2018
deals
with
generating
fair
data
points
and
labels
which
can
be
used
as
training
or
test
sets
approaches
in
this
space
may
be
123
algorithmic
fairness
datasets
the
story
so
far
2097
used
to
ensure
an
equitable
representation
of
protected
categories
in
data
generation
processes
learnt
from
biased
datasets
celeba
ibm
hr
analytics
and
to
evaluate
biases
in
existing
classifiers
ms-celeb-1m
data
generation
may
also
be
limited
to
synthesizing
artificial
sensitive
attributes
burke
et
al
2018a
fair
graph
mining
kang
et
al
2020
focuses
on
representations
and
prediction
tasks
on
graph
structures
fairness
may
be
defined
either
as
lack
of
bias
in
represen
tations
or
with
respect
to
final
inference
task
defined
on
the
graph
fair
graph
mining
approaches
have
been
applied
to
knowledge
bases
freebase15k-237
wikidata
col
laboration
networks
citeseer
paper
academic
collaboration
networks
and
social
network
datasets
facebook
large
network
twitch
social
networks
fair
pricing
kallus
and
zhou
2021
concerns
learning
and
deploying
an
optimal
pricing
policy
for
revenue
while
maintaining
equity
of
access
to
services
and
con
sumer
welfare
across
sensitive
groups
datasets
employed
in
fair
pricing
are
from
the
economics
credit
elasticities
italian
car
insurance
transportation
chicago
ridesharing
and
public
health
domains
willingness-to-pay
for
vaccine
fair
advertising
celis
et
al
2019a
is
also
concerned
with
access
to
goods
and
services
it
comprises
both
bidding
strategies
and
auction
mechanisms
which
may
be
modified
to
reduce
discrimination
with
respect
to
the
gender
or
race
composition
of
the
audience
that
sees
an
ad
one
publicly
available
dataset
for
this
subtask
is
yahoo
a1
search
marketing
fair
routing
qian
et
al
2015
is
the
task
of
suggesting
an
optimal
path
from
starting
location
to
destination
for
this
task
experimentation
has
been
carried
out
on
semi-synthetic
traffic
dataset
shanghai
taxi
trajectories
the
proposed
fairness
measure
requires
equalizing
the
driving
cost
per
customer
across
all
drivers
fair
entity
resolution
cotter
et
al
2019
is
task
focused
on
deciding
whether
multiple
records
refer
to
the
same
entity
which
is
useful
for
instance
for
the
construc
tion
and
maintenance
of
knowledge
bases
business
entity
resolution
is
proprietary
dataset
for
fair
entity
resolution
where
constraints
of
performance
equality
across
chain
and
non-chain
businesses
can
be
tested
winogender
and
winobias
are
publicly
available
datasets
developed
to
study
gender
biases
in
pronoun
resolution
fair
sentiment
analysis
kiritchenko
and
mohammad
2018
is
well-established
instance
of
fair
classification
where
text
snippets
are
typically
classified
as
positive
negative
or
neutral
depending
on
the
sentiment
they
express
fairness
is
intended
with
respect
to
the
entities
mentioned
in
the
text
men
and
women
the
central
idea
is
that
the
estimated
sentiment
for
sentence
should
not
change
if
female
entities
her
woman
mary
are
substituted
with
their
male
counterparts
him
man
james
the
equity
evaluation
corpus
is
benchmark
developed
to
assess
gender
and
race
bias
in
sentiment
analysis
models
bias
in
word
embeddings
wes
bolukbasi
et
al
2016
is
the
study
of
unde
sired
semantics
and
stereotypes
captured
by
vectorial
representations
of
words
wes
are
typically
trained
on
large
text
corpora
wikipedia
dumps
and
audited
for
associ
ations
between
gendered
words
or
other
words
connected
to
sensitive
attributes
and
stereotypical
or
harmful
concepts
such
as
the
ones
encoded
in
weat
bias
in
language
models
lms
bordia
and
bowman
2019
is
quite
similarly
the
study
of
biases
in
lms
which
are
flexible
models
of
human
language
based
on
contextualized
word
representations
which
can
be
employed
in
variety
of
linguistics
123
2098
fabris
et
al
and
nlp
tasks
lms
are
trained
on
large
text
corpora
from
which
they
may
learn
spu
rious
correlations
and
stereotypes
the
bold
dataset
is
an
evaluation
benchmark
for
lms
based
on
prompts
that
mention
different
socio-demographic
groups
lms
com
plete
these
prompts
into
full
sentences
which
can
be
tested
along
different
dimensions
sentiment
regard
toxicity
emotion
and
gender
polarity
fair
machine
translation
mt
stanovsky
et
al
2019
concerns
automatic
trans
lation
of
text
from
source
language
into
target
one
mt
systems
can
exhibit
gender
biases
such
as
tendency
to
translate
gender-neutral
pronouns
from
the
source
language
into
gendered
pronouns
of
the
target
language
in
accordance
with
gender
stereotypes
for
example
nurse
mentioned
in
gender-neutral
context
in
the
source
sentence
may
be
rendered
with
feminine
grammar
in
the
target
language
bias
in
translation
templates
is
set
of
short
templates
to
test
such
biases
fair
speech
recognition
tatman
2017
requires
accurate
annotation
of
spoken
language
into
text
across
different
demographics
youtube
dialect
accuracy
is
dataset
developed
to
audit
the
accuracy
of
youtube
automatic
captions
across
two
genders
and
five
dialects
of
english
similarly
timit
is
classic
speech
recognition
dataset
annotated
with
american
english
dialect
and
gender
of
speaker
5.2
setting
as
noted
at
the
beginning
of
this
section
there
are
several
settings
or
challenges
that
run
across
different
tasks
described
above
some
of
these
settings
are
specific
to
fair
ml
such
as
ensuring
fairness
across
an
exponential
number
of
groups
or
in
the
presence
of
noisy
labels
for
sensitive
attributes
other
settings
are
connected
with
common
ml
challenges
including
few-shot
and
privacy-preserving
learning
below
we
describe
common
settings
encountered
in
the
surveyed
articles
most
of
these
settings
are
tested
on
fairness
datasets
which
are
popular
overall
adult
compas
and
german
credit
we
highlight
situations
where
this
is
not
the
case
potentially
due
to
given
challenge
arising
naturally
in
some
other
dataset
rich-subgroup
fairness
kearns
et
al
2018
is
setting
where
fairness
properties
are
required
to
hold
not
only
for
limited
number
of
protected
groups
but
across
an
exponentially
large
number
of
subpopulations
this
line
of
work
represents
an
attempt
to
bridge
the
normative
reasoning
underlying
individual
and
group
fairness
fairness
under
unawareness
is
general
expression
to
indicate
problems
where
sensitive
attributes
are
missing
chen
et
al
2019a
encrypted
kilbertus
et
al
2018
or
corrupted
by
noise
lamy
et
al
2019
these
problems
respond
to
real-world
chal
lenges
related
to
the
confidential
nature
of
protected
attributes
that
individuals
may
wish
to
hide
encrypt
or
obfuscate
this
setting
is
most
commonly
studied
on
highly
popular
fairness
dataset
adult
compas
moderately
popular
ones
law
school
and
credit
card
default
and
dataset
about
home
mortgage
applications
in
the
us
hmda
limited-label
fairness
comprises
settings
with
limited
information
on
the
target
variable
including
situations
where
labelled
instances
are
few
ji
et
al
2020
noisy
wang
et
al
2021
or
only
available
in
aggregate
form
sabato
and
yom-tov
2020
robust
fairness
problems
arise
under
perturbations
to
the
training
set
huang
and
vishnoi
2019
adversarial
attacks
nanda
et
al
2021
and
dataset
shift
singh
123
algorithmic
fairness
datasets
the
story
so
far
2099
et
al
2021
this
line
of
research
is
often
connected
with
work
in
robust
machine
learning
extending
the
stability
requirements
beyond
accuracy-related
metrics
to
fairness-related
ones
dynamical
fairness
liu
et
al
2018
amour
et
al
2020
entails
repeated
deci
sions
in
changing
environments
potentially
affected
by
the
very
algorithm
that
is
being
studied
works
in
this
space
study
the
co-evolution
of
algorithms
and
populations
on
which
they
act
over
time
for
example
an
algorithm
that
achieves
equality
of
accep
tance
rates
across
protected
groups
in
static
setting
may
generate
further
incentives
for
the
next
generation
of
individuals
from
historically
disadvantaged
groups
popular
resources
for
this
setting
are
fico
and
the
ml
fairness
gym
preference-based
fairness
zafar
et
al
2017b
denotes
work
informed
explicitly
or
implicitly
by
the
preferences
of
stakeholders
for
people
subjected
to
decision
this
is
related
to
notions
of
envy-freeness
and
loss
aversion
ali
et
al
2019b
alternatively
policy-makers
can
express
indications
on
how
to
trade-off
different
fairness
measures
zhang
et
al
2020c
or
experts
can
provide
demonstrations
of
fair
outcomes
galhotra
et
al
2021
multi-stage
fairness
madras
et
al
2018b
refers
to
settings
where
several
deci
sion
makers
coexist
in
compound
decision-making
process
decision
makers
both
humans
and
algorithmic
may
act
with
different
levels
of
coordination
fundamen
tal
question
in
this
setting
is
how
to
ensure
fairness
under
composition
of
different
decision
mechanisms
fair
few-shot
learning
zhao
et
al
2020b
aims
at
developing
fair
ml
solutions
in
the
presence
of
small
amount
of
data
samples
the
problem
is
closely
related
to
and
possibly
solved
by
fair
transfer
learning
coston
et
al
2019
where
the
goal
is
to
exploit
the
knowledge
gained
on
problem
to
solve
different
but
related
one
datasets
where
this
setting
arises
naturally
are
communities
and
crime
where
one
may
restrict
the
training
set
to
subset
of
us
states
and
mobile
money
loans
which
consists
of
data
from
different
african
countries
fair
private
learning
bagdasaryan
et
al
2019
jagielski
et
al
2019
studies
the
interplay
between
privacy-preserving
mechanisms
and
fairness
constraints
works
in
this
space
consider
the
equity
of
machine
learning
models
designed
to
avoid
leakage
of
information
about
individuals
in
the
training
set
common
domains
for
datasets
employed
in
this
setting
are
face
analysis
utk
face
fairface
diversity
in
face
and
medicine
chexpert
siim-isic
melanoma
classification
mimic-cxr-jpg
additional
settings
that
are
less
common
include
fair
federated
learning
li
et
al
2020b
where
algorithms
are
trained
across
multiple
decentralized
devices
fair
incre
mental
learning
zhao
et
al
2020a
where
novel
classes
may
be
added
to
the
learning
problem
over
time
fair
active
learning
noriega-campero
et
al
2019
allowing
for
the
acquisition
of
novel
information
during
inference
and
fair
selective
classification
jones
et
al
2021
where
predictions
are
issued
only
if
model
confidence
is
above
certain
threshold
overall
we
found
variety
of
tasks
defined
on
fairness
datasets
ranging
from
generic
such
as
fair
classification
to
narrow
and
specifically
defined
on
certain
datasets
such
as
fair
districting
on
mggg
states
and
fair
truth
discovery
on
crowd
judgement
orthogonally
to
this
dimension
many
settings
or
challenges
may
arise
to
complicate
these
tasks
including
noisy
labels
system
dynamics
and
privacy
concerns
123
2100
fabris
et
al
quite
clearly
algorithmic
fairness
research
has
been
expanding
in
both
directions
by
studying
variety
of
tasks
under
diverse
and
challenging
settings
in
the
next
section
we
analyze
the
roles
played
in
scholarly
works
by
the
surveyed
datasets
5.3
role
the
datasets
used
in
algorithmic
fairness
research
can
play
different
roles
for
example
some
may
be
used
to
train
novel
algorithms
while
others
are
suited
to
test
existing
algorithms
from
specific
point
of
view
chapter
of
barocas
et
al
2019
describes
six
different
roles
of
datasets
in
machine
learning
we
adopt
their
framework
to
analyse
fair
ml
datasets
adding
to
the
taxonomy
two
roles
that
are
specific
to
fairness
research
source
of
real
data
while
synthetic
datasets
and
simulations
may
be
suited
to
demonstrate
specific
properties
of
novel
method
the
usefulness
of
an
algorithm
is
typically
established
on
data
from
the
real
world
more
than
sign
of
immediate
applicability
to
important
challenges
good
performance
on
real-world
sources
of
data
signals
that
the
researchers
did
not
make
up
the
data
to
suit
the
algorithm
this
is
likely
the
most
common
role
for
fairness
datasets
especially
common
for
the
ones
hosted
on
the
uci
ml
repository
including
adult
german
credit
communities
and
crime
diabetes
130
us
hospitals
bank
marketing
credit
card
default
us
census
data
1990
these
resources
owe
their
popularity
in
fair
ml
research
to
being
product
of
human
processes
and
to
encoding
protected
attributes
quite
simply
they
are
sources
of
real
human
data
catalyst
of
domain-specific
progress
datasets
can
spur
algorithmic
insight
and
bring
about
domain-specific
progress
civil
comments
is
great
example
of
this
role
powering
the
jigsaw
unintended
bias
in
toxicity
classification
challenge
the
chal
lenge
responds
to
specific
need
in
the
space
of
automated
moderation
against
toxic
comments
in
online
discussion
early
attempts
at
toxicity
detection
resulted
in
models
which
associate
mentions
of
frequently
attacked
identities
gay
with
toxicity
due
to
spurious
correlations
in
training
sets
the
dataset
and
associated
challenge
tackle
this
issue
by
providing
toxicity
ratings
for
comments
along
with
labels
encoding
whether
members
of
certain
group
are
mentioned
favouring
measurement
of
undesired
bias
many
other
datasets
can
play
similar
role
including
winogender
winobias
and
the
equity
evaluation
corpus
in
broader
sense
compas
and
the
accompanying
study
angwin
et
al
2016
have
been
an
important
catalyst
not
for
specific
task
but
for
fairness
research
overall
way
to
numerically
track
progress
on
problem
this
role
is
common
for
machine
learning
benchmarks
that
also
provide
human
performance
baselines
algorithmic
methods
approaching
or
surpassing
these
baselines
are
often
considered
sign
that
the
task
is
solved
and
that
harder
benchmarks
are
required
barocas
et
al
2019
algorithmic
fairness
is
complicated
context-dependent
contested
construct
whose
correct
measurement
is
continuously
debated
due
to
this
reason
we
are
unaware
of
any
dataset
having
similar
role
in
the
algorithmic
fairness
literature
resource
to
compare
models
practitioners
interested
in
solving
specific
problem
may
take
large
set
of
algorithms
and
test
them
on
group
of
datasets
that
are
represen
tative
of
their
problem
in
order
to
select
the
most
promising
ones
for
well-established
123
algorithmic
fairness
datasets
the
story
so
far
2101
ml
challenges
there
are
often
leaderboards
providing
concise
comparison
between
algorithms
for
given
task
which
may
be
used
for
model
selection
this
setting
is
rare
in
the
fairness
literature
also
due
to
inherent
difficulties
in
establishing
sin
gle
measure
of
interest
in
the
field
one
notable
exception
is
represented
by
friedler
et
al
2019
who
employed
suite
of
four
datasets
adult
compas
german
credit
ricci
to
compare
the
performance
of
four
different
approaches
to
fair
classification
source
of
pre-training
data
flexible
general-purpose
models
are
often
pre
trained
to
encode
useful
representations
which
are
later
fine-tuned
for
specific
tasks
in
the
same
domain
for
example
large
text
corpora
are
often
employed
to
train
language
models
and
word
embeddings
which
are
later
specialized
to
support
variety
of
down
stream
nlp
applications
wikipedia
dumps
for
instance
are
often
used
to
train
word
embeddings
and
investigate
their
biases
brunet
et
al
2019
liang
and
acuna
2020
papakyriakopoulos
et
al
2020
several
algorithmic
fairness
works
aim
to
study
and
mitigate
undesirable
biases
in
learnt
representations
corpora
like
wikipedia
dumps
are
used
to
obtain
representations
via
realistic
pretraining
procedures
that
mimic
com
mon
machine
learning
practice
as
closely
as
possible
source
of
training
data
models
for
specific
task
are
typically
learnt
from
training
sets
that
encode
relations
between
features
and
target
variable
in
representative
fashion
one
example
from
the
fairness
literature
is
large
movie
review
used
to
train
sentiment
analysis
models
later
audited
for
fairness
liang
and
acuna
2020
for
fairness
audits
one
alternative
would
be
resorting
to
publicly
available
models
but
sometimes
close
control
on
the
training
corpus
and
procedure
is
necessary
indeed
it
is
interesting
to
study
issues
of
model
fairness
in
relation
to
biases
present
in
the
respective
training
corpora
which
can
help
explain
the
causes
of
bias
brunet
et
al
2019
some
works
measure
biases
in
internal
model
representations
before
and
after
fine-tuning
on
training
set
and
regard
the
difference
as
measure
of
bias
in
the
training
set
babaeianjelodar
et
al
2020
employ
this
approach
to
measure
biases
in
rtgender
civil
comments
and
datasets
from
glue
representative
summary
of
service
much
important
work
in
the
fairness
lit
erature
is
focused
on
measuring
fairness
and
harms
in
the
real
world
this
line
of
work
includes
audits
of
products
and
services
which
rely
on
datasets
extracted
from
the
application
of
interest
datasets
created
for
this
purpose
include
amazon
recom
mendations
pymetrics
bias
group
occupations
in
google
images
zillow
searches
online
freelance
marketplaces
bing
us
queries
youtube
dialect
accuracy
sev
eral
other
datasets
were
originally
created
for
this
purpose
and
later
repurposed
in
the
fairness
literature
as
sources
of
real
data
including
stop
question
and
frisk
hmda
law
school
and
compas
an
important
source
of
data
some
datasets
acquire
pivotal
role
in
research
and
industry
to
the
point
of
being
considered
de-facto
standard
for
given
purpose
this
status
warrants
closer
scrutiny
of
the
dataset
through
which
researchers
aim
to
uncover
potential
biases
and
problematic
aspects
that
may
impact
models
and
insights
derived
from
the
dataset
imagenet
for
instance
is
dataset
with
millions
of
images
across
thousands
of
categories
since
its
release
in
2011
this
resource
has
been
used
to
train
benchmark
and
compare
hundreds
of
computer
vision
models
given
its
sta
tus
in
machine
learning
research
imagenet
has
been
the
subject
of
two
quantitative
investigations
analyzing
its
biases
and
other
problematic
aspects
in
the
person
subtree
123
2102
fabris
et
al
uncovering
issues
of
representation
yang
et
al
2020b
and
non-consensuality
prabhu
and
birhane
2020
different
data
bias
audit
was
carried
out
on
safegraph
research
release
safegraph
data
captures
mobility
patterns
in
the
us
with
data
from
nearly
50
million
mobile
devices
obtained
and
maintained
by
safegraph
private
data
com
pany
their
recent
academic
release
has
become
fundamental
resource
for
pandemic
research
to
the
point
of
being
used
by
the
centers
for
disease
control
and
prevention
to
measure
the
effectiveness
of
social
distancing
measures
moreland
et
al
2020
to
evaluate
its
representativeness
for
the
overall
us
population
coston
et
al
2021
have
studied
selection
biases
in
this
dataset
in
algorithmic
fairness
research
datasets
play
similar
roles
to
the
ones
they
play
in
machine
learning
according
to
barocas
et
al
2019
including
training
catalyzing
attention
and
signalling
awareness
of
common
data
practices
one
notable
exception
is
that
fairness
datasets
are
not
used
to
track
algorithmic
progress
on
problem
over
time
likely
due
to
the
fact
that
there
is
no
consensus
on
single
measure
to
be
reported
on
the
other
hand
two
roles
peculiar
to
fairness
research
are
summarizing
service
or
product
that
is
being
audited
and
representing
an
important
resource
whose
biases
and
ethical
aspects
are
particularly
worthy
of
attention
we
note
that
these
roles
are
not
mutually
exclusive
and
that
datasets
can
play
multiple
roles
compas
for
example
was
originally
curated
to
perform
an
audit
of
pretrial
risk
assessment
tools
and
was
later
used
extensively
in
fair
ml
research
as
source
of
real
human
data
becoming
overall
catalyst
for
fairness
research
and
debate
in
sum
existing
fairness
datasets
originate
from
variety
of
domains
support
diverse
tasks
and
play
different
roles
in
the
algorithmic
fairness
literature
we
hope
our
work
will
contribute
to
establishing
principled
data
practices
in
the
field
to
guide
an
optimal
usage
of
these
resources
in
the
next
section
we
continue
our
discussion
on
the
key
features
of
these
datasets
with
change
of
perspective
asking
which
lessons
can
be
learnt
from
existing
resources
for
the
curation
of
novel
ones
best
practices
for
dataset
curation
in
this
section
we
analyze
the
surveyed
datasets
from
different
perspectives
typ
ical
of
critical
data
studies
human
computer
interaction
and
computer-supported
cooperative
work
in
particular
we
discuss
concerns
of
re-identification
sect
6.1
consent
sect
6.2
inclusivity
sect
6.3
sensitive
attribute
labeling
sect
6.4
and
transparency
sect
6.5
we
describe
range
of
approaches
and
consideration
to
these
topics
ranging
from
negligent
to
conscientious
our
aim
is
to
make
these
concerns
and
related
desiderata
more
visible
and
concrete
to
help
inform
responsible
curation
of
novel
fairness
resources
whose
number
has
been
increasing
in
recent
years
fig
6.1
re-identification
motivation
data
re-identification
or
de-anonymization
is
practice
through
which
instances
in
dataset
theoretically
representing
people
in
an
anonymized
fashion
are
successfully
mapped
back
to
the
respective
individuals
their
identity
is
thus
discov
123
algorithmic
fairness
datasets
the
story
so
far
2103
fig
most
datasets
employed
in
algorithmic
fairness
were
created
or
updated
after
2015
with
clear
growth
in
recent
years
ered
and
associated
with
the
information
encoded
in
the
dataset
features
examples
of
external
re-identification
attacks
include
de-anonymization
of
movie
ratings
from
the
netflix
prize
dataset
narayanan
and
shmatikov
2008
identification
of
profiles
based
on
social
media
group
membership
wondracek
et
al
2010
and
identification
of
people
depicted
in
verifiably
pornographic
categories
of
imagenet
prabhu
and
birhane
2020
these
analyses
were
carried
out
as
attacks
by
external
teams
for
demonstrative
purposes
but
dataset
curators
and
stakeholders
may
undertake
similar
efforts
internally
mckenna
2019b
there
are
multiple
harms
connected
to
data
re-identification
especially
the
ones
featured
in
algorithmic
fairness
research
due
to
their
social
significance
depending
on
the
domain
and
breadth
of
information
provided
by
dataset
malicious
actors
may
acquire
information
about
mobility
patterns
consumer
habits
political
leaning
psychological
traits
and
medical
conditions
of
individuals
just
to
name
few
the
potential
for
misuse
is
tremendous
including
phishing
attacks
blackmail
threat
and
manipulation
kröger
et
al
2021
face
recognition
datasets
are
especially
prone
to
successful
re-identification
as
by
definition
they
contain
information
strongly
con
nected
with
person
identity
the
problem
also
extends
to
general
purpose
computer
vision
datasets
in
recent
dataset
audit
prabhu
and
birhane
2020
found
images
of
beach
voyeurism
and
other
non-consensual
depictions
in
imagenet
and
were
able
to
identify
the
victims
using
reverse
image
search
engines
highlighting
downstream
risks
of
blackmail
and
other
forms
of
abuse
disparate
consideration
in
this
work
we
find
that
fairness
datasets
are
proofed
against
re-identification
with
full
range
of
measures
and
care
perhaps
surprisingly
some
datasets
allow
for
straightforward
re-identification
of
individuals
providing
their
full
names
we
do
not
discuss
these
resources
here
to
avoid
amplifying
the
harms
dis
cussed
above
other
datasets
afford
plausible
re-identification
providing
social
media
handles
and
aliases
such
as
twitter
abusive
behavior
sentiment140
facebook
large
123
2104
fabris
et
al
network
and
google
local
columbia
university
speed
dating
may
also
fall
in
this
category
due
to
restricted
population
from
which
the
sample
is
drawn
and
provision
of
age
field
of
study
and
zip
code
where
participants
grew
up
in
addition
in
con
trast
many
datasets
come
with
strong
guarantees
against
de-anonymization
which
is
especially
typical
of
health
data
such
as
mimic-iii
and
heritage
health
el
emam
et
al
2012
indeed
health
is
domain
where
culture
of
patient
record
confiden
tiality
is
widely
established
and
there
is
strong
attention
to
harm
avoidance
also
datasets
describing
scholarly
works
and
academic
collaboration
networks
academic
collaboration
networks
pubmed
diabetes
papers
cora
citeseer
are
typically
de
identified
with
numerical
ids
substituting
names
this
is
possibly
sign
of
attention
to
anonymization
from
curators
when
the
data
represents
potential
colleagues
as
consequence
researchers
are
protected
from
related
harms
but
posterior
annotation
of
sensitive
attributes
similarly
to
biega
et
al
2019
becomes
difficult
or
impossible
one
notable
exception
is
arnetminer
citation
network
derived
from
an
online
plat
form
which
is
especially
focused
on
data
mining
from
academic
social
networks
and
profiling
of
researchers
mitigating
factors
wide
range
of
factors
summarized
in
table
may
help
to
reduce
the
risk
of
re-identification
first
set
of
approaches
concerns
the
distribution
of
data
artefacts
some
datasets
are
simply
kept
private
minimizing
risks
in
this
regard
these
include
unige
us
student
performance
apnea
symptoms
in
queries
and
pymetrics
bias
group
the
last
two
being
proprietary
datasets
that
are
not
disclosed
to
preserve
intellectual
property
twitter
online
harrassment
is
available
upon
request
to
protect
the
identities
of
twitter
users
that
were
included
another
interesting
approach
are
mixed
release
strategies
nlsy
has
some
publicly
available
data
while
access
to
further
information
that
may
favour
re-identification
zip
code
and
census
tract
is
restricted
for
crawl-based
datasets
it
is
possible
to
keep
resource
private
while
providing
code
to
recreate
it
bias
in
bios
while
this
may
alleviate
some
concerns
it
will
not
deter
motivated
actors
as
post-hoc
remedy
proactive
removal
of
problematic
instances
is
also
possibility
as
shown
by
recent
work
on
imagenet
yang
et
al
2020b
another
family
of
approaches
is
based
on
redaction
aggregation
and
injection
of
noise
obfuscation
typically
involves
the
distribution
of
proprietary
company
data
at
level
of
abstraction
which
maintains
utility
to
company
while
hindering
recon
struction
of
the
underlying
human-readable
data
which
also
makes
re-identification
highly
unlikely
yahoo
c14b
learn
to
rank
microsoft
learning
to
rank
noise
injection
can
take
many
forms
such
as
top-coding
adult
saturation
of
certain
variables
and
blurring
chicago
ridesharing
disclosure
at
coarse
granularity
targeted
scrubbing
of
identifiable
information
is
also
rather
common
with
ad-hoc
techniques
applied
in
different
domains
for
example
the
curators
of
asap
dataset
featuring
student
essays
removed
personally
identifying
information
from
the
essays
using
named
entity
recognition
and
several
heuristics
finally
aggregation
of
data
into
subpopulations
of
interest
also
supports
the
anonymity
of
the
underlying
individuals
fico
so
far
we
have
covered
datasets
that
feature
human
data
derived
from
real-world
processes
toy
datasets
on
the
other
hand
are
perfectly
safe
from
this
point
of
view
however
their
social
relevance
is
inevitably
low
in
this
work
we
survey
four
popular
123
algorithmic
fairness
datasets
the
story
so
far
2105
table
mitigating
factors
against
re-identification
mitigating
factor
example
datasets
controlled
distribution
private
dataset
unige
pymetrics
bias
group
availability
upon
request
twitter
online
harrassment
mixed
release
strategy
nlsy
code-based
reconstruction
bias
in
bios
data
perturbation
obfuscation
yahoo
c14b
learn
to
rank
microsoft
learning
to
rank
top-coding
adult
blurring
chicago
ridesharing
targeted
scrubbing
asap
aggregation
fico
synthesis
synthetic
data
toy
dataset
semi-synthetic
data
antelope
valley
networks
kidney
matching
hypothetical
profiles
italian
car
insurance
age
german
credit
ones
taken
from
zafar
et
al
2017c
donini
et
al
2018
lipton
et
al
2018
singh
and
joachims
2019
semi-synthetic
datasets
aim
for
the
best
of
both
worlds
by
generating
artificial
data
from
models
that
emulate
the
key
characteristics
of
the
underlying
processes
as
is
the
case
with
antelope
valley
networks
kidney
matching
and
the
generative
adversarial
network
trained
by
mcduff
et
al
2019
on
ms-celeb
1m
data
synthesis
may
also
be
applied
to
augment
datasets
with
artificial
sensitive
attributes
in
principled
fashion
movielens
burke
et
al
2018a
finally
resources
designed
to
externally
probe
services
algorithms
and
platforms
to
estimate
the
direct
effect
of
feature
of
interest
gender
race
may
rely
on
hypothetical
profiles
bertrand
and
mullainathan
2004
fabris
et
al
2021
this
approach
can
support
evaluations
of
fairness
through
unawareness
grgic-hlaca
et
al
2016
of
which
italian
car
insurance
is
an
example
one
last
important
factor
is
the
age
of
dataset
re-identification
of
old
information
about
individuals
requires
matching
with
auxiliary
resources
from
the
same
period
which
are
less
likely
to
be
maintained
than
comparable
resources
from
recent
years
moreover
even
if
successful
the
consequences
of
re-identification
are
likely
mitigated
by
dataset
age
as
old
information
about
individuals
is
less
likely
to
support
harm
against
them
the
german
credit
dataset
for
example
represents
loan
applicants
from
1973
to
1975
whose
re-identification
and
subsequent
harm
appears
less
likely
than
re-identification
for
more
recent
datasets
in
the
same
domain
anonymization
vs
social
relevance
utility
and
privacy
are
typically
considered
conflicting
objectives
for
dataset
wieringa
et
al
2021
if
we
define
social
rele
vance
as
the
breadth
and
depth
of
societally
useful
insights
that
can
be
derived
from
dataset
similar
conflict
with
privacy
becomes
clear
old
datasets
hardly
afford
123
2106
fabris
et
al
any
insight
that
is
actionable
and
relevant
to
current
applications
insight
derived
from
synthetic
datasets
is
inevitably
questionable
noise
injection
increases
uncertainty
and
reduces
the
precision
of
claims
obfuscation
hinders
subsequent
annotation
of
sensitive
attributes
conservative
release
strategies
increase
friction
and
deter
from
obtaining
and
analyzing
the
data
the
most
socially
relevant
fairness
datasets
typically
feature
confidential
information
criminal
history
and
financial
situation
in
conjunction
with
sensitive
attributes
of
individuals
race
and
sex
for
these
reasons
the
social
impact
afforded
by
dataset
and
the
safety
against
re-identification
of
included
indi
viduals
are
potentially
conflicting
objectives
that
require
careful
balancing
in
the
next
section
we
discuss
informed
consent
another
important
aspect
for
the
privacy
of
data
subjects
6.2
consent
motivation
in
the
context
of
data
informed
consent
is
an
agreement
between
data
processor
and
subject
aimed
at
allowing
collection
and
use
of
personal
information
while
guaranteeing
some
control
to
the
subject
it
is
emphasized
in
article
and
recitals
42
and
43
of
the
general
data
protection
regulation
european
union
2016
requiring
it
to
be
freely
given
specific
informed
and
unambiguous
paullada
et
al
2020
note
that
in
the
absence
of
individual
control
on
personal
information
anyone
with
access
to
the
data
can
process
it
with
little
oversight
possibly
against
the
interest
and
well-being
of
data
subjects
consent
is
thus
an
important
tool
in
healthy
data
ecosystem
that
favours
development
trust
and
dignity
negative
examples
separate
framework
often
conflated
with
consent
is
copy
right
licenses
such
as
creative
commons
discipline
how
academic
and
creative
works
can
be
shared
and
built
upon
with
proper
credit
attribution
according
to
the
creative
commons
organization
however
their
licenses
are
not
suited
to
protect
privacy
and
cover
research
ethics
merkley
2019
in
computer
vision
and
especially
in
face
recog
nition
consent
and
copyright
are
often
considered
and
discussed
jointly
and
creative
commons
licenses
are
frequently
taken
as
an
all-inclusive
permit
encompassing
intel
lectual
property
consent
and
ethics
prabhu
and
birhane
2020
merler
et
al
2019
for
example
mention
privacy
and
copyright
concerns
in
the
construction
of
diver
sity
in
faces
these
concerns
are
apparently
jointly
solved
by
obtaining
images
from
yfcc-100m
due
to
the
fact
that
large
portion
of
the
photos
have
creative
com
mons
license
indeed
lack
of
consent
is
widespread
and
far-reaching
problem
in
face
recognition
datasets
keyes
et
al
2019
prabhu
and
birhane
2020
find
several
examples
of
non-consensual
images
in
large
scale
computer
vision
datasets
partic
ularly
egregious
example
covered
in
this
survey
is
ms-celeb-1m
released
in
2016
as
the
largest
publicly
available
training
set
for
face
recognition
in
the
world
guo
et
al
2016b
as
suggested
by
its
name
the
dataset
should
feature
only
celebrities
to
enable
our
training
testing
and
re-distributing
under
certain
licenses
guo
et
al
2016b
however
the
dataset
was
later
found
to
feature
several
people
who
are
in
no
way
celebrities
and
must
simply
maintain
an
online
presence
the
dataset
was
retracted
for
this
reason
murgia
2019
123
algorithmic
fairness
datasets
the
story
so
far
2107
positive
examples
faces
an
experimental
psychology
dataset
on
emotion
related
stimuli
represents
positive
exception
in
the
face
analysis
domain
due
its
small
cardinality
it
was
possible
to
obtain
informed
consent
from
every
participant
one
domain
where
informed
consent
doctrine
has
been
well-established
for
decades
is
medicine
fairness
datasets
from
this
space
are
typically
sensitive
to
the
topic
exper
iments
such
as
randomized
controlled
trials
always
require
consent
elicitation
and
often
discuss
the
process
in
the
respective
articles
infant
health
and
development
program
ihdp
for
instance
is
dataset
used
to
study
fair
risk
assessment
it
was
collected
through
the
ihdp
program
carried
out
between
1985
and
1988
in
the
us
to
evaluate
the
effectiveness
of
comprehensive
early
intervention
in
reducing
devel
opmental
and
health
problems
in
low
birth
weight
premature
infants
brooks-gunn
et
al
1992
clearly
state
that
of
the
1302
infants
who
met
enrollment
criteria
274
21
had
parents
who
refused
consent
and
43
were
withdrawn
before
entry
into
the
assigned
group
longitudinal
studies
require
trust
and
continued
participation
they
typically
produce
insights
and
data
thanks
to
participants
who
have
read
and
signed
an
informed
consent
form
examples
of
such
datasets
include
framingham
stem
ming
from
study
on
cardiovascular
disease
and
the
national
longitudinal
survey
of
youth
following
the
lives
of
representative
samples
of
us
citizens
focusing
on
their
labor
market
activities
and
other
significant
life
events
field
studies
and
derived
datasets
drugnet
homeless
youths
social
networks
are
also
attentive
to
informed
consent
the
fries
framework
according
to
the
consentful
tech
project
consent
should
be
freely
given
reversible
informed
enthusiastic
and
specific
fries
below
we
expand
on
these
points
and
discuss
some
fairness
datasets
through
the
fries
lens
pokec
social
network
summarizes
the
networks
of
pokec
users
pop
ular
social
network
in
slovakia
and
czech
republic
due
to
default
privacy
settings
being
predefined
as
public
wealth
of
information
for
each
profile
was
collected
by
curators
including
information
on
demographics
politics
education
marital
status
and
children
takac
and
zabovsky
2012
while
privacy
settings
are
useful
tool
to
control
personal
data
default
public
settings
are
arguably
misleading
and
do
not
amount
to
freely
given
consent
in
the
presence
of
more
conservative
predefined
set
tings
user
can
explicitly
choose
to
publicly
share
their
information
this
may
be
interpreted
as
consent
to
share
one
information
here
and
now
with
other
users
more
loose
interpretations
favouring
data
collection
and
distribution
are
also
possible
but
they
seem
rather
lacking
in
specificity
it
is
far
from
clear
that
choosing
public
profile
settings
entails
consent
to
become
part
of
study
and
publicly
available
dataset
for
years
to
come
this
stands
in
contrast
with
framingham
and
other
datasets
derived
from
medical
studies
where
consent
may
be
provided
or
refused
with
fine
granularity
levy
et
al
2010
in
this
regard
let
us
consider
consent
form
from
recent
framingham
exam
framingham
heart
study
2021
the
form
comes
with
five
different
consent
boxes
which
cover
participation
in
examination
use
of
resulting
data
participation
in
genetic
studies
sharing
of
data
with
external
entities
and
notification
of
findings
to
subject
before
the
consent
boxes
well-structured
document
informs
participants
on
the
https://www.consentfultech.io/.
123
2108
fabris
et
al
reasons
for
this
study
clarifies
that
they
can
choose
to
drop
out
without
penalties
at
any
point
provides
point
of
contact
explains
what
will
happen
in
the
study
and
what
are
the
risks
to
the
subject
some
examples
of
accessible
language
and
open
explanations
include
the
following
you
have
the
right
to
refuse
to
allow
your
data
and
samples
to
be
used
or
shared
for
further
research
please
check
the
appropriate
box
in
the
selection
below
there
is
potential
risk
that
your
genetic
information
could
be
used
to
your
disadvantage
for
example
if
genetic
research
findings
suggest
serious
health
problem
that
could
be
used
to
make
it
harder
for
you
to
get
or
keep
job
or
insurance
however
we
cannot
guarantee
total
privacy
once
information
is
given
to
outside
parties
we
cannot
promise
that
it
will
be
kept
private
moreover
the
consent
form
is
accessible
from
website
that
promises
to
deliver
spanish
version
showing
attention
to
linguistic
minorities
overall
this
approach
seems
geared
towards
trust
and
truly
informed
consent
in
some
cases
consent
is
made
unapplicable
by
necessity
allegheny
child
wel
fare
for
instance
stems
from
an
initiative
by
the
allegheny
county
department
of
human
services
to
develop
assistive
tools
to
support
child
maltreatment
hotline
screening
decisions
individuals
who
resort
to
this
service
are
in
situation
of
need
and
emergency
that
makes
enthusiastic
consent
highly
unlikely
similar
considera
tions
arise
in
any
situations
where
data
subjects
are
in
state
of
need
and
can
only
access
service
by
providing
their
data
clear
example
is
harvey
rescue
the
result
of
crowdsourced
efforts
to
connect
rescue
parties
with
people
requesting
help
in
the
houston
area
moreover
the
provision
of
data
is
mandatory
in
some
cases
such
as
the
us
census
which
conflicts
with
meaningful
let
alone
enthusiastic
consent
finally
consent
should
be
reversible
giving
individuals
chance
to
revoke
it
and
be
removed
from
dataset
this
is
an
active
area
of
research
studying
specific
tools
for
consent
management
albanese
et
al
2020
and
approaches
for
retroactive
removal
of
an
instance
from
model
training
set
ginart
et
al
2019
unfortunately
even
when
discontinued
or
redacted
some
datasets
remain
available
through
backchannels
and
derivatives
ms-celeb-1m
is
again
negative
example
in
this
regard
the
dataset
was
removed
by
microsoft
after
widespread
criticism
and
claims
of
privacy
infringe
ment
despite
this
fact
it
remains
available
via
academic
torrents
peng
et
al
2021
moreover
ms-celeb-1m
was
used
as
source
of
images
for
several
datasets
derived
from
it
including
the
bupt
faces
and
racial
faces
in
the
wild
datasets
covered
in
this
survey
this
fact
demonstrates
that
harms
related
to
data
artefacts
are
not
simply
remedied
via
retirement
or
redaction
ethical
considerations
about
consent
and
poten
tial
harms
to
people
must
be
more
than
an
afterthought
and
need
to
enter
the
discussion
during
design
6.3
inclusivity
motivation
issues
of
representation
inclusion
and
diversity
are
central
to
the
fair
ml
community
due
to
historical
biases
stemming
from
structural
inequalities
some
pop
ulations
and
their
perspectives
are
underrepresented
in
certain
domains
and
in
related
123
algorithmic
fairness
datasets
the
story
so
far
2109
data
artefacts
jo
and
gebru
2020
for
example
the
person
subtree
of
imagenet
contains
images
that
skew
toward
male
young
and
light
skin
individuals
yang
et
al
2020b
female
entities
were
found
to
be
underrepresented
in
popular
datasets
for
coreference
resolution
zhao
et
al
2018
even
datasets
that
match
natural
group
pro
portions
may
support
the
development
of
biased
tools
with
low
accuracy
for
minorities
recent
works
have
demonstrated
the
disparate
performance
of
tools
on
sensitive
subpopulations
in
domains
such
as
health
care
obermeyer
and
mullainathan
2019
speech
recognition
tatman
2017
and
computer
vision
buolamwini
and
gebru
2018
inclusivity
and
diversity
are
often
considered
primary
solution
in
this
regard
both
in
training
sets
which
support
the
development
of
better
models
and
test
sets
capable
of
flagging
such
issues
positive
examples
ideally
inclusivity
should
begin
with
clear
definition
of
data
collection
objectives
jo
and
gebru
2020
indeed
we
find
that
diversity
and
repre
sentation
are
strong
points
of
datasets
that
were
creted
to
assess
biases
in
services
products
and
algorithms
bold
hmda
fico
law
school
scientist
painter
cvs
from
singapore
youtube
dialect
accuracy
pilot
parliaments
benchmark
which
were
designed
and
curated
with
special
attention
to
sensitive
groups
we
also
find
instances
of
ex-post
remedies
to
issues
of
diversity
as
an
example
the
curators
of
imagenet
proposed
demographic
balancing
solution
based
on
web
interface
that
removes
the
images
of
overrepresented
categories
yang
et
al
2020b
natural
alter
native
is
the
collection
of
novel
instances
solution
adopted
for
framingham
this
dataset
stems
from
study
of
key
factors
that
contribute
to
cardiovascular
disease
with
participants
recruited
in
framingham
massachusetts
over
multiple
decades
recent
cohorts
were
especially
designed
to
reflect
greater
racial
and
ethnic
diversity
in
the
town
tsao
and
vasan
2015
negative
examples
among
the
datasets
we
surveyed
we
highlight
one
whose
low
inclusivity
is
rather
obvious
webtext
is
40
gb
text
dataset
that
supported
training
of
the
gpt-2
language
model
radford
et
al
2019
the
authors
crawled
every
document
reachable
from
outbound
reddit
links
that
collected
at
least
karma
while
this
was
considered
useful
heuristic
to
achieve
size
and
quality
it
ended
up
skewing
this
resource
towards
content
appreciated
by
reddit
users
who
are
predominantly
male
young
and
enjoy
good
internet
access
this
should
act
as
reminder
that
size
does
not
guarantee
diversity
bender
et
al
2021
and
that
sampling
biases
are
almost
inevitable
inclusivity
is
nuanced
while
inclusivity
surely
requires
an
attention
to
subpop
ulations
more
precise
definition
may
depend
on
context
and
application
based
on
the
task
at
hand
an
ideal
sample
may
feature
all
subpopulations
with
equal
presence
or
proportionally
to
their
share
in
the
overall
population
let
us
call
these
the
equal
and
proportional
approach
to
diversity
the
equal
approach
is
typical
of
datasets
that
are
meant
to
be
evaluation
benchmarks
pilot
parliaments
benchmark
winobias
and
allow
for
statistically
significant
statements
on
performance
differences
across
groups
on
the
other
hand
the
proportional
approach
is
rather
common
in
datasets
collected
by
census
offices
such
as
us
census
data
1990
and
in
resources
aimed
precisely
at
studying
issues
of
representation
in
services
and
products
occupations
in
google
images
open-ended
collection
of
data
is
ideal
to
ensure
that
various
cultures
are
represented
in
the
manner
in
which
they
would
like
to
be
seen
jo
and
gebru
2020
unfortunately
123
2110
fabris
et
al
we
found
no
instance
of
datasets
where
sensitive
labels
were
self-reported
according
to
open-ended
responses
on
the
contrary
individuals
with
non-conforming
gender
identities
were
excluded
from
some
datasets
and
analyses
bing
us
queries
is
proprietary
dataset
used
to
study
differential
user
satisfaction
with
the
bing
search
engine
across
different
demographic
groups
it
consists
of
subset
of
bing
users
who
provided
their
gender
at
registration
according
to
binary
categorization
which
misrepresents
or
simply
excludes
non-binary
users
from
the
subset
moreover
dataset
may
be
inclusive
and
encode
gender
in
non-binary
gender
fashion
climate
assembly
uk
but
if
used
in
conjunction
with
an
auxiliary
dataset
where
gender
has
binary
encoding
common
solution
is
removing
instances
whose
gender
is
neither
female
nor
male
flanigan
et
al
2020
inclusivity
does
not
guarantee
benefits
to
avoid
downstream
harms
inclusion
by
itself
is
insufficient
the
context
in
which
people
and
sensitive
groups
are
repre
sented
should
always
be
taken
into
account
despite
its
overall
skew
towards
male
subjects
imagenet
has
high
female-to-male
ratio
in
classes
such
as
bra
bikini
and
maillot
which
often
feature
images
that
are
voyeuristic
pornographic
and
non
consensual
prabhu
and
birhane
2020
similarly
in
ms-coco
famous
dataset
for
object
recognition
there
is
roughly
female-to-male
ratio
increasing
to
0.95
for
images
of
kitchens
hendricks
et
al
2018
this
sort
of
representation
is
unlikely
to
benefit
women
in
any
way
and
on
the
contrary
may
contribute
to
reinforce
stereotypes
and
support
harmful
biases
another
clear
but
often
ignored
disconnect
between
the
inclusion
of
group
and
benefits
to
it
is
represented
by
the
task
at
hand
and
more
in
general
by
possible
uses
afforded
by
dataset
in
this
regard
we
find
many
datasets
from
the
face
recognition
domain
which
are
presented
as
resources
geared
towards
inclusion
diversity
in
faces
bupt
faces
utk
face
fairface
racial
faces
in
the
wild
attention
to
subpopu
lations
in
this
context
is
still
called
diversity
diversity
in
faces
fairface
racial
faces
in
the
wild
or
social
awareness
bupt
faces
but
is
driven
by
business
imperatives
and
goals
of
robustness
for
technology
that
can
very
easily
be
employed
for
surveillance
purposes
and
become
detrimental
to
vulnerable
populations
included
in
datasets
in
similar
vein
the
faces
dataset
has
been
used
to
measure
age
bias
in
emotion
detection
task
whose
applications
and
benefits
for
individuals
remain
dubious
overall
attention
to
subpopulations
is
an
upside
of
many
datasets
we
surveyed
however
inclusion
representation
and
diversity
can
be
defined
in
different
ways
according
to
the
problem
at
hand
individuals
would
rather
be
included
on
their
own
terms
and
decide
whether
and
how
they
should
be
represented
the
problems
of
diversity
and
robustness
have
some
clear
commonalities
as
the
former
can
be
seen
as
means
towards
the
latter
but
it
seems
advisable
to
maintain
clear
separation
between
the
two
and
to
avoid
equating
either
one
with
fairness
algorithmic
fairness
will
not
be
solved
by
simply
collecting
more
data
or
granting
equal
performance
across
different
groups
identified
by
given
sensitive
attribute
123
algorithmic
fairness
datasets
the
story
so
far
2111
table
approaches
to
demographic
data
procurement
approach
example
datasets
self-reported
labels
bing
us
queries
movielens
libimset
adult
hmda
law
school
sushi
willingness-to-pay
for
vaccine
expert
labels
pilot
parliaments
benchmark
non-expert
labels
celebfaces
attributes
diversity
in
faces
fairface
occupations
in
google
images
ml
algorithm
racial
faces
in
the
wild
instagram
photos
bupt
faces
utk
face
ml
algorithm
annotators
fairface
open
images
dataset
rule
knowledge-based
algorithm
rtgender
bias
in
bios
demographics
on
twitter
twitteraae
6.4
sensitive
attribute
labelling
motivation
datasets
are
often
taken
as
factual
information
that
supports
objective
computation
and
pattern
extraction
the
etymology
of
the
word
data
meaning
given
is
rather
revealing
in
this
sense
on
the
contrary
research
in
human
computer
interaction
computer-supported
cooperative
work
and
critical
data
studies
argues
that
this
belief
is
superficial
limited
and
potentially
harmful
muller
et
al
2019
crawford
and
paglen
2021
data
is
quite
simply
human-influenced
entity
miceli
et
al
2021
determined
by
chain
of
discretionary
decisions
on
measurement
sampling
and
categorization
which
shape
how
and
by
whom
data
will
be
collected
and
annotated
according
to
which
taxonomy
and
based
on
which
guidelines
data
science
professionals
often
more
cognizant
of
the
context
surrounding
data
than
theoretical
researchers
report
significant
awareness
of
how
curation
and
annotation
choices
influence
their
data
and
its
relation
with
the
underlying
phenomena
muller
et
al
2019
in
an
interview
senior
text
classification
researcher
responsible
for
ground
truth
annotation
shows
consciousness
of
their
own
influence
on
datasets
by
stating
am
the
ground
truth
muller
et
al
2019
sensitive
attributes
such
as
race
and
gender
are
no
exception
in
this
regard
incon
sistencies
in
racial
annotation
are
rather
common
within
the
same
system
lum
et
al
2020
and
even
more
so
across
different
systems
scheuerman
et
al
2020
khan
and
fu
2021
external
annotation
either
human
or
algorithmic
is
essentially
based
on
co-occurrence
of
specific
traits
with
membership
in
group
thus
running
the
risk
of
encoding
and
reinforcing
stereotypes
self-reported
labels
overcome
this
issue
although
they
are
still
based
on
an
imposed
taxonomy
unless
provided
in
an
open-ended
fashion
in
this
section
we
discuss
the
practices
through
which
sensitive
attributes
are
annotated
in
datasets
used
in
algorithmic
fairness
research
which
are
summarized
in
table
procurement
of
sensitive
attributes
self-reported
labels
for
sensitive
attributes
are
typical
of
datasets
that
represent
users
of
service
who
may
report
their
demo
graphics
during
registration
bing
us
queries
movielens
libimseti
or
that
were
123
2112
fabris
et
al
gathered
through
surveys
hmda
adult
law
school
sushi
willingness-to-pay
for
vaccine
these
are
all
resources
for
which
collection
of
protected
attributes
was
envi
sioned
at
design
potentially
as
an
optional
step
however
when
sensitive
attributes
are
not
available
their
annotation
may
be
possible
through
different
mechanisms
common
approach
is
having
sensitive
attributes
labelled
by
non-experts
often
workers
hired
on
crowdsourcing
platforms
celebfaces
attributes
dataset
celeba
features
images
of
celebrities
from
the
celebfaces
dataset
augmented
with
annota
tions
of
landmark
location
and
categorical
attributes
including
gender
skin
tone
and
age
which
were
annotated
by
professional
labeling
company
liu
et
al
2015
in
similar
fashion
diversity
in
faces
consists
of
images
labeled
with
gender
and
age
by
workers
hired
through
the
figure
eight
crowd-sourcing
platform
while
the
creators
of
fairface
hired
workers
on
amazon
mechanical
turk
to
annotate
gender
race
and
age
in
public
image
dataset
this
practice
also
raises
concerns
of
fair
compensation
of
labour
which
are
not
discussed
in
this
work
some
creators
employ
algorithms
to
obtain
sensitive
labels
face
datasets
curators
often
resort
to
the
face
api
racial
faces
in
the
wild
instagram
photos
bupt
faces
or
other
algorithms
utk
face
fairface
in
essence
labeling
is
classifying
hence
measuring
and
reporting
accuracy
for
this
procedure
would
be
in
order
but
rarely
happens
creators
occasionally
note
that
automated
labels
were
validated
fairface
or
substantially
enhanced
open
images
dataset
by
human
annotators
and
very
seldom
report
inter-annotator
agreement
occupations
in
google
images
other
examples
of
external
labels
include
the
geographic
origin
of
candidates
in
resumes
cvs
from
singapore
political
leaning
of
us
twitter
profiles
twitter
politi
cal
searches
english
dialect
of
tweets
twitteraae
and
gender
of
subjects
featured
in
image
search
results
for
professions
occupations
in
google
images
annota
tion
may
also
rely
on
external
knowledge
bases
such
as
wikipedia
as
is
the
case
with
rtgender
in
situations
where
text
written
by
individuals
is
available
rule-based
approaches
exploiting
gendered
nouns
woman
or
pronouns
she
are
also
appli
cable
bias
in
bios
demographics
on
twitter
some
datasets
may
simply
have
no
sensitive
attribute
these
are
often
used
in
works
of
individual
fairness
but
may
occasionally
support
studies
of
group
fairness
for
example
dsprites
is
synthetic
computer
vision
dataset
where
regular
covariates
may
play
the
role
of
sensitive
variables
locatello
et
al
2019
alternatively
datasets
can
be
augmented
with
simulated
demographics
as
done
by
madnani
et
al
2017
who
randomly
assigned
native
language
to
test-takers
in
asap
or
through
the
technique
of
burke
et
al
2018a
which
they
demonstrate
on
movielens
face
datasets
posterior
annotation
is
especially
common
in
computer
vision
datasets
the
pilot
parliaments
benchmark
for
instance
was
devised
as
testbed
for
face
analysis
algorithms
it
consists
of
images
of
parliamentary
representatives
from
three
african
and
three
european
countries
that
were
labelled
by
surgical
der
matologist
with
the
fitzpatrick
skin
type
of
the
subjects
fitzpatrick
1988
this
is
dermatological
scale
for
skin
color
which
can
be
retrieved
from
people
appearance
on
the
contrary
annotations
of
race
or
ethnicity
from
photo
are
simplistic
at
best
and
it
should
be
clear
that
they
actually
capture
perceived
race
from
the
perspective
https://en.wikipedia.org/wiki/category:american_female_tennis_players.
123
algorithmic
fairness
datasets
the
story
so
far
2113
of
the
annotator
fairface
bupt
faces
careful
nomenclature
is
an
important
first
step
to
improve
the
transparency
of
dataset
and
make
the
underlying
context
more
visible
similarly
to
scheuerman
et
al
2020
we
find
that
documentation
accompanying
face
recognition
datasets
hardly
ever
describes
how
specific
taxonomies
for
gender
and
race
were
chosen
conveying
false
impression
of
objectivity
description
of
the
annotation
process
is
typically
present
but
minimal
for
multi-task
facial
landmark
for
instance
we
only
know
that
the
ground
truths
of
the
related
tasks
are
labeled
manually
zhang
et
al
2014
annotation
trade-offs
it
is
worth
re-emphasizing
that
sensitive
label
assignment
is
classification
task
that
rests
on
assumptions
annotation
of
race
and
gender
in
images
for
example
is
based
on
the
idea
that
they
can
be
accurately
ascertained
from
pictures
which
is
an
oversimplification
of
these
constructs
the
envisioned
classes
binary
gender
are
another
subjective
choice
stemming
from
the
point
of
view
of
dataset
curators
and
may
reflect
narrow
or
outdated
conceptions
and
potentially
harm
the
data
subjects
in
this
regard
quote
from
the
curators
of
ms-celeb-1m
who
do
not
annotate
race
but
consider
it
for
their
sampling
strategy
is
particularly
striking
we
cover
all
the
major
races
in
the
world
caucasian
mongoloid
and
negroid
guo
et
al
2016b
for
these
reasons
external
annotation
of
sensitive
attributes
is
controversial
and
inevitably
influenced
by
dataset
curators
on
the
other
hand
external
annotation
may
be
the
only
way
to
test
specific
biases
occupations
in
google
images
for
instance
is
an
image
dataset
collected
to
study
gender
and
skin
tone
diversity
in
image
search
results
for
various
professions
the
creators
hired
workers
on
amazon
mechanical
turk
to
label
the
gender
male
female
and
fitzpatrick
skin
tone
type
of
the
primary
person
in
each
image
the
pilot
parliaments
benchmark
was
also
annotated
externally
to
obtain
benchmark
for
the
evaluation
of
face
analysis
technology
with
balanced
representation
of
gender
and
skin
type
different
purposes
can
motivate
data
collection
and
annotation
of
sensitive
attributes
purposes
and
aims
should
be
documented
clearly
while
also
reflecting
on
other
uses
and
potential
for
misuse
of
dataset
gebru
et
al
2018
dataset
curators
may
use
documentation
to
discuss
these
aspects
and
specify
limitations
for
the
intended
use
of
resource
peng
et
al
2021
in
the
next
section
we
focus
on
documentation
and
why
it
represents
key
component
of
data
curation
6.5
transparency
motivation
transparent
and
accurate
documentation
is
fundamental
part
of
data
quality
its
absence
may
lead
to
serious
issues
including
lack
of
reproducibility
con
cerns
of
scientific
validity
ethical
problems
and
harms
barocas
et
al
2019
clear
documentation
can
shine
light
on
inevitable
choices
made
by
dataset
creators
and
on
the
context
surrounding
the
data
in
the
absence
of
this
information
the
curation
mechanism
mediating
reality
and
data
is
hidden
the
data
becomes
one
with
its
context
in
this
article
we
typically
discuss
sensitive
attributes
following
the
naming
convention
in
the
accompa
nying
documentation
of
dataset
avoiding
critical
terminology
discussion
123
2114
fabris
et
al
to
the
point
that
interpretation
of
numerical
results
can
be
misleading
and
overarching
bao
et
al
2021
the
ground
truth
labels
typically
indicated
with
the
letter
which
are
the
target
of
prediction
tasks
in
some
datasets
such
as
indications
of
recidivism
in
compas
are
especially
sensitive
in
this
regard
indeed
not
only
accuracy
and
related
quality
metrics
but
also
measures
of
algorithmic
fairness
such
as
sufficiency
and
separation
barocas
et
al
2019
are
based
on
labels
and
the
ability
of
ml
algorithms
to
replicate
them
implicitly
granting
them
special
status
of
truthfulness
in
reality
however
these
labels
may
be
biased
and
incorrect
due
to
multiple
causes
including
very
frequently
disconnect
between
what
we
aim
to
measure
in
an
ideal
construct
space
offense
in
the
case
of
compas
and
what
we
can
actually
measure
in
the
observed
space
arrest
friedler
et
al
2021
fair
ml
algorithms
measures
can
only
partly
overcome
catch
these
biases
and
actually
run
the
risk
of
further
reifying
them
proper
documentation
does
not
solve
this
issue
but
equips
practitioners
and
researchers
with
the
necessary
awareness
to
handle
these
biases
more
broadly
good
documentation
should
discuss
and
explain
features
providing
context
about
who
collected
and
annotated
the
data
how
and
for
which
purpose
gebru
et
al
2018
denton
et
al
2020
this
provides
dataset
users
with
information
they
can
leverage
to
select
appropriate
datasets
for
their
tasks
and
avoid
unintentional
misuse
gebru
et
al
2018
other
actors
such
as
reviewers
may
also
access
the
official
documentation
of
dataset
to
ensure
that
it
is
employed
in
compliance
with
its
stated
purpose
guidelines
and
terms
of
use
peng
et
al
2021
positive
examples
in
this
survey
we
find
examples
of
excellent
documentation
in
datasets
related
to
studies
and
experiments
including
chexpert
framingham
and
nlsy
indeed
datasets
curated
by
medical
institutions
and
census
offices
are
often
well-documented
the
ideal
source
of
good
documentation
are
descriptor
articles
published
in
conjunction
with
dataset
mimic-iii
typically
offering
stronger
guarantees
than
web
pages
in
terms
of
quality
and
permanence
official
websites
hosting
and
distributing
datasets
are
also
important
to
collect
updates
errata
and
additional
information
that
may
not
be
available
at
the
time
of
release
the
million
song
dataset
and
goodreads
reviews
for
instance
are
available
on
websites
which
contain
useful
overview
of
the
respective
dataset
list
of
updates
code
samples
pointers
to
documentation
and
contacts
for
further
questions
negative
examples
on
the
other
hand
some
datasets
are
opaque
and
poorly
docu
mented
among
publicly
available
ones
arrhythmia
is
distributed
with
description
of
the
features
but
no
context
about
the
purposes
actors
and
subjects
involved
in
the
data
collection
similarly
the
whole
curation
process
and
composition
of
multi-task
facial
landmark
is
described
in
short
paragraph
explaining
it
consists
of
10
000
outdoor
face
images
from
the
web
that
were
labelled
manually
with
gender
most
face
datasets
suffer
from
opaque
documentation
especially
concerning
the
choice
of
sensi
tive
labels
and
their
annotation
for
semi-synthetic
resources
proper
documentation
is
especially
important
to
let
users
understand
the
broader
applicability
and
implications
of
numerical
analyses
performed
on
dataset
ibm
hr
analytics
is
resource
about
employee
attrition
which
the
hosting
website
describes
as
containing
fictional
data
without
any
additional
information
nonetheless
this
data
was
plausibly
generated
in
123
algorithmic
fairness
datasets
the
story
so
far
2115
principled
fashion
and
even
partial
disclosure
of
the
underlying
data
generation
mechanism
would
benefit
dataset
users
retrospective
documentation
good
documentation
may
also
be
produced
ret
rospectively
bandy
and
vincent
2021
garbin
et
al
2021
german
credit
is
an
interesting
example
of
dataset
that
was
poorly
documented
for
decades
until
the
recent
publication
of
report
correcting
severe
coding
mistakes
grömping
2019
as
mentioned
in
sect
4.3
from
the
old
documentation
it
seemed
possible
to
retrieve
the
sex
of
data
subjects
from
feature
jointly
encoding
sex
and
marital
status
the
dataset
archaeology
work
by
grömping
2019
shows
that
this
is
not
the
case
which
has
par
ticular
relevance
for
many
algorithmic
fairness
works
using
this
dataset
with
sex
as
protected
feature
as
this
feature
is
simply
not
available
numerical
results
obtained
in
this
setting
may
be
an
artefact
of
the
wrong
coding
with
which
the
dataset
has
been
and
still
is
officially
distributed
in
the
uci
machine
learning
repository
1994
until
the
report
and
the
new
redacted
dataset
uci
machine
learning
repository
2019
become
well-known
the
old
version
will
remain
prevalent
and
more
mistakes
will
be
made
in
other
words
while
the
documentation
debt
for
this
particular
dataset
has
been
ret
rospectively
addressed
opacity
many
algorithmic
fairness
works
published
after
the
report
continue
to
use
the
german
credit
dataset
with
sex
as
protected
attribute
he
et
al
2020b
yang
et
al
2020a
baharlouei
et
al
2020
lohaus
et
al
2020
martinez
et
al
2020
wang
et
al
2021
this
is
an
issue
of
documentation
sparsity
where
the
right
information
exists
but
does
not
reach
interested
parties
including
researchers
and
reviewers
documentation
is
fundamental
part
of
data
curation
with
most
responsibility
resting
on
creators
however
dataset
users
can
also
play
role
in
mitigating
the
documentation
debt
by
proactively
looking
for
information
about
the
resources
they
plan
to
use
brief
summaries
discussing
and
motivating
the
chosen
datasets
can
be
included
in
scholarly
articles
at
least
in
supplementary
materials
when
conflicting
with
page
limitations
indeed
documentation
debt
is
problem
for
the
whole
research
community
which
can
be
addressed
collectively
with
retrospective
contributions
and
clarifications
we
argue
that
it
is
also
up
to
individual
researchers
to
seek
contextual
information
for
situating
the
data
they
want
to
use
broader
relevance
to
the
community
along
with
the
analyses
presented
in
this
work
through
the
lens
of
tasks
supported
domains
spanned
and
roles
played
by
algorithmic
fairness
datasets
we
are
releasing
the
underlying
data
briefs
as
further
contribution
for
the
research
community
data
briefs
are
short
documentation
format
providing
essential
information
on
datasets
used
in
fairness
research
data
briefs
are
composed
of
ten
fields
detailed
in
appendix
derived
from
shared
vocabularies
such
as
data
catalog
vocabulary
dcat
to
be
compliant
with
the
fair
data
principles
wilkinson
et
al
2016
we
also
defined
schema
called
fdo
to
model
the
relationships
between
the
terms
and
to
make
the
links
to
external
vocabularies
explicit
we
leverage
fdo
to
format
the
data
briefs
as
resource
description
framework
rdf
miller
1998
and
to
make
them
available
123
2116
fabris
et
al
as
linked
open
data
thus
supporting
data
reuse
interoperability
and
interpretability
10
our
final
aim
is
to
release
update
and
maintain
web
app
which
can
be
queried
by
researchers
and
practitioners
to
find
the
most
relevant
datasets
according
to
their
specific
needs
11
we
envision
several
benefits
for
the
algorithmic
fairness
and
data
studies
communities
such
as
informing
the
choice
of
datasets
for
experimental
evaluations
of
fair
ml
methods
including
domain-oriented
and
task-oriented
search
directing
studies
of
data
bias
and
other
quantitative
and
qualitative
analyses
including
retrospective
documentation
efforts
towards
popular
or
otherwise
important
resources
identifying
areas
and
sub-problems
that
are
understudied
in
the
algorithmic
fair
ness
literature
supporting
multi-dataset
studies
focused
on
resources
united
by
common
char
acteristic
such
as
encoding
given
sensitive
attribute
scheuerman
et
al
2020
concerning
computer
vision
fabbrizzi
et
al
2021
or
being
popular
in
the
fairness
literature
le
quy
et
al
2022
conclusions
and
recommendations
algorithmic
fairness
is
young
research
area
undergoing
fast
expansion
with
diverse
contributions
in
terms
of
methodology
and
applications
progress
in
the
field
hinges
on
different
resources
including
very
prominently
datasets
in
this
work
extending
fabris
et
al
2022
we
have
surveyed
hundreds
of
datasets
used
in
the
fair
ml
and
algorithmic
equity
literature
to
help
the
research
community
reduce
its
documentation
debt
improve
the
utilization
of
existing
datasets
and
the
curation
of
novel
ones
with
respect
to
existing
resources
we
have
shown
that
the
most
popular
datasets
in
the
fairness
literature
adult
compas
and
german
credit
have
limited
merits
beyond
originating
from
human
processes
and
encoding
protected
attributes
on
the
other
hand
several
negative
aspects
call
into
question
their
current
status
of
general
purpose
fairness
benchmarks
including
contrived
prediction
tasks
noisy
data
severe
coding
mistakes
limitations
in
encoding
sensitive
attributes
and
age
we
have
documented
over
two
hundred
datasets
to
provide
viable
alternatives
annotating
their
domain
the
tasks
they
support
and
discussing
the
roles
they
play
in
works
of
algorithmic
fairness
we
have
shown
that
the
processes
generating
the
data
belong
to
many
different
domains
including
for
instance
criminal
justice
education
search
engines
online
marketplaces
emergency
response
social
media
medicine
hiring
and
finance
at
the
same
time
we
have
described
variety
of
tasks
studied
on
these
resources
ranging
from
generic
such
as
fair
classification
to
narrow
such
as
fair
districting
and
fair
truth
discovery
overall
such
diversity
of
domains
and
tasks
provides
glimpse
into
the
variety
of
human
activities
and
applications
that
can
be
impacted
by
automated
decision
making
and
that
can
benefit
from
algorithmic
10
schema
publicly
available
at
https://fairnessdatasets.dei.unipd.it/schema/;
rdf
publicly
available
at
https://zenodo.org/record/6518370#.ynoskftmjhf.
11
this
resource
will
be
released
at
https://fairnessdatasets.dei.unipd.it/.
123
algorithmic
fairness
datasets
the
story
so
far
2117
fairness
research
tasks
and
domain
annotations
are
made
available
in
our
data
briefs
to
facilitate
the
work
of
researchers
and
practitioners
interested
in
the
study
of
algorithmic
fairness
applied
to
specific
domains
or
tasks
by
assembling
sparse
information
on
hundreds
of
datasets
into
single
document
we
aim
to
provide
useful
reference
to
support
both
domain-oriented
and
task-oriented
dataset
search
at
the
same
time
we
have
analyzed
issues
connected
to
re-identification
consent
inclusivity
labeling
and
transparency
running
across
these
datasets
by
describing
range
of
approaches
and
attentiveness
to
these
topics
we
aim
to
make
them
more
visible
and
concrete
on
one
hand
this
may
prove
valuable
to
inform
post-hoc
data
interventions
aimed
at
mitigating
potential
harms
caused
by
existing
datasets
on
the
other
hand
as
novel
datasets
are
increasingly
curated
published
and
adopted
in
fairness
research
it
is
important
to
motivate
these
concerns
make
them
tangible
and
distill
existing
approaches
into
best
practices
which
we
summarize
below
for
future
endeavours
of
data
curation
our
recommendations
complement
and
do
not
replace
growing
body
of
work
studying
key
aspects
in
the
life
cycle
of
datasets
gebru
et
al
2018
jo
and
gebru
2020
prabhu
and
birhane
2020
crawford
and
paglen
2021
peng
et
al
2021
social
relevance
of
data
intended
as
the
breadth
and
depth
of
societally
useful
insights
afforded
by
datasets
is
central
requirement
in
fairness
research
unfor
tunately
this
may
conflict
with
user
privacy
favouring
re-identification
or
leaving
consideration
of
consent
in
the
background
consent
should
be
considered
during
the
initial
design
of
dataset
in
accordance
with
existing
frameworks
such
as
the
fries
framework
outlined
in
the
consentful
tech
project
moreover
different
strategies
are
available
to
alleviate
concerns
of
re-identification
including
noise
injection
conser
vative
release
and
semi
synthetic
data
generation
algorithmic
fairness
is
motivated
by
aims
of
justice
and
harm
avoidance
for
people
which
should
be
extended
to
data
subjects
inclusivity
is
also
important
for
social
relevance
as
it
allows
for
wider
repre
sentation
and
supports
analyses
that
take
into
account
important
groups
however
inclusivity
is
insufficient
in
itself
possible
uses
afforded
by
dataset
should
always
be
considered
evaluating
costs
and
benefits
for
the
data
subjects
and
the
wider
pop
ulation
in
the
absence
of
these
considerations
acritical
inclusivity
runs
the
risk
of
simply
supporting
system
robustness
across
sensitive
attributes
such
as
race
and
gen
der
rebranded
as
fairness
sensitive
attributes
are
key
ingredient
to
measure
inclusion
and
increase
the
social
relevance
of
dataset
although
often
impractical
it
is
typically
preferable
for
sen
sitive
attributes
to
be
self-reported
by
data
subjects
externally
assigned
labels
and
taxonomies
can
harm
individuals
by
erasing
their
needs
and
points
of
view
sensi
tive
attribute
labelling
is
thus
shortcut
whose
advantages
and
disadvantages
should
be
carefully
weighted
and
if
chosen
it
should
be
properly
documented
possible
approaches
based
on
human
labour
include
expert
and
non-expert
annotation
while
automated
approaches
range
from
simple
rule-based
systems
to
complex
and
opaque
algorithms
to
label
is
to
classify
hence
measuring
and
reporting
per-group
accuracy
is
in
order
some
labeling
endeavours
are
more
sensible
than
others
while
skin
tone
can
arguably
be
retrieved
from
pictures
annotations
of
race
from
an
image
actually
123
2118
fabris
et
al
capture
perceived
race
from
the
perspective
of
the
annotator
rigorous
nomenclature
favours
better
understanding
and
clarifies
the
subjectivity
of
certain
labels
reliable
documentation
shines
light
on
inevitable
choices
made
by
dataset
creators
and
on
the
context
surrounding
the
data
this
provides
dataset
users
with
information
they
can
leverage
to
select
appropriate
datasets
for
their
tasks
and
avoid
unintentional
misuse
datasets
for
which
some
curation
choices
are
poorly
documented
may
appear
more
objective
at
first
sight
however
it
should
be
clear
that
objective
data
and
turbid
data
are
very
different
things
proper
documentation
increases
transparency
trust
and
understanding
at
minimum
it
should
include
the
purpose
of
data
artifact
description
of
the
sample
the
features
and
related
annotation
procedures
along
with
an
explicit
discussion
of
the
associated
task
if
any
it
should
also
clarify
who
was
involved
in
the
different
stages
of
the
data
development
procedure
with
special
attention
to
annotation
data
documentation
also
supports
reviewers
and
readers
of
academic
research
in
assessing
whether
dataset
was
selected
with
good
reason
and
utilized
in
compliance
with
creators
guidelines
understanding
and
budgeting
for
all
these
aspects
during
early
design
phases
rather
than
after
collection
or
release
can
be
invaluable
for
data
subjects
data
users
and
soci
ety
while
possible
remedies
exist
data
is
an
extremely
fluid
asset
allowing
for
easy
reproduction
and
derivatives
of
all
sorts
remedies
applied
to
dataset
do
not
neces
sarily
benefit
its
derivates
in
this
work
we
have
targeted
the
collective
documentation
debt
of
the
algorithmic
fairness
community
resulting
from
the
opacity
surrounding
certain
resources
and
the
sparsity
of
existing
documentation
we
have
mainly
targeted
sparsity
in
centralized
documentation
effort
as
result
we
have
found
and
described
range
of
weaknesses
and
best
practices
that
can
be
adopted
to
reduce
opacity
and
mitigate
concerns
of
privacy
and
inclusion
similarly
to
other
types
of
data
interven
tions
useful
documentation
can
be
produced
after
release
but
as
shown
in
this
work
the
documentation
debt
may
propagate
nonetheless
in
mature
research
community
curators
users
and
reviewers
can
all
contribute
to
cultivating
data
documentation
culture
and
keep
the
overall
documentation
debt
in
check
supplementary
information
the
online
version
contains
supplementary
material
available
at
https://doi.
org
10.1007
s10618-022-00854-z
acknowledgements
the
authors
would
like
to
thank
the
following
researchers
and
dataset
creators
for
the
useful
feedback
on
the
data
briefs
alain
barrat
luc
behaghel
asia
biega
marko
bohanec
chris
burgess
robin
burke
alejandro
noriega
campero
margarida
carvalho
abhijnan
chakraborty
robert
cheetham
won
ik
cho
paulo
cortez
thomas
davidson
maria
de-arteaga
lucas
dixon
danijela
djordjevic
michele
donini
marco
duarte
natalie
ebner
elaine
fehrman
altay
guvenir
moritz
hardt
irina
higgins
yu
hen
hu
rachel
huddart
lalana
kagal
dean
karlan
vijay
keswani
been
kim
hyunjik
kim
jiwon
kim
svetlana
kiritchenko
pang
wei
koh
joseph
konstan
varun
kumar
jeremy
andrew
irvin
jamie
larson
jure
leskovec
jonathan
levy
andrea
lodi
oisin
mac
aodha
loic
matthey
julian
mcauley
brendan
mcmahan
sergio
moro
luca
oneto
orestis
papakyriakopoulos
stephen
robert
pfohl
christopher
potts
mike
redmond
kit
rodolfa
ben
roshan
veronica
rotemberg
rachel
rudinger
sivan
sabato
kate
saenko
mark
shermis
daniel
slunge
david
solans
luca
soldaini
efstathios
stamatatos
ryan
steed
rachael
tatman
schrasing
tong
alan
tsang
sathishkumar
andreas
van
cranenburgh
lucy
vasserman
roland
vollgraf
alex
wang
zeerak
waseem
kellie
webster
bryan
wilder
nick
wilson
i-cheng
yeh
elad
yom-tov
neil
yorke-smith
michal
zabovsky
yukun
zhu
123
algorithmic
fairness
datasets
the
story
so
far
2119
funding
open
access
funding
provided
by
università
degli
studi
di
padova
within
the
crui-care
agree
ment
open
access
this
article
is
licensed
under
creative
commons
attribution
4.0
international
license
which
permits
use
sharing
adaptation
distribution
and
reproduction
in
any
medium
or
format
as
long
as
you
give
appropriate
credit
to
the
original
author
and
the
source
provide
link
to
the
creative
commons
licence
and
indicate
if
changes
were
made
the
images
or
other
third
party
material
in
this
article
are
included
in
the
article
creative
commons
licence
unless
indicated
otherwise
in
credit
line
to
the
material
if
material
is
not
included
in
the
article
creative
commons
licence
and
your
intended
use
is
not
permitted
by
statutory
regulation
or
exceeds
the
permitted
use
you
will
need
to
obtain
permission
directly
from
the
copyright
holder
to
view
copy
of
this
licence
visit
http://creativecommons.org/licenses/by/4.0/.
references
abbasi
bhaskara
venkatasubramanian
2021
fair
clustering
via
equitable
group
representations
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
504
514
https://doi.org/10.1145/3442188.3445913
adragna
creager
madras
zemel
2020
fairness
and
robustness
in
invariant
learning
case
study
in
toxicity
classification
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2011.06485
agarwal
beygelzimer
dudik
langford
wallach
2018a
reductions
approach
to
fair
classification
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
60
69
http://proceedings.mlr.press/v80/agarwal18a.html
agrawal
zitnik
leskovec
et
al
2018b
large-scale
analysis
of
disease
pathways
in
the
human
interactome
in
psb
world
scientific
pp
111
122
agarwal
dudik
wu
zs
2019
fair
regression
quantitative
definitions
and
reduction-based
algorithms
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
california
usa
proceedings
of
machine
learning
research
vol
97
pp
120
129
http://proceedings.mlr.press/v97/agarwal19d.html
ahmadian
epasto
knittel
kumar
mahdian
moseley
pham
vassilvitskii
wang
2020
fair
hierarchical
clustering
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.
cc
paper
2020
hash
f10f2da9a238b746d2bac55759915f0d-abstract
html
aka
burke
bauerle
greer
mitchell
2021
measuring
model
biases
in
the
absence
of
ground
truth
association
for
computing
machinery
new
york
pp
327
335
https://doi.org/10.1145/
3461702.3462557
albanese
calbimonte
jp
schumacher
calvaresi
2020
dynamic
consent
management
for
clinical
trials
via
private
blockchain
technology
amb
intell
human
comput
18
ali
babaei
chakraborty
mirzasoleiman
gummadi
kp
singla
2019a
on
the
fairness
of
time-critical
influence
maximization
in
social
networks
neurips
2019
workshop
human-centric
machine
learning
arxiv
1905.06618
ali
zafar
mb
singla
gummadi
kp
2019b
loss-aversively
fair
classification
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
211
218
https://doi.org/10.1145/3306618.3314266,
ali
lahoti
gummadi
kp
2021
accounting
for
model
uncertainty
in
algorithmic
discrimination
association
for
computing
machinery
new
york
pp
336
345
https://doi.org/10.1145/3461702.
3462630
amini
soleimany
ap
schwarting
bhatia
sn
rus
2019
uncovering
and
mitigating
algorithmic
bias
through
learned
latent
structure
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
289
295
https
doi
org
10.1145
3306618.3314243
anderson
1936
the
species
problem
in
iris
ann
mo
bot
gard
23
457
509
123
2120
fabris
et
al
andrus
spitzer
brown
xiang
2021
what
we
can
measure
we
can
understand
challenges
to
demographic
data
procurement
in
the
pursuit
of
fairness
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
249
260
https://doi.org/10.1145/3442188.3445888
andrzejak
rg
lehnertz
mormann
rieke
david
elger
ce
2001
indications
of
nonlinear
deterministic
and
finite-dimensional
structures
in
time
series
of
brain
electrical
activity
dependence
on
recording
region
and
brain
state
phys
rev
64
061907
angwin
larson
mattu
kirchner
2016
machine
bias
https://www.propublica.org/article/machine-
bias-risk-assessments-in-criminal-sentencing
arjovsky
bottou
gulrajani
lopez-paz
2020
invariant
risk
minimization
arxiv
1907.02893
atwood
srinivasan
halpern
sculley
2019
fair
treatment
allocations
in
social
networks
neurips
2019
workshop
fair
ml
for
health
arxiv
1911.05489
awasthi
beutel
kleindessner
morgenstern
wang
2021
evaluating
fairness
of
machine
learning
models
under
uncertain
and
incomplete
information
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
206
214
https://doi.org/10.1145/3442188.3445884
babaeianjelodar
lorenz
gordon
matthews
freitag
2020
quantifying
gender
bias
in
dif
ferent
corpora
in
companion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
752
759
https://doi.org/10.1145/3366424.3383559
babaioff
nisan
talgam-cohen
2019
fair
allocation
through
competitive
equilibrium
from
generic
incomes
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
180
https://doi.org/10.1145/3287560.3287582
backurs
indyk
onak
schieber
vakilian
wagner
2019
scalable
fair
clustering
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
california
proceedings
of
machine
learning
research
vol
97
pp
405
413
http
proceedings
mlr
press
v97
backurs19a
html
bagdasaryan
poursaeed
shmatikov
2019
differential
privacy
has
disparate
impact
on
model
accu
racy
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
32
https://proceedings.neurips.
cc
paper
2019
file
fc0de4e0396fff257ea362983c2dda5a-paper
pdf
baharlouei
nouiehed
beirami
razaviyayn
2020
rényi
fair
inference
in
international
con
ference
on
learning
representations
https://openreview.net/forum?id=hkgsujrtdb
bakker
ma
tu
dp
valdés
hr
gummadi
kp
varshney
kr
weller
pentland
2019
dadi
dynamic
discovery
of
fair
information
with
adversarial
reinforcement
learning
neurips
2019
workshop
human-centric
machine
learning
arxiv
1910.13983
bakker
ma
tu
dp
gummadi
kp
pentland
as
varshney
kr
weller
2021
beyond
reasonable
doubt
improving
fairness
in
budget-constrained
decision
making
using
confidence
thresholds
association
for
computing
machinery
new
york
pp
346
356
https://doi.org/10.1145/3461702.3462575
ball-burack
lee
msa
cobbe
singh
2021
differential
tweetment
mitigating
racial
dialect
bias
in
harmful
tweet
detection
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
116
128
https
doi
org
10.1145
3442188.3445875
bandy
vincent
2021
addressing
documentation
debt
in
machine
learning
research
retrospective
datasheet
for
bookcorpus
arxiv
2105.05241
bao
zhou
zottola
brubach
desmarais
horowitz
lum
venkatasubramanian
2021
it
compaslicated
the
messy
relationship
between
rai
datasets
and
algorithmic
fairness
benchmarks
arxiv
2106.05498
barabas
dinakar
doyle
2019
the
problems
with
risk
assessment
tools
https://www.nytimes.com/
2019
07
17
opinion
pretrial-ai
html
barbaro
2007
in
apparel
all
tariffs
aren
created
equal
https://www.nytimes.com/2007/04/28/
business
28gender
html
barenstein
2019
propublica
compas
data
revisited
arxiv
1906.04711
barman-adhikari
begun
rice
yoshioka-maxwell
perez-portillo
2016
sociometric
network
structure
and
its
association
with
methamphetamine
use
norms
among
homeless
youth
soc
sci
res
58
292
308
barocas
hardt
narayanan
2019
fairness
and
machine
learning
fairmlbook
org
http://www.
fairmlbook
org
123
algorithmic
fairness
datasets
the
story
so
far
2121
baudry
jp
cardoso
celeux
amorim
mj
ferreira
as
2015
enhancing
the
selection
of
model-based
clustering
with
external
categorical
variables
adv
data
anal
classif
177
196
behaghel
crépon
gurgand
2014
private
and
public
provision
of
counseling
to
job
seekers
evidence
from
large
controlled
experiment
am
econ
appl
econ
142
74
https://doi.org/10.
1257
app
6.4
142
belitz
jiang
bosch
2021
automating
procedurally
fair
feature
selection
in
machine
learning
association
for
computing
machinery
new
york
pp
379
389
https://doi.org/10.1145/3461702.
3462585
bender
em
friedman
2018
data
statements
for
natural
language
processing
toward
mitigating
system
bias
and
enabling
better
science
trans
assoc
comput
linguist
587
604
https://doi.org/10.1162/
tacl_a_00041
https://www.aclweb.org/anthology/q18-1041
bender
em
gebru
mcmillan-major
shmitchell
2021
on
the
dangers
of
stochastic
parrots
can
language
models
be
too
big
association
for
computing
machinery
new
york
facct
21
pp
610
623
https://doi.org/10.1145/3442188.3445922,
benenson
popov
ferrari
2019
large-scale
interactive
object
segmentation
with
human
annotators
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pp
11700
11709
bera
chakrabarty
flores
negahbani
2019
fair
algorithms
for
clustering
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
32
pp
4954
4965
https://proceedings.neurips.cc/
paper
2019
file
fc192b0c0d270dbf41870a63a8c76c2f-paper
pdf
beretta
vetrò
lepri
martin
jcd
2021
detecting
discriminatory
risk
through
data
annotation
based
on
bayesian
inferences
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
794
804
https://doi.
org
10.1145
3442188.3445940
berk
heidari
jabbari
joseph
kearns
morgenstern
neel
roth
2017
convex
framework
for
fair
regression
in
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1706.02409
bertin-mahieux
ellis
dpw
whitman
lamere
2011
the
million
song
dataset
in
proceedings
of
the
12th
international
society
for
music
information
retrieval
conference
ismir
miami
pp
591
596
https://doi.org/10.5281/zenodo.1415820
bertrand
mullainathan
2004
are
emily
and
greg
more
employable
than
lakisha
and
jamal
field
experiment
on
labor
market
discrimination
am
econ
rev
94
991
1013
beutel
chen
zhao
chi
eh
2017
data
decisions
and
theoretical
implications
when
adversarially
learning
fair
representations
in
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1707.00075
biega
aj
diaz
ekstrand
md
kohlmeier
2019
overview
of
the
trec
2019
fair
ranking
track
in
the
twenty-eighth
text
retrieval
conference
trec
2019
proceedings
biswas
mukherjee
2021
ensuring
fairness
under
prior
probability
shifts
association
for
computing
machinery
new
york
pp
414
424
https://doi.org/10.1145/3461702.3462596
black
fredrikson
2021
leave-one-out
unfairness
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
285
295
https://doi.org/10.1145/3442188.3445894
black
yeom
fredrikson
2020
fliptest
fairness
testing
via
optimal
transport
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
111
121
https://doi.org/10.1145/3351095.3372845
blodget
sl
connor
2017
racial
disparity
in
natural
language
processing
case
study
of
social
media
african-american
english
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1707.00061
blodgett
sl
green
connor
2016
demographic
dialectal
variation
in
social
media
case
study
of
african-american
english
in
proceedings
of
the
2016
conference
on
empirical
methods
in
natural
language
processing
association
for
computational
linguistics
austin
pp
1119
1130
https://doi.
org
10.18653
v1
d16-1120
https://www.aclweb.org/anthology/d16-1120
bolukbasi
chang
kw
zou
jy
saligrama
kalai
at
2016
man
is
to
computer
program
mer
as
woman
is
to
homemaker
debiasing
word
embeddings
in
lee
sugiyama
luxburg
guyon
garnett
eds
advances
in
neural
information
processing
systems
123
2122
fabris
et
al
curran
associates
inc
vol
29
pp
4349
4357
https://proceedings.neurips.cc/paper/2016/file/
a486cd07e4ac3d270571622f4f316ec5-paper
pdf
bordes
usunier
garcia-duran
weston
yakhnenko
2013
translating
embeddings
for
mod
eling
multi-relational
data
in
burges
cjc
bottou
welling
ghahramani
weinberger
kq
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
26
https
proceedings
neurips
cc
paper
2013
file
1cecc7a77928ca8133fa24680a88d2f9-paper
pdf
bordia
bowman
sr
2019
identifying
and
reducing
gender
bias
in
word-level
language
models
in
proceedings
of
the
2019
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
student
research
workshop
association
for
computational
linguistics
minneapolis
pp
15
https://doi.org/10.18653/v1/n19-3002,
https://aclanthology.org/n19-3002
borkan
dixon
sorensen
thain
vasserman
2019
nuanced
metrics
for
measuring
unintended
bias
with
real
data
for
text
classification
in
companion
proceedings
of
the
2019
world
wide
web
conference
association
for
computing
machinery
new
york
www
19
pp
491
500
https://doi.
org
10.1145
3308560.3317593
bose
hamilton
2019
compositional
fairness
constraints
for
graph
embeddings
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
715
724
http://proceedings.mlr.
press
v97
bose19a
html
bower
niss
sun
vargo
2018
debiasing
representations
by
removing
unwanted
variation
due
to
protected
attributes
in
icml
2018
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1807.00461
bower
eftekhari
yurochkin
sun
2021
individually
fair
rankings
in
international
conference
on
learning
representations
https://openreview.net/forum?id=71zcsp_hubn
brennan
dieterich
ehret
2009
evaluating
the
predictive
validity
of
the
compas
risk
and
needs
assessment
system
crim
justice
behav
36
21
40
https://doi.org/10.1177/0093854808326545
brockman
cheung
pettersson
schneider
schulman
tang
zaremba
2016
openai
gym
arxiv
1606.01540
brooks-gunn
fr
klebanov
pk
1992
effects
of
early
intervention
on
cognitive
function
of
low
birth
weight
preterm
infants
pediatr
120
350
359
brožovský
2006
recommender
system
for
dating
service
master
thesis
charles
university
in
prague
prague
http://colfi.wz.cz/colfi.pdf
brozovsky
petricek
2007
recommender
system
for
online
dating
service
arxiv
0703042
cs
brubach
chakrabarti
dickerson
khuller
srinivasan
tsepenekas
2020
pairwise
fair
and
community-preserving
approach
to
k-center
clustering
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
1178
1189
http://proceedings.mlr.press/v119/brubach20a.html
brunet
me
alkalay-houlihan
anderson
zemel
2019
understanding
the
origins
of
bias
in
word
embeddings
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
803
811
http://proceedings.mlr.press/v97/brunet19a.html
buolamwini
gebru
2018
gender
shades
intersectional
accuracy
disparities
in
commercial
gender
classification
in
friedler
sa
wilson
eds
proceedings
of
the
1st
conference
on
fairness
account
ability
and
transparency
pmlr
new
york
proceedings
of
machine
learning
research
vol
81
pp
77
91
http://proceedings.mlr.press/v81/buolamwini18a.html
burke
kontny
sonboli
2018a
synthetic
attribute
data
for
evaluating
consumer-side
fairness
recsys
2018
workshop
workshop
on
responsible
recommendation
fat
rec
arxiv
1809.04199
burke
sonboli
ordonez-gauger
2018b
balanced
neighborhoods
for
multi-sided
fairness
in
recommendation
in
friedler
sa
wilson
eds
proceedings
of
the
1st
conference
on
fairness
accountability
and
transparency
pmlr
new
york
proceedings
of
machine
learning
research
vol
81
pp
202
214
http://proceedings.mlr.press/v81/burke18a.html
buyl
de
bie
2020
debayes
bayesian
method
for
debiasing
network
embeddings
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
1220
1229
http://proceedings.mlr.press/v119/
buyl20a
html
cai
gaebler
garg
goel
2020
fair
allocation
through
selective
information
acquisition
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
22
28
https://doi.org/10.1145/3375627.3375823,
123
algorithmic
fairness
datasets
the
story
so
far
2123
caldas
duddu
smk
wu
li
konec
ny
mcmahan
hb
smith
talwalkar
2018
leaf
benchmark
for
federated
settings
arxiv
1812.01097
calders
verwer
2010
three
naive
bayes
approaches
for
discrimination-free
classification
data
min
knowl
discov
21
277
292
https://doi.org/10.1007/s10618-010-0190-x
calders
kamiran
pechenizkiy
2009
building
classifiers
with
independency
constraints
in
2009
ieee
international
conference
on
data
mining
workshops
pp
13
18
https://doi.org/10.1109/icdmw.
2009.83
caliskan
bryson
narayanan
2017
semantics
derived
automatically
from
language
corpora
contain
human-like
biases
science
356
6334
183
186
https://doi.org/10.1126/science.aal4230
calmon
wei
vinzamuri
natesan
ramamurthy
varshney
kr
2017
optimized
pre
processing
for
discrimination
prevention
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
sys
tems
curran
associates
inc
vol
30
pp
3992
4001
https://proceedings.neurips.cc/paper/2017/file/
9a49a25d845a483fae4be7e341368e36-paper
pdf
canetti
cohen
dikkala
ramnarayan
scheffler
smith
2019
from
soft
classifiers
to
hard
decisions
how
fair
can
we
be
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
309
318
https://doi.
org
10.1145
3287560.3287561
caragiannis
kurokawa
moulin
procaccia
ad
shah
wang
2016
the
unreasonable
fairness
of
maximum
nash
welfare
in
proceedings
of
the
2016
acm
conference
on
economics
and
computation
association
for
computing
machinery
new
york
ec
16
pp
305
322
https://doi.org/10.1145/
2940716.2940726
cardoso
rl
meira
jr
almeida
zaki
mj
2019
framework
for
benchmarking
discrimination-aware
models
in
machine
learning
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
437
444
https://doi.org/10.
1145
3306618.3314262
carvalho
lodi
2019
game
theoretical
analysis
of
kidney
exchange
programs
arxiv
1911.09207
caton
haas
2020
fairness
in
machine
learning
survey
arxiv
2010.04053
celis
le
keswani
2020
implicit
diversity
in
image
summarization
proc
acm
hum
comput
interact
cscw2
28
https://doi.org/10.1145/3415210
celis
le
deshpande
kathuria
vishnoi
nk
2016
how
to
be
fair
and
diverse
dtl
2016
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1610.07183
celis
keswani
straszak
deshpande
kathuria
vishnoi
2018
fair
and
diverse
dpp-based
data
summarization
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
716
725
http://proceedings.mlr.press/v80/celis18a.html
celis
mehrotra
vishnoi
2019a
toward
controlling
discrimination
in
online
ad
auctions
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
4456
4465
http://proceedings.mlr.press/v97/mehrotra19a.html
celis
le
huang
keswani
vishnoi
nk
2019b
classification
with
fairness
constraints
meta
algorithm
with
provable
guarantees
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
319
328
https://doi.
org
10.1145
3287560.3287586
celis
le
keswani
vishnoi
2020a
data
preprocessing
to
mitigate
bias
maximum
entropy
based
approach
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
1349
1359
http
proceedings
mlr
press
v119
celis20a
html
celis
le
mehrotra
vishnoi
nk
2020b
interventions
for
ranking
in
the
presence
of
implicit
bias
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
369
380
https://doi.org/10.1145/3351095.3372858
celma
2010
music
recommendation
and
discovery
in
the
long
tail
springer
berlin
chaibub
neto
2020
causal
look
at
statistical
definitions
of
discrimination
in
proceedings
of
the
26th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
20
pp
873
881
https://doi.org/10.1145/3394486.3403130
chakraborty
patro
gk
ganguly
gummadi
kp
loiseau
2019
equality
of
voice
towards
fair
representation
in
crowdsourced
top-k
recommendations
in
proceedings
of
the
conference
on
fairness
123
2124
fabris
et
al
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
129
138
https://doi.org/10.1145/3287560.3287570
chapelle
chang
2010
yahoo
learning
to
rank
challenge
overview
in
proceedings
of
the
2010
international
conference
on
yahoo
learning
to
rank
challenge-volume
14
jmlr
org
ylrc
10
pp
24
chaudhari
ha
lin
linda
2020
general
framework
for
fairness
in
multistakeholder
recom
mendations
recsys
2020
workshop
3rd
facctrec
workshop
on
responsible
recommendation
arxiv
2009.02423
chelba
mikolov
schuster
ge
brants
koehn
robinson
2014
one
billion
word
benchmark
for
measuring
progress
in
statistical
language
modeling
in
interspeech-2014
chen
deng
shen
2018a
virtual
class
enhanced
discriminative
embedding
learning
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
31
https://proceedings.neurips.cc/paper/2018/file/
d79aac075930c83c2f1e369a511148fe-paper
pdf
chen
cw
lamere
schedl
zamani
2018b
recsys
challenge
2018
automatic
music
playlist
continuation
association
for
computing
machinery
new
york
recsys
18
pp
527
528
https://doi.
org
10.1145
3240323.3240342
chen
johansson
fd
sontag
2018c
why
is
my
classifier
discriminatory
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
31
https://proceedings.neurips.cc/paper/2018/file/
1f1baa5b8edac74eb4eaa329f14a0361-paper
pdf
chen
kallus
mao
svacha
udell
2019a
fairness
under
unawareness
assessing
disparity
when
protected
class
is
unobserved
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
339
348
https://doi.
org
10.1145
3287560.3287594
chen
fain
lyu
munagala
2019b
proportionally
fair
clustering
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
1032
1041
http://proceedings.mlr.press/v97/
chen19d
html
chen
mahoney
grasso
wali
matthews
middleton
njie
matthews
2021
gender
bias
and
under-representation
in
natural
language
processing
across
human
languages
association
for
computing
machinery
new
york
pp
24
34
https://doi.org/10.1145/3461702.3462530
cheng
hao
yuan
si
carin
2021a
fairfil
contrastive
neural
debiasing
method
for
pretrained
text
encoders
in
international
conference
on
learning
representations
https://openreview.net/forum?
id
n6jecd-pi5w
cheng
suriyakumar
vm
dullerud
joshi
ghassemi
2021b
can
you
fake
it
until
you
make
it
impacts
of
differentially
private
synthetic
data
on
downstream
classification
fairness
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
149
160
https://doi.org/10.1145/3442188.3445879
chierichetti
kumar
lattanzi
vassilvitskii
2017
fair
clustering
through
fairlets
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
30
pp
5029
5037
https://proceedings.
neurips
cc
paper
2017
file
978fce5bcc4eccc88ad48ce3914124a2-paper
pdf
chiplunkar
kale
ramamoorthy
sn
2020
how
to
solve
fair
k-center
in
massive
data
models
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
1877
1886
http://proceedings.mlr.
press
v119
chiplunkar20a
html
cho
hwang
suh
2020
fair
classifier
using
kernel
density
estimation
in
larochelle
ran
zato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
33
pp
15088
15099
https://proceedings.neurips.cc/paper/2020/file/
ac3870fcad1cfc367825cda0101eee62-paper
pdf
cho
wi
kim
yang
kim
ns
2021
towards
cross-lingual
generalization
of
translation
gender
bias
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
449
457
https://doi.org/10.1145/3442188.3445907
choi
grover
singh
shu
ermon
2020a
fair
generative
modeling
via
weak
supervision
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
123
algorithmic
fairness
datasets
the
story
so
far
2125
virtual
proceedings
of
machine
learning
research
vol
119
pp
1887
1898
http://proceedings.mlr.
press
v119
choi20a
html
choi
dang
den
broeck
gv
2020b
group
fairness
by
probabilistic
modeling
with
latent
fair
deci
sions
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2009.09031
chouldechova
2017
fair
prediction
with
disparate
impact
study
of
bias
in
recidivism
prediction
instruments
big
data
153
163
chouldechova
sell
2017
fairer
and
more
accurate
but
for
whom
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1707.00046
chouldechova
roth
2020
snapshot
of
the
frontiers
of
fairness
in
machine
learning
commun
acm
63
82
89
https://doi.org/10.1145/3376898
chuang
cy
mroueh
2021
fair
mixup
fairness
via
interpolation
in
international
conference
on
learning
representations
https://openreview.net/forum?id=dnl5s5bxebn
chzhen
denis
hebiri
oneto
pontil
2019
leveraging
labeled
and
unlabeled
data
for
consistent
fair
binary
classification
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
32
pp
12760
12770
https://proceedings.neurips.cc/paper/2019/file/ba51e6158bcaf80fd0d834950251e693-paper.
pdf
chzhen
denis
hebiri
oneto
pontil
2020a
fair
regression
via
plug-in
estimator
and
recal
ibration
with
statistical
guarantees
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
pro
cessing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/
paper
2020
hash
ddd808772c035aed16d42ad3559be5f-abstract
html
chzhen
denis
hebiri
oneto
pontil
2020b
fair
regression
with
wasserstein
barycenters
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
33
pp
7321
7331
https://proceedings.neurips.cc/
paper
2020
file
51cdbd2611e844ece5d80878eb770436-paper
pdf
cohany
sr
polivka
ae
rothgeb
jm
1994
revisions
in
the
current
population
survey
effective
january
1994
emp
earn
41
13
corbett-davies
pierson
feller
goel
huq
2017
algorithmic
decision
making
and
the
cost
of
fairness
in
proceedings
of
the
23rd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
17
pp
797
806
https
doi
org
10.1145
3097983.3098095
cortez
silva
amg
2008
using
data
mining
to
predict
secondary
school
student
performance
in
proceedings
of
5th
future
business
technology
conference
coston
ramamurthy
kn
wei
varshney
kr
speakman
mustahsan
chakraborty
2019
fair
transfer
learning
with
missing
protected
attributes
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
91
98
https://doi.org/10.1145/3306618.3314236
coston
mishler
kennedy
eh
chouldechova
2020
counterfactual
risk
assessments
evaluation
and
fairness
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
582
593
https://doi.org/10.1145/
3351095.3372851
coston
guha
ouyang
lu
chouldechova
ho
de
2021
leveraging
administrative
data
for
bias
audits
assessing
disparate
coverage
with
mobility
data
for
covid-19
policy
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
173
184
https://doi.org/10.1145/3442188.3445881
cotter
gupta
jiang
srebro
sridharan
wang
woodworth
you
2018
training
fairness
constrained
classifiers
to
generalize
icml
2018
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
cotter
gupta
jiang
srebro
sridharan
wang
woodworth
you
2019
training
well
generalizing
classifiers
for
fairness
metrics
and
other
data-dependent
constraints
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
1397
1405
http://proceedings.
mlr
press
v97
cotter19b
html
crawford
paglen
2021
excavating
ai
the
politics
of
images
in
machine
learning
training
sets
https
excavating
ai
123
2126
fabris
et
al
creager
madras
jacobsen
jh
weis
swersky
pitassi
zemel
2019
flexibly
fair
represen
tation
learning
by
disentanglement
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
1436
1445
http://proceedings.mlr.press/v97/creager19a.html
creager
madras
pitassi
zemel
2020
causal
modeling
for
fairness
in
dynamical
systems
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
2185
2195
http://proceedings.mlr.
press
v119
creager20a
html
creager
jacobsen
jh
zemel
2021
exchanging
lessons
between
algorithmic
fairness
and
domain
gen
eralization
https://openreview.net/forum?id=dc1im3mkgg,
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
amour
srinivasan
atwood
baljekar
sculley
halpern
2020
fairness
is
not
static
deeper
understanding
of
long
term
fairness
via
simulation
studies
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
525
534
https://doi.org/10.1145/3351095.3372878
dash
chakraborty
ghosh
mukherjee
gummadi
kp
2021
when
the
umpire
is
also
player
bias
in
private
label
product
recommendations
on
e-commerce
marketplaces
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
873
884
https://doi.org/10.1145/3442188.3445944
datta
posada
olson
li
reilly
balraj
mesterhazy
pallas
desai
shah
2020
new
paradigm
for
accelerating
clinical
data
science
at
stanford
medicine
arxiv
2003.10534
david
ke
liu
fong
2020
debiasing
convolutional
neural
networks
via
meta
orthogonalization
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2011.07453
davidson
ravi
ss
2020
framework
for
determining
the
fairness
of
outlier
detection
in
ecai
2020
ios
press
pp
2465
2472
davidson
warmsley
macy
mw
weber
2017
automated
hate
speech
detection
and
the
problem
of
offensive
language
in
proceedings
of
the
eleventh
international
conference
on
web
and
social
media
icwsm
2017
montréal
may
15
18
2017
aaai
press
pp
512
515
https://aaai.org/ocs/index.php/
icwsm
icwsm17
paper
view
15665
de-arteaga
romanov
wallach
chayes
borgs
chouldechova
geyik
kenthapadi
kalai
at
2019
bias
in
bios
case
study
of
semantic
representation
bias
in
high-stakes
setting
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
120
128
https://doi.org/10.1145/3287560.3287572
delobelle
temple
perrouin
frénay
heymans
berendt
2020
ethical
adversaries
towards
mitigating
unfairness
with
adversarial
machine
learning
ecmlpkdd
2020
workshop
bias
2020
bias
and
fairness
in
ai
arxiv
2005.06852
deng
dong
socher
li
kai
li
li
fei-fei
2009
imagenet
large-scale
hierarchical
image
database
in
2009
ieee
conference
on
computer
vision
and
pattern
recognition
pp
248
255
https
doi
org
10.1109
cvpr
2009.5206848
denton
hanna
amironesei
smart
nicole
scheuerman
mk
2020
bringing
the
people
back
in
contesting
benchmark
machine
learning
datasets
arxiv
2007.07399
deshpande
kv
pan
foulds
jr
2020
mitigating
demographic
bias
in
ai-based
resume
filtering
in
adjunct
publication
of
the
28th
acm
conference
on
user
modeling
adaptation
and
personalization
association
for
computing
machinery
new
york
umap
20
adjunct
pp
268
275
https://doi.org/
10.1145
3386392.3399569
detrano
janosi
steinbrunn
pfisterer
schmid
jj
sandhu
guppy
kh
lee
froelicher
1989
international
application
of
new
probability
algorithm
for
the
diagnosis
of
coronary
artery
disease
am
cardiol
64
304
310
devlin
chang
mw
lee
toutanova
2019
bert
pre-training
of
deep
bidirectional
transformers
for
language
understanding
in
proceedings
of
the
2019
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
long
and
short
papers
association
for
computational
linguistics
minneapolis
minnesota
pp
4171
4186
dhamala
sun
kumar
krishna
pruksachatkun
chang
kw
gupta
2021
bold
dataset
and
metrics
for
measuring
biases
in
open-ended
language
generation
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
862
872
https://doi.org/10.1145/3442188.3445924
123
algorithmic
fairness
datasets
the
story
so
far
2127
diana
gill
kearns
kenthapadi
roth
2021
minimax
group
fairness
algorithms
and
experi
ments
association
for
computing
machinery
new
york
pp
66
76
https://doi.org/10.1145/3461702.
3462523
diciccio
vasudevan
basu
kenthapadi
agarwal
2020
evaluating
fairness
using
permuta
tion
tests
association
for
computing
machinery
new
york
pp
1467
1477
https://doi.org/10.1145/
3394486.3403199
dickens
singh
getoor
2020
hyperfair
soft
approach
to
integrating
fairness
criteria
recsys
2020
workshop
3rd
facctrec
workshop
on
responsible
recommendation
arxiv
2009.08952
dieterich
mendoza
brennan
2016
compas
risk
scales
demonstrating
accuracy
equity
and
pre
dictive
parity
ding
hardt
miller
schmidt
2021
retiring
adult
new
datasets
for
fair
machine
learning
in
advances
in
neural
information
processing
systems
34
dixon
li
sorensen
thain
vasserman
2018
measuring
and
mitigating
unintended
bias
in
text
classification
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
asso
ciation
for
computing
machinery
new
york
aies
18
pp
67
73
https://doi.org/10.1145/3278721.
3278729
donini
oneto
ben-david
shawe-taylor
js
pontil
2018
empirical
risk
minimization
under
fairness
constraints
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
31
pp
2791
2801
https://proceedings.neurips.cc/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-paper.pdf
dressel
farid
2018
the
accuracy
fairness
and
limits
of
predicting
recidivism
sci
adv
eaao5580
duarte
mf
hu
yh
2004
vehicle
classification
in
distributed
sensor
networks
parallel
distrib
comput
64
826
838
https://doi.org/10.1016/j.jpdc.2004.03.020
dwork
hardt
pitassi
reingold
zemel
2012
fairness
through
awareness
in
proceedings
of
the
3rd
innovations
in
theoretical
computer
science
conference
association
for
computing
machinery
new
york
itcs
12
pp
214
226
https://doi.org/10.1145/2090236.2090255
dwork
immorlica
kalai
at
leiserson
2017
decoupled
classifiers
for
fair
and
efficient
machine
learning
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1707.06613
dwork
immorlica
kalai
at
leiserson
2018
decoupled
classifiers
for
group-fair
and
efficient
machine
learning
in
friedler
sa
wilson
eds
proceedings
of
the
1st
conference
on
fairness
accountability
and
transparency
pmlr
new
york
ny
usa
proceedings
of
machine
learning
research
vol
81
pp
119
133
http://proceedings.mlr.press/v81/dwork18a.html
ebner
nc
riediger
lindenberger
2010
faces-a
database
of
facial
expressions
in
young
middle
aged
and
older
women
and
men
development
and
validation
behav
res
methods
42
351
362
eidinger
enbar
hassner
2014
age
and
gender
estimation
of
unfiltered
faces
ieee
trans
inf
forensics
secur
12
2170
2179
https://doi.org/10.1109/tifs.2014.2359646
ekstrand
md
tian
azpiazu
im
ekstrand
jd
anuyah
mcneill
pera
ms
2018
all
the
cool
kids
how
do
they
fit
in
popularity
and
demographic
biases
in
recommender
evaluation
and
effectiveness
in
friedler
sa
wilson
eds
proceedings
of
the
1st
conference
on
fairness
accountability
and
transparency
pmlr
new
york
proceedings
of
machine
learning
research
vol
81
pp
172
186
http://proceedings.mlr.press/v81/ekstrand18b.html
el
emam
arbuckle
koru
eze
gaudette
neri
rose
howard
gluck
2012
de
identification
methods
for
open
health
data
the
case
of
the
heritage
health
prize
claims
dataset
med
internet
res
14
e33
el
halabi
mitrovic
norouzi-fard
tardos
tarnawski
jm
2020
fairness
in
streaming
submodular
maximization
algorithms
and
hardness
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
33
pp
13609
13622
https://proceedings.neurips.cc/paper/2020/file/9d752cb08ef466fc480fba981cfa44a1-
paper
pdf
elzayn
jabbari
jung
kearns
neel
roth
schutzman
2019
fair
algorithms
for
learning
in
allocation
problems
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
170
179
https://doi.org/10.1145/
3287560.3287571
epstein
landes
posner
2013
the
behavior
of
federal
judges
theoretical
and
empirical
study
of
rational
choice
harvard
university
press
https://books.google.it/books?id=rcqebeic3ecc
123
2128
fabris
et
al
equivant
2019
practitioner
guide
to
compas
core
https://www.equivant.com/wp-content/uploads/
practitioners-guide-to-compas-core-040419
pdf
esmaeili
brubach
tsepenekas
dickerson
2020
probabilistic
fair
clustering
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
sys
tems
curran
associates
inc
vol
33
pp
12743
12755
https://proceedings.neurips.cc/paper/2020/
file
95f2b84de5660ddf45c8a34933a2e66f-paper
pdf
european
union
2016
regulation
eu
2016
679
of
the
european
parliament
and
of
the
council
of
27
april
2016
on
the
protection
of
natural
persons
with
regard
to
the
processing
of
personal
data
and
on
the
free
movement
of
such
data
and
repealing
directive
95
46
ec
general
data
protection
regulation
https
eur-lex
europa
eu
eli
reg
2016
679
2016
05
04
fabbrizzi
papadopoulos
ntoutsi
kompatsiaris
2021
survey
on
bias
in
visual
datasets
arxiv
2107.07919
fabris
mishler
gottardi
carletti
daicampi
susto
ga
silvello
2021
algorithmic
audit
of
italian
car
insurance
evidence
of
unfairness
in
access
and
pricing
association
for
computing
machinery
new
york
pp
458
468
https://doi.org/10.1145/3461702.3462569
fabris
messina
silvello
susto
ga
2022
tackling
documentation
debt
survey
on
algorithmic
fairness
datasets
in
equity
and
access
in
algorithms
mechanisms
and
optimization
association
for
computing
machinery
new
york
ny
https://doi.org/10.1145/3551624.3555286
farnad
babaki
gendreau
2020
unifying
framework
for
fairness-aware
influence
maximization
in
companion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
714
722
https://doi.org/10.1145/3366424.3383555
farnadi
kouki
thompson
sk
srinivasan
getoor
2018
fairness-aware
hybrid
recom
mender
system
recsys
2018
workshop
workshop
on
responsible
recommendation
fat
rec
arxiv
1809.09030
farnadi
babaki
carvalho
2019
enhancing
fairness
in
kidney
exchange
program
by
ranking
solutions
neurips
2019
workshop
fair
ml
for
health
arxiv
1911.05489
fehrman
muhammad
ak
mirkes
em
egan
gorban
an
2017
the
five
factor
model
of
personality
and
evaluation
of
drug
consumption
risk
in
palumbo
montanari
vichi
eds
data
science
springer
cham
pp
231
242
fehrman
egan
gorban
an
levesley
mirkes
em
muhammad
ak
2019
personality
traits
and
drug
consumption
story
told
by
data
springer
berlin
feldman
friedler
sa
moeller
scheidegger
venkatasubramanian
2015
certifying
and
removing
disparate
impact
in
proceedings
of
the
21th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
15
pp
259
268
https://doi.org/10.1145/2783258.2783311
ferraro
bogdanov
serra
yoon
2019
artist
and
style
exposure
bias
in
collaborative
filtering
based
music
recommendations
ismir
2019
workshop
workshop
on
designing
human-centric
mir
systems
arxiv
1911.04827
fish
kun
lelkes
2015
fair
boosting
case
study
in
icml
2015
workshop
fairness
account
ability
and
transparency
in
machine
learning
fat
ml
fisher
ra
1936
the
use
of
multiple
measurements
in
taxonomic
problems
ann
eugen
179
188
fisher
palfrey
christodoulopoulos
mittal
2020
measuring
social
bias
in
knowledge
graph
embed
dings
akbc
2020
workshop
bias
in
automatic
knowledge
graph
construction
arxiv
1912.02761
fisman
iyengar
kamenica
simonson
2006
gender
differences
in
mate
selection
evidence
from
speed
dating
experiment
econ
121
673
697
https://doi.org/10.1162/qjec.2006.121.2.673
fitzpatrick
tb
1988
the
validity
and
practicality
of
sun-reactive
skin
types
through
vi
arch
dermatol
124
869
871
flanigan
gölz
gupta
procaccia
ad
2020
neutralizing
self-selection
bias
in
sampling
for
sortition
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/2020/hash/
48237d9f2dea8c74c2a72126cf63d933-abstract
html
florez
ou
2019
on
the
unintended
social
bias
of
training
language
generation
models
with
data
from
local
media
neurips
2019
workshop
human-centric
machine
learning
arxiv
1911.00461
fogliato
xiang
lipton
nagin
chouldechova
2021
on
the
validity
of
arrest
as
proxy
for
offense
race
and
the
likelihood
of
arrest
for
violent
crimes
in
proceedings
of
the
4th
aaai
acm
123
algorithmic
fairness
datasets
the
story
so
far
2129
conference
on
ai
ethics
and
society
aies
2021
virtual
event
pp
100
111
https://doi.org/10.
1145
3461702.3462538
founta
djouvas
chatzakou
leontiadis
blackburn
stringhini
vakali
sirivianos
kourtellis
2018
large
scale
crowdsourcing
and
characterization
of
twitter
abusive
behavior
in
proceedings
of
the
twelfth
international
conference
on
web
and
social
media
icwsm
2018
stan
ford
june
25
28
2018
aaai
press
pp
491
500
https://aaai.org/ocs/index.php/icwsm/icwsm18/
paper
view
17909
framingham
heart
study
2021
framingham
heart
study
offspring
exam
10
omni
exam
research
consent
form
https://framinghamheartstudy.org/files/2021/01/fhs-offspring-exam-10-
omni-1-exam-5-informed-consent-english-language-v21
pdf
friedler
sa
scheidegger
venkatasubramanian
choudhary
hamilton
ep
roth
2019
compar
ative
study
of
fairness-enhancing
interventions
in
machine
learning
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
329
338
https://doi.org/10.1145/3287560.3287589
friedler
sa
scheidegger
venkatasubramanian
2021
the
im
possibility
of
fairness
different
value
systems
require
different
mechanisms
for
fair
decision
making
commun
acm
64
136
143
https
doi
org
10.1145
3433949
galhotra
saisubramanian
zilberstein
2021
learning
to
generate
fair
clusters
from
demonstrations
association
for
computing
machinery
new
york
pp
491
501
https://doi.org/10.1145/3461702.
3462558
garbin
rajpurkar
irvin
lungren
mp
marques
2021
structured
dataset
documentation
datasheet
for
chexpert
arxiv
2105.03020
garg
perot
limtiaco
taly
chi
eh
beutel
2019
counterfactual
fairness
in
text
classification
through
robustness
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
219
226
https://doi.org/10.1145/
3306618.3317950
gastwirth
jl
miao
2009
formal
statistical
analysis
of
the
data
in
disparate
impact
cases
provides
sounder
inferences
than
the
us
government
four-fifths
rule
an
examination
of
the
statistical
evidence
in
ricci
destefano
law
probab
risk
171
191
ge
caverlee
lu
2016
taper
contextual
tensor-based
approach
for
personalized
expert
rec
ommendation
in
proceedings
of
the
10th
acm
conference
on
recommender
systems
association
for
computing
machinery
new
york
recsys
16
pp
261
268
https://doi.org/10.1145/2959100.
2959151
gebru
morgenstern
vecchione
vaughan
jw
wallach
daumé
iii
crawford
2018
datasheets
for
datasets
arxiv
1803.09010
geiger
rs
yu
yang
dai
qiu
tang
huang
2020
garbage
in
garbage
out
do
machine
learning
application
papers
in
social
computing
report
where
human-labeled
training
data
comes
from
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
325
336
https://doi.org/10.1145/3351095.3372862
gelman
fagan
kiss
2007
an
analysis
of
the
new
york
city
police
department
stop-and-frisk
policy
in
the
context
of
claims
of
racial
bias
am
stat
assoc
102
479
813
823
gerritse
ej
de
vries
ap
2020
effect
of
debiasing
on
information
retrieval
in
boratto
faralli
marras
stilo
eds
bias
and
social
aspects
in
search
and
recommendation
springer
cham
pp
35
42
ghadiri
samadi
vempala
2021
socially
fair
k-means
clustering
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
438
448
https://doi.org/10.1145/3442188.3445906
ginart
guan
valiant
zou
jy
2019
making
ai
forget
you
data
deletion
in
machine
learning
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
https://proceedings.neurips.cc/paper/
2019
file
cb79f8fa58b91d3af6c9c991f63962d3-paper
pdf
go
bhayani
huang
2009
twitter
sentiment
classification
using
distant
supervision
processing
150
goel
faltings
2019
crowdsourcing
with
fairness
diversity
and
budget
constraints
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
297
304
https://doi.org/10.1145/3306618.3314282
goel
rao
jm
shroff
et
al
2016
precinct
or
prejudice
understanding
racial
disparities
in
new
york
city
stop-and-frisk
policy
ann
appl
stat
10
365
394
123
2130
fabris
et
al
goel
perelman
shroff
sklansky
2017
combatting
police
discrimination
in
the
age
of
big
data
new
crim
law
rev
20
181
232
https://doi.org/10.1525/nclr.2017.20.2.181
goel
yaghini
faltings
2018
non-discriminatory
machine
learning
through
convex
fairness
cri
teria
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
18
pp
116
https://doi.org/10.1145/3278721.3278722
goel
amayuelas
deshpande
sharma
2020
the
importance
of
modeling
data
missingness
in
algorithmic
fairness
causal
perspective
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2012.11448
goelz
kahng
procaccia
ad
2019
paradoxes
in
fair
machine
learning
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
32
pp
8342
8352
https://proceedings.neurips.cc/paper/2019/
file
bbc92a647199b832ec90d7cf57074e9e-paper
pdf
golbeck
ashktorab
banjo
ro
berlinger
bhagwan
buntain
cheakalos
geller
aa
gergory
gnanasekaran
rk
gunasekaran
rr
hoffman
km
hottle
jienjitlert
khare
lau
martindale
mj
naik
nixon
hl
ramachandran
rogers
km
rogers
sarin
ms
shahane
thanki
vengataraman
wan
wu
dm
2017
large
labeled
corpus
for
online
harassment
research
in
proceedings
of
the
2017
acm
on
web
science
conference
association
for
computing
machinery
new
york
websci
17
pp
229
233
https://doi.org/10.1145/3091478.3091509
goldstein
1991
multilevel
modelling
of
survey
data
stat
soc
ser
statist
40
235
244
http
www.jstor.org/stable/2348496
gong
liu
jain
ak
2021
mitigating
face
recognition
bias
via
group
adaptive
classifier
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pp
3414
3424
gordaliza
barrio
ed
fabrice
loubes
jm
2019
obtaining
fairness
using
optimal
transport
theory
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
california
proceedings
of
machine
learning
research
vol
97
pp
2357
2365
http://proceedings.mlr.press/v97/gordaliza19a.html
gordon
babaeianjelodar
matthews
2020
studying
political
bias
via
word
embeddings
in
com
panion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
760
764
https://doi.org/10.1145/3366424.3383560
goyal
khot
summers-stay
batra
parikh
2017
making
the
in
vqa
matter
elevating
the
role
of
image
understanding
in
visual
question
answering
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
pp
6904
6913
graffam
shinkfield
aj
hardcastle
2008
the
perceived
employability
of
ex-prisoners
and
offenders
int
offender
ther
comp
criminol
52
673
685
https://doi.org/10.1177/0306624x07307783
green
chen
2019
disparate
interactions
an
algorithm-in-the-loop
analysis
of
fairness
in
risk
assess
ments
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
ny
fat
19
pp
90
99
https://doi.org/10.1145/3287560.
3287563
greenwald
ag
mcghee
de
schwartz
jl
1998
measuring
individual
differences
in
implicit
cognition
the
implicit
association
test
pers
soc
psychol
74
1464
grgic-hlaca
zafar
gummadi
weller
2016
the
case
for
process
fairness
in
learning
feature
selection
for
fair
decision
making
neurips
2016
workshop
machine
learning
and
the
law
grömping
2019
south
german
credit
data
correcting
widely
used
data
set
report
tech
rep
beuth
university
of
applied
sciences
berlin
http://www1.beuth-hochschule.de/fb_ii/reports/
report-2019-004
pdf
gulla
ja
zhang
liu
özgöbek
su
2017
the
adressa
dataset
for
news
recommendation
in
pro
ceedings
of
the
international
conference
on
web
intelligence
association
for
computing
machinery
new
york
wi
17
pp
1042
1048
https://doi.org/10.1145/3106426.3109436
gungor
2018
benchmarking
authorship
attribution
techniques
using
over
thousand
books
by
fifty
victorian
era
novelists
master
thesis
purdue
university
guo
caliskan
2021
detecting
emergent
intersectional
biases
contextualized
word
embeddings
contain
distribution
of
human-like
biases
association
for
computing
machinery
new
york
pp
122
133
https://doi.org/10.1145/3461702.3462536
guo
zhang
yorke-smith
2016a
novel
evidence-based
bayesian
similarity
measure
for
recom
mender
systems
acm
trans
web
https://doi.org/10.1145/2856037
123
algorithmic
fairness
datasets
the
story
so
far
2131
guo
zhang
hu
he
gao
2016b
ms-celeb-1m
dataset
and
benchmark
for
large-scale
face
recognition
in
leibe
matas
sebe
welling
eds
computer
vision
eccv
2016
springer
cham
pp
87
102
guvenir
ha
acar
demiroz
cekin
1997
supervised
machine
learning
algorithm
for
arrhythmia
analysis
in
computers
in
cardiology
1997
ieee
pp
433
436
han
jain
ak
2014
age
gender
and
race
estimation
from
unconstrained
face
images
http
biometrics
cse
msu
edu
publications
face
hanjain_unconstrainedagegenderraceestimation_
msutechreport2014
pdf
hannák
wagner
garcia
mislove
strohmaier
wilson
2017
bias
in
online
freelance
marketplaces
evidence
from
taskrabbit
and
fiverr
in
proceedings
of
the
2017
acm
conference
on
computer
supported
cooperative
work
and
social
computing
association
for
computing
machinery
new
york
cscw
17
pp
1914
1933
https://doi.org/10.1145/2998181.2998327
har-peled
mahabadi
2019
near
neighbor
who
is
the
fairest
of
them
all
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
13176
13187
https://proceedings.neurips.cc/paper/2019/
file
742141ceda6b8f6786609d31c8ef129f-paper
pdf
harb
lam
hs
2020
kfc
scalable
approximation
algorithm
for
k-center
fair
clustering
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
sys
tems
vol
33
curran
associates
inc
pp
14509
14519
https://proceedings.neurips.cc/paper/2020/
file
a6d259bfbfa2062843ef543e21d7ec8e-paper
pdf
hardt
price
price
srebro
2016
equality
of
opportunity
in
supervised
learning
in
lee
sugiyama
luxburg
guyon
garnett
eds
advances
in
neural
information
processing
sys
tems
vol
29
curran
associates
inc
pp
3315
3323
https://proceedings.neurips.cc/paper/2016/file/
9d2682367c3935defcb1f9e247a97c0d-paper
pdf
harper
fm
konstan
ja
2015
the
movielens
datasets
history
and
context
acm
trans
interact
intell
syst
https://doi.org/10.1145/2827872
hashimoto
srivastava
namkoong
liang
2018
fairness
without
demographics
in
repeated
loss
minimization
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
1929
1938
http://proceedings.mlr.press/v80/hashimoto18a.html
he
mcauley
2016
ups
and
downs
proceedings
of
the
25th
international
conference
on
world
wide
web
https://doi.org/10.1145/2872427.2883037
he
kang
wc
mcauley
2017
translation-based
recommendation
in
proceedings
of
the
eleventh
acm
conference
on
recommender
systems
association
for
computing
machinery
new
york
recsys
17
pp
161
169
https://doi.org/10.1145/3109859.3109882
he
burghardt
guo
lerman
2020a
inherent
trade-offs
in
the
fair
allocation
of
treatments
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2010.16409
he
burghardt
lerman
2020b
geometric
solution
to
fair
representations
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
279
285
https://doi.org/10.1145/3375627.3375864
heidari
ferrari
gummadi
krause
2018
fairness
behind
veil
of
ignorance
wel
fare
analysis
for
automated
decision
making
in
bengio
wallach
larochelle
grau
man
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
vol
31
curran
associates
inc
pp
1265
1276
https://proceedings.neurips.cc/paper/2018/file/
be3159ad04564bfb90db9e32851ebf9c-paper
pdf
heidari
loi
gummadi
kp
krause
2019a
moral
framework
for
understanding
fair
ml
through
economic
models
of
equality
of
opportunity
in
proceedings
of
the
conference
on
fairness
account
ability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
181
190
https://doi.org/10.1145/3287560.3287584
heidari
nanda
gummadi
2019b
on
the
long-term
impact
of
algorithmic
decision
policies
effort
unfairness
and
feature
segregation
through
social
learning
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceed
ings
of
machine
learning
research
vol
97
pp
2692
2701
http://proceedings.mlr.press/v97/heidari19a.
html
123
2132
fabris
et
al
hendricks
la
burns
saenko
darrell
rohrbach
2018
women
also
snowboard
overcoming
bias
in
captioning
models
in
ferrari
hebert
sminchisescu
weiss
eds
computer
vision-eccv
2018
springer
cham
pp
793
811
higgins
matthey
pal
burgess
glorot
botvinick
mohamed
lerchner
2017
beta-vae
learning
basic
visual
concepts
with
constrained
variational
framework
in
iclr
holland
hosny
newman
joseph
chmielinski
2018
the
dataset
nutrition
label
framework
to
drive
higher
data
quality
standards
arxiv
1805.03677
hollywood
mckay
woods
agniel
2019
real
time
crime
centers
in
chicago
https://www.rand.
org
content
dam
rand
pubs
research_reports
rr3200
rr3242
rand_rr3242
pdf
holmes
md
smith
bw
freng
ab
muñoz
ea
2008
minority
threat
crime
control
and
police
resource
allocation
in
the
southwestern
united
states
crime
delinq
54
128
152
https://doi.org/10.1177/
0011128707309718
holstein
wortman
vaughan
daumé
iii
dudik
wallach
2019
improving
fairness
in
machine
learning
systems
what
do
industry
practitioners
need
in
proceedings
of
the
acm
conference
on
human
factors
in
computing
systems
chi
2019
glasgow
pp
16
houvardas
stamatatos
2006
n-gram
feature
selection
for
authorship
identification
in
international
conference
on
artificial
intelligence
methodology
systems
and
applications
springer
pp
77
86
hu
chen
2020
fair
classification
and
social
welfare
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
535
545
https://doi.org/10.1145/3351095.3372857
hu
wu
zhang
wu
2020
fair
multiple
decision
making
through
soft
interventions
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
infor
mation
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/2020/hash/
d0921d442ee91b89ad95059d13df618-abstract
html
huan
wu
zhang
wu
2020
fairness
through
equality
of
effort
in
companion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
743
751
https://doi.org/10.1145/3366424.3383558,
huang
vishnoi
2019
stable
and
fair
classification
in
chaudhuri
salakhutdinov
eds
proceed
ings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
2879
2890
http://proceedings.mlr.press/v97/huang19e.html
huang
gb
ramesh
berg
learned-miller
2007
labeled
faces
in
the
wild
database
for
studying
face
recognition
in
unconstrained
environments
huang
jiang
vishnoi
2019
coresets
for
clustering
with
fairness
constraints
in
advances
in
neural
information
processing
systems
pp
7589
7600
huang
wei
celis
2020
towards
just
fair
and
interpretable
methods
for
judicial
subset
selection
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
293
299
https://doi.org/10.1145/3375627.3375848
hull
1994
database
for
handwritten
text
recognition
research
ieee
trans
pattern
anal
mach
intell
16
550
554
https://doi.org/10.1109/34.291440
hussain
dahan
na
ba-alwib
fm
ribata
2018
educational
data
mining
and
analysis
of
students
academic
performance
using
weka
indones
electr
eng
comput
sci
447
459
hutchinson
prabhakaran
denton
webster
zhong
denuyl
2020
unintended
machine
learning
biases
as
social
barriers
for
persons
with
disabilities
in
sigaccess
access
comput
vol
125https
doi
org
10.1145
3386296.3386305
häußler
walter
1979
empirische
ergebnisse
zu
diskriminationsverfahren
bei
kreditscoringsystemen
https://link.springer.com/article/10.1007/bf01917956
international
warfarin
pharmacogenetics
consortium
2009
estimation
of
the
warfarin
dose
with
clinical
and
pharmacogenetic
data
engl
med
360
753
764
irvin
rajpurkar
ko
yu
ciurea-ilcus
chute
marklund
haghgoo
ball
shpanskaya
seekins
mong
halabi
sandberg
jones
larson
langlotz
patel
lungren
ng
2019
chexpert
large
chest
radiograph
dataset
with
uncertainty
labels
and
expert
compari
son
in
33rd
aaai
conference
on
artificial
intelligence
aaai
2019
31st
innovative
applications
of
artificial
intelligence
conference
iaai
2019
and
the
9th
aaai
symposium
on
educational
advances
in
artificial
intelligence
eaai
2019
aaai
press
33rd
aaai
conference
on
artificial
intelligence
aaai
2019
31st
innovative
applications
of
artificial
intelligence
conference
iaai
2019
and
the
9th
aaai
symposium
on
educational
advances
in
artificial
intelligence
eaai
2019
pp
590
597
pub
123
algorithmic
fairness
datasets
the
story
so
far
2133
lisher
copyright
2019
association
for
the
advancement
of
artificial
intelligence
www.aaai.org).
all
rights
reserved
33rd
aaai
conference
on
artificial
intelligence
aaai
2019
31st
annual
con
ference
on
innovative
applications
of
artificial
intelligence
iaai
2019
and
the
9th
aaai
symposium
on
educational
advances
in
artificial
intelligence
eaai
2019
conference
date
27
01
2019
through
01
02
2019
islam
pan
foulds
jr
2021
can
we
obtain
fairness
for
free
association
for
computing
machinery
new
york
pp
586
596
https://doi.org/10.1145/3461702.3462614
jabbari
ou
hc
lakkaraju
tambe
2020
an
empirical
study
of
the
trade-offs
between
interpretabil
ity
and
fairness
in
icml
2020
workshop
on
human
interpretability
in
machine
learning
preliminary
version
icml
2020
workshop
workshop
on
human
interpretability
in
machine
learning
whi
jagielski
kearns
mao
oprea
roth
malvajerdi
ss
ullman
2019
differentially
private
fair
learning
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
3000
3008
http://proceedings.mlr.press/v97/jagielski19a.html
ji
smyth
steyvers
2020
can
trust
my
fairness
metric
assessing
fairness
with
unlabeled
data
and
bayesian
inference
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/
2020
hash
d83de59e10227072a9c034ce10029c39-abstract
html
jiang
pardos
za
2021
towards
equity
and
algorithmic
fairness
in
student
grade
prediction
association
for
computing
machinery
new
york
pp
608
617
https://doi.org/10.1145/3461702.3462623
jo
es
gebru
2020
lessons
from
archives
strategies
for
collecting
sociocultural
data
in
machine
learning
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
associ
ation
for
computing
machinery
new
york
fat
20
pp
306
316
https://doi.org/10.1145/3351095.
3372829
johnson
ae
pollard
tj
shen
lehman
lh
feng
ghassemi
moody
szolovits
celi
la
mark
rg
2016
mimic-iii
freely
accessible
critical
care
database
sci
data
160035
johnson
ae
pollard
tj
greenbaum
nr
lungren
mp
deng
cy
peng
lu
mark
rg
berkowitz
sj
horng
2019
mimic-cxr-jpg
large
publicly
available
database
of
labeled
chest
radiographs
arxiv
1901.07042
jones
nguyen
nguyen
2020
fair
k-centers
via
maximum
matching
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
4940
4949
http://proceedings.mlr.press/v119/jones20a.html
jones
sagawa
koh
pw
kumar
liang
2021
selective
classification
can
magnify
disparities
across
groups
in
international
conference
on
learning
representations
https://openreview.net/forum?
id
n0m_4bkq05i
jung
lee
park
moon
2021
fair
feature
distillation
for
visual
recognition
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pp
12115
12124
kallus
zhou
2018
residual
unfairness
in
fair
machine
learning
from
prejudiced
data
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stock
holmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
2439
2448
http://proceedings.mlr.press/v80/kallus18a.html
kallus
zhou
2019a
assessing
disparate
impact
of
personalized
interventions
identifiability
and
bounds
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
3426
3437
https
proceedings
neurips
cc
paper
2019
file
d54e99a6c03704e95e6965532dec148b-paper
pdf
kallus
zhou
2019b
the
fairness
of
risk
scores
beyond
classification
bipartite
ranking
and
the
xauc
metric
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
3438
3448
https://proceedings.neurips.cc/paper/2019/file/73e0f7487b8e5297182c5a711d20bf26-paper.pdf
kallus
zhou
2021
fairness
welfare
and
equity
in
personalized
pricing
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
296
314
https://doi.org/10.1145/3442188.3445895
kallus
mao
zhou
2020
assessing
algorithmic
fairness
with
unobserved
protected
class
using
data
combination
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
110
https://doi.org/10.1145/3351095.
3373154
123
2134
fabris
et
al
kamishima
2003
nantonac
collaborative
filtering
recommendation
based
on
order
responses
in
proceedings
of
the
ninth
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
03
pp
583
588
https://doi.org/10.
1145
956750.956823
kang
he
maciejewski
tong
2020
inform
individual
fairness
on
graph
mining
association
for
computing
machinery
new
york
pp
379
389
https://doi.org/10.1145/3394486.3403080
kannel
wb
mcgee
dl
1979
diabetes
and
cardiovascular
disease
the
framingham
study
jama
241
19
2035
2038
karako
manggala
2018
using
image
fairness
representations
in
diversity-based
re-ranking
for
rec
ommendations
umap
2018
workshop
fairness
in
user
modeling
adaptation
and
personalization
fairumap
arxiv
1809.03577
karkkainen
joo
2021
fairface
face
attribute
dataset
for
balanced
race
gender
and
age
for
bias
measurement
and
mitigation
in
proceedings
of
the
ieee
cvf
winter
conference
on
applications
of
computer
vision
pp
1548
1558
karlan
ds
zinman
2008
credit
elasticities
in
less-developed
economies
implications
for
microfinance
am
econ
rev
98
1040
68
kasy
abebe
2021
fairness
equality
and
power
in
algorithmic
decision-making
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
576
586
https://doi.org/10.1145/3442188.3445919
kato
teshima
honda
2019
learning
from
positive
and
unlabeled
data
with
selection
bias
in
international
conference
on
learning
representations
https://openreview.net/forum?id=rjzlcicqkm
kearns
neel
roth
wu
zs
2018
preventing
fairness
gerrymandering
auditing
and
learning
for
subgroup
fairness
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
2564
2572
http://proceedings.mlr.press/v80/kearns18a.html
kearns
neel
roth
wu
zs
2019
an
empirical
study
of
rich
subgroup
fairness
for
machine
learn
ing
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
100
109
https://doi.org/10.1145/3287560.3287592
keswani
lease
kenthapadi
2021
towards
unbiased
and
accurate
deferral
to
multiple
experts
association
for
computing
machinery
new
york
pp
154
165
https://doi.org/10.1145/3461702.
3462516
keyes
stevens
wernimont
2019
the
government
is
using
the
most
vulnerable
people
to
test
facial
recognition
software
khan
fu
2021
one
label
one
billion
faces
usage
and
consistency
of
racial
categories
in
computer
vision
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
587
597
https://doi.org/10.1145/
3442188.3445920
kilbertus
gascon
kusner
veale
gummadi
weller
2018
blind
justice
fairness
with
encrypted
sensitive
attributes
in
dy
krause
eds
proceedings
of
the
35th
international
confer
ence
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
2630
2639
http://proceedings.mlr.press/v80/kilbertus18a.html
kim
mnih
2018
disentangling
by
factorising
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
proceedings
of
machine
learning
research
vol
80
pp
2649
2658
http://proceedings.mlr.press/v80/kim18b.html
kim
mp
ghorbani
zou
2019
multiaccuracy
black-box
post-processing
for
fairness
in
classifica
tion
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
247
254
https://doi.org/10.1145/3306618.3314287
kim
js
chen
talwalkar
2020
fact
diagnostic
for
group
fairness
trade-offs
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceed
ings
of
machine
learning
research
vol
119
pp
5264
5274
http://proceedings.mlr.press/v119/kim20a.
html
kim
bryant
srikanth
howard
2021
age
bias
in
emotion
detection
an
analysis
of
facial
emotion
recognition
performance
on
young
middle-aged
and
older
adults
association
for
computing
machinery
new
york
pp
638
644
https://doi.org/10.1145/3461702.3462609
kiritchenko
mohammad
2018
examining
gender
and
race
bias
in
two
hundred
sentiment
analysis
systems
in
proceedings
of
the
seventh
joint
conference
on
lexical
and
computational
semantics
123
algorithmic
fairness
datasets
the
story
so
far
2135
association
for
computational
linguistics
new
orleans
pp
43
53
https://doi.org/10.18653/v1/s18-
2005
https://aclanthology.org/s18-2005
kizhner
terras
rumyantsev
khokhlova
demeshkova
rudov
afanasieva
2020
digital
cultural
colonialism
measuring
bias
in
aggregated
digitized
content
held
in
google
arts
and
culture
digital
scholarship
in
the
humanities
36
607
640
https://doi.org/10.1093/llc/fqaa055,
https
academic
oup
com
dsh
article-pdf
36
607
40873280
fqaa055
pdf
klare
bf
klein
taborsky
blanton
cheney
allen
grother
mah
jain
ak
2015
pushing
the
frontiers
of
unconstrained
face
detection
and
recognition
iarpa
janus
benchmark
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
cvpr
kleindessner
awasthi
morgenstern
2019a
fair
k-center
clustering
for
data
summarization
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
california
usa
proceedings
of
machine
learning
research
vol
97
pp
3448
3457
http://proceedings.mlr.press/v97/kleindessner19a.html
kleindessner
samadi
awasthi
morgenstern
2019b
guarantees
for
spectral
clustering
with
fairness
constraints
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
3458
3467
http://proceedings.mlr.press/v97/kleindessner19b.html
knees
hübler
2019
towards
uncovering
dataset
biases
investigating
record
label
diversity
in
music
playlists
ismir
2019
workshop
workshop
on
designing
human-centric
mir
systems
kobren
saha
mccallum
2019
paper
matching
with
local
fairness
constraints
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
19
pp
1247
1257
https://doi.org/10.1145/3292500.
3330899
kocijan
camburu
om
lukasiewicz
2020
the
gap
on
gap
tackling
the
problem
of
differing
data
distributions
in
bias-measuring
datasets
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2011.01837
kohavi
1996
scaling
up
the
accuracy
of
naive-bayes
classifiers
decision-tree
hybrid
in
proceedings
of
the
second
international
conference
on
knowledge
discovery
and
data
mining
aaai
press
kdd
96
pp
202
207
komiyama
takeda
honda
shimao
2018
nonconvex
optimization
for
regression
with
fairness
constraints
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
2737
2746
http://proceedings.mlr.press/v80/komiyama18a.html
konstantakis
promponas
dretakis
papadakos
2020
bias
goggles
exploring
the
bias
of
web
domains
through
the
eyes
of
users
in
boratto
faralli
marras
stilo
eds
bias
and
social
aspects
in
search
and
recommendation
springer
cham
pp
66
71
koolen
2018
reading
beyond
the
female
the
relationship
between
perception
of
author
gender
and
literary
quality
phd
thesis
university
of
amsterdam
koolen
van
cranenburgh
2017
these
are
not
the
stereotypes
you
are
looking
for
bias
and
fairness
in
authorial
gender
attribution
in
proceedings
of
the
first
acl
workshop
on
ethics
in
natural
language
processing
association
for
computational
linguistics
valencia
pp
12
22
https://doi.org/10.18653/
v1
w17-1602
https://www.aclweb.org/anthology/w17-1602
krizhevsky
2009
learning
multiple
layers
of
features
from
tiny
images
kröger
jl
miceli
müller
2021
how
data
can
be
used
against
people
classification
of
personal
data
misuses
available
at
ssrn
3887097
kuhlman
rundensteiner
2020
rank
aggregation
algorithms
for
fair
consensus
proc
vldb
endow
13
12
2706
2719
https://doi.org/10.14778/3407790.3407855
kuhlman
gerych
rundensteiner
2021
measuring
group
advantage
comparative
study
of
fair
ranking
metrics
in
proceedings
of
the
2021
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
21
pp
674
682
https://doi.org/10.1145/
3461702.3462588
kulshrestha
eslami
messias
zafar
mb
ghosh
gummadi
kp
karahalios
2017
quantifying
search
bias
investigating
sources
of
bias
for
political
searches
in
social
media
in
proceedings
of
the
2017
acm
conference
on
computer
supported
cooperative
work
and
social
computing
association
for
computing
machinery
new
york
cscw
17
pp
417
432
https://doi.org/10.1145/2998181.
2998321
123
2136
fabris
et
al
kushmerick
1999
learning
to
remove
internet
advertisements
in
proceedings
of
the
third
annual
conference
on
autonomous
agents
association
for
computing
machinery
new
york
agents
99
pp
175
181
https://doi.org/10.1145/301136.301186,
kusner
mj
loftus
russell
silva
2017
counterfactual
fairness
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
pp
4066
4076
https://proceedings.neurips.cc/paper/2017/
file
a486cd07e4ac3d270571622f4f316ec5-paper
pdf
kuznetsova
rom
alldrin
uijlings
krasin
pont-tuset
kamali
popov
malloci
kolesnikov
et
al
2020
the
open
images
dataset
v4
int
comput
vis
128
1956
1981
kügelgen
jv
karimi
ah
bhatt
valera
weller
schölkopf
2021
on
the
fairness
of
causal
algorithmic
recourse
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2010.06529
lahoti
beutel
chen
lee
prost
thain
wang
chi
2020
fairness
without
demographics
through
adversarially
reweighted
learning
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
vol
33
curran
associates
inc
pp
728
740
https://proceedings.neurips.cc/paper/2020/file/07fc15c9d169ee48573edd749d25945d-paper.pdf
lake
bm
salakhutdinov
tenenbaum
jb
2015
human-level
concept
learning
through
probabilistic
program
induction
science
350
6266
1332
1338
https://doi.org/10.1126/science.aab3050
lamy
zhong
menon
ak
verma
2019
noise-tolerant
fair
classification
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
294
306
https://proceedings.neurips.cc/paper/2019/file/
8d5e957f297893487bd98fa830fa6413-paper
pdf
lan
huan
2017
discriminatory
transfer
kdd
2017
workshop
fairness
accountability
and
trans
parency
in
machine
learning
fat
ml
arxiv
1707.00780
larson
mattu
kirchner
angwin
2016
how
we
analyzed
the
compas
recidivism
algorithm
https
www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm
le
quy
roy
iosifidis
zhang
ntoutsi
2022
survey
on
datasets
for
fairness-aware
machine
learning
wires
data
mining
and
knowledge
discovery
e1452
https://doi.org/10.1002/
widm
1452
https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1452
leavy
meaney
wade
greene
2019
curatr
platform
for
semantic
analysis
and
curation
of
historical
literary
texts
pp
354
366
https://doi.org/10.1007/978-3-030-36599-8_31
leavy
meaney
wade
greene
2020
mitigating
gender
bias
in
machine
learning
data
sets
in
boratto
faralli
marras
stilo
eds
bias
and
social
aspects
in
search
and
recommendation
springer
cham
pp
12
26
lecun
bottou
bengio
haffner
1998
gradient-based
learning
applied
to
document
recognition
proc
ieee
86
11
2278
2324
https://doi.org/10.1109/5.726791
lecun
fu
jie
huang
bottou
2004
learning
methods
for
generic
object
recognition
with
invariance
to
pose
and
lighting
in
proceedings
of
the
2004
ieee
computer
society
conference
on
computer
vision
and
pattern
recognition
2004
cvpr
2004
vol
pp
ii
104
https://doi.org/10.1109/cvpr.
2004.1315150
lee
kizilcec
rf
2020
evaluation
of
fairness
trade-offs
in
predicting
student
success
in
interna
tional
conference
on
educational
data
mining
workshop
fairness
accountability
and
transparency
in
educational
data
mining
arxiv
2007.00088
leonelli
tempini
2020
data
journeys
in
the
sciences
springer
berlin
leskovec
mcauley
2012
learning
to
discover
social
circles
in
ego
networks
in
pereira
burges
cjc
bottou
weinberger
kq
eds
advances
in
neural
information
pro
cessing
systems
vol
25
curran
associates
inc
https://proceedings.neurips.cc/paper/2012/file/
7a614fd06c325499f1680b9896beedeb-paper
pdf
leskovec
kleinberg
faloutsos
2007
graph
evolution
densification
and
shrinking
diameters
acm
trans
knowl
discov
from
data
tkdd
lesmana
ns
zhang
bei
2019
balancing
efficiency
and
fairness
in
on-demand
ridesourcing
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
5309
5319
https://proceedings.
neurips
cc
paper
2019
file
3070e6addcd702cb58de5d7897bfdae1-paper
pdf
levy
splansky
gl
strand
nk
atwood
ld
benjamin
ej
blease
cupples
la
agostino
rb
sr
fox
cs
kelly-hayes
et
al
2010
consent
for
genetic
research
in
the
framingham
heart
study
am
med
genet
152
1250
1256
123
algorithmic
fairness
datasets
the
story
so
far
2137
li
zhao
liu
huang
mei
chen
2018
learning
from
history
and
present
next-item
recommendation
via
discriminatively
exploiting
user
behaviors
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
com
puting
machinery
new
york
kdd
18
pp
1734
1743
https://doi.org/10.1145/3219819.3220014
li
zhao
liu
2020a
deep
fair
clustering
for
visual
learning
in
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
li
sanjabi
beirami
smith
2020b
fair
resource
allocation
in
federated
learning
in
international
conference
on
learning
representations
https://openreview.net/forum?id=byexelsydr
li
ning
liu
wu
hui
wang
2020c
fairness
of
classification
using
users
social
relationships
in
online
peer-to-peer
lending
in
companion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
733
742
https://doi.org/10.1145/3366424.
3383557
li
sun
wang
wh
2020d
towards
fair
truth
discovery
from
biased
crowdsourced
answers
association
for
computing
machinery
new
york
pp
599
607
https://doi.org/10.1145/3394486.3403102
li
wang
zhao
hong
liu
2021
on
dyadic
fairness
exploring
and
mitigating
bias
in
graph
connections
in
international
conference
on
learning
representations
https://openreview.net/forum?
id
xggs6pmznq6
liang
acuna
de
2020
artificial
mental
phenomena
psychophysics
as
framework
to
detect
per
ception
biases
in
ai
models
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
403
412
https://doi.
org
10.1145
3351095.3375623
lin
ty
maire
belongie
hays
perona
ramanan
dollár
zitnick
cl
2014
microsoft
coco
common
objects
in
context
in
fleet
pajdla
schiele
tuytelaars
eds
computer
vision-eccv
2014
springer
cham
pp
740
755
lipton
mcauley
chouldechova
2018
does
mitigating
ml
impact
disparity
require
treatment
dis
parity
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
vol
31
curran
associates
inc
https://proceedings.neurips.
cc
paper
2018
file
8e0384779e58ce2af40eb365b318cc32-paper
pdf
liu
burke
2018
personalizing
fairness-aware
re-ranking
recsys
2018
workshop
workshop
on
responsible
recommendation
fat
rec
arxiv
1809.02921
liu
luo
wang
tang
2015
deep
learning
face
attributes
in
the
wild
arxiv
1411.7766
liu
lt
dean
rolf
simchowitz
hardt
2018
delayed
impact
of
fair
machine
learning
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
stockholm
sweden
proceedings
of
machine
learning
research
vol
80
pp
3150
3158
http://proceedings.mlr.press/v80/liu18c.html
liu
lt
simchowitz
hardt
2019
the
implicit
fairness
criterion
of
unconstrained
learning
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
4051
4060
http://proceedings.mlr.press/v97/liu19f.html
liu
lt
wilson
haghtalab
kalai
at
borgs
chayes
2020
the
disparate
equilibria
of
algorithmic
decision
making
when
individuals
invest
rationally
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
381
391
https://doi.org/10.1145/3351095.3372861
liu
shafi
fleisher
eliassi-rad
alfeld
2021
rawlsnet
altering
bayesian
networks
to
encode
rawlsian
fair
equality
of
opportunity
association
for
computing
machinery
new
york
pp
745
755
https://doi.org/10.1145/3461702.3462618
locatello
abbati
rainforth
bauer
schölkopf
bachem
2019
on
the
fairness
of
disentangled
representations
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
14611
14624
https://proceedings.neurips.cc/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-paper.pdf
lohaus
perrot
luxburg
uv
2020
too
relaxed
to
be
fair
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
6360
6369
http://proceedings.mlr.press/v119/lohaus20a.html
louizos
swersky
li
welling
zemel
rs
2016
the
variational
fair
autoencoder
in
bengio
lecun
eds
4th
international
conference
on
learning
representations
iclr
2016
san
juan
puerto
rico
may
2016
conference
track
proceedings
arxiv
1511.00830
123
2138
fabris
et
al
lowe
ferris
ta
hernandez
weber
2009
stride
an
integrated
standards-based
translational
research
informatics
platform
amia
ann
sympos
proc
amia
sympos
2009
391
lu
getoor
2003
link-based
classification
in
proceedings
of
the
twentieth
international
conference
on
international
conference
on
machine
learning
aaai
press
icml
03
pp
496
503
lum
johndrow
2016
statistical
framework
for
fair
predictive
algorithms
dtl
2016
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1610.08077
lum
boudin
price
2020
the
impact
of
overbooking
on
pre-trial
risk
assessment
tool
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
482
491
https://doi.org/10.1145/3351095.3372846,
luong
bt
ruggieri
turini
2016
classification
rule
mining
supported
by
ontology
for
discrimination
discovery
in
2016
ieee
16th
international
conference
on
data
mining
workshops
icdmw
pp
868
875
https://doi.org/10.1109/icdmw.2016.0128
maas
al
daly
re
pham
pt
huang
ng
ay
potts
2011
learning
word
vectors
for
sentiment
analysis
in
proceedings
of
the
49th
annual
meeting
of
the
association
for
computational
linguistics
human
language
technologies
association
for
computational
linguistics
portland
pp
142
150
https
www.aclweb.org/anthology/p11-1015
madnani
loukina
von
davier
burstein
cahill
2017
building
better
open-source
tools
to
support
fairness
in
automated
scoring
in
proceedings
of
the
first
acl
workshop
on
ethics
in
natural
language
processing
association
for
computational
linguistics
valencia
pp
41
52
https://doi.org/
10.18653
v1
w17-1605
https://www.aclweb.org/anthology/w17-1605
madras
creager
pitassi
zemel
2018a
learning
adversarially
fair
and
transferable
representations
in
dy
krause
eds
proceedings
of
the
35th
international
conference
on
machine
learning
pmlr
stockholmsmässan
proceedings
of
machine
learning
research
vol
80
pp
3384
3393
http
proceedings
mlr
press
v80
madras18a
html
madras
pitassi
zemel
2018b
predict
responsibly
improving
fairness
and
accuracy
by
learning
to
defer
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
vol
31
curran
associates
inc
pp
6147
6157
https
proceedings
neurips
cc
paper
2018
file
09d37c08f7b129e96277388757530c72-paper
pdf
madras
creager
pitassi
zemel
2019
fairness
through
causal
awareness
learning
causal
latent
variable
models
for
biased
data
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
pp
349
358
https://doi.
org
10.1145
3287560.3287564
mahabadi
vakilian
2020
individual
fairness
for
k-clustering
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
6586
6596
http://proceedings.mlr.press/v119/mahabadi20a.html
maity
xue
yurochkin
sun
2021
statistical
inference
for
individual
fairness
in
international
conference
on
learning
representations
https://openreview.net/forum?id=z9k8bwl-_2u
mandal
deng
jana
wing
jm
hsu
dj
2020
ensuring
fairness
beyond
the
training
data
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
infor
mation
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/2020/hash/
d6539d3b57159bab6a72e106beb45bd-abstract
html
manjunatha
saini
davis
ls
2019
explicit
bias
discovery
in
visual
question
answering
models
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
martinez
bertran
sapiro
2020
minimax
pareto
fairness
multi
objective
perspective
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
6755
6764
http://proceedings.mlr.press/v119/
martinez20a
html
mary
calauzènes
karoui
ne
2019
fairness-aware
learning
for
continuous
attributes
and
treatments
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
4382
4391
http
proceedings
mlr
press
v97
mary19a
html
mastrandrea
fournet
barrat
2015
contact
patterns
in
high
school
comparison
between
data
collected
using
wearable
sensors
contact
diaries
and
friendship
surveys
plos
one
10
e0136497
https://doi.org/10.1371/journal.pone.0136497
123
algorithmic
fairness
datasets
the
story
so
far
2139
mattei
saffidine
walsh
2018a
an
axiomatic
and
empirical
analysis
of
mechanisms
for
online
organ
matching
in
proceedings
of
the
7th
international
workshop
on
computational
social
choice
comsoc
mattei
saffidine
walsh
2018b
fairness
in
deceased
organ
matching
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
18
pp
236
242
https://doi.org/10.1145/3278721.3278749
mayson
sg
2018
bias
in
bias
out
yale
lj
128
2218
mcauley
targett
shi
van
den
hengel
2015
image-based
recommendations
on
styles
and
substi
tutes
in
proceedings
of
the
38th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
association
for
computing
machinery
new
york
sigir
15
pp
43
52
https://doi.org/10.1145/2766462.2767755,
mccallum
ak
nigam
rennie
seymore
2000
automating
the
construction
of
internet
portals
with
machine
learning
inf
retr
127
163
mcduff
ma
song
kapoor
2019
characterizing
bias
in
classifiers
using
generative
models
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
5403
5414
https://proceedings.
neurips
cc
paper
2019
file
7f018eb7b301a66658931cb8a93fd6e8-paper
pdf
mcfee
bertin-mahieux
ellis
dp
lanckriet
gr
2012
the
million
song
dataset
challenge
in
proceed
ings
of
the
21st
international
conference
on
world
wide
web
association
for
computing
machinery
new
york
www
12
companion
pp
909
916
https://doi.org/10.1145/2187980.2188222
mckenna
2019a
history
of
the
current
population
survey
and
disclosure
avoidance
https://www2.
census
gov
adrm
ced
papers
fy20
2019
04
mckenna-cps
20and
20da
pdf
mckenna
2019b
history
of
the
us
census
bureau
disclosure
review
board
https://www2.census.
gov
adrm
ced
papers
fy20
2019
04
mckenna-drb
pdf
mcmahan
moore
ramage
hampson
arcas
ba
2017
communication-efficient
learning
of
deep
networks
from
decentralized
data
in
singh
zhu
eds
proceedings
of
the
20th
international
conference
on
artificial
intelligence
and
statistics
pmlr
fort
lauderdale
proceedings
of
machine
learning
research
vol
54
pp
1273
1282
http://proceedings.mlr.press/v54/mcmahan17a.html
mcnamara
2019
equalized
odds
implies
partially
equalized
outcomes
under
realistic
assumptions
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
313
320
https://doi.org/10.1145/3306618.3314290
meek
thiesson
heckerman
2002
the
learning-curve
sampling
method
applied
to
model-based
clustering
mach
learn
res
feb
397
418
mehrabi
morstatter
saxena
lerman
galstyan
2021
survey
on
bias
and
fairness
in
machine
learning
acm
comput
surv
https://doi.org/10.1145/3457607
mehrotra
celis
le
2021
mitigating
bias
in
set
selection
with
noisy
protected
attributes
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
237
248
https://doi.org/10.1145/3442188.3445887
mehrotra
anderson
diaz
sharma
wallach
yilmaz
2017
auditing
search
engines
for
differential
satisfaction
across
demographics
in
proceedings
of
the
26th
international
conference
on
world
wide
web
companion
international
world
wide
web
conferences
steering
committee
republic
and
canton
of
geneva
che
www
17
companion
pp
626
633
https://doi.org/10.1145/3041021.
3054197
merkley
2019
use
and
fair
use
statement
on
shared
images
in
facial
recognition
ai
merler
ratha
feris
rs
smith
jr
2019
diversity
in
faces
arxiv
1901.10436
metevier
giguere
brockman
kobren
brun
brunskill
thomas
ps
2019
offline
con
textual
bandits
with
high
probability
fairness
guarantees
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
14922
14933
https://proceedings.neurips.cc/paper/2019/file/
d69768b3da745b77e82cdbddcc8bac98-paper
pdf
mhasawade
chunara
2021
causal
multi-level
fairness
association
for
computing
machinery
new
york
pp
784
794
https://doi.org/10.1145/3461702.3462587
miao
2010
did
the
results
of
promotion
exams
have
disparate
impact
on
minorities
using
statistical
evidence
in
ricci
destefano
stat
educ
18
miceli
yang
naudts
schuessler
serbanescu
hanna
2021
documenting
computer
vision
datasets
an
invitation
to
reflexive
data
practices
in
proceedings
of
the
2021
acm
conference
on
123
2140
fabris
et
al
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
161
172
miller
1998
an
introduction
to
the
resource
description
framework
d-lib
magazine
mirkin
nowson
brun
perez
2015
motivating
personality-aware
machine
translation
in
pro
ceedings
of
the
2015
conference
on
empirical
methods
in
natural
language
processing
association
for
computational
linguistics
lisbon
pp
1102
1108
https://doi.org/10.18653/v1/d15-1130,
https
www.aclweb.org/anthology/d15-1130
mishler
kennedy
eh
chouldechova
2021
fairness
in
risk
assessment
instruments
post-processing
to
achieve
counterfactual
equalized
odds
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
386
400
https://doi.org/10.1145/3442188.3445902
mishra
he
belli
2020
assessing
demographic
bias
in
named
entity
recognition
akbc
2020
workshop
bias
in
automatic
knowledge
graph
construction
arxiv
2008.03415
mislove
viswanath
gummadi
kp
druschel
2010
you
are
who
you
know
inferring
user
profiles
in
online
social
networks
in
proceedings
of
the
third
acm
international
conference
on
web
search
and
data
mining
association
for
computing
machinery
new
york
wsdm
10
pp
251
260
https
doi
org
10.1145
1718487.1718519
moore
jc
stinson
ll
welniak
ej
2000
income
measurement
error
in
surveys
review
off
stat
16
331
362
moreland
herlihy
tynan
ma
sunshine
mccord
rf
hilton
poovey
werner
ak
jones
cd
fulmer
eb
et
al
2020
timing
of
state
and
territorial
covid-19
stay-at-home
orders
and
changes
in
population
movement-united
states
march
may
31
2020
morb
mortal
wkly
rep
69
35
1198
moro
cortez
rita
2014
data-driven
approach
to
predict
the
success
of
bank
telemarketing
decis
support
syst
62
22
31
mozannar
ohannessian
srebro
2020
fair
learning
with
private
demographic
data
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
7066
7075
http://proceedings.mlr.press/v119/
mozannar20a
html
mukherjee
yurochkin
banerjee
sun
2020
two
simple
ways
to
learn
individual
fairness
metrics
from
data
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
7097
7107
http
proceedings
mlr
press
v119
mukherjee20a
html
muller
lange
wang
piorkowski
tsay
liao
qv
dugan
erickson
2019
how
data
science
workers
work
with
data
discovery
capture
curation
design
creation
association
for
computing
machinery
new
york
pp
15
https://doi.org/10.1145/3290605.3300356
murgia
2019
microsoft
quietly
deletes
largest
public
face
recognition
data
set
https://www.ft.com/
content
7d3e0d6a-87a0-11e9-a028-86cea8523dc2
nabi
malinsky
shpitser
2019
learning
optimal
fair
policies
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
4674
4682
http://proceedings.mlr.press/v97/
nabi19a
html
namata
london
getoor
huang
edu
2012
query-driven
active
surveying
for
collective
classification
in
10th
international
workshop
on
mining
and
learning
with
graphs
vol
nanda
xu
sankararaman
ka
dickerson
jp
srinivasan
2020
balancing
the
tradeoff
between
profit
and
fairness
in
rideshare
platforms
during
high-demand
hours
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
131
https://doi.org/10.1145/3375627.3375818
nanda
dooley
singla
feizi
dickerson
jp
2021
fairness
through
robustness
investigating
robustness
disparity
in
deep
learning
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
466
477
https://doi.org/10.1145/3442188.3445910
narayanan
shmatikov
2008
robust
de-anonymization
of
large
sparse
datasets
in
2008
ieee
symposium
on
security
and
privacy
sp
2008
ieee
pp
111
125
nasr
tschantz
mc
2020
bidding
strategies
with
gender
nondiscrimination
constraints
for
online
ad
auctions
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
337
347
https://doi.org/10.1145/
3351095.3375783
123
algorithmic
fairness
datasets
the
story
so
far
2141
ngong
ic
maughan
near
jp
2020
towards
auditability
for
fairness
in
deep
learning
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2012.00106
nlst
trial
research
team
2011
the
national
lung
screening
trial
overview
and
study
design
radiology
258
243
253
noriega-campero
bakker
ma
garcia-bulle
pentland
as
2019
active
fairness
in
algorithmic
decision
making
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
asso
ciation
for
computing
machinery
new
york
aies
19
pp
77
83
https://doi.org/10.1145/3306618.
3314277
noriega-campero
garcia-bulle
cantu
lf
bakker
ma
tejerina
pentland
2020
algorithmic
targeting
of
social
policies
fairness
accuracy
and
distributed
governance
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
241
251
https://doi.org/10.1145/3351095.3375784
nuttall
dl
goldstein
prosser
rasbash
1989
differential
school
effectiveness
int
educ
res
13
769
776
https://doi.org/10.1016/0883-0355(89)90027-x
obermeyer
mullainathan
2019
dissecting
racial
bias
in
an
algorithm
that
guides
health
decisions
for
70
million
people
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
19
89
https://doi.org/10.1145/3287560.
3287593
ogura
takeda
2020
convex
fairness
constrained
model
using
causal
effect
estimators
in
companion
proceedings
of
the
web
conference
2020
association
for
computing
machinery
new
york
www
20
pp
723
732
https://doi.org/10.1145/3366424.3383556
olave
rajkovic
bohanec
1989
an
application
for
admission
in
public
school
systems
expert
syst
pub
admin
145
160
oneto
siri
luria
anguita
2017
dropout
prediction
at
university
of
genoa
privacy
preserving
data
driven
approach
in
esann
oneto
donini
elders
pontil
2019a
taking
advantage
of
multitask
learning
for
fair
classifica
tion
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
227
237
https://doi.org/10.1145/3306618.3314255
oneto
donini
maurer
pontil
2019b
learning
fair
and
transferable
representations
neurips
2019
workshop
human-centric
machine
learning
oneto
donini
luise
ciliberto
maurer
pontil
2020
exploiting
mmd
and
sinkhorn
divergences
for
fair
and
transferable
representation
learning
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https
proceedings
neurips
cc
paper
2020
hash
af9c0e0c1dee63e5cad8b7ed1a5be96-abstract
html
pandey
caliskan
2021
disparate
impact
of
artificial
intelligence
bias
in
ridehailing
economy
price
discrimination
algorithms
association
for
computing
machinery
new
york
pp
822
833
https
doi
org
10.1145
3461702.3462561
papakyriakopoulos
hegelich
serrano
jcm
marco
2020
bias
in
word
embeddings
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
446
457
https://doi.org/10.1145/3351095.3372843
paraschakis
nilsson
2020
matchmaking
under
fairness
constraints
speed
dating
case
study
ecir
2020
workshop
international
workshop
on
algorithmic
bias
in
search
and
recommendation
bias
2020
patro
gk
chakraborty
ganguly
gummadi
kp
2019
incremental
fairness
in
two-sided
market
plat
forms
on
smoothly
updating
recommendations
neurips
2019
workshop
human-centric
machine
learning
arxiv
1909.10005
paullada
raji
id
bender
em
denton
hanna
2020
data
and
its
dis
contents
survey
of
dataset
development
and
use
in
machine
learning
research
arxiv
2012.05345
pedreshi
ruggieri
turini
2008
discrimination-aware
data
mining
in
proceedings
of
the
14th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
08
pp
560
568
https://doi.org/10.1145/1401890.1401959
peng
mathur
narayanan
2021
mitigating
dataset
harms
requires
stewardship
lessons
from
1000
papers
arxiv
2108.02922
123
2142
fabris
et
al
perrone
donini
zafar
mb
schmucker
kenthapadi
archambeau
2021
fair
bayesian
optimization
association
for
computing
machinery
new
york
pp
854
863
https://doi.org/10.1145/
3461702.3462629
pessach
shmueli
2020
algorithmic
fairness
arxiv
2001.09784
peters
me
lecocq
2013
content
extraction
using
diverse
feature
sets
in
proceedings
of
the
22nd
international
conference
on
world
wide
web
pp
89
90
peters
neumann
iyyer
gardner
clark
lee
zettlemoyer
2018
deep
contextual
ized
word
representations
in
proceedings
of
the
2018
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
vol
long
papers
association
for
computational
linguistics
new
orleans
pp
2227
2237
pfohl
marafino
coulet
rodriguez
palaniappan
shah
nh
2019
creating
fair
models
of
atherosclerotic
cardiovascular
disease
risk
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
271
278
https://doi.org/10.1145/3306618.3314278
pinard
2010
collateral
consequences
of
criminal
convictions
confronting
issues
of
race
and
dignity
nyul
rev
85
457
pitoura
stefanidis
koutrika
2021
fairness
in
rankings
and
recommendations
an
overview
vldb
28
pleiss
raghavan
wu
kleinberg
weinberger
kq
2017
on
fairness
and
calibration
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
pp
5680
5689
https://proceedings.
neurips
cc
paper
2017
file
b8b9c74ac526fffbeb2d39ab038d1cd7-paper
pdf
pont-tuset
uijlings
changpinyo
soricut
ferrari
2020
connecting
vision
and
language
with
localized
narratives
in
european
conference
on
computer
vision
springer
pp
647
664
prabhu
vu
birhane
2020
large
image
datasets
pyrrhic
win
for
computer
vision
arxiv
2006.16923
preot
iuc-pietro
ungar
2018
user-level
race
and
ethnicity
predictors
from
twitter
text
in
proceedings
of
the
27th
international
conference
on
computational
linguistics
association
for
computational
linguistics
santa
fe
pp
1534
1545
https://www.aclweb.org/anthology/c18-1130
propublica
2016
compas
analysis
github
repository
https://github.com/propublica/compas-analysis
propublica
2021
propublica
data
store
terms
https://www.propublica.org/datastore/terms
pujol
mckenna
kuppam
hay
machanavajjhala
miklau
2020
fair
decision
making
using
privacy-protected
data
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
189
199
https://doi.
org
10.1145
3351095.3372872
qian
cao
mouël
fl
sahel
li
2015
scram
sharing
considered
route
assignment
mechanism
for
fair
taxi
route
recommendations
in
proceedings
of
the
21th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
15
pp
955
964
https://doi.org/10.1145/2783258.2783261
qin
liu
ty
2013
introducing
letor
4.0
datasets
arxiv
1306.2597
quadrianto
sharmanska
2017
recycling
privileged
learning
and
distribution
matching
for
fair
ness
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
pp
677
688
https://proceedings.neurips.cc/paper/2017/file/250cf8b51c773f3f8dc8b4be867a9a02-paper.pdf
quadrianto
sharmanska
thomas
2019
discovering
fair
representations
in
the
data
domain
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
radford
narasimhan
salimans
sutskever
2018
improving
language
understanding
by
generative
pre-training
radford
wu
child
luan
amodei
sutskever
2019
language
models
are
unsupervised
multitask
learners
radin
2017
digital
natives
how
medical
and
indigenous
histories
matter
for
big
data
osiris
32
43
64
raff
sylvester
2018
gradient
reversal
against
discrimination
icml
2018
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1807.00392
raff
sylvester
mills
2018
fair
forests
regularized
tree
induction
to
minimize
model
bias
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
18
pp
243
250
https://doi.org/10.1145/3278721.3278742,
123
algorithmic
fairness
datasets
the
story
so
far
2143
rahmattalabi
vayanos
fulginiti
rice
wilder
yadav
tambe
2019
exploring
algo
rithmic
fairness
in
robust
graph
covering
problems
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
15776
15787
https://proceedings.neurips.cc/paper/2019/file/
1d7c2aae840867027b7edd17b6aaa0e9-paper
pdf
raj
wood
montoly
ekstrand
md
2020
comparing
fair
ranking
metrics
recsys
2020
workshop
3rd
facctrec
workshop
on
responsible
recommendation
arxiv
2009.01311
raji
id
buolamwini
2019
actionable
auditing
investigating
the
impact
of
publicly
naming
biased
performance
results
of
commercial
ai
products
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
429
435
https://doi.org/10.1145/3306618.3314244
ramachandran
gs
brugere
varshney
lr
xiong
2021
gaea
graph
augmentation
for
equitable
access
via
reinforcement
learning
association
for
computing
machinery
new
york
pp
884
894
https://doi.org/10.1145/3461702.3462615
ramaswamy
vv
kim
ssy
russakovsky
2021
fair
attribute
classification
through
latent
space
de
biasing
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pp
9301
9310
red
kelsic
ed
mucha
pj
porter
ma
2011
comparing
community
structure
to
characteristics
in
online
collegiate
social
networks
siam
rev
53
526
543
redmond
baveja
2002
data-driven
software
tool
for
enabling
cooperative
information
sharing
among
police
departments
eur
oper
res
141
660
678
https://doi.org/10.1016/s0377-
2217
01
00264
redmond
cunningham
2013
temporal
network
analysis
reveals
the
unprofitability
of
arbitrage
in
the
prosper
marketplace
expert
syst
appl
40
3715
3721
https://doi.org/10.1016/j.eswa.2012.12.
077
reed
se
zhang
zhang
lee
2015
deep
visual
analogy-making
in
cortes
lawrence
lee
sugiyama
garnett
eds
advances
in
neural
information
processing
systems
vol
28
curran
associates
inc
pp
1252
1260
https://proceedings.neurips.cc/paper/2015/file/
e07413354875be01a996dc560274708e-paper
pdf
rezaei
liu
memarrast
ziebart
2021
robust
fairness
under
covariate
shift
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2010.05166
riederer
chaintreau
2017
the
price
of
fairness
in
location
based
advertising
https://doi.org/10.
18122
b2md8c
recsys
2017
workshop
workshop
on
responsible
recommendation
fat
rec
rocher
hendrickx
jm
de
montjoye
ya
2019
estimating
the
success
of
re-identifications
in
incomplete
datasets
using
generative
models
nat
commun
10
rodolfa
kt
salomon
haynes
mendieta
ih
larson
ghani
2020
case
study
predictive
fairness
to
reduce
misdemeanor
recidivism
through
social
service
interventions
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
142
153
https://doi.org/10.1145/3351095.3372863
roh
lee
whang
suh
2020
fr-train
mutual
information-based
approach
to
fair
and
robust
training
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
8147
8157
http
proceedings
mlr
press
v119
roh20a
html
roh
lee
whang
se
suh
2021
fairbatch
batch
selection
for
model
fairness
in
international
conference
on
learning
representations
https://openreview.net/forum?id=ynnpaakecfx
romano
bates
candes
2020
achieving
equalized
odds
by
resampling
sensitive
attributes
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
pro
cessing
systems
vol
33
curran
associates
inc
pp
361
371
https://proceedings.neurips.cc/paper/
2020
file
03593ce517feac573fdaafa6dcedef61-paper
pdf
romei
ruggieri
2014
multidisciplinary
survey
on
discrimination
analysis
knowl
eng
rev
29
582
638
https://doi.org/10.1017/s0269888913000039
rotemberg
kurtansky
betz-stablein
caffery
chousakos
codella
combalia
dusza
guitera
gutman
et
al
2021
patient-centric
dataset
of
images
and
metadata
for
identifying
melanomas
using
clinical
context
sci
data
rozemberczki
allen
sarkar
2021
multi-scale
attributed
node
embedding
complex
netw
cnab014
123
2144
fabris
et
al
rudinger
may
van
durme
2017
social
bias
in
elicited
natural
language
inferences
in
proceedings
of
the
first
acl
workshop
on
ethics
in
natural
language
processing
association
for
computational
lin
guistics
pp
74
79
https://doi.org/10.18653/v1/w17-1609,
https://www.aclweb.org/anthology/w17-
1609
rudinger
naradowsky
leonard
van
durme
2018
gender
bias
in
coreference
resolution
in
proceedings
of
the
2018
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
short
papers
association
for
computational
linguistics
new
orleans
pp
14
https://doi.org/10.18653/v1/n18-2002,
https://www.aclweb.org/
anthology
n18-2002
ruoss
balunovic
fischer
vechev
2020
learning
certified
individually
fair
representations
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
vol
33
curran
associates
inc
pp
7584
7596
https://proceedings.neurips.cc/
paper
2020
file
55d491cf951b1b920900684d71419282-paper
pdf
russell
kusner
mj
loftus
silva
2017
when
worlds
collide
integrating
different
coun
terfactual
assumptions
in
fairness
in
guyon
luxburg
uv
bengio
wallach
fer
gus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
pp
6414
6423
https://proceedings.neurips.cc/paper/2017/file/
1271a7029c9df08643b631b02cf9e116-paper
pdf
sabato
yom-tov
2020
bounding
the
fairness
and
accuracy
of
classifiers
from
population
statistics
in
iii
hd
singh
eds
proceedings
of
the
37th
international
conference
on
machine
learning
pmlr
virtual
proceedings
of
machine
learning
research
vol
119
pp
8316
8325
http://proceedings.mlr.
press
v119
sabato20a
html
saenko
kulis
fritz
darrell
2010
adapting
visual
category
models
to
new
domains
in
pro
ceedings
of
the
11th
european
conference
on
computer
vision
part
iv
springer
berlin
heidelberg
eccv
10
pp
213
226
sagawa
koh
pw
hashimoto
tb
liang
2020
distributionally
robust
neural
networks
in
international
conference
on
learning
representations
https://openreview.net/forum?id=ryxgujrfvs
samadi
tantipongpipat
morgenstern
jh
singh
vempala
2018
the
price
of
fair
pca
one
extra
dimension
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
vol
31
curran
associates
inc
pp
10976
10987
https://proceedings.neurips.cc/paper/2018/file/cc4af25fa9d2d5c953496579b75f6f6c-paper.pdf
savani
white
govindarajulu
ns
2020
intra-processing
methods
for
debiasing
neural
net
works
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/2020/hash/
1d8d70dddf147d2d92a634817f01b239-abstract
html
scheuerman
mk
wade
lustig
brubaker
jr
2020
how
we
ve
taught
algorithms
to
see
identity
constructing
race
and
gender
in
image
databases
for
facial
analysis
in
proceedings
of
the
acm
human
computer
interaction
cscw1
https://doi.org/10.1145/3392866
schumann
ricco
prabhu
ferrari
pantofaru
2021
step
toward
more
inclusive
people
annotations
for
fairness
association
for
computing
machinery
new
york
pp
916
925
https://doi.
org
10.1145
3461702.3462594
schutzman
2020
trade-offs
in
fair
redistricting
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
159
165
https
doi
org
10.1145
3375627.3375802
segal
adi
pinkas
baum
ganesh
keshet
2021
fairness
in
the
eyes
of
the
data
certifying
machine-learning
models
association
for
computing
machinery
new
york
pp
926
935
https://doi.
org
10.1145
3461702.3462554
sen
namata
bilgic
getoor
galligher
eliassi-rad
2008
collective
classification
in
network
data
ai
mag
29
93
93
shah
gupta
deshpande
bhattacharyya
2021
rawlsian
fair
adaptation
of
deep
learning
classifiers
association
for
computing
machinery
new
york
pp
936
945
https://doi.org/10.1145/3461702.
3462592
shang
sun
lam
ns
2020
list-wise
fairness
criterion
for
point
processes
association
for
computing
machinery
new
york
pp
1948
1958
https://doi.org/10.1145/3394486.3403246
sharifi-malvajerdi
kearns
roth
2019
average
individual
fairness
algorithms
generalization
and
experiments
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
123
algorithmic
fairness
datasets
the
story
so
far
2145
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
8242
8251
https://proceedings.neurips.cc/paper/2019/file/0e1feae55e360ff05fef58199b3fa521-paper.pdf
sharma
henderson
ghosh
2020a
certifai
common
framework
to
provide
explanations
and
analyse
the
fairness
and
robustness
of
black-box
models
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
166
172
https://doi.org/10.1145/3375627.3375812
sharma
zhang
ríos
aliaga
jm
bouneffouf
muthusamy
varshney
kr
2020b
data
augmentation
for
discrimination
prevention
and
bias
disambiguation
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
358
364
https://doi.org/10.1145/3375627.3375865
sharma
gee
ah
paydarfar
ghosh
2021
fair-n
fair
and
robust
neural
networks
for
structured
data
association
for
computing
machinery
new
york
pp
946
955
https://doi.org/10.1145/3461702.
3462559
shekhar
shah
akoglu
2021
fairod
fairness-aware
outlier
detection
in
proceedings
of
the
2021
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
21
pp
210
220
https://doi.org/10.1145/3461702.3462517
shen
jh
fratamico
rahwan
rush
am
2018
darling
or
babygirl
investigating
stylistic
bias
in
senti
ment
analysis
kdd
2018
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
shermis
md
2014
state-of-the-art
automated
essay
scoring
competition
results
and
future
directions
from
united
states
demonstration
assess
writ
20
53
76
https://doi.org/10.1016/j.asw.2013.04.001
singh
joachims
2018
fairness
of
exposure
in
rankings
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machin
ery
new
york
kdd
18
pp
2219
2228
https://doi.org/10.1145/3219819.3220088
singh
joachims
2019
policy
learning
for
fairness
in
ranking
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
process
ing
systems
vol
32
curran
associates
inc
pp
5426
5436
https://proceedings.neurips.cc/paper/
2019
file
9e82757e9a1c12cb710ad680db11f6f1-paper
pdf
singh
ramamurthy
kn
2019
understanding
racial
bias
in
health
using
the
medical
expenditure
panel
survey
data
neurips
2019
workshop
fair
ml
for
health
arxiv
1911.01509
singh
singh
mhasawade
chunara
2021
fairness
violations
and
mitigation
under
covariate
shift
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
13
https://doi.org/10.1145/3442188.3445865
slack
friedler
givental
2019a
fair
meta-learning
learning
how
to
learn
fairly
https
drive
google
com
file
1f5yf1ar1hj7l2h7zisc35szxowquylvw
view
neurips
2019
workshop
human-centric
machine
learning
slack
friedler
givental
2019b
fairness
warnings
https://drive.google.com/file/d/
1eeu703ulwkehk0weepydwxg2kxswozc2
view
neurips
2019
workshop
human-centric
machine
learning
slack
friedler
sa
givental
2020
fairness
warnings
and
fair-maml
learning
fairly
with
minimal
data
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
200
209
https://doi.org/10.1145/3351095.3372839
slunge
2015
the
willingness
to
pay
for
vaccination
against
tick-borne
encephalitis
and
implications
for
public
health
policy
evidence
from
sweden
plos
one
10
12
e0143875
smith
jw
everhart
dickson
knowler
johannes
1988
using
the
adap
learning
algorithm
to
forecast
the
onset
of
diabetes
mellitus
in
proceedings
symposium
on
computer
applications
in
medical
care
pp
261
265
https://europepmc.org/articles/pmc2245318
solans
fabbri
calsamiglia
castillo
bonchi
2021
comparing
equity
and
effectiveness
of
dif
ferent
algorithms
in
an
application
for
the
room
rental
market
association
for
computing
machinery
new
york
pp
978
988
https://doi.org/10.1145/3461702.3462600
sonboli
burke
2019
localized
fairness
in
recommender
systems
in
adjunct
publication
of
the
27th
conference
on
user
modeling
adaptation
and
personalization
association
for
computing
machinery
new
york
umap
19
adjunct
pp
295
300
https://doi.org/10.1145/3314183.3323845
sonboli
burke
mattei
eskandanian
gao
2020
and
the
winner
is
dynamic
lotteries
for
multi-group
fairness-aware
recommendation
recsys
2020
workshop
3rd
facctrec
workshop
on
responsible
recommendation
arxiv
2009.02590
123
2146
fabris
et
al
speakman
sridharan
markus
2018
three
population
covariate
shift
for
mobile
phone-based
credit
scoring
in
proceedings
of
the
1st
acm
sigcas
conference
on
computing
and
sustainable
societies
association
for
computing
machinery
new
york
ny
usa
compass
18
speicher
ali
venkatadri
ribeiro
fn
arvanitakis
benevenuto
gummadi
kp
loiseau
mislove
2018a
potential
for
discrimination
in
online
targeted
advertising
in
friedler
sa
wilson
eds
proceedings
of
the
1st
conference
on
fairness
accountability
and
transparency
pmlr
new
york
proceedings
of
machine
learning
research
vol
81
pp
19
http://proceedings.mlr.press/v81/
speicher18a
html
speicher
heidari
grgic-hlaca
gummadi
kp
singla
weller
zafar
mb
2018b
uni
fied
approach
to
quantifying
algorithmic
unfairness
measuring
individual
and
group
unfairness
via
inequality
indices
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowl
edge
discovery
anddata
mining
association
for
computing
machinery
new
york
kdd
18
pp
2239
2248
https://doi.org/10.1145/3219819.3220046
squire
rf
2019
measuring
and
correcting
sampling
bias
in
safegraph
patterns
for
more
accurate
demographic
analysis
https://www.safegraph.com/blog/measuring-and-correcting-sampling-
bias-for-accurate-demographic-analysis
utm_source
content
utm_medium
referral
utm_
campaign
colabnotebook
utm_content
panel_bias
stanovsky
smith
na
zettlemoyer
2019
evaluating
gender
bias
in
machine
translation
in
pro
ceedings
of
the
57th
annual
meeting
of
the
association
for
computational
linguistics
association
for
computational
linguistics
florence
pp
1679
1684
https://doi.org/10.18653/v1/p19-1164,
https
aclanthology
org
p19-1164
steed
caliskan
2021
image
representations
learned
with
unsupervised
pre-training
contain
human
like
biases
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
701
713
https://doi.org/10.1145/
3442188.3445932
strack
deshazo
gennings
olmo
ortiz
jl
ventura
cios
clore
2014
impact
of
hba1c
mea
surement
on
hospital
readmission
rates
analysis
of
70
000
clinical
database
patient
records
biomed
res
int
2014
781670
https://doi.org/10.1155/2014/781670
sühr
biega
aj
zehlike
gummadi
kp
chakraborty
2019
two-sided
fairness
for
repeated
match
ings
in
two-sided
markets
case
study
of
ride-hailing
platform
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
com
puting
machinery
new
york
kdd
19
pp
3082
3092
https://doi.org/10.1145/3292500.3330793
sühr
hilgard
lakkaraju
2021
does
fair
ranking
improve
minority
outcomes
understanding
the
interplay
of
human
and
algorithmic
biases
in
online
hiring
association
for
computing
machinery
new
york
pp
989
999
https://doi.org/10.1145/3461702.3462602
sun
han
gao
yu
2009
itopicmodel
information
network-integrated
topic
modeling
in
2009
ninth
ieee
international
conference
on
data
mining
pp
493
502
https://doi.org/10.1109/icdm.
2009.43
sun
gaut
tang
huang
elsherief
zhao
mirza
belding
chang
kw
wang
wy
2019
mitigating
gender
bias
in
natural
language
processing
literature
review
in
proceedings
of
the
57th
annual
meeting
of
the
association
for
computational
linguistics
association
for
computational
lin
guistics
florence
pp
1630
1640
https://doi.org/10.18653/v1/p19-1159,
https://aclanthology.org/
p19-1159
swinger
de-arteaga
heffernan
iv
nt
leiserson
md
kalai
at
2019
what
are
the
biases
in
my
word
embedding
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
19
pp
305
311
https://doi.org/10.1145/
3306618.3314270
takac
zabovsky
2012
data
analysis
in
public
social
networks
in
international
scientific
conference
and
international
workshop
present
day
trends
of
innovations
vol
tan
yc
celis
le
2019
assessing
social
and
intersectional
biases
in
contextualized
word
representations
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
13230
13241
https
proceedings
neurips
cc
paper
2019
file
201d546992726352471cfea6b0df0a48-paper
pdf
tang
zhang
yao
li
zhang
su
2008
arnetminer
extraction
and
mining
of
academic
social
networks
pp
990
998
https://doi.org/10.1145/1401890.1402008
tantipongpipat
samadi
singh
morgenstern
jh
vempala
2019
multi-criteria
dimen
sionality
reduction
with
applications
to
fairness
in
wallach
larochelle
beygelzimer
123
algorithmic
fairness
datasets
the
story
so
far
2147
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
15161
15171
https://proceedings.neurips.cc/paper/2019/file/
2201611d7a08ffda97e3e8c6b667a1bc-paper
pdf
taskesen
blanchet
kuhn
nguyen
va
2021
statistical
test
for
probabilistic
fairness
in
pro
ceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
648
665
https://doi.org/10.1145/3442188.3445927
tatman
2017
gender
and
dialect
bias
in
youtube
automatic
captions
in
proceedings
of
the
first
acl
workshop
on
ethics
in
natural
language
processing
association
for
computational
linguis
tics
valencia
pp
53
59
https://doi.org/10.18653/v1/w17-1606,
https://www.aclweb.org/anthology/
w17-1606
tavallaee
bagheri
lu
ghorbani
aa
2009
detailed
analysis
of
the
kdd
cup
99
data
set
in
2009
ieee
symposium
on
computational
intelligence
for
security
and
defense
applications
pp
https://doi.org/10.1109/cisda.2009.5356528
team
conduent
public
safety
solutions
2018
real
time
crime
forecasting
challenge
post-mortem
analysis
challenge
performance
tjong
kim
sang
ef
de
meulder
2003
introduction
to
the
conll-2003
shared
task
language
independent
named
entity
recognition
in
proceedings
of
the
seventh
conference
on
natural
language
learning
at
hlt-naacl
2003
pp
142
147
https://www.aclweb.org/anthology/w03-0419
tong
kagal
2020
investigating
bias
in
image
classification
using
model
explanations
icml
2020
workshop
workshop
on
human
interpretability
in
machine
learning
whi
arxiv
2012.05463
toutanova
chen
pantel
poon
choudhury
gamon
2015
representing
text
for
joint
embedding
of
text
and
knowledge
bases
https://doi.org/10.18653/v1/d15-1174
tsang
wilder
rice
tambe
zick
2019
group-fairness
in
influence
maximization
in
inter
national
joint
conference
on
artificial
intelligence
tsao
cw
vasan
rs
2015
cohort
profile
the
framingham
heart
study
fhs
overview
of
milestones
in
cardiovascular
epidemiology
int
epidemiol
44
1800
1813
tschandl
rosendahl
kittler
2018
the
ham10000
dataset
large
collection
of
multi-source
der
matoscopic
images
of
common
pigmented
skin
lesions
sci
data
tziavelis
giannakopoulos
doka
koziris
karras
2019
equitable
stable
matchings
in
quadratic
time
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
457
467
https
proceedings
neurips
cc
paper
2019
file
cb70ab375662576bd1ac5aaf16b3fca4-paper
pdf
uci
machine
learning
repository
1994
statlog
german
credit
data
data
set
https://archive.ics.uci.edu/
ml
datasets
statlog
german
credit
data
uci
machine
learning
repository
1996
adult
data
set
https://archive.ics.uci.edu/ml/datasets/adult
uci
machine
learning
repository
2019
south
german
credit
data
set
https://archive.ics.uci.edu/ml/
datasets
south
german
credit
us
dept
of
commerce
bureau
of
the
census
1978
the
current
population
survey
design
and
methodology
us
dept
of
commerce
bureau
of
the
census
1995
current
population
survey
annual
demographic
file
1994
us
federal
reserve
2007
report
to
the
congress
on
credit
scoring
and
its
effects
on
the
availability
and
affordability
of
credi
ustun
westover
mb
rudin
bianchi
mt
2016
clinical
prediction
models
for
sleep
apnea
the
importance
of
medical
history
over
symptoms
clin
sleep
med
12
02
161
168
https://doi.org/10.
5664
jcsm
5476
ustun
liu
parkes
2019
fairness
without
harm
decoupled
classifiers
with
preference
guarantees
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
6373
6382
http
proceedings
mlr
press
v97
ustun19a
html
ve
cho
2020
rule-based
model
for
seoul
bike
sharing
demand
prediction
using
weather
data
eur
remote
sens
53
sup1
166
183
https://doi.org/10.1080/22797254.2020.1725789
park
cho
2020
using
data
mining
techniques
for
bike
sharing
demand
prediction
in
metropoli
tan
city
comput
commun
153
353
366
https://doi.org/10.1016/j.comcom.2020.02.007
vaithianathan
putnam-hornstein
jiang
nand
maloney
2017
developing
predictive
models
to
support
child
maltreatment
hotline
screening
decisions
allegheny
county
methodol
ogy
and
implementation
https://www.alleghenycountyanalytics.us/wp-content/uploads/2019/05/16-
acdhs-26_predictiverisk_package_050119_final-2
pdf
123
2148
fabris
et
al
valera
singla
gomez
rodriguez
2018
enhancing
the
accuracy
and
fairness
of
human
decision
mak
ing
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
31
pp
1769
1778
https
proceedings
neurips
cc
paper
2018
file
0a113ef6b61820daa5611c870ed8d5ee-paper
pdf
van
horn
mac
aodha
song
cui
sun
shepard
adam
perona
belongie
2018
the
inaturalist
species
classification
and
detection
dataset
arxiv
1707.06642
van
horn
cole
beery
wilber
belongie
mac
aodha
2021
benchmarking
representation
learning
for
natural
world
image
collections
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pp
12884
12893
vargo
zhang
yurochkin
sun
2021
individually
fair
gradient
boosting
in
international
con
ference
on
learning
representations
https://openreview.net/forum?id=jbaa9we1al
vig
gehrmann
belinkov
qian
nevo
singer
shieber
sm
2020
investigating
gender
bias
in
language
models
using
causal
mediation
analysis
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https
proceedings
neurips
cc
paper
2020
hash
92650b2e92217715fe312e6fa7b90d82-abstract
html
vijayaraghavan
vosoughi
roy
2017
twitter
demographic
classification
using
deep
multi-modal
multi-task
learning
in
proceedings
of
the
55th
annual
meeting
of
the
association
for
computational
linguistics
volume
short
papers
association
for
computational
linguistics
vancouver
pp
478
483
https://doi.org/10.18653/v1/p17-2076,
https://www.aclweb.org/anthology/p17-2076
voigt
jurgens
prabhakaran
jurafsky
tsvetkov
2018
rtgender
corpus
for
studying
differen
tial
responses
to
gender
in
proceedings
of
the
eleventh
international
conference
on
language
resources
and
evaluation
lrec
2018
european
language
resources
association
elra
miyazaki
https
www.aclweb.org/anthology/l18-1445
voorhees
2005
overview
of
the
trec
2005
robust
retrieval
track
https://trec.nist.gov/pubs/trec13/papers/
robust
overview
pdf
wadsworth
vera
piech
2018
achieving
fairness
through
adversarial
learning
an
application
to
recidivism
prediction
icml
2018
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1807.00199
wah
branson
welinder
perona
belongie
2011
the
caltech-ucsd
birds200-2011
dataset
advances
in
water
resources-adv
water
resour
wan
mcauley
2018
item
recommendation
on
monotonic
behavior
chains
in
proceedings
of
the
12th
acm
conference
on
recommender
systems
association
for
computing
machinery
new
york
recsys
18
pp
86
94
https://doi.org/10.1145/3240323.3240369
wang
deng
2020
mitigating
bias
in
face
recognition
using
skewness-aware
reinforcement
learning
in
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
wang
saar-tsechansky
2020
augmented
fairness
an
interpretable
model
augmenting
decision
makers
fairness
neurips
2020
workshop
algorithmic
fairness
through
the
lens
of
causality
and
interpretability
afci
arxiv
2011.08398
wang
pruksachatkun
nangia
singh
michael
hill
levy
bowman
2019a
super
glue
stickier
benchmark
for
general-purpose
language
understanding
systems
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
https://proceedings.neurips.cc/paper/2019/file/
4496bf24afe7fab6f046bf4923da8de6-paper
pdf
wang
singh
michael
hill
levy
bowman
sr
2019b
glue
multi-task
benchmark
and
analysis
platform
for
natural
language
understanding
in
international
conference
on
learning
representations
https://openreview.net/forum?id=rj4km2r5t7
wang
grgic-hlaca
lahoti
gummadi
kp
weller
2019c
an
empirical
study
on
learning
fairness
metrics
for
compas
data
with
human
supervision
neurips
2019
workshop
human-centric
machine
learning
arxiv
1910.10255
wang
ustun
calmon
2019d
repairing
without
retraining
avoiding
disparate
impact
with
coun
terfactual
distributions
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
6618
6627
http://proceedings.mlr.press/v97/wang19l.html
wang
deng
hu
tao
huang
2019e
racial
faces
in
the
wild
reducing
racial
bias
by
infor
mation
maximization
adaptation
network
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
pp
692
702
123
algorithmic
fairness
datasets
the
story
so
far
2149
wang
guo
narasimhan
cotter
gupta
jordan
2020a
robust
optimization
for
fairness
with
noisy
protected
groups
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
processing
systems
vol
33
curran
associates
inc
pp
5190
5203
https
proceedings
neurips
cc
paper
2020
file
37d097caf1299d9aa79c2c2b843d2d78-paper
pdf
wang
qinami
karakozis
ic
genova
nair
hata
russakovsky
2020b
towards
fairness
in
visual
recognition
effective
strategies
for
bias
mitigation
in
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
wang
liu
levy
2021
fair
classification
with
group-dependent
label
noise
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
526
536
https://doi.org/10.1145/3442188.3445915,
waseem
hovy
2016
hateful
symbols
or
hateful
people
predictive
features
for
hate
speech
detection
on
twitter
in
proceedings
of
the
naacl
student
research
workshop
association
for
computational
linguistics
san
diego
pp
88
93
https://doi.org/10.18653/v1/n16-2013,
https://www.aclweb.org/
anthology
n16-2013
webster
recasens
axelrod
baldridge
2018
mind
the
gap
balanced
corpus
of
gendered
ambiguous
pronouns
arxiv
1810.05201
weeks
clair
borgatti
radda
schensul
2002
social
networks
of
drug
users
in
high-risk
sites
finding
the
connections
aids
behav
193
206
https://doi.org/10.1023/a:1015457400897
wick
panda
tristan
jb
2019
unlocking
fairness
trade-off
revisited
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
curran
associates
inc
vol
32
pp
8783
8792
https://proceedings.neurips.cc/paper/2019/
file
373e4c5d8edfa8b74fd4b6791d0cf6dc-paper
pdf
wieringa
kannan
ma
reutterer
risselada
skiera
2021
data
analytics
in
privacy-concerned
world
bus
res
122
915
925
https://doi.org/10.1016/j.jbusres.2019.05.005
wightman
ramsey
council
lsa
1998
lsac
national
longitudinal
bar
passage
study
lsac
research
report
series
law
school
admission
council
https://books.google.it/books?
id
wda7aqaaiaaj
wilder
ou
hc
de
la
haye
tambe
2018
optimizing
network
structure
for
preventative
health
in
proceedings
of
the
17th
international
conference
on
autonomous
agents
and
multiagent
systems
international
foundation
for
autonomous
agents
and
multiagent
systems
richland
sc
aamas
18
pp
841
849
wilkinson
md
dumontier
aalbersberg
ij
appleton
axton
baak
blomberg
boiten
jw
da
silva
santos
lb
bourne
pe
et
al
2016
the
fair
guiding
principles
for
scientific
data
management
and
stewardship
sci
data
williams
jv
razavian
2019
quantification
of
bias
in
machine
learning
for
healthcare
case
study
of
renal
failure
prediction
https://drive.google.com/file/d/1dvjfvvliqveekalrmlxfx6lcvtzhkdq0/
view
neurips
2019
workshop
fair
ml
for
health
williamson
menon
2019
fairness
risk
measures
in
chaudhuri
salakhutdinov
eds
proceedings
of
the
36th
international
conference
on
machine
learning
pmlr
long
beach
proceedings
of
machine
learning
research
vol
97
pp
6786
6797
http://proceedings.mlr.press/v97/williamson19a.html
wilson
ghosh
jiang
mislove
baker
szary
trindel
polli
2021
tbuilding
and
auditing
fair
algorithms
case
study
in
candidate
screening
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
666
677
https://doi.org/10.1145/3442188.3445928
wondracek
holz
kirda
kruegel
2010
practical
attack
to
de-anonymize
social
network
users
in
2010
ieee
symposium
on
security
and
privacy
pp
223
238
https://doi.org/10.1109/sp.2010.21
wu
zhang
wu
2018
on
discrimination
discovery
and
removal
in
ranked
data
using
causal
graph
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
18
pp
2536
2544
https://doi.org/
10.1145
3219819.3220087
wu
zhang
wu
tong
2019
pc-fairness
unified
framework
for
measuring
causality-based
fairness
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
3404
3414
https
proceedings
neurips
cc
paper
2019
file
44a2e0804995faf8d2e3b084a1e2db1d-paper
pdf
xiao
rasul
vollgraf
2017
fashion-mnist
novel
image
dataset
for
benchmarking
machine
learning
algorithms
arxiv
1708.07747
123
2150
fabris
et
al
xiao
zhao
pan
song
zheng
vw
yang
2019
beyond
personalization
social
content
recommendation
for
creator
equality
and
consumer
satisfaction
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
com
puting
machinery
new
york
kdd
19
pp
235
245
https://doi.org/10.1145/3292500.3330965,
xie
lauritsen
jl
2012
racial
context
and
crime
reporting
test
of
black
stratification
hypothesis
quant
criminol
28
265
293
xu
yuan
zhang
wu
2018
fairgan
fairness-aware
generative
adversarial
networks
in
2018
ieee
international
conference
on
big
data
big
data
ieee
pp
570
575
xu
cui
kuang
li
zhou
shen
cui
2020
algorithmic
decision
making
with
conditional
fairness
association
for
computing
machinery
new
york
pp
2125
2135
https://doi.org/10.1145/
3394486.3403263
xu
huang
shen
li
li
huang
li
cui
2021
consistent
instance
false
positive
improves
fairness
in
face
recognition
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pp
578
586
yang
stoyanovich
2017
measuring
fairness
in
ranked
outputs
in
proceedings
of
the
29th
international
conference
on
scientific
and
statistical
database
management
association
for
computing
machinery
new
york
ssdbm
17
https://doi.org/10.1145/3085504.3085526
yang
kim
2019
benchmarking
attribution
methods
with
relative
feature
importance
arxiv
1907.09701
yang
cisse
koyejo
2020a
fairness
with
overlapping
groups
probabilistic
perspective
in
larochelle
ranzato
hadsell
balcan
mf
lin
eds
advances
in
neural
information
pro
cessing
systems
vol
33
curran
associates
inc
pp
4067
4078
https://proceedings.neurips.cc/paper/
2020
file
29c0605a3bab4229e46723f89cf59d83-paper
pdf
yang
qinami
fei-fei
deng
russakovsky
2020b
towards
fairer
datasets
filtering
and
balancing
the
distribution
of
the
people
subtree
in
the
imagenet
hierarchy
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
fat
20
pp
547
558
https://doi.org/10.1145/3351095.3375709
yao
huang
2017a
beyond
parity
fairness
objectives
for
collaborative
filtering
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
pp
2921
2930
https://proceedings.neurips.cc/
paper
2017
file
e6384711491713d29bc63fc5eeb5ba4f-paper
pdf
yao
huang
2017b
new
fairness
metrics
for
recommendation
that
embrace
differences
kdd
2017
workshop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1706.09838
yeh
ic
hui
lien
2009
the
comparisons
of
data
mining
techniques
for
the
predictive
accuracy
of
probability
of
default
of
credit
card
clients
expert
syst
appl
36
part
2473
2480
https://doi.org/
10.1016
eswa
2007.12
020
yi
xiaogang
xiaoou
2013
deep
convolutional
network
cascade
for
facial
point
detection
in
2013
ieee
conference
on
computer
vision
and
pattern
recognition
pp
3476
3483
https://doi.org/10.
1109
cvpr
2013.446
yi
wang
joshi
ghassemi
2019
fair
and
robust
treatment
effect
estimates
estimation
under
treatment
and
outcome
disparity
with
deep
neural
models
https://drive.google.com/file/d/
1huhrovnfzxnpaseltczzuqfvgu9jbti1
view
neurips
2019
workshop
fair
ml
for
health
yurochkin
sun
2021
sensei
sensitive
set
invariance
for
enforcing
individual
fairness
in
interna
tional
conference
on
learning
representations
https://openreview.net/forum?id=dktzb97_fx
yurochkin
bower
sun
2020
training
individually
fair
ml
models
with
sensitive
subspace
robustness
in
international
conference
on
learning
representations
https://openreview.net/forum?
id
b1gdkxhfdh
zafar
mb
valera
gomez
rodriguez
gummadi
kp
2017a
fairness
beyond
disparate
treatment
and
disparate
impact
learning
classification
without
disparate
mistreatment
in
proceedings
of
the
26th
international
conference
on
world
wide
web
international
world
wide
web
conferences
steering
committee
republic
and
canton
of
geneva
che
www
17
pp
1171
1180
https://doi.org/10.
1145
3038912.3052660
zafar
mb
valera
rodriguez
gummadi
weller
2017b
from
parity
to
preference-based
notions
of
fairness
in
classification
in
guyon
luxburg
uv
bengio
wallach
fergus
vishwanathan
garnett
eds
advances
in
neural
information
processing
systems
vol
30
curran
associates
inc
123
algorithmic
fairness
datasets
the
story
so
far
2151
pp
229
239
https://proceedings.neurips.cc/paper/2017/file/82161242827b703e6acf9c726942a1e4-
paper
pdf
zafar
mb
valera
rogriguez
mg
gummadi
kp
2017c
fairness
constraints
mechanisms
for
fair
clas
sification
in
artificial
intelligence
and
statistics
pmlr
pp
962
970
zehlike
yang
stoyanovich
2021
fairness
in
ranking
survey
arxiv
2103.14000
zhang
2005
bayesian
graphical
model
for
adaptive
information
filtering
phd
thesis
carnegie
mellon
university
zhang
bareinboim
2018
equality
of
opportunity
in
classification
causal
approach
in
bengio
wallach
larochelle
grauman
cesa-bianchi
garnett
eds
advances
in
neural
information
processing
systems
vol
31
curran
associates
inc
pp
3671
3681
https://proceedings.neurips.cc/
paper
2018
file
ff1418e8cc993fe8abcfe3ce2003e5c5-paper
pdf
zhang
davidson
2021
towards
fair
deep
anomaly
detection
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
association
for
computing
machinery
new
york
facct
21
pp
138
148
https://doi.org/10.1145/3442188.3445878
zhang
neill
db
2017
identifying
significant
predictive
bias
in
classifiers
kdd
2017
workshop
fair
ness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1611.08292
zhang
luo
loy
cc
tang
2014
facial
landmark
detection
by
deep
multi-task
learning
https
doi
org
10.1007
978
319
10599
4_7
zhang
luo
loy
cc
tang
2015
learning
deep
representation
for
face
alignment
with
auxiliary
attributes
ieee
trans
pattern
anal
mach
intell
38
918
930
zhang
wu
wu
2017a
achieving
non-discrimination
in
data
release
in
proceedings
of
the
23rd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
association
for
computing
machinery
new
york
kdd
17
pp
13350
1344
https://doi.org/10.1145/3097983.
3098167
zhang
song
qi
2017b
age
progression
regression
by
conditional
adversarial
autoencoder
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
cvpr
zhang
bh
lemoine
mitchell
2018
mitigating
unwanted
biases
with
adversarial
learning
in
pro
ceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
18
pp
335
340
https://doi.org/10.1145/3278721.3278779
zhang
khaliligarekani
tekin
liu
2019
group
retention
when
using
machine
learning
in
sequen
tial
decision
making
the
interplay
between
user
dynamics
and
fairness
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
15269
15278
https://proceedings.neurips.cc/paper/2019/
file
7690dd4db7a92524c684e3191919eb6b-paper
pdf
zhang
lu
ax
abdalla
mcdermott
ghassemi
2020a
hurtful
words
quantifying
biases
in
clinical
contextual
word
embeddings
in
proceedings
of
the
acm
conference
on
health
inference
and
learning
association
for
computing
machinery
new
york
chil
20
pp
110
120
https://doi.
org
10.1145
3368555.3384448
zhang
tu
liu
liu
kjellström
zhang
zhang
2020b
how
do
fair
decisions
fare
in
long-term
qualification
in
larochelle
ranzato
hadsell
balcan
lin
eds
advances
in
neural
information
processing
systems
33
annual
conference
on
neural
information
processing
systems
2020
neurips
2020
december
12
2020
virtual
https://proceedings.neurips.cc/paper/
2020
hash
d6d231705f96d5a3aeb3a76402e49a3-abstract
html
zhang
bellamy
varshney
2020c
joint
optimization
of
ai
fairness
and
utility
human-centered
approach
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
association
for
computing
machinery
new
york
aies
20
pp
400
406
https://doi.org/10.1145/3375627.3375862
zhao
gordon
2019
inherent
tradeoffs
in
learning
fair
representations
in
wallach
larochelle
beygelzimer
alché-buc
fox
garnett
eds
advances
in
neural
information
processing
systems
vol
32
curran
associates
inc
pp
15675
15685
https://proceedings.neurips.cc/paper/2019/
file
b4189d9de0fb2b9cce090bd1a15e3420-paper
pdf
zhao
wang
yatskar
ordonez
chang
kw
2017
men
also
like
shopping
reducing
gender
bias
amplification
using
corpus-level
constraints
in
proceedings
of
the
2017
conference
on
empiri
cal
methods
in
natural
language
processing
association
for
computational
linguistics
copenhagen
denmark
pp
2979
2989
https://doi.org/10.18653/v1/d17-1323,
https://www.aclweb.org/anthology/
d17-1323
zhao
wang
yatskar
ordonez
chang
kw
2018
gender
bias
in
coreference
resolution
evaluation
and
debiasing
methods
in
proceedings
of
the
2018
conference
of
the
north
american
chapter
of
123
2152
fabris
et
al
the
association
for
computational
linguistics
human
language
technologies
volume
short
papers
association
for
computational
linguistics
new
orleans
pp
15
20
https://doi.org/10.18653/v1/n18-
2003
https://www.aclweb.org/anthology/n18-2003
zhao
xiao
gan
zhang
xia
st
2020a
maintaining
discrimination
and
fairness
in
class
incre
mental
learning
in
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
zhao
li
li
chen
2020b
fair
meta-learning
for
few-shot
classification
in
2020
ieee
inter
national
conference
on
knowledge
graph
ickg
pp
275
282
https://doi.org/10.1109/icbk50248.
2020.00047
zhao
coston
adel
gordon
gj
2020c
conditional
learning
of
fair
representations
in
international
conference
on
learning
representations
https://openreview.net/forum?id=hkekl0nfpr
zhao
kong
fowlkes
2021
camera
pose
matters
improving
depth
prediction
by
mitigating
pose
distribution
bias
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recog
nition
cvpr
pp
15759
15768
zheng
dave
mishra
kumar
2018
fairness
in
reciprocal
recommendations
speed-dating
study
in
adjunct
publication
of
the
26th
conference
on
user
modeling
adaptation
and
personalization
association
for
computing
machinery
new
york
umap
18
pp
29
34
https://doi.org/10.1145/
3213586.3226207
zhong
deng
wang
hu
peng
tao
huang
2019
unequal-training
for
deep
face
recognition
with
long-tailed
noisy
data
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
zhou
lapedriza
khosla
oliva
torralba
2018
places
10
million
image
database
for
scene
recognition
ieee
trans
pattern
anal
mach
intell
40
1452
1464
https://doi.org/10.1109/tpami.
2017.2723009
zhu
kiros
zemel
salakhutdinov
urtasun
torralba
fidler
2015
aligning
books
and
movies
towards
story-like
visual
explanations
by
watching
movies
and
reading
books
arxiv
1506.06724
zhu
wang
zhang
caverlee
2018
fairness-aware
recommendation
of
information
curators
recsys
2018
workshop
workshop
on
responsible
recommendation
fat
rec
arxiv
1809.03040
žliobaité
2015
on
the
relation
between
accuracy
and
fairness
in
binary
classification
icml
2015
work
shop
fairness
accountability
and
transparency
in
machine
learning
fat
ml
arxiv
1505.05723
žliobaité
kamiran
calders
2011
handling
conditional
discrimination
in
2011
ieee
11th
interna
tional
conference
on
data
mining
pp
992
1001
https://doi.org/10.1109/icdm.2011.72
publisher
note
springer
nature
remains
neutral
with
regard
to
jurisdictional
claims
in
published
maps
and
institutional
affiliations
123