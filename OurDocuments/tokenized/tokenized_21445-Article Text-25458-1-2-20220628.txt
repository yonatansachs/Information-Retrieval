the
thirty-sixth
aaai
conference
on
artificial
intelligence
aaai-22
has
ceo
gender
bias
really
been
fixed
adversarial
attacking
and
improving
gender
fairness
in
image
search
yunhe
feng
chirag
shah
information
school
university
of
washington
yunhe@uw.edu
chirags@uw.edu
abstract
gender
bias
is
one
of
the
most
common
and
well-studied
demographic
biases
in
information
retrieval
and
in
general
in
ai
systems
after
discovering
and
reporting
that
gender
bias
for
certain
professions
could
change
searchers
worldviews
mainstreaming
image
search
engines
such
as
google
quickly
took
action
to
correct
and
fix
such
bias
however
given
the
nature
of
these
systems
viz
being
opaque
it
is
unclear
if
they
addressed
unequal
gender
representation
and
gender
stereotypes
in
image
search
results
systematically
and
in
sustainable
way
in
this
paper
we
propose
adversarial
attack
queries
composed
of
professions
and
countries
ceo
united
states
to
investigate
whether
gender
bias
is
thoroughly
mitigated
by
image
search
engines
our
experiments
on
google
baidu
naver
and
yandex
image
search
show
that
the
proposed
attack
can
trigger
high
levels
of
gender
bias
in
image
search
results
very
effectively
to
defend
against
such
attacks
and
mitigate
gender
bias
we
design
and
implement
three
novel
re-ranking
algorithms
epsilon-greedy
algorithm
relevance-aware
swapping
algorithm
and
fairness-greedy
algorithm
to
re-rank
returned
images
for
given
image
queries
experiments
on
both
simulated
three
typical
gender
distributions
and
real-world
datasets
demonstrate
the
proposed
algorithms
can
mitigate
gender
bias
effectively
introduction
the
web
biggest
image
search
engines
such
as
google
and
bing
provide
an
important
information-seeking
interface
for
people
to
explore
the
world
according
to
internet
live
stats1
google
processes
more
than
3.5
billion
queries
per
day
and
1.2
trillion
searches
per
year
google
image
searches
account
for
22.6
of
all
searches2
given
the
volume
and
importance
in
our
daily
lives
image
search
results
can
significantly
influence
how
people
perceive
and
view
the
world
images
are
often
more
than
useful
objects
of
information
they
provide
visual
representation
of
phenomenon
concept
and
perceived
reality
of
the
world
around
us
given
this
it
is
not
sufficient
to
assess
the
quality
of
image
search
results
using
relevance
metrics
we
also
need
to
consider
how
this
visual
information
carries
various
perceptions
and
prejudice
for
example
lam
et
al
2018
showed
that
searching
for
ceo
in
google
image
search
resulted
in
predominantly
white
males
while
the
same
query
in
web
search
provides
diverse
set
copyright
2022
association
for
the
advancement
of
artificial
intelligence
www.aaai.org).
all
rights
reserved
https://www.internetlivestats.com/google-search-statistics/
https://firstsiteguide.com/google-search-stats/
11882
of
information
objects
definition
wikipedia
article
questions
and
answers
image
search
results
are
mono-media
and
appeal
to
one
visual
perceptions
which
can
more
quickly
affect
their
worldview
hibbing
and
rankin-erickson
2003
if
these
results
carry
biases
such
as
those
shown
by
lam
et
al
2018
and
kay
matuszek
and
munson
2015
they
are
much
easier
to
perpetuate
than
regular
web
search
results
therefore
while
evaluating
image
search
results
we
need
to
look
beyond
their
relevance
we
must
also
look
at
inherent
disparity
and
biases
carried
out
in
them
among
different
types
of
image
biases
gender
bias
is
one
of
the
most
common
and
well-studied
demographic
biases
not
surprisingly
this
also
gets
scrutiny
and
attention
from
scholars
and
media
that
often
makes
the
service
providers
take
immediate
actions
to
fix
such
biases
in
an
ad
hoc
manner
for
example
after
the
work
described
before
kay
matuszek
and
munson
2015
received
lot
of
attention
google
shifted
the
gender
distributions
in
image
search
results
for
ceo
and
some
other
occupations
for
instance
the
famous
query
of
ceo
in
image
search
has
been
fixed
for
long
time
see
figure
and
figure
however
mozilla
recent
internet
health
report
mozilla
2021
points
out
that
the
default
internet
user
is
still
viewed
as
white
male
and
cisgender
and
big
tech
has
not
done
enough
to
fix
it
griffith
2021
but
that
is
only
one
way
the
gender
bias
problem
is
not
fixed
in
this
paper
we
revisit
gender
bias
in
image
search
results
for
professional
occupations
for
some
search
terms
such
as
ceo
gender
fairness
is
observed
in
image
search
results
but
have
image
search
engines
mitigated
gender
biases
in
search
results
systematically
to
further
illustrate
this
research
question
we
present
the
relevant
adversarial
attack
queries
of
the
ceo
in
google
and
bing
as
shown
in
figure
both
google
and
bing
image
search
engines
have
already
fixed
the
gender
stereotypes
for
the
occupation
of
ceo
however
such
gender
bias
resurfaces
when
appending
the
country
names
such
as
united
states
and
uk
to
the
original
keyword
of
ceo
this
finding
inspires
us
to
dive
into
the
exploration
of
gender
fairness
in
image
search
engines
to
reveal
superficial
bias
mitigation
ten
occupations
also
investigated
by
kay
matuszek
and
munson
2015
seven
years
ago
are
chosen
as
image
search
terms
in
our
study
we
design
two
search
keywords
for
each
occupation
the
original
occupation
name
and
the
adversarial
attack
keyword
that
consists
of
occupation
name
and
the
country
name
of
united
states
the
latter
aims
to
trigger
gender
bias
in
image
search
results
for
each
keyword
we
retrieve
the
top
200
images
if
available
from
four
widely
ceo
google
ceo
google
ceo
uk
google
ceo
bing
ceo
bing
ceo
uk
bing
figure
image
search
results
of
ceo
ceo
united
states
almost
all
males
and
ceo
uk
all
males
by
google
and
bing
used
image
search
engines
namely
google
from
usa
baidu
from
china
naver
from
south
korea
and
yandex
from
russia
in
total
we
collected
more
than
18
000
images
when
image
retrieval
is
complete
we
attempt
to
leverage
image
gender
detection
apis
to
recognize
the
genders
of
people
in
these
images
to
be
specific
five
popular
gender
detection
apis
including
amazon
rekognition
luxand
face
microsoft
azure
and
facebook
deepface
are
selected
to
calculate
gender
distributions
of
returned
occupation
images
we
compared
the
gender
labels
detected
by
apis
with
human
annotations
and
found
that
only
amazon
rekognition
apis
were
acceptable
but
still
failed
to
handle
the
images
with
low
ratio
of
detected
faces
therefore
we
propose
hybrid
approach
to
detect
face
genders
in
search
images
by
combining
amazon
rekognition
results
and
crowdsourced
human
annotations
through
amazon
mechanical
turk
to
mitigate
gender
bias
we
present
three
generalized
reranking
algorithms
including
the
epsilon-greedy
algorithm
relevance-aware
swapping
algorithm
and
fairness-greedy
algorithm
to
balance
the
trade-off
between
gender
fairness
and
image
relevance
evaluations
of
the
proposed
gender
bias
mitigation
algorithms
on
both
simulated
and
real-world
datasets
demonstrated
that
it
is
feasible
and
advisable
to
address
bias
in
image
search
and
perhaps
in
other
types
of
search
as
well
in
systematic
and
more
meaningful
way
than
doing
individual
query
fixes
in
an
ad
hoc
manner
our
contributions
are
summarized
as
follows
we
design
adversarial
attacks
with
regard
to
gender
bias
in
image
search
and
determine
that
gender
bias
is
not
fixed
systematically
by
search
engines
cirf
an
open-sourced
cross-search-engine
image
retrieval
framework
to
collect
images
from
multiple
search
engines
for
given
search
terms
is
developed
we
find
image
gender
detection
apis
cannot
always
perform
well
on
search
images
in
the
wild
so
hybrid
approach
combining
automatic
gender
detection
and
manual
annotation
is
presented
we
propose
and
validate
three
re-ranking
algorithms
to
mitigate
gender
bias
in
image
search
results
related
work
this
section
presents
the
importance
of
gender
fairness
in
image
search
results
summarizes
the
gender
bias-related
research
findings
from
multiple
perspectives
discusses
the
existing
approaches
to
mitigate
image
gender
biases
and
highlights
the
difference
between
existing
works
and
ours
the
gender
fairness
or
biases
demonstrated
in
image
search
results
affect
people
perceptions
and
views
significantly
ellemers
2018
metaxa
et
al
2021
kay
matuszek
11883
and
munson
2015
are
one
of
the
first
to
investigate
gender
biases
in
professional
occupation
image
search
results
they
reported
that
such
image
search
results
for
occupations
slightly
exaggerate
gender
stereotypes
and
people
thought
image
search
results
were
better
if
they
agreed
with
the
stereotype
more
importantly
this
research
work
pointed
out
that
the
biased
representation
of
gender
in
image
search
results
could
shift
people
perceptions
about
real-world
distributions
otterbacher
bates
and
clough
2017
proposed
trait
adjective
checklist
inspired
method
further
to
identify
the
existence
of
gender
biases
in
image
search
they
found
that
images
of
men
were
more
often
retrieved
for
agentic
traits
whereas
warm
traits
were
demonstrated
in
photos
of
women
in
addition
photos
of
stereotype-incongruent
individuals
exhibited
backlash
effect
competent
women
were
less
likely
to
be
portrayed
positively
otterbacher
et
al
2018
measured
the
user
perception
of
gender
bias
in
image
search
from
the
perspective
of
sexism
and
found
search
engine
users
who
were
more
sexist
were
less
likely
to
perceive
gender
biases
there
also
exist
many
research
studies
exploring
gender
bias
in
different
types
of
images
by
detecting
gender
labels
of
the
photographs
of
members
of
congress
and
their
tweeted
images
schwemmer
et
al
2020
concluded
google
cloud
vision
gcv
could
produce
correct
and
biased
labels
at
the
same
time
because
subset
of
many
possible
true
labels
was
selectively
reported
wijnhoven
2021
found
gender
bias
toward
stereotypically
female
jobs
for
women
but
also
for
men
when
searching
jobs
via
google
search
engine
by
examining
four
professions
across
digital
platforms
singh
et
al
2020
concluded
gender
stereotypes
were
most
likely
to
be
challenged
when
users
acted
directly
to
create
and
curate
content
and
algorithmic
approaches
for
content
curation
showed
little
inclination
towards
breaking
stereotypes
makhortykh
urman
and
ulloa
2021
conducted
cross-engine
comparison
of
racial
and
gender
bias
in
the
visual
representation
of
the
search
term
artificial
intelligence
and
gender
representation
of
ai
is
more
diverse
than
its
racial
representation
hashemi
and
hall
2020
reported
no
gender
bias
was
identified
when
detecting
criminal
tendency
based
on
mugshot
images
of
arrested
individuals
in
the
last
couple
of
years
many
approaches
have
been
proposed
to
detect
and
mitigate
gender
bias
in
images
for
training
deep
learning
models
wang
et
al
2020
xu
et
al
2020
hwang
et
al
2020
adeli
et
al
2021
for
example
serna
et
al
2021
showed
how
bias
in
face
images
impacted
the
activations
of
gender
detection
models
and
developed
insidebias
to
detect
biased
models
to
reduce
gender
bias
in
deep
image
representations
an
adversarial
method
for
the
removal
of
features
associated
with
protected
variable
gender
from
0.0
0.2
0.4
0.6
0.8
1.0
ratio
of
images
containing
faces
amazon
rekognition
0.0
0.2
0.4
0.6
0.8
1.0
ratio
of
images
containing
faces
engineer
engineer
nurse
nurse
police
officer
police
officer
0.0
0.2
0.4
0.6
0.8
1.0
ratio
of
images
containing
faces
luxand
face
primary
school
teacher
primary
school
teacher
0.0
0.2
0.4
0.6
0.8
1.0
ratio
of
images
containing
faces
microsoft
azure
software
developer
software
developer
normalized
difference
cook
cook
normalized
difference
computer
programmer
computer
programmer
normalized
difference
chief
executive
officer
chief
executive
officer
normalized
difference
normalized
difference
biologist
biologist
truck
driver
truck
driver
0.0
0.2
0.4
0.6
0.8
1.0
ratio
of
images
containing
faces
facebook
deepface
figure
normalized
female
ratio
difference
compared
with
mturk
results
vs
the
ratio
of
detected
faces
in
images
the
intermediate
convolutional
neural
network
based
representations
was
presented
wang
et
al
2019
many
other
image
gender
bias
mitigation
approaches
such
as
posterior
regularization
based
gender
de-biasing
framework
jia
et
al
2020
fairness-aware
disentangling
variational
autoencoder
fd-vae
park
et
al
2021
and
an
adversarial
gender
de-biasing
algorithm
agenda
dhar
et
al
2020
are
also
proposed
besides
some
post-processing
bias
mitigation
methods
such
as
fa
ir
zehlike
et
al
2017
and
multi-task
learning
for
fair
regression
zhao
and
chen
2019
have
been
proposed
our
study
adds
to
the
literature
on
exploring
gender
bias
in
image
search
results
in
the
following
ways
first
similar
to
kay
matuszek
and
munson
2015
we
investigate
the
gender
distribution
in
professional
occupation
image
results
still
we
also
design
an
adversarial
search
attack
by
adding
the
country
information
into
the
occupation
search
terms
we
find
evidence
that
image
search
engines
do
not
fix
the
reported
gender
bias
in
search
results
systematically
second
we
not
only
examine
the
performance
of
five
popular
image
gender
detection
apis
but
also
propose
hybrid
approach
that
combines
automatic
detection
amazon
rekognition
services
and
manual
annotations
amazon
mechanical
turk
to
improve
gender
distribution
estimation
finally
we
develop
three
re-ranking
algorithms
epsilon-greedy
relevanceaware
swapping
and
fairness-greedy
methods
to
mitigate
gender
biases
in
image
search
results
image
retrieval
and
gender
detection
source
lnms
tbm
isch
as
url
template
for
google
image
search
where
keyword
is
the
placeholder
of
search
terms
as
cirf
is
able
to
handle
multiple
search
engines
we
also
design
similar
url
templates
for
popular
search
engines
including
baidu
from
china
naver
from
south
korea
and
yandex
from
russia
the
keyword
can
be
written
in
any
language
because
browsers
will
encode
it
in
the
utf-8
format
data
downloader
we
collect
two
types
of
search
image
data
based
on
the
built
urls
the
web
page
html
file
that
captures
the
layout
and
names
of
images
returned
by
search
engines
ii
individual
image
files
embedded
in
the
image
gallery
cirf
adopts
the
web
framework
selenium
webdriver
to
open
urls
in
chrome
browser
with
incognito
mode
to
display
and
cache
more
search
images
cirf
scrolls
up
and
down
the
web
page
by
sending
page_up
and
page_down
commands
to
the
html
entities
cirf
leverages
pyautogui
cross-platform
gui
automation
module
to
save
the
web
page
html
file
and
all
supplementary
materials
including
images
image
parser
this
component
is
responsible
for
extracting
the
images
and
their
orders
from
the
downloaded
html
file
in
general
three
types
of
images
are
collected
standard
images
base64
encoded
images
and
image
urls
for
the
latter
two
types
of
images
cirf
decodes
them
into
standard
images
and
retrieves
images
via
urls
respectively
when
all
images
are
ready
cirf
renames
them
according
to
their
orders
in
the
html
file
for
further
analysis
gender
detection
in
this
section
we
describe
how
to
build
the
image
search
datasets
examine
the
performance
of
image
based
gender
detection
apis
and
propose
hybrid
approach
to
strike
balance
between
detection
accuracy
and
efficiency
search
image
retrieval
we
propose
and
develop
an
open-sourced
cross-searchengine
image
retrieval
framework
cirf
to
automatically
collect
images
from
multiple
search
engines
for
given
search
terms
cirf
mainly
consists
of
three
components
url
builder
data
downloader
and
image
parser
url
builder
to
enable
automatic
data
download
we
first
construct
image
search
urls
based
on
search-enginespecific
url
templates
for
given
search
terms
for
example
we
use
https://www.google.com/search?q=keyword&
image-based
gender
detection
has
widely
been
adopted
in
diverse
domains
so
many
commercial
and
open-sourced
gender
detection
apis
have
been
developed
and
released
considering
the
scalability
and
efficiency
we
intended
to
rely
on
these
available
tools
to
label
search
images
automatically
to
evaluate
their
performance
we
randomly
selected
and
searched
ten
occupations
and
their
corresponding
adversarial
attack
search
terms
appending
united
states
in
google
image
search
then
we
conducted
an
irb-approved
user
study
to
recruit
participants
from
amazon
mechanical
turk
mturk
to
build
the
ground
truth
of
genders
we
paid
each
participant
0.5
for
annotating
50
images
and
each
image
was
assigned
to
three
workers
five
popular
gender
detection
apis
including
amazon
rekognition
luxand
face
microsoft
azure
and
facebook
deepface
were
chosen
to
https://github.com/yunhefeng/cirf
11884
https://www.selenium.dev/documentation/webdriver/
https://pyautogui.readthedocs.io/
calculate
gender
distributions
of
the
top
200
google
search
images
for
each
given
search
term
figure
demonstrates
the
normalized
female
ratio
difference
between
mturk
results
and
face
gender
detection
apis
in
general
amazon
rekognition
outperforms
the
rest
of
apis
in
terms
of
face
detection
ratios
see
x-axis
and
the
female
ratio
errors
see
y-axis
when
the
face
detection
ratio
is
above
0.5
the
normalized
difference
of
amazon
rekognition
is
below
15
therefore
amazon
rekognition
was
chosen
to
identify
the
genders
of
people
in
images
thus
we
propose
two-step
hybrid
method
to
annotate
image
gender
labels
use
amazon
rekognition
to
detect
image
genders
for
search
terms
that
suffer
from
low
face
detection
ratio
below
0.5
we
still
rely
on
mturk
to
manually
label
them
exploring
unsystematic
gender
bias
fixing
we
investigate
whether
gender
bias
in
image
search
results
is
systematically
fixed
by
designing
adversarial
search
attacks
and
measuring
the
degree
of
gender
fairness
adversarial
search
attack
design
as
mentioned
before
we
are
motivated
to
investigate
whether
image
search
engines
fix
gender
bias
in
different
occupation
queries
systematically
therefore
we
follow
the
occupation
list
on
bureau
of
labor
statistics
and
choose
occupation
names
as
the
baseline
search
keywords
when
constructing
adversarial
searches
we
append
the
country
name
of
united
states
to
each
occupation
name
to
build
the
attacking
search
term
if
both
baseline
and
attacking
searches
demonstrate
no
difference
with
the
gender
distribution
ground
truth
for
one
occupation
we
think
search
engines
mitigate
gender
bias
systematically
for
that
occupation
otherwise
we
argue
that
such
fixes
and
mitigation
of
gender
bias
are
just
hit-or-miss
as
the
previously
existing
gender
biases
in
image
search
queries
such
as
ceo
drew
huge
attention
of
the
public
and
academia
mainstream
search
engines
had
already
mitigated
such
biases
accordingly
however
our
analytics
show
that
gender
biases
crossing
over
all
occupations
are
not
fixed
in
systematic
way
gender
bias
measurement
it
is
very
intuitive
and
straightforward
to
compare
the
normalized
difference
between
gender
probability
distribution
in
image
search
results
and
the
ground
truth
gender
probability
for
each
occupation
for
top
images
returned
by
search
engines
we
calculate
the
kullback-leibler
divergence
dkl
between
these
images
and
the
ground
truth
the
average
kullback-leibler
divergence
is
used
to
represent
the
existing
bias
pn
dkl
epsilon-greedy
algorithm
inspired
by
the
exploitation
and
exploration
trade-off
idea
in
reinforcement
learning
berry
and
fristedt
1985
sutton
and
barto
2018
and
re-ranking
gao
and
shah
2020
we
propose
the
epsilon-greedy
re-ranking
algorithm
which
swaps
items
in
the
image
rank
list
with
controllable
degree
of
randomness
the
randomized
swapping
breaks
original
gender
distributions
and
might
improve
fairness
especially
when
items
with
the
same
attribution
values
are
gathered
together
densely
male
ceo
images
fully
occupy
the
top
20
ceo
image
search
results
this
algorithm
has
two
main
advantages
simplicity
and
generalizability
it
is
straightforward
and
simple
to
randomly
shuffle
items
without
considering
other
factors
in
addition
no
prior
knowledge
such
as
the
optimal
gender
distribution
is
required
to
apply
this
algorithm
in
the
proposed
epsilon-greedy
algorithm
the
randomness
is
specified
by
the
parameter
representing
the
probability
of
swapping
two
items
each
item
has
probability
of
to
exchange
positions
with
random
item
that
follows
it
larger
introduces
more
randomness
leading
to
re-ranked
list
that
is
more
different
from
the
original
list
relevance-aware
swapping
algorithm
normally
the
items
with
large
relevance
weights
are
ranked
at
the
top
of
search
engines
returned
image
list
if
unrelated
or
less
related
images
are
ranked
high
it
harms
user
experience
and
utility
the
epsilon-greedy
algorithm
is
very
straightforward
and
simple
but
it
ignores
the
relevance
of
search
items
during
re-ranking
therefore
we
propose
the
relevance-aware
swapping
algorithm
to
consider
both
the
randomness
and
relevance
weight
of
image
items
to
re-rank
the
image
list
to
keep
the
utility
of
the
re-ranked
list
an
image
with
larger
relevance
weight
is
less
likely
to
be
swapped
with
an
image
item
that
follows
it
relevance
weight
modeling
we
grade
an
image
relevance
weight
based
on
its
index
in
the
image
list
returned
by
search
engines
similar
to
the
mean
reciprocal
rank
mrr
voorhees
et
al
1999
the
relevance
weight
of
an
image
with
rank
index
can
be
modeled
by
its
reciprocal
rank
1i
however
such
relevance
weight
decreases
too
fast
with
the
growth
of
the
rank
index
instead
we
model
the
relevance
weight
distribution
in
linear
manner
suppose
we
have
an
image
list
containing
images
the
linear
relevance
weight
of
the
ith
image
is
estimated
as
inspired
by
the
discounted
cumulative
gain
dcg
järvelin
and
kekäläinen
2002
we
further
introduce
discount
factor
of
log2
to
smooth
the
decay
of
relevance
weights
of
bottom
images
in
finally
the
relevance
weight
of
image
li
is
expressed
as
wi
log2
algorithms
to
mitigate
gender
bias
we
propose
three
interpretable
and
lightweight
re-ranking
algorithms
to
mitigate
gender
biases
in
image
search
results
https://www.bls.gov/oes/current/oes_nat.htm#00-0000
11885
swapping
probability
the
swapping
probability
of
image
li
is
determined
by
its
relevance
weight
wi
to
ensure
that
the
image
with
high
relevance
weight
is
less
likely
to
be
swapped
we
can
use
wi
to
represent
the
swapping
algorithm
relevance-aware
swapping
algorithm
algorithm
fairness-greedy
algorithm
input
the
original
image
list
the
sensitivity
of
swapping
two
items
output
the
re-ranked
image
list
initialize
as
empty
for
do
input
the
original
image
list
the
ground
truth
of
gender
distribution
the
set
of
gender
features
output
the
re-ranked
image
list
l1
initialize
as
l1
for
do
gender
distribution
on
gr1
gri
lag
alse
xmin
one
most
underrep
feat
set
checked
features
as
while
lag
alse
and
do
10
dmin
11
add
xmin
to
update
select
most
underrep
feature
12
for
do
13
px
tx
diff
in
feat
14
if
dmin
then
15
xmin
underrep
feat
16
end
find
1st
item
underrep
feat
17
for
do
18
if
glj
xmin
then
find
an
item
19
temp
lj
save
lj
20
for
do
21
lk
lk
move
down
22
end
23
li
temp
update
li
24
append
li
to
update
25
lag
rue
find
the
item
26
break
27
end
28
end
29
end
30
return
10
11
12
13
14
15
16
17
wi
log
relevance
weight
random
number
between
and
if
wi
then
swap
items
temp
li
random
number
between
and
li
lj
lj
temp
append
li
to
add
swapped
item
else
keep
the
original
item
append
li
to
add
unswapped
item
end
end
return
probability
for
image
li
in
addition
we
design
coefficient
to
further
control
the
swapping
sensitivity
and
the
swapping
probability
of
image
li
is
expressed
as
wi
the
detailed
implementation
is
illustrated
in
algorithm
fairness-greedy
algorithm
considering
more
than
90
of
users
do
not
go
past
the
first
page
of
the
google
search
results
sharma
et
al
2019
and
the
first
three
items
displayed
in
amazon
search
results
account
for
64
of
all
clicks
baker
2018
we
think
it
is
of
great
significance
to
ensure
gender
fairness
in
images
ranked
top
in
search
results
therefore
we
propose
the
fairness-greedy
algorithm
to
guarantee
gender
fairness
in
the
first
few
pages
with
high
priority
accordingly
the
gender
distribution
of
images
displayed
on
the
last
pages
to
which
users
pay
lesser
attention
is
given
less
consideration
the
main
idea
of
the
fairness-greedy
algorithm
is
to
narrow
the
difference
in
gender
distributions
between
top-ranked
images
and
the
ground
truth
by
moving
images
up
and
down
unlike
epsilon-greedy
and
relevance-aware
swapping
algorithms
the
fairness-greedy
algorithm
needs
to
know
the
ground
truth
the
gender
distribution
of
search
terms
in
real
life
and
list
of
gender
labels
for
returned
images
in
search
engines
the
ground
truth
of
searched
profession
is
usually
available
through
open
data
such
as
census
data
the
image
gender
labels
which
can
be
estimated
by
available
computer
vision
based
gender
apis
are
required
to
calculate
the
gender
distribution
of
top-ranked
images
the
detailed
implementation
of
the
fairness-greedy
algorithm
is
shown
in
algorithm
to
make
our
algorithm
more
general
we
use
to
represent
all
involved
features
such
as
gender
features
of
female
and
male
note
that
the
fairnessgreedy
algorithm
is
capable
of
handling
more
than
two
different
features
we
keep
the
first
item
in
the
original
rank
list
as
it
is
at
the
beginning
see
line
starting
from
the
second
item
we
calculate
the
gender
distribution
over
the
latest
re-ranked
list
px
represents
the
ratio
of
feature
11886
and
tx
is
the
ground
truth
of
feature
in
the
real
world
next
we
take
two-step
re-ranking
method
to
mitigate
feature
biases
step
identify
the
most
underrepresented
feature
xmin
by
comparing
the
difference
between
px
and
tx
see
line
12
16
step
find
the
first
item
lj
with
feature
of
xmin
glj
xmin
in
li
and
move
it
forward
as
the
new
li
see
line
17
27
if
such
an
item
lj
does
not
exist
we
exclude
the
feature
xmin
by
adding
it
into
the
checked
feature
set
and
continue
the
re-ranking
see
line
11
experiments
and
evaluation
this
section
presents
the
evaluations
of
the
three
proposed
bias
mitigation
approaches
on
synthetic
and
real
datasets
evaluation
on
synthetic
data
we
generated
three
synthetic
datasets
with
different
gender
distribution
patterns
uniform
dataset
female
and
male
items
are
distributed
evenly
across
the
whole
list
heavyheaded
dataset
female
items
are
aggregated
at
the
top
of
the
list
heavy-tailed
dataset
female
items
are
aggregated
at
the
bottom
of
the
list
for
these
experiments
we
created
list
with
length
of
200
and
set
the
female
ratio
as
0.5
100
items
are
labeled
as
female
on
the
heavy-headed
and
heavy-tailed
datasets
the
100
female
items
are
distributed
at
computer
programmer
0.4
0.25
50
100
top
150
200
nurse
nurse
truck
driver
1.0
0.8
0.8
0.8
0.6
0.4
0.2
0.0
google
and
50
100
top
150
200
female
ratio
0.50
engineer
1.0
0.6
0.4
0.2
0.0
baidu
and
50
100
top
150
200
naver
and
truck
driver
0.6
0.4
0.2
0.0
50
100
top
150
200
yandex
and
0.2
0.1
0.0
0.1
0.2
0.3
50
100
top
biologist
150
google
naver
baidu
yandex
200
chief
executive
computer
officer
programmer
cook
engineer
nurse
police
officer
primary
school
teacher
software
developer
truck
driver
difference
in
the
female
ratio
between
image
search
results
united
states
in
search
terms
and
the
ground
truth
figure
gender
distributions
of
ten
occupations
in
google
baidu
naver
and
yandex
image
search
engines
evaluation
on
real-world
data
we
conducted
adversarial
attacks
on
gender
fairness
in
four
major
image
search
engines
where
various
gender
distributions
are
observed
for
the
same
search
term
we
also
found
that
image
search
engines
are
sensitive
to
the
search
term
variants
that
convey
the
same
semantics
finally
we
evaluated
the
performances
of
the
three
proposed
bias
mitigated
algorithms
on
the
collected
dataset
this
subsection
presents
the
details
of
these
evaluations
gender
bias
in
cross-culture
search
engines
besides
the
google
image
search
engine
we
evaluated
the
same
occupation
terms
in
baidu
from
china
naver
from
south
korea
and
yandex
from
russia
using
the
hybrid
image
gender
detection
method
see
the
subsection
of
gender
detection
similar
to
google
all
the
above
three
image
search
engines
are
deemed
to
have
gender
bias
with
search
terms
that
include
united
states
in
them
see
figure
where
positive
value
indicates
over-representing
females
and
neg11887
ative
value
indicates
under-representing
females
the
effectiveness
of
the
proposed
adversarial
attack
approach
in
cross-culture
search
engines
is
demonstrated
in
figure
to
figure
where
the
difference
in
female
ratios
between
search
terms
with
and
without
united
states
is
evident
especially
among
the
top
50
items
we
can
also
observe
that
distinct
occupations
demonstrate
different
gender
distribution
patterns
in
the
same
search
engine
and
the
same
occupation
may
demonstrate
different
patterns
across
search
engines
these
findings
led
us
to
consider
that
such
gender
bias
exists
across
cultures
and
needs
attention
globally
diff
in
ratio
the
top
50
and
the
bottom
50
on
the
list
respectively
we
set
the
ground
truth
of
gender
distribution
as
emale
0.5
male
0.5
the
bias
mitigation
performance
of
the
three
proposed
algorithms
with
1000
runs
and
widely
used
fair
top-k
ranking
algorithm
named
fa
ir
zehlike
et
al
2017
is
shown
in
table
recall
that
we
used
equation
to
measure
the
bias
as
expected
neither
epsilon-greedy
nor
relevanceaware
swapping
algorithms
can
mitigate
bias
by
introducing
randomness
on
the
uniform
dataset
because
the
original
list
has
already
been
randomized
entirely
for
the
same
reason
fa
ir
also
fails
to
improve
the
fairness
of
the
uniform
dataset
on
heavy-headed
and
heavy-tailed
datasets
if
more
randomness
is
introduced
larger
in
epsilon-greedy
algorithm
and
larger
in
relevance-aware
swapping
algorithm
the
bias
is
more
mitigated
fa
ir
also
reduces
gender
bias
significantly
the
fairness-greedy
algorithm
performs
best
on
all
three
datasets
female
ratio
0.0
0.75
0.00
0.6
0.2
female
ratio
female
ratio
0.8
diff
btw
ground
truth
female
ratio
1.0
engineer
1.0
female
ratio
computer
programmer
1.00
0.3
0.2
0.1
ceo
chief
executive
officer
ceo
chief
executive
officer
50
100
top
150
200
0.0
0.2
ceo
chief
executive
officer
ceo
chief
executive
officer
50
100
top
150
200
ceo
vs
chief
exec
officer
difference
in
female
ratio
figure
sensitive
to
variant
search
terms
sensitive
to
variant
search
terms
another
evidence
of
the
unsystematic
mitigation
of
gender
bias
is
that
image
search
engines
are
sensitive
to
variant
search
terms
as
shown
in
figure
the
female
ratios
of
image
search
results
between
ceo
and
chief
executive
officer
are
significantly
different
especially
when
search
terms
include
united
states
however
with
the
increase
of
top
the
difference
in
the
female
ratio
demonstrates
trend
to
become
stable
and
small
especially
for
search
terms
containing
united
states
see
figure
gender
bias
mitigation
we
deployed
the
three
proposed
algorithms
on
the
image
search
datasets
collected
from
google
baidu
naver
and
yandex
to
illustrate
how
the
proposed
algorithms
work
we
take
the
epsilon-greedy
algorithm
as
an
example
to
show
the
dynamic
fairness
achievements
on
biologist
datasets
as
shown
in
figure
as
increases
original
uniform
heavy-headed
heavy-tailed
0.066
2.046
2.046
relevance-aware
swapping
epsilon-greedy
0.2
0.4
0.6
0.2
0.4
fair-greedy
0.6
fa
ir
0.5
0.1
0.059
0.019
0.055
0.025
0.052
0.028
0.065
0.013
0.064
0.018
0.063
0.022
0.426
0.189
0.203
0.107
0.105
0.063
0.553
0.222
0.316
0.143
0.198
0.095
0.423
0.199
0.194
0.096
0.102
0.061
0.548
0.219
0.312
0.136
0.198
0.098
0.020
0.020
0.020
0.066
0.142
0.142
table
bias
mitigation
performance
on
synthetic
datasets
the
bias
value
in
the
table
is
measured
by
equation
original
biologist
ceo
comp
programmer
cook
engineer
nurse
police
officer
prim
school
teacher
software
developer
truck
driver
0.138
0.172
0.114
0.149
0.04
0.115
0.049
0.135
0.189
0.056
relevance-aware
swapping
epsilon-greedy
0.2
0.4
0.6
0.2
0.4
0.6
0.102
0.044
0.175
0.055
0.119
0.027
0.131
0.051
0.044
0.011
0.119
0.011
0.053
0.015
0.136
0.007
0.193
0.066
0.067
0.044
0.087
0.046
0.160
0.082
0.120
0.030
0.109
0.064
0.053
0.019
0.119
0.015
0.054
0.016
0.136
0.010
0.171
0.078
0.088
0.062
0.071
0.049
0.144
0.087
0.135
0.062
0.101
0.070
0.063
0.036
0.128
0.023
0.055
0.018
0.137
0.011
0.156
0.082
0.088
0.067
0.128
0.032
0.169
0.048
0.113
0.021
0.148
0.049
0.045
0.022
0.118
0.009
0.048
0.008
0.137
0.006
0.193
0.035
0.070
0.044
0.108
0.046
0.167
0.052
0.114
0.030
0.133
0.052
0.048
0.016
0.121
0.015
0.047
0.011
0.136
0.008
0.180
0.061
0.074
0.048
0.114
0.046
0.160
0.054
0.120
0.035
0.128
0.064
0.052
0.022
0.124
0.017
0.046
0.013
0.137
0.009
0.184
0.067
0.087
0.064
fair-greedy
fa
ir
0.5
0.1
0.018
0.021
0.034
0.017
0.02
0.066
0.015
0.1
0.055
0.007
0.072
0.084
0.071
0.102
0.027
0.076
0.088
0.085
0.094
0.02
table
bias
mitigation
performance
on
google
occupation
image
datasets
the
bias
value
is
measured
by
equation
more
randomness
is
introduced
the
gender
distribution
of
the
re-ranked
list
becomes
more
likely
to
be
different
from
the
original
one
see
the
shaded
range
implying
more
fairness
will
be
achieved
if
the
raw
image
search
list
suffers
from
severe
gender
bias
with
the
increase
of
top
the
female
ratio
becomes
more
stable
and
finally
converges
when
top
reaches
200
0.0
100
top
200
2.5
0.0
0.0
yandex
orig
yandex
greedy
1.0
0.5
epsilon
0.2
5.0
naver
orig
naver
greedy
1.0
0.5
female
ratio
7.5
1.0
female
ratio
0.0
baidu
orig
baidu
greedy
female
ratio
google
orig
google
greedy
0.5
100
top
200
epsilon
0.4
0.0
100
top
200
epsilon
0.6
figure
performance
of
the
epsilon-greedy
algorithm
on
google
baidu
naver
and
yandex
biologist
datasets
similar
to
the
evaluations
on
synthetic
datasets
we
explored
the
performance
of
our
algorithms
and
fa
ir
zehlike
200
et
al
2017
on
100
real-world
datasets
table
illustrates
the
genk
der
mitigation
top
performance
of
each
algorithm
on
10
google
image
datasets
which
were
collected
with
the
search
keywords
of
10
occupations
plus
united
states
when
the
original
bias
is
larger
than
0.1
biologist
united
states
gender
bias
normally
decreases
along
with
the
increase
of
in
the
epsilon-greedy
algorithm
and
in
the
relevance-aware
swapping
algorithm
however
if
the
original
bias
is
small
engineer
united
states
epsilon-greedy
algorithm
and
relevance-aware
swapping
algorithm
cannot
mitigate
gender
bias
we
can
observe
that
the
fairness-greedy
algorithm
consistently
achieves
low
bias
because
it
gives
the
highest
priority
to
fairness
during
re-ranking
fa
ir
also
demonstrates
stable
and
good
performance
regardless
of
the
original
bias
in
addition
comparing
the
result
columns
of
original
and
11888
fairness-greedy
in
table
can
tell
the
degree
of
gender
bias
hidden
in
the
original
image
list
conclusion
and
limitation
bias
in
ai
systems
has
become
an
increasingly
prevalent
and
complex
issue
to
address
often
the
system
developers
fix
problem
by
creating
superfluous
solution
without
addressing
the
underlying
issue
in
this
paper
we
used
an
adversarial
query
attack
method
by
appending
additional
information
like
country
names
to
trigger
potential
gender
bias
in
image
search
an
open-sourced
cross-search-engine
image
retrieval
framework
cirf
was
developed
to
retrieve
data
from
google
baidu
naver
and
yandex
to
recognize
the
gender
of
people
in
photos
five
popular
image
gender
detection
apis
namely
amazon
rekognition
luxand
face
microsoft
azure
and
facebook
deepface
were
evaluated
although
these
apis
are
endorsed
by
ai
giants
they
could
not
always
handle
images
in
the
wild
with
high
accuracy
therefore
hybrid
method
combining
automatic
gender
detection
apis
and
crowdsourced
human
workforce
was
designed
to
label
image
genders
to
mitigate
gender
bias
we
proposed
three
lightweight
and
interpretable
re-ranking
algorithms
and
evaluated
their
performance
on
both
synthetic
and
real-world
datasets
our
results
demonstrated
that
it
is
possible
and
advisable
to
address
bias
in
image
search
and
perhaps
in
other
types
of
search
as
well
in
systematic
sustainable
and
more
meaningful
way
than
doing
individual
query
fixes
in
an
ad
hoc
fashion
in
this
paper
we
treated
gender
as
binary
attribute
inferred
by
either
gender
apis
or
humans
however
we
acknowledge
that
gender
is
different
from
biological
sex
and
is
non-binary
it
is
also
something
that
third-party
be
it
human
or
program
is
not
always
in
position
to
detect
genders
correctly
our
reliance
on
the
binary
gender
norm
and
third-party
annotation
is
limitation
of
this
research
references
adeli
zhao
pfefferbaum
sullivan
fei-fei
niebles
and
pohl
2021
representation
learning
with
statistical
independence
to
mitigate
bias
in
proceedings
of
the
ieee
cvf
winter
conference
on
applications
of
computer
vision
2513
2523
baker
2018
amazon
search
engine
ranking
algorithm
what
marketers
need
to
know
https://www.searchenginejournal.com/amazon-searchengine-ranking-algorithm-explained/265173/.
accessed
2022
02
10
berry
and
fristedt
1985
bandit
problems
sequential
allocation
of
experiments
monographs
on
statistics
and
applied
probability
london
chapman
and
hall
71
87
dhar
gleason
souri
castillo
and
chellappa
2020
towards
gender-neutral
face
descriptors
for
mitigating
bias
in
face
recognition
arxiv
preprint
arxiv
2006.07845
ellemers
2018
gender
stereotypes
annual
review
of
psychology
69
275
298
gao
and
shah
2020
toward
creating
fairer
ranking
in
search
engine
results
information
processing
management
57
102138
griffith
2021
algorithms
still
have
bias
problem
and
big
tech
isn
doing
enough
to
fix
it
https://www.pcmag.com/news/algorithms-still-have-abias-problem-and-big-tech-isnt-doing-enough-to.
accessed
2022
02
10
hashemi
and
hall
2020
retracted
article
criminal
tendency
detection
from
facial
images
and
the
gender
bias
effect
journal
of
big
data
16
hibbing
and
rankin-erickson
2003
picture
is
worth
thousand
words
using
visual
images
to
improve
comprehension
for
middle
school
struggling
readers
the
reading
teacher
56
758
770
hwang
park
kim
do
and
byun
2020
fairfacegan
fairness-aware
facial
image-to-image
translation
arxiv
preprint
arxiv
2012.00282
järvelin
and
kekäläinen
2002
cumulated
gain-based
evaluation
of
ir
techniques
acm
transactions
on
information
systems
tois
20
422
446
jia
meng
zhao
and
chang
2020
mitigating
gender
bias
amplification
in
distribution
by
posterior
regularization
arxiv
preprint
arxiv
2005.06251
kay
matuszek
and
munson
2015
unequal
representation
and
gender
stereotypes
in
image
search
results
for
occupations
in
proceedings
of
the
33rd
annual
acm
conference
on
human
factors
in
computing
systems
lam
broderick
wojcik
and
hughes
2018
gender
and
jobs
in
online
image
searches
pew
social
trends
retrieved
march
14
2020
makhortykh
urman
and
ulloa
2021
detecting
race
and
gender
bias
in
visual
representation
of
ai
on
web
search
engines
in
international
workshop
on
algorithmic
bias
in
search
and
recommendation
36
50
springer
11889
metaxa
gan
goh
hancock
and
landay
2021
an
image
of
society
gender
and
racial
representation
and
impact
in
image
search
results
for
occupations
proceedings
of
the
acm
on
human-computer
interaction
cscw1
23
mozilla
2021
internet
health
report
2020
https
creativecommons
org
licenses
by
4.0
accessed
2021
0915
otterbacher
bates
and
clough
2017
competent
men
and
warm
women
gender
stereotypes
and
backlash
in
image
search
results
in
proceedings
of
the
2017
chi
conference
on
human
factors
in
computing
systems
otterbacher
checco
demartini
and
clough
2018
investigating
user
perception
of
gender
bias
in
image
search
the
role
of
sexism
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
933
936
park
hwang
kim
and
byun
2021
learning
disentangled
representation
for
fair
facial
attribute
classification
via
fairness-aware
information
alignment
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
35
2403
2411
schwemmer
knight
bello-pardo
oklobdzija
schoonvelde
and
lockhart
2020
diagnosing
gender
bias
in
image
recognition
systems
socius
2378023120967171
serna
peña
morales
and
fierrez
2021
insidebias
measuring
bias
in
deep
networks
and
application
to
face
gender
biometrics
in
2020
25th
international
conference
on
pattern
recognition
icpr
3720
3727
ieee
sharma
shukla
giri
and
kumar
2019
brief
review
on
search
engine
optimization
in
2019
9th
international
conference
on
cloud
computing
data
science
engineering
confluence
687
692
ieee
singh
chayko
inamdar
and
floegel
2020
female
librarians
and
male
computer
programmers
gender
bias
in
occupational
images
on
digital
media
platforms
journal
of
the
association
for
information
science
and
technology
71
11
1281
1294
sutton
and
barto
2018
reinforcement
learning
an
introduction
mit
press
voorhees
et
al
1999
the
trec-8
question
answering
track
report
in
trec
volume
99
77
82
citeseer
wang
zhao
yatskar
chang
and
ordonez
2019
balanced
datasets
are
not
enough
estimating
and
mitigating
gender
bias
in
deep
image
representations
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
5310
5319
wang
qinami
karakozis
genova
nair
hata
and
russakovsky
2020
towards
fairness
in
visual
recognition
effective
strategies
for
bias
mitigation
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
8919
8928
wijnhoven
2021
search
engine
gender
bias
frontiers
in
big
data
29
xu
white
kalkan
and
gunes
2020
investigating
bias
and
fairness
in
facial
expression
recognition
in
european
conference
on
computer
vision
506
523
springer
zehlike
bonchi
castillo
hajian
megahed
and
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
in
proceedings
of
the
2017
acm
on
conference
on
information
and
knowledge
management
1569
1578
zhao
and
chen
2019
rank-based
multi-task
learning
for
fair
regression
in
2019
ieee
international
conference
on
data
mining
icdm
916
925
ieee
11890