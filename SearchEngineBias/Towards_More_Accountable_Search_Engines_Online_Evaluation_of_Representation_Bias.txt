Towards More Accountable Search Engines:
Online Evaluation of Representation Bias
Aldo Lipani

Florina Piroi

Emine Yilmaz

University College London
London, United Kingdom
aldo.lipani@ucl.ac.uk

TU Wien
Vienna, Austria
florina.piroi@tuwien.ac.at

University College London
London, United Kingdom
emine.yilmaz@ucl.ac.uk

arXiv:2110.08835v1 [cs.IR] 17 Oct 2021

ABSTRACT
Information availability affects peopleâ€™s behavior and perception
of the world. Notably, people rely on search engines to satisfy their
need for information. Search engines deliver results relevant to user
requests usually without being or making themselves accountable
for the information they deliver, which may harm peopleâ€™s lives
and, in turn, society. This potential risk urges the development
of evaluation mechanisms of bias in order to empower the user
in judging the results of search engines. In this paper, we give a
possible solution to measuring representation bias with respect to
societal features for search engines and apply it to evaluating the
gender representation bias for Googleâ€™s Knowledge Graph Carousel
for listing occupations.

KEYWORDS
representation bias, rank bias measure, gender bias

1

INTRODUCTION

Nowadays, search engines are increasingly used as primary tools
to access information. The information that search engines point
their users too often helps people take decisions that guide them
through their lives, while it may also affect their judgement and
perception of the world [7]. Towards the end of the last century it
has been observed that information systems can be biased [4] and
that biased information (i.e., information that does not represent
the reality of the world people live in) can be harmful to how people
relate to each-other and to how they evaluate their own decisions
and opportunities [8]. The essential role that search engines have in
influencing peopleâ€™s access to information urges the need to design
evaluation mechanisms to assess search systems for potential biases
that may negatively impact society. This urge is further accentuated
by the lack of transparency in how results to user searches are
selected and displayed. Such a bias assessment mechanism can be
used as a diagnostic tool to monitor search systems results and
make the systems more accountable, a desire that is raising to the
attention of the research community [1].
Various measures to quantify bias in ranking have been previously proposed, e.g., Yang and Stoyanovich [9] define 3 measures
starting from utility-based IR evaluation measures like RBP and
nDCG; Zehlike et al. [10] define a statistical based method to evaluate and produce fair rankings. These metrics were then further
discussed and improved by Gezici et al. [5]. In this short paper we
focus on representation bias, also known as equality of opportunity [6] which is rank-independent, while the mentioned works
focus on statistical parity, i.e. a fair ranking system retrieves the
same proportion of individuals across groups. S. C. Geyik et al [3]

Figure 1: Googleâ€™s KGC as shown by Google when querying:
â€˜list of philosophers.â€™
define a ratio based metric for the feature representation at cut-off
ğ‘˜ but all experiments further on are wrt. ranking and not rank-free
representation bias.
Many countries enforce legislation to protect people from discrimination, be it at work or in the wider society. More concretely,
such legislation aims to protect individuals from being discriminated as to societal features like outer aspect, education, sexual
orientation, or geographical location. In this context, search engines
bias evaluation can also be used by regulatory bodies in order to
combat potential discrimination during search for information.
Traditionally search engines are evaluated in terms of effectiveness and efficiency. An effective system retrieves relevant information while an efficient one is a system that does it fast and with few
resources. We argue, here, that an orthogonal form of search engine
evaluation is needed where the fair representation of relevant societal features in search results is assessed. In other words, we argue
for a measure to assess a search systemâ€™s feature representation bias.
In this work we formalize the representation bias for categorical
features and illustrate its use to analyze the gender representation
bias in exponents of professions as displayed by the Knowledge
Graph Carousel (KGC) featured by Google (Figure 1).

2

REPRESENTATION BIAS

The representation bias of a search system is the preference of
the system for or against a group of results relevant to a user
query, that manifest a specific feature value ğ‘ âˆˆ C, with C a set of
categorical features like age, education, gender. For a given feature
ğ‘ âˆˆ C, we compute a systemâ€™s representation bias with respect
to this feature as the difference between the proportion of result
documents manifesting the ğ‘ feature value (the model ğ‘-ratio) and
the proportion of all documents indexed by a search system that
present the ğ‘ feature value (the target ğ‘-ratio).
We give, now, the definitions and rationale behind the representation bias and its two components, model and target ğ‘-ratios. In
this work, and for the sake of formalization simplicity, we consider
C to be binary. The formalization can be generalized to any C size.

Definition 2.1. Given the set of documents, D, relevant to a userâ€™s
input query, and a binary feature set, C, we define the target ğ‘-ratio
as:
|Dğ‘ |
ğœ‡ğ‘ =
,
(1)
|D|
ğ‘
where D is the set of documents in D that manifest feature ğ‘ âˆˆ C.

This equation is defined in the set {ğ‘˜/ğ‘› : ğ‘˜, ğ‘› âˆˆ Z, ğ‘› > 0, |ğ‘˜ | â‰¤ ğ‘›}.
When ğ›½ğ‘ğ‘Ÿ @ğ‘› = 0 we consider that ğ‘Ÿ is bias-free with respect to
the feature ğ‘. When ğ›½ğ‘ğ‘Ÿ @ğ‘› > 0 the system result is biased towards
ğ‘, with maximal bias when ğ›½@ğ‘› = 1. When ğ›½ğ‘ğ‘Ÿ @ğ‘› < 0 the system
result is biased against ğ‘, with maximal bias when ğ›½@ğ‘› = âˆ’1.
We can now compute the Mean representation Bias (MB) and its
Standard deviation (SB) over a set of user queries (topics) for each
feature value ğ‘. Note that when a search engine is biased towards
one ğ‘ value for one topic and towards the other value for another
topic, these two topics may cancel each other out when computing
MB. To compensate for this effect we compute also the Mean of the
Absolute Bias (MAB).

Since search engines, however, show only a lower size ğ‘› subset
U of the document set D found relevant to a query, we could define
the target ğ‘-ratio at cut-off ğ‘› similarly, as:
|Uğ‘ |
,
(2)
ğ‘›
with ğ‘› = |U| and Uğ‘ is the set of documents in U with feature ğ‘ 1 .
This definition is, though, insufficient explained below.
To measure a search systemâ€™s representation bias our method
compares the target ğ‘-ratio at cut-off n with the model ğ‘-ratio where
the latter is defined as:
ğœ‡ğ‘ @ğ‘› =

3

Using the feature ratios defined in the previous section, we measure
the representation bias for gender (a binary feature) at cut-off ğ‘› = 10.
The user topics we consider are professions. We measure the gender
representation bias for Googleâ€™s KGS. The code used to collect
the dataset and ran the analysis is available at the following weblink: https://github.com/aldolipani/TMASE. We note here that, as
Googleâ€™s results are profile dependent - to say the least - we have
crawled the KGC for professions using a United States location
and an incognito browsing mode. Figure 1 shows an example of
KGC display, located between the Google search bar and the results
list. Such a KGC is displayed only when users give specific word
patterns as input queries.
Though the list of word patterns that trigger the KGC display is
not disclosed, based on our tests, we found the following triggering patterns: â€˜list of [professionsâ€™ noun]â€™, â€˜top [professionsâ€™
noun]â€™, â€˜important [professionsâ€™ noun]â€™. In this analysis we focus
only on the first pattern that triggers the KGC display, due to its
(gender) neutral surface formulation. The connotative meaning of
the â€˜topâ€™ or â€˜importantâ€™ terms bring emotional and other associations we want to avoid. In this context, the neutral connotations
for queries like â€˜list of [professionsâ€™ noun]â€™ demonstrate a neutral intent to retrieve personsâ€™ associated with the professionsâ€™.
Although the search engine may return their results based on a
predicted user preference, we believe that these queries in particular, due to their neutral intent, and based on the fact that Google
does not make itself accountable by disclosing the feature used
to rank these results (like â€˜historical importanceâ€™, â€˜yearly incomeâ€™,
etc.), should give results free of bias.
During our initial experiments, we found out that not all queries
to list persons with a certain occupations trigger the KGC display.
Therefore we extracted all professions available in Wikidata [2],
together with their gender feature annotations. This gave us a list
of 3,374 professions for which we generated Google word patterns,
and retained 454 professions which did trigger the KGC display.
The analysis further on is done on these 474 professions. For cases
where Googleâ€™s KG gender annotation did not match Wikidataâ€™s,
we did manual labelling.
As defined in the previous section, to calculate the target ğ‘-ratio
for the results displayed in the KGC we first compute the target
ğ‘-ratio, ğœ‡ğ‘ , for each profession. We select two document sets: (a)
the full-length Google search result for professions, denoted by R;

Definition 2.2. Given the set of documents, D, relevant to a
userâ€™s input query, a binary feature set, C, and a search result,
ğ‘Ÿ = [ğ‘‘ 1, . . . , ğ‘‘ğ‘š ] with ğ‘‘ 1, . . . , ğ‘‘ğ‘š âˆˆ D of documents ranked by
the search system, we define the model ğ‘-ratio at cut-off ğ‘› with
respect to ğ‘Ÿ as:
|Dğ‘Ÿğ‘@ğ‘› |
ğœ‡Ë†ğ‘ğ‘Ÿ @ğ‘› =
,
(3)
ğ‘›
ğ‘
where ğ‘Ÿ @ğ‘› is ğ‘Ÿ cut at rank ğ‘›, Dğ‘Ÿ @ğ‘› is the set of documents in ğ‘Ÿ @ğ‘›
that have the feature ğ‘.
Ideally, ğœ‡ğ‘ and ğœ‡ğ‘ @ğ‘› are the same, making a comparison between
ğœ‡ğ‘ @ğ‘› and ğœ‡Ë†ğ‘ğ‘Ÿ @ğ‘›, which we aim for with our method, straightforward. However, since |U| = ğ‘› is usually much lower than |D|, this
is not always the case. Consider, for example, ğœ‡ğ‘ = 0.5 and ğ‘› = 11.
Two result sets Dğ‘Ÿ @ğ‘› , one that contains 5 ğ‘-feature documents
while the other displays 6 ğ‘-feature documents must be considered
as equal from a representational bias point of view. Using the naive
estimator in (2), in certain circumstances, we would be measuring
an non existing bias. To this end, we define the unbiased version of
the target ğ‘-ratio as follows:
Definition 2.3. For a given set of documents, D, relevant to a
userâ€™s input query, a binary feature set, C, and a search result, ğ‘Ÿ ,
the cut-off ğ‘›, let ğ›¿ğ‘› = ğœ‡ğ‘ Â· ğ‘› âˆ’ âŒŠğœ‡ğ‘ Â· ğ‘›âŒ‹. We define the target ğ‘-ratio
at cut-off ğ‘› with respect to ğ‘Ÿ as:
ï£±
ï£´ âŒŠğœ‡ğ‘ Â· ğ‘›âŒ‹
ğ›¿ğ‘› < 0.5
ï£´
ï£²
1ï£´
ğ‘Ÿ
ğœ‡ğ‘ @ğ‘› =
arg min { âŒŠğœ‡ğ‘ Â·ğ‘›âŒ‹, âŒˆğœ‡ğ‘ Â·ğ‘›âŒ‰ } (|ğ‘¥ âˆ’ ğœ‡Ë†ğ‘ğ‘Ÿ @ğ‘›|) ğ›¿ğ‘› = 0.5 . (4)
ğ‘›ï£´
ï£´
ï£´ âŒˆğœ‡ğ‘ Â· ğ‘›âŒ‰
ğ›¿ğ‘› > 0.5
ï£³
The target ğ‘-ratio at cut-off ğ‘› here defined is the â€˜idealâ€™ model ğ‘-ratio,
that is the observed ğ‘Ÿ is free of representational bias. The way this
is defined ensures that when computing the ideal ğ‘-ratio for a given
ğ‘Ÿ , in case of ambiguity (ğ›¿ğ‘› = 0.5) the solution given by the search
engine is accepted as correct.
Definition 2.4. The ğ‘-feature representation bias for a result
set ğ‘Ÿ is the difference between the model and the target ğ‘-ratios
with respect to ğ‘Ÿ :
ğ›½ğ‘ğ‘Ÿ @ğ‘› = ğœ‡ğ‘ğ‘Ÿ @ğ‘› âˆ’ ğœ‡Ë†ğ‘ğ‘Ÿ @ğ‘›
1 Note

CASE STUDY

(5)

that ğœ‡ğ‘ @ğ‘› has values in the set {ğ‘˜/ğ‘›; ğ‘˜, ğ‘› âˆˆ N, 0 â‰¤ ğ‘›, 0 â‰¤ ğ‘˜ â‰¤ ğ‘› }.
2

0.15
0.2

ğ‘

0.10

ğ‘
female

female
0.1

male

0.05

0.00

male

0.0
-1.0

-0.5

0.0

0.5

1.0

-1.0

-0.5

ğ›½ğ‘ğ‘Ÿ @10

0.0

0.5

1.0

ğ›½ğ‘ğ‘Ÿ @10

Figure 2: Gender representation bias histograms for K (left) and R (right)
Table 1: Summary of the data visualized in Fig. 2.
ğ‘
K
R

female
male
female
male

MB@10

SB@10

MAB@10

min

max

-0.042
0.026
-0.015
0.001

0.156
0.170
0.097
0.113

0.113
0.121
0.062
0.072

-0.5
-0.8
-0.4
-0.5

0.8
0.5
0.5
0.4

concrete values in these tables represent the female-ratios2 . Table 2 identifies â€˜announcerâ€™ as the profession over-represented by
the male gender feature, and â€˜archivistsâ€™ as the profession overrepresented by the female gender feature. For the K data â€˜archivistsâ€™
is over-represented by females, and â€˜librarianâ€˜ by the male feature.
From the data in these tables we observe that while the professions biased in favour of the male gender have a target genderratio of around 0.5, the target ratios for professions that are female
gender-biased is around 0.1. This indicates that females, when fairly
represented in the larger data (R or K) are less represented in the
KGC results, and when they are not represented in the larger data,
their representation ratio in the KGC results is higher.

and (b) the Wikidata collaboratively edited knowledge base (KB)
hosted by the Wikimedia Foundation, denoted by K.
The results shown by KGC, as many online systems, change
over time, so the following analysis is relative to data crawled on
December 1st, 2019.
Figure 2 shows the per gender histograms of KGC representation
bias values in relation to R and K target c-ratios. Considering the
binary characteristic of gender we expected these histograms to
be symmetric to each other, for each of the two cases. However,
these are not perfectly symmetric as for some queries the KGC has
shown less than 10 results. Still, had the KGC been free of bias,
we would now observe two histograms centered on zero with no
spread. Since this is not the case we may conclude that the KGC is
not free of gender representation bias. Additionally, the pair-wise
asymmetry of the histograms indicates that when the search result
is biased, it is more likely that the bias favours the male gender
over the female one. Looking at the distribution bias wrt. the K
target gender-ratio we observe a stronger bias than the one wrt. the
R target gender-ration of the full search result set. Moreover, we
also observe that when we base our target distribution based on
the KB we measure more bias than what we measure when the
target distribution is calculated over the full search results. Table 1
summarizes the data visualized in Figure 2.
Figure 3 compares the target (x-axis) and the model (y-axis)
gender-ratios for each profession in K and R. To every data point
we added a controlled jitter that makes them fall within a square
of the grid surrounding the original data point. Looking at the dispersion of points along the x-axis, we note that the male gender
dominates many professions. The number of points per grid square
decreases when moving towards the center, these are those professions equally represented by both genders. Only 4 professions are
female-dominated. Would the KGC be free of bias, it would have
placed every profession within the green squares, where the target
and model gender-ratios are equal, i.e. bias is null.
Tables 2 and 3 show the top 11 professions extracted from K
and R for which the highest representation bias towards the male
gender was observed, no gender-ratio bias was observed, and highest bias towards female gender was observed, respectively. The

4

CONCLUSION

We have defined concepts to help evaluate the representation bias
with respect to specific societal features in online search engines.
Such evaluations, if provided together with search results, will allow users to assess potential representation biases. In a concrete
case study, we have used the defined concepts to assess the representation bias of the Googleâ€™s KGC with respect to gender and
professions. Assuming the correctness of the population frequency
of the documents available to KGC, we conclude that it suffer from
gender representation bias.
A key aspect of the method introduced in this paper is the assumption of complete knowledge of the population frequency for
each feature value. In our case study, the feature ratios have been
calculated based on data that the search system provider made available and on a collaboratively knowledge base (Wikidata). However,
better statistics for ğ‘-ratios could be aimed for, e.g. official statistics
published by government agencies.
Finally, we have shown how such a representation bias evaluation can be performed using a concrete case study. In this case study
we have analyzed the gender representation bias of the Googleâ€™s
KGC, and shown that, assuming the correctness of the population
means, it suffers from gender representation bias. We must be aware,
though, that the systemsâ€™ selection of documents to be displayed
(by KGC, for example), is most of the time not transparent, and
other features not openly accounted for, like â€™historical relevanceâ€™
or â€™yearly incomeâ€™ may affect the final result shown to a user. At
the same time we must distinguish, between detecting the source
of bias and measuring a representation bias. The work described
in this paper may help a user hypothesize on sources of bias once
they have been detected by measuring the representation bias.
2 Female-

3

and male-ratio sum up to 1.

ğ‘
male
female

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡Ë†ğ‘ğ‘Ÿ @10

1.1
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

1.1
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

1.1

ğ‘
male
female

0.0

0.1

0.2

ğœ‡ğ‘ğ‘Ÿ @10

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

1.1

ğœ‡ğ‘ğ‘Ÿ @10

Figure 3: Target vs. model gender-ratio for K (left) and R (right). Every point represents a profession. A jitter is added to every
data point, i.e. points occurring in the same square of the grid have the same coordinates. The two sets of points for the male
and female gender are symmetric. The points in the green squares are those free of bias.

Table 2: Values are calculated with ğ‘ = female. The column Unbiased spans across every target ratio, from 0.0 to 1.0. In this
column, when more than one profession is found with the same target ratio, we selected the one with the largest population
in K. The first and last columns are generated by ordering each profession based on its representation bias.
Biased Towards Males
Profession
announcer
long jumper
high jumper
science writer
rugby sevens player
cell biologist
clinical psychologist
piano teacher
water polo player
middle-distance runner
botanical illustrator

Unbiased

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.0
0.0
0.0
0.1
0.0
0.0
0.0
0.0
0.0
0.0
0.3

0.5
0.5
0.5
0.6
0.5
0.5
0.5
0.5
0.4
0.4
0.7

-0.5
-0.5
-0.5
-0.5
-0.5
-0.5
-0.5
-0.5
-0.4
-0.4
-0.4

Profession
american football player
historian
songwriter
illustrator
choreographer
badminton player
artistic gymnast
model
flight attendant
rhythmic gymnast
glamour model

Biased Towards Females

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.0
0.1
0.2
0.3
0.4
0.5
0.5
0.8
0.9
1.0
1.0

0.0
0.1
0.2
0.3
0.4
0.5
0.5
0.8
0.9
1.0
1.0

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

Profession
archivist
baker
school teacher
modern pentathlete
church musician
drama teacher
television presenter
scenographer
track cyclist
skeleton racer
game author

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.9
0.6
0.6
0.5
0.4
0.7
0.7
0.5
0.4
0.7
0.3

0.1
0.1
0.2
0.1
0.0
0.3
0.4
0.2
0.1
0.4
0.0

0.8
0.5
0.4
0.4
0.4
0.4
0.3
0.3
0.3
0.3
0.3

Table 3: Values are arranged as in Table 2 but selected using R.
Biased Towards Males
Profession
librarian
draughts player
long jumper
handball player
translator
classical archaeologist
high jumper
talk show host
sound artist
executive
science journalist

Unbiased

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.2
0.0
0.0
0.1
0.1
0.0
0.0
0.0
0.0
0.0
0.1

0.6
0.4
0.3
0.4
0.4
0.2
0.2
0.2
0.2
0.2
0.3

-0.4
-0.4
-0.3
-0.3
-0.3
-0.2
-0.2
-0.2
-0.2
-0.2
-0.2

Profession
officer of the french navy
war photographer
table tennis player
fashion designer
alpine skier
vj
sex educator
beach volleyball player
softball player
domestic worker
ballerina

REFERENCES

Biased Towards Females

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.6
0.8
0.9
1.0

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.6
0.8
0.9
1.0

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

Profession
archivist
scenographer
video blogger
drama teacher
sailor
lighting designer
chemist
fighter pilot
musical theatre actor
television presenter
skeleton racer

ğœ‡Ë†ğ‘ğ‘Ÿ @10

ğœ‡ğ‘ğ‘Ÿ @10

ğ›½ğ‘ğ‘Ÿ @10

0.9
0.5
0.7
0.7
0.2
0.3
0.3
0.2
0.6
0.7
0.7

0.4
0.2
0.4
0.4
0.0
0.1
0.1
0.0
0.4
0.5
0.5

0.5
0.3
0.3
0.3
0.2
0.2
0.2
0.2
0.2
0.2
0.2

24, 2 (2021).
[6] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in
Supervised Learning. In Proc. of NeuIPS â€™16.
[7] Matthew Kay, Cynthia Matuszek, and Sean A. Munson. 2015. Unequal Repr. and
Gender Stereotypes in Image Search Results for Occupations. In Proc. of CHI â€™15.
[8] Steven J. Spencer, Claude M. Steele, and Diane M. Quinn. 1999. Stereotype Threat
and Womenâ€™s Math Performance. Journal of Experimental Social Psych. 35 (1999).
[9] Ke Yang and Julia Stoyanovich. 2017. Measuring fairness in ranked outputs. In
Proc. of SSDBM â€™17.
[10] M. Zehlike et al. 2017. FA*IR: A fair top-k ranking algorithm. In Proc. CIKM â€™17.

[1] Adam Roegiest et al. 2019. FACTS-IR: Fairness, Accountability, Confidentiality,
Transparency, and Safety in Information Retrieval. SIGIR Forum 53, 2 (2019).
[2] Pellissier Tanon et al. 2016. From Freebase to Wikidata: The Great Migration. In
Proc. of WWW â€™16.
[3] S. C. Geyik et al. 2019. Fairness-Aware Ranking in Search & Recommendation
Systems with Application to LinkedIn Talent Search. In Proc. of KDDâ€™19.
[4] Batya Friedman and Helen Nissenbaum. 1996. Bias in Computer Systems. ACM
Trans. Inf. Syst. 14, 3 (July 1996), 330â€“347.
[5] Gizem Gezici, Aldo Lipani, Yucel Saygin, and Emine Yilmaz. 2021. Evaluation
metrics for measuring bias in search engine results. Information Retrieval Journal
4

