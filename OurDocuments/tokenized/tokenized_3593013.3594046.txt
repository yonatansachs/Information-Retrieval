unfair
search
engine
manipulation
undetectable
by
amortized
inequity
abstract
tim
de
jonge
tim.dejonge@ru.nl
radboud
university
nijmegen
nijmegen
the
netherlands
djoerd
hiemstra
hiemstra@cs.ru.nl
radboud
university
nijmegen
nijmegen
the
netherlands
retrieval
systems
have
some
fairness
concerns
and
there
is
broad
modern
society
increasingly
relies
on
information
retrieval
systems
to
answer
various
information
needs
since
this
impacts
society
in
many
ways
there
has
been
great
deal
of
work
to
ensure
the
fairness
of
these
systems
and
to
prevent
societal
harms
there
is
prevalent
risk
of
failing
to
model
the
entire
system
where
nefarious
actors
can
produce
harm
outside
the
scope
of
fairness
metrics
we
demonstrate
the
practical
possibility
of
this
risk
through
unfair
ranking
system
that
achieves
performance
and
measured
fairness
competitive
with
current
state-of-the-art
while
simultaneously
be
ing
manipulative
in
setup
unfair
demonstrates
how
adhering
to
fairness
metric
amortized
equity
can
be
insufficient
to
prevent
search
engine
manipulation
this
possibility
of
manipulation
by
passing
fairness
metric
discourages
imposing
fairness
metric
ahead
of
time
and
motivates
instead
more
holistic
approach
to
fairness
assessments
ccs
concepts
information
systems
learning
to
rank
keywords
fairness
information
retrieval
search
engine
manipulation
effect
exposure
unfair
acm
reference
format
tim
de
jonge
and
djoerd
hiemstra
2023
unfair
search
engine
manip
ulation
undetectable
by
amortized
inequity
in
2023
acm
conference
on
fairness
accountability
and
transparency
facct
23
june
12
15
2023
chicago
il
usa
acm
new
york
ny
usa
10
pages
https://doi.org/10.
1145
3593013.3594046
introduction
modern
society
increasingly
relies
on
information
retrieval
systems
to
answer
various
information
needs
ranging
from
high
impact
applications
like
healthcare
16
and
automated
fact
checking
39
to
more
everyday
problems
such
as
fashion
matching
46
and
music
recommendation
25
since
information
retrieval
systems
impact
society
in
many
ways
there
has
been
great
deal
of
work
to
ensure
the
fairness
of
these
systems
while
it
is
easy
to
see
why
automated
fact
checking
requires
some
care
nearly
all
information
spectrum
of
efforts
to
increase
fairness
in
these
systems
22
while
these
efforts
are
all
for
the
greater
good
many
authors
do
not
cite
the
harms
they
aim
to
address
15
this
has
the
risk
of
fairness
interventions
being
deployed
in
situations
where
they
do
not
mitigate
the
harm
they
ought
to
address
or
where
fairness
metric
is
kept
in
use
although
it
does
not
adequately
measure
the
harm
in
the
situation
selbst
et
al
provide
an
excellent
overview
of
difficulties
in
ab
stracting
the
complicated
notion
of
fairness
by
indicating
five
traps
that
fair-ml
work
falls
into
first
and
foremost
of
these
is
the
fram
ing
trap
or
the
failure
to
model
the
entire
system
over
which
social
criterion
such
as
fairness
will
be
enforced
this
means
an
algorithm
can
appear
fair
to
model
while
the
algorithm
causes
societal
harm
outside
the
scope
of
assessment
44
while
the
theo
retical
possibility
of
the
framing
trap
is
well-established
there
is
little
practical
research
demonstrating
where
and
how
these
harms
could
enter
system
we
demonstrate
the
dangers
of
the
framing
trap
through
un
fair
novel
ranking
system
that
achieves
performance
and
mea
sured
fairness
competitive
with
current
state-of-the-art
while
being
clearly
manipulative
in
setup
unfair
demonstrates
how
failing
to
model
the
entire
system
can
result
in
search
engine
manipulation
while
adhering
to
specific
fairness
metric
amortized
equity
if
manipulating
amortized
inequity
is
possible
imposing
fairness
metric
ahead
of
time
might
be
insufficient
to
prevent
societal
harm
which
motivates
more
holistic
approach
to
fairness
assessments
26
in
section
we
will
introduce
search
engine
manipulation
as
harm
in
information
retrieval
in
section
we
provide
technical
specification
of
the
model
in
which
we
have
built
our
information
retrieval
system
in
section
we
look
at
amortized
equity
common
fairness
metric
and
introduce
our
own
metric
to
assess
search
engine
manipulation
in
section
we
introduce
our
own
ranking
system
unfair
to
illustrate
how
ranking
systems
could
bypass
carelessly
applied
fairness
metrics
in
section
we
show
our
experimental
setup
and
the
results
thereof
in
section
we
discuss
our
findings
and
provide
an
outlook
for
research
in
fair
ranking
this
work
is
licensed
under
creative
commons
attribution
international
4.0
license
facct
23
june
12
15
2023
chicago
il
usa
2023
copyright
held
by
the
owner
author
acm
isbn
979
4007
0192
23
06
https://doi.org/10.1145/3593013.3594046
facct
23
june
12
15
2023
chicago
il
usa
tim
de
jonge
and
djoerd
hiemstra
search
engine
manipulation
as
illustration
of
the
societal
harms
that
can
result
from
infor
mation
retrieval
systems
we
use
the
search
engine
manipulation
effect
epstein
and
robertson
13
held
an
experiment
in
which
they
showed
participants
the
results
of
mock
search
engine
manipu
lated
to
be
heavily
politically
skewed
their
experimental
design
can
be
found
in
figure
first
they
polled
their
participants
on
prospective
election
they
then
asked
the
participants
to
interact
with
the
search
engine
results
page
that
was
biased
towards
one
of
the
candidates
they
then
polled
the
participants
again
to
see
whether
shift
had
taken
place
the
results
varied
strongly
with
participants
features
and
their
prior
knowledge
of
the
election
the
net
probability
of
influencing
vote
in
the
direction
of
the
manipulation
could
be
as
high
as
25
to
the
most
susceptible
sub
group
of
the
population
poorly
informed
moderate
voters
13
by
contrast
the
least
susceptible
subgroup
showed
slight
negative
response
making
it
less
likely
for
the
participant
to
vote
for
the
candidate
favoured
by
the
search
results
figure
experimental
design
of
the
search
engine
manipu
lation
effect
taken
from
13
condition
is
neutral
con
dition
with
conditions
and
skew
towards
left
and
right
respectively
conditions
and
are
altered
versions
of
con
narrowly
cooperated
with
cambridge
analytica
on
facebook
to
micro-target
advertisements
specifically
to
those
most
susceptible
to
them
cambridge
analytica
claims
to
have
done
the
same
in
several
other
elections
around
the
globe
38
while
these
ex
amples
are
not
necessarily
search
engine
manipulation
and
their
effectiveness
unclear
11
17
20
36
they
show
that
these
tech
giants
are
moving
in
political
spheres
with
the
intent
of
granting
electoral
advantage
model
specification
3.1
learning-to-rank
we
must
first
formalize
an
abstraction
of
an
information
retrieval
system
to
investigate
how
societal
harm
may
result
from
informa
tion
retrieval
systems
table
contains
an
overview
of
the
notation
used
in
this
paper
the
core
task
of
an
information
retrieval
system
is
to
rank
set
of
documents
optimally
given
query
primarily
we
want
to
show
users
those
documents
they
want
to
see
but
as
section
will
show
other
conditions
diversity
of
results
non-discrimination
fairness
can
be
imposed
as
well
we
use
an
online
learning-to-rank
model
which
entails
that
at
the
start
of
session
the
model
does
not
have
any
information
on
how
much
the
population
is
interested
in
each
document
it
will
have
to
learn
by
doing
23
learning-to
rank
is
common
in
information
retrieval
where
frequently
corpora
are
too
large
to
rely
on
expert
judgements
instead
learning-to
rank
algorithms
are
used
to
dynamically
ensure
that
users
of
model
are
shown
good
results
even
with
relatively
sparse
feedback
we
build
our
information
retrieval
system
in
the
model
used
in
morik
et
al
27
following
morik
et
al
27
we
observe
political
news
site
assume
the
editors
of
the
site
have
selected
30
documents
to
be
displayed
to
users
and
the
learning-to-rank
algorithm
must
rank
these
documents
by
estimated
relevance
using
the
data
users
provide
in
this
case
clicks
sequentially
users
come
to
the
site
and
are
presented
with
ranking
of
documents
looking
dition
to
mask
the
bias
in
the
results
through
these
documents
the
user
will
give
attention
to
docu
the
manipulation
of
search
results
can
be
difficult
to
detect
for
end-users
since
each
user
only
interacts
with
the
search
engine
limited
amount
of
times
it
can
be
hard
to
find
patterns
in
the
results
provided
by
search
engine
additionally
we
cannot
expect
users
to
have
the
expertise
required
to
detect
bias
in
search
results
if
they
use
search
engine
to
acquire
knowledge
in
the
first
place
the
powerful
position
in
which
large
tech
companies
find
themselves
combined
with
the
inability
for
individuals
to
realize
something
is
afoot
makes
for
nasty
problem
if
manipulation
through
search
engines
does
take
place
although
there
are
no
direct
examples
of
search
engine
ma
nipulation
being
used
with
malicious
intent
there
are
adjacent
cases
in
the
2012
us
election
the
obama
administration
coop
erated
narrowly
with
google
to
ensure
electoral
advantage
47
google
could
leverage
their
swathes
of
personal
data
to
estimate
persuasion
scores
they
could
then
target
advertisements
towards
those
individuals
that
were
easiest
to
persuade
to
ensure
maximal
ments
or
alternatively
phrased
the
documents
get
exposure
as
user
is
exposed
to
these
documents
they
will
click
on
whichever
documents
they
are
interested
in
these
clicks
are
registered
by
the
model
as
the
ranking
system
target
is
then
to
interpret
these
clicks
to
produce
the
best
ranking
according
to
some
quality
standard
relevance
fairness
or
any
other
definition
morik
et
al
27
introduces
two
rankers
d-ultr
glob
standing
for
dynamic-unbiased
learning-to-rank
global
and
fairco
we
will
now
take
look
at
d-ultr
glob
we
take
closer
look
at
fairco
in
section
5.1
both
fairco
and
unfair
are
extensions
of
this
d-ultr
glob
system
d-ultr
glob
mitigates
bias
users
have
where
they
inter
act
with
documents
because
they
are
at
the
top
of
the
ranking
not
because
the
documents
are
especially
relevant
to
them
19
this
means
we
cannot
use
click
data
as
direct
proxy
for
rele
vance
the
users
clicked
more
on
documents
that
were
near
the
top
of
the
ranking
than
these
documents
deserved
if
we
assume
an
attention
distribution
we
can
use
inverse
propensity
scoring
to
impact
similarly
in
the
2016
us
election
the
trump
administration
calculate
the
estimated
relevance
from
the
users
clicks
30
43
d-ultr
glob
then
ranks
the
documents
by
27
we
use
finding
document
relevant
we
have
ultr
glob
as
baseline
that
is
not
concerned
with
fairness
issues
we
illustrate
d-ultr
glob
with
simple
example
user
enters
the
site
clicks
on
the
articles
in
position
and
18
and
leaves
the
site
the
site
registers
these
clicks
and
passes
them
on
for
further
processing
since
the
user
clicked
on
the
articles
on
position
and
18
we
assume
the
user
was
interested
in
these
articles
and
we
put
article
and
18
at
the
top
of
the
results
page
for
the
next
the
actual
relevance
of
document
to
user
is
then
with
probability
and
otherwise
user
since
users
are
most
likely
to
scroll
through
results
from
the
first
to
the
last
the
user
is
much
more
likely
to
interact
with
article
bernoulli
than
with
article
18
for
the
next
user
inverse
propensity
score
then
makes
article
18
the
new
article
article
the
new
article
and
article
the
new
article
we
do
not
have
any
information
on
any
of
the
other
documents
so
they
remain
in
the
order
they
were
after
enough
iterations
of
additive
feedback
the
model
will
converge
to
ranking
that
has
high
probability
of
producing
relevant
documents
for
any
given
user
joining
the
site
3.2
technical
specification
in
this
section
we
lay
out
the
details
of
the
model
the
ranking
system
will
operate
in
unfortunately
outside
of
laboratorium
setting
we
cannot
di
rectly
observe
relevance
rather
we
are
restricted
to
observing
user
interactions
with
documents
for
user
to
interact
with
docu
ment
they
must
both
find
the
document
relevant
and
have
seen
it
in
the
first
place
we
assume
the
attention
user
gives
an
article
is
dependent
on
the
position
document
gets
in
the
ranking
the
higher
the
document
ranks
the
more
likely
it
is
for
user
to
see
it
19
we
denote
the
rank
document
gets
for
user
under
system
as
we
conceptualize
the
probability
that
user
sees
document
as
the
attention
or
exposure
user
gives
document
at
time
user
visits
the
page
and
requires
ranking
of
and
calculated
as
the
documents
the
user-base
is
made
up
of
two
smaller
sub-populations
the
left-wing
sub-population
is
drawn
from
normal
distribution
centered
around
0.5
and
analogously
the
log2
right-wing
sub-population
is
drawn
from
normal
distribution
centered
around
0.5
we
truncate
the
distributions
of
users
to
for
ease
of
graphing
this
does
not
result
in
any
substantial
changes
to
the
results
of
any
experiments
the
portion
of
left-wing
users
is
represented
by
left
in
this
model
we
assume
that
everyone
is
either
left-wing
or
right-wing
so
the
portion
of
right-wing
users
is
given
by
left
combining
these
concepts
the
political
lean
of
users
is
given
by
note
that
indeed
attention
has
the
range
meaning
interpret
ing
it
as
probability
is
consistent
the
attention
function
indicates
that
the
top-rated
document
is
very
likely
to
be
seen
with
rapid
initial
drop
this
drop
then
levels
out
after
the
first
few
results
meaning
there
is
less
difference
between
the
documents
on
rank
20
and
rank
21
than
there
is
between
the
documents
on
rank
and
rank
as
before
we
require
binary
value
for
user
seeing
document
leading
to
the
following
variable
for
user
seeing
document
leftn
0.5
0.2
left
0.5
0.2
bernoulli
finally
user
clicks
on
the
document
if
and
where
is
restricted
to
here
political
lean
of
only
if
they
have
seen
the
document
and
they
deem
it
relevant
means
that
the
user
is
maximally
left
wing
political
lean
of
the
model
can
only
observe
click
feedback
it
does
not
means
that
the
user
is
maximally
right-wing
the
documents
political
lean
are
normally
distributed
around
0.5
similarly
restricted
to
the
phrasing
here
implies
two-party
system
this
need
not
be
the
case
in
pluralistic
political
system
one
can
run
this
same
analysis
by
substituting
left-wing
with
the
party
of
one
choice
and
right-wing
by
fitting
comparison
group
section
4.2
motivates
the
choice
for
only
two
groups
and
gives
an
indication
of
how
these
groups
ought
to
be
chosen
we
describe
by
users
openness
their
willingness
to
interact
with
documents
that
do
not
align
with
their
political
preferences
the
have
access
to
any
of
the
aforementioned
underlying
variables
as
an
exception
we
assume
that
unfair
has
access
to
the
susceptibility
of
the
users
using
it
only
to
ensure
the
manipulation
described
in
section
5.2
again
let
us
illustrate
this
model
with
an
example
user
robin
comes
to
the
page
they
are
from
the
left-wing
portion
of
the
pop
ulation
robin
political
lean
robin
is
0.4
and
their
openness
robin
is
0.3
they
come
across
document
from
propublica
pp
which
has
political
lean
pp
0.2
the
probability
that
robin
0.4
0.2
users
openness
is
uniformly
distributed
between
0.05
and
0.55
finds
the
document
pp
relevant
is
then
robin
pp
or
about
80
0.32
0.05
0.55
we
also
interpret
users
openness
as
their
susceptibility
their
likelihood
to
change
their
vote
based
on
the
here
robin
found
the
document
on
position
pp
robin
the
odds
of
robin
seeing
the
propublica
article
are
then
robin
results
page
pp
these
factors
interact
in
natural
way
user
is
more
likely
to
find
document
relevant
if
the
political
lean
of
the
document
is
close
to
the
political
lean
of
the
user
the
more
open
user
is
the
more
receptive
they
are
for
documents
that
do
not
align
with
their
own
political
preference
operationalizing
the
probability
of
user
log2
or
50
now
robin
is
person
rather
than
concept
so
we
can
actually
be
sure
whether
they
see
the
article
and
whether
they
find
it
relevant
it
turns
out
that
they
do
find
the
article
relevant
and
that
they
see
it
they
click
on
it
and
the
system
registers
robin
click
on
the
propublica
article
evaluation
4.1
relevance
multi-lateral
inequity
tem
primarily
an
information
retrieval
system
ought
to
provide
users
with
those
documents
they
deem
relevant
to
assess
relevance
we
use
ndcg
10
18
the
technical
specification
of
ndcg
can
be
found
in
appendix
although
delivering
users
the
articles
they
want
to
see
is
an
important
interpretation
of
performance
it
is
not
the
sole
possible
interpretation
we
should
also
give
documents
fair
treatment
as
such
we
assess
ranking
systems
on
fairness
through
amortized
inequity
the
details
of
amortized
inequity
can
be
found
in
the
subsequent
section
4.2
finally
to
assess
electoral
impact
we
introduce
metric
to
measure
specifically
search
engine
manipulation
as
introduced
in
section
section
4.3
provides
the
details
of
this
metric
with
multi-lateral
inequity
we
compare
between
all
groups
and
then
average
out
the
deviations
27
31
as
mentioned
by
biega
et
al
multi-lateral
inequity
is
primarily
useful
as
an
optimiza
tion
target
when
unfairness
is
zero
all
groups
are
treated
equitably
according
to
the
chosen
definition
of
merit
however
multi-lateral
inequity
is
less
convenient
as
metric
of
the
level
of
skew
in
the
system
it
is
impossible
to
see
which
deviations
from
the
average
are
taking
place
since
all
pairwise
differences
are
summed
the
case
where
all
groups
are
scattered
closely
around
the
mean
is
differ
ent
in
analysis
than
the
case
in
which
one
group
is
discriminated
against
and
one
group
favoured
to
ease
analysis
we
suggest
using
bi-lateral
fairness
definition
for
measuring
inequity
41
in
so
doing
one
has
to
take
care
to
appropriately
choose
the
groups
tested
one
group
for
which
one
suspects
unfair
inequality
and
an
appropriate
comparison
group
in
the
case
with
two
groups
we
denoted
inequity
as
follows
bi-lateral
inequity
table
notations
used
in
this
paper
and
short
representa
tion
of
their
meanings
4.2
fairness
assessing
whether
any
system
is
fair
is
not
trivial
undertaking
if
users
click
on
document
slightly
more
than
they
click
on
document
it
seems
sensible
that
the
system
shows
document
slightly
more
if
users
only
have
minimal
preference
for
document
but
the
system
always
ranks
document
higher
than
document
this
seems
to
violate
proportionality
the
main
thing
to
determine
in
assessing
system
is
then
the
appropriate
level
to
which
there
we
can
attach
different
meanings
to
merit
dependent
on
cir
cumstance
more
restrictive
than
equity
of
attention
is
equality
of
attention
special
case
where
we
give
each
group
equal
merit
equality
of
attention
can
be
good
choice
in
cases
where
we
can
not
accurately
measure
or
estimate
document
value
but
it
is
too
restrictive
to
use
much
otherwise
more
frequently
merit
is
attached
to
relevance
show
document
to
users
proportional
to
the
extent
that
users
want
to
see
it
relevance
is
conceptually
attractive
but
noisy
to
the
point
of
being
practically
intractable
this
means
that
any
assessment
of
real-world
information
retrieval
systems
on
relevance
will
have
to
either
estimate
relevance
from
user
behaviour
or
recruit
judges
have
to
investigate
all
documents
and
mark
down
the
relevance
of
each
document
to
their
query
trec
the
largest
applied
text
retrieval
conference
historically
used
only
binary
relevance
which
lacks
nuance
compares
to
the
different
ways
in
which
users
might
experience
documents
in
the
real
world
work
to
comprehensively
define
relevance
in
the
best
way
to
assess
information
retrieval
systems
is
ongoing
and
has
proven
to
be
challenging
24
35
40
to
combat
the
noise
in
relevance
we
can
assess
fairness
across
multiple
queries
here
we
accumulate
relevance
and
attention
across
the
queries
that
the
system
has
seen
and
only
compare
the
totals
against
each
other
through
amortized
inequity
can
be
differentiation
between
documents
before
this
difference
in
attention
becomes
harmful
amortized
inequity
which
partition
the
set
of
documents
additionally
we
conceptu
biega
et
al
introduced
the
term
amortized
inequity
but
the
alize
merit
as
the
extent
of
differentiation
that
is
appropriate
accumulation
of
inequity
across
queries
is
commonplace
either
if
we
treat
each
group
according
to
their
merit
this
is
fair
and
the
further
we
stray
from
treatment
according
to
merit
the
more
unfair
it
becomes
this
notion
of
treating
each
group
according
to
merit
is
called
equity
of
attention
if
we
have
more
than
two
groups
we
can
calculate
the
multi-lateral
inequity
as
follows
through
direct
summation
over
visited
queries
10
27
31
33
42
45
or
an
expected
value
over
query
space
29
34
amortized
inequity
can
then
be
used
as
target
as
long
as
the
amortized
inequity
is
close
to
zero
both
groups
have
received
attention
in
proportion
to
their
merit
4.3
electoral
impact
as
suggested
in
section
it
might
be
possible
to
build
system
that
adheres
to
the
restrictions
posed
by
fairness
metrics
while
resulting
in
societal
harm
to
assess
whether
search
engine
manipulation
takes
place
we
devise
the
following
measure
unfair
we
set
out
to
demonstrate
that
ranking
system
can
result
in
societal
harm
while
adhering
to
the
restrictions
imposed
by
fair
ness
metric
to
this
end
we
create
the
undetectably
nefarious
fair
ranker
or
unfair
it
is
important
to
understand
that
our
intention
is
not
to
contribute
to
the
state-of-the-art
of
electoral
manipulation
rather
the
possibility
of
the
unfair
algorithm
existing
motivates
amortized
impact
careful
use
of
fairness
metrics
careless
application
of
fairness
metrics
might
not
stop
the
harm
it
intends
to
stop
the
code
to
run
unfair
can
be
found
on
https://github.com/unfairpaper/unfair.
unfair
is
system
with
conditions
baseline
positive
and
here
we
assume
every
document
has
impact
on
the
reader
equal
to
the
attention
the
document
received
the
reader
is
then
impacted
proportional
to
their
susceptibility
as
mentioned
in
section
google
and
facebook
have
both
attempted
to
estimate
users
susceptibility
in
previous
political
contexts
with
the
explicit
intention
of
influencing
elections
amortized
impact
is
continuous
extension
of
the
theory
es
tablished
in
epstein
and
robertson
13
systems
can
manipulate
people
electoral
preference
by
up
to
25
with
heavily
skewed
search
results
pages
which
matches
the
range
of
amortized
impact
negative
unfair
assigns
one
of
these
conditions
to
each
user
and
ranks
the
documents
based
on
the
condition
the
user
is
in
in
the
baseline
condition
unfair
uses
an
appropriate
high-performing
ranker
in
this
case
we
use
d-ultr
glob
as
introduced
in
section
3.1
27
the
baseline
ranker
does
not
need
to
be
fair
unfair
solves
for
the
fairness
metric
through
use
of
the
positive
and
negative
conditions
the
positive
condition
is
direct
modification
of
the
baseline
condition
here
unfair
first
ranks
all
documents
according
to
the
baseline
ranker
then
the
positive
ranker
finds
the
first
documents
with
positive
political
lean
and
places
to
account
for
queries
that
are
less
heavily
skewed
amortized
impact
scales
the
impact
by
the
difference
in
attention
the
groups
them
on
the
first
positions
of
the
results
page
pushing
other
receive
we
acknowledge
that
amortized
impact
makes
simplifying
as
sumptions
it
has
embedded
in
itself
the
disputed
claims
that
one
can
assess
ahead
of
time
how
much
user
vote
will
be
swayed
by
search
engine
28
that
every
results
page
has
non-zero
effect
on
user
vote
11
or
the
more
fundamental
complaint
that
the
likelihood
of
someone
vote
might
not
be
desirable
or
possible
to
capture
in
data
even
with
these
limitations
there
is
value
in
examining
this
metric
as
proxy
for
real-world
impact
most
model
analysis
stops
at
the
model
borders
whereas
value
can
be
found
in
observing
the
layer
beyond
the
model
output
44
other
choices
can
be
made
on
the
exact
specification
of
amortized
impact
the
following
discus
sion
still
holds
fair
rankers
5.1
fairco
fairco
27
is
learning-to-rank
system
that
targets
amortized
inequity
as
fairness
metric
since
this
paper
has
received
the
best
paper
award
at
sigir
2020
and
is
general-purpose
ranker
we
will
use
it
as
state-of-the-art
baseline
to
understand
fairco
first
recall
from
section
3.1
how
d-ultr
glob
operates
d-ultr
glob
estimates
the
relevance
of
document
corrected
for
position
bias
we
denote
this
d-ultr
glob
then
creates
results
page
by
ranking
documents
by
this
estimated
rel
evance
to
ensure
that
fairco
does
not
allow
for
any
inequity
of
attention
morik
et
al
modify
this
base
ranker
to
do
so
they
use
amortized
inequity
equation
amortized
inequity
is
transformed
into
an
error
term
err
err
whenever
is
in
the
privileged
group
and
err
is
equal
to
the
amortized
inequity
otherwise
rank
results
down
as
necessary
this
results
page
is
then
presented
to
the
user
the
positive
condition
corresponds
to
case
in
figure
the
negative
condition
is
defined
similarly
but
replacing
all
instances
of
positive
with
negative
and
corresponding
to
case
in
figure
unfair
assigns
conditions
to
users
by
susceptibility
score
unfair
offers
the
positive
condition
to
those
users
with
the
highest
susceptibility
scores
and
the
negative
ranker
to
those
with
the
lowest
susceptibility
scores
herein
lies
the
manipulation
since
not
all
attention
is
equal
the
algorithm
aims
to
have
all
high-impact
attention
go
towards
the
privileged
group
while
all
low
impact
attention
goes
towards
the
maltreated
group
this
means
that
the
voters
that
can
be
influenced
will
be
influenced
towards
the
positive
group
there
is
formal
equity
of
attention
while
the
system
can
produce
electoral
impact
to
formalize
this
we
keep
thresholds
and
if
unfair
uses
the
positive
condition
likewise
if
unfair
uses
the
negative
condition
if
unfair
uses
the
baseline
condition
recall
that
we
are
working
inside
learning-to-rank
model
which
means
that
at
the
start
of
an
experiment
we
do
not
know
anything
about
the
user
distribution
this
means
we
have
to
update
and
throughout
run
to
maintain
balance
between
the
different
conditions
unfairsmooth
is
the
first
mechanism
of
choosing
thresholds
here
the
thresholds
move
smoothly
within
the
domain
after
every
timestep
unfairsmooth
sets
err
and
similarly
err
this
ensures
that
unfairsmooth
chooses
the
negative
condition
more
frequently
when
the
discrepancy
in
exposure
is
positive
and
vice
versa
we
investigate
the
results
of
choosing
the
parameter
in
section
ing
by
err
then
produces
ranking
system
that
converges
in
the
limit
to
having
zero
amortized
inequity
unfaircoarse
is
the
other
mechanism
of
choosing
thresholds
unfaircoarse
uses
only
limited
set
of
possible
threshold
val
ues
the
consequence
of
this
is
that
we
can
execute
more
explicit
control
over
the
model
and
ensure
convergence
at
the
cost
of
some
flexibility
for
convenience
we
use
the
notation
amortized
inequity
where
is
the
amount
of
users
if
we
can
ensure
that
for
some
we
guarantee
that
amortized
inequity
remains
below
and
as
such
that
it
tends
to
parameter
can
be
interpreted
as
spring
constant
we
explore
the
effects
of
varying
in
section
6.4
6.3
how
does
performance
vary
at
different
levels
of
evaluation
for
completeness
we
investigated
how
the
relevance
assessment
changes
as
we
vary
the
amount
of
documents
evaluated
in
figure
to
ensure
that
divide
the
population
into
evenly
we
can
see
the
differences
between
the
different
evaluations
unfairsmooth
performs
well
on
ndcg
but
unfairsmooth
sized
chunks
along
the
susceptibility
axis
then
when
the
system
is
in
balance
the
system
only
uses
the
left
and
right
ranker
for
the
outermost
and
uses
the
baseline
condition
otherwise
if
10
there
is
an
inequity
of
10
towards
the
positive
the
negative
ranker
takes
over
the
lowest
susceptibility
chunk
of
the
performance
declines
as
more
results
are
evaluated
fairco
and
unfaircoarse
perform
about
equally
well
throughout
but
always
worse
than
d-ultr
glob
it
is
notable
that
ndcg
30
is
much
higher
than
ndcg
for
all
population
as
rises
unfaircoarse
will
assign
more
chunks
rankers
this
is
not
reflective
of
the
rankers
being
more
proficient
at
ranking
all
documents
instead
this
is
quirk
of
the
model
chosen
of
population
to
the
negative
condition
thus
stopping
the
rise
of
this
will
result
in
amortized
inequity
tending
to
as
previously
explained
results
6.1
experimental
setup
in
the
following
section
we
will
demonstrate
that
unfair
can
attain
similar
performance
and
fairness
to
fairco
while
reaching
substantial
electoral
impact
the
code
to
reproduce
these
results
can
be
found
on
https://github.com/unfairpaper/unfair.
unless
otherwise
listed
we
set
fairco
parameter
to
0.01
as
suggested
in
morik
et
al
27
unfair
parameter
to
0.005
and
left
to
0.5
each
experiment
uses
30
random
seeds
that
have
not
been
previously
used
in
developing
unfair
or
in
previous
experiments
within
each
experiment
we
then
evaluate
all
systems
on
the
same
random
seeds
resulting
in
the
same
experimental
conditions
6.2
can
unfair
achieve
electoral
impact
while
maintaining
competitive
performance
and
fairness
figure
shows
long
run
of
these
systems
evaluated
on
rele
vance
fairness
and
electoral
impact
for
this
experimental
setup
we
learn-to-rank
the
same
30
documents
for
10000
simulated
users
in
figure
the
classical
trade-off
between
fairness
and
performance
is
clearly
visible
12
d-ultr
glob
performs
best
in
terms
of
relevance
but
is
unfair
according
to
exposure
inequity
unfair
coarse
and
fairco
have
very
similar
performance
on
relevance
and
fairness
achieving
inequity
with
the
same
performance
loss
furthermore
we
can
see
that
fairco
reduces
electoral
impact
to
as
well
and
that
d-ultr
glob
does
not
stray
far
from
neutral
electoral
impact
both
implementations
of
unfair
do
deviate
from
and
in
unfairsmooth
we
can
see
oscillating
behaviour
as
more
users
interact
with
the
system
this
oscillating
behaviour
is
due
to
the
delay
between
an
imbalance
on
amortized
equity
and
un
fairsmooth
response
unfairsmooth
thresholds
move
in
the
opposite
direction
of
the
error
term
err
if
err
is
positive
but
already
decreasing
unfairsmooth
will
still
change
the
thresholds
to
be
more
skewed
towards
the
negative
side
thus
overshooting
the
target
this
results
in
spring-like
behavior
as
amortized
equity
will
turn
negative
rather
than
finding
equilibrium
at
unfair
the
performance
of
purely
random
ranker
is
also
significantly
higher
when
evaluated
with
ndcg
30
as
such
comparisons
be
tween
systems
on
the
same
metric
are
valid
comparisons
between
metrics
mostly
are
not
6.4
what
are
the
effects
of
unfair
lambda
parameter
in
the
following
experiment
we
only
vary
in
unfairsmooth
without
comparison
against
baseline
figure
shows
similar
per
formance
between
all
rankers
0.05
performs
poorly
while
0.003
performs
best
higher
results
in
heavier
oscillation
in
electoral
impact
where
exposure
inequity
is
initially
driven
down
but
does
not
continue
steady
decline
conversely
low
results
in
steady
but
slow
behaviour
these
observations
are
consistent
with
the
interpretation
of
as
spring
constant
of
system
as
middle
ground
between
these
two
extremes
we
retain
0.005
for
the
other
experiments
6.5
is
unfair
effective
at
different
user
distributions
finally
it
is
worth
observing
what
happens
when
populations
shift
in
figure
we
vary
the
left-wing
proportion
of
the
population
left
between
35
and
65
in
intervals
of
the
same
four
systems
used
before
are
evaluated
here
on
30
previously
unseen
seeds
plotted
are
the
values
the
system
produces
after
3000
users
have
used
the
system
in
figure
in
terms
of
ndcg
both
unfairsmooth
and
un
faircoarse
closely
mimic
fairco
performance
in
terms
of
rele
vance
although
all
are
lower
than
the
baseline
we
see
here
that
fairco
slightly
outperforms
unfair
on
minimizing
inequity
un
fair
does
still
achieve
serious
reduction
in
inequity
compared
to
d-ultr
glob
in
terms
of
electoral
impact
d-ultr
glob
has
the
most
polarized
scores
while
fairco
mitigates
the
impact
of
the
population
skew
unfair
does
not
in
cases
where
the
system
is
already
skewed
towards
the
positive
side
unfair
does
not
reduce
the
impact
much
in
cases
where
the
system
is
balanced
or
skewed
towards
the
negative
side
unfair
moves
this
impact
even
further
towards
the
positive
side
than
fairco
does
figure
ndcg
10
left
amortized
inequity
middle
and
amortized
impact
right
averaged
over
30
trials
of
10000
simulated
users
figure
from
left
to
right
ndcg
ndcg
ndcg
30
averaged
over
30
trials
of
3000
simulated
users
figure
ndcg
10
left
amortized
inequity
middle
and
amortized
impact
right
of
unfairsmooth
with
0.001
0.003
0.005
0.01
0.02
0.05
averaged
over
30
trials
of
3000
users
figure
ndcg
10
left
amortized
inequity
middle
and
amortized
impact
right
averaged
over
30
trials
of
3000
simulated
users
for
each
value
of
left
between
35
and
65
discussion
and
moving
forward
throughout
the
results
unfair
reaches
performance
close
to
fairco
while
being
similarly
fair
according
to
amortized
inequity
while
achieving
that
unfair
does
not
mitigate
any
of
the
electoral
impact
d-ultr
glob
has
as
baseline
and
when
reasonable
unfair
adds
positive
electoral
impact
that
said
unfair
certainly
has
imperfections
for
practical
use
one
could
create
more
refined
unfair
ranker
however
the
point
of
this
paper
is
not
to
provide
new
state-of-the-art
for
manipulating
elections
the
point
is
to
illustrate
that
current
fairness
metrics
need
careful
application
although
amortized
inequity
is
plausible
metric
for
search
engine
manipulation
we
see
that
nefarious
actor
could
create
system
that
appears
fair
to
amortized
inequity
while
not
reducing
search
engine
manipulation
in
any
way
the
unfair
algorithm
is
start
towards
investigating
the
limita
tions
of
fairness
metrics
in
information
retrieval
but
it
leaves
some
questions
yet
unanswered
this
paper
only
demonstrates
unfair
on
one
simulated
dataset
on
one
metric
while
the
algorithm
is
agnostic
to
the
metric
used
and
the
dataset
future
research
will
have
to
point
out
whether
unfair
functions
equally
well
in
other
circumstances
in
models
that
classify
rather
than
rank
we
have
theorems
proving
the
impossibility
of
satisfying
multiple
fairness
metrics
at
the
same
time
21
similar
theorems
for
ranking
would
be
very
powerful
tool
to
understand
the
trade-offs
inherent
in
fair
ranking
scholars
have
been
pondering
justice
or
fairness
for
centuries
given
that
definitive
answers
on
justice
are
few
and
far
between
distilling
fairness
down
to
single
number
seems
overzealous
32
rather
any
practical
application
of
fairness
algorithms
needs
to
be
grounded
in
the
relevant
literature
outside
of
just
information
retrieval
14
and
supported
by
the
normative
assumptions
that
clarify
which
substantive
fairness
we
are
benefiting
it
appears
that
fairness
metrics
have
very
fitting
role
as
canary
in
the
coal
mine
once
the
canary
falls
to
the
bottom
of
the
cage
it
is
time
to
evacuate
the
mine
however
live
canary
does
not
mean
the
mine
is
in
any
way
safe
and
building
policy
to
keep
the
canary
safe
is
fool
errand
just
because
there
are
limitations
to
the
usage
of
fairness
metrics
that
does
not
mean
the
practice
has
to
be
abolished
37
energy
invested
towards
the
greater
good
is
laudable
and
the
problem
remains
very
complex
the
suggestion
here
is
that
different
people
working
towards
the
greater
good
could
have
very
different
goals
and
making
these
goals
explicit
allows
us
to
judge
systems
more
adequately
15
in
naively
optimizing
towards
chosen
metric
one
could
miss
their
target
of
reducing
societal
harm
and
be
left
with
system
that
only
looks
better
to
the
metric
more
holistic
algorithmic
design
and
assessment
might
be
necessary
to
ensure
fair
algorithms
mökander
et
al
26
suggest
ethics
based
auditing
as
method
to
ensure
morally
sustainable
algorithms
ethics
based
auditing
expands
on
the
auditing
pro
cess
of
trying
to
assess
the
system
from
within
the
system
first
one
identifies
moral
values
and
potential
harms
so
there
can
be
continuous
evaluation
of
whether
the
system
upholds
these
moral
values
this
enables
us
to
evaluate
whether
performance
indicators
used
still
measure
the
underlying
norm
rather
than
only
evaluating
whether
the
system
still
adheres
to
the
performance
indicators
conclusion
we
investigated
amortized
equity
of
attention
as
definition
of
fair
ness
and
showed
how
system
could
manipulate
electoral
results
while
being
fair
to
amortized
equity
the
proposed
algorithm
un
fair
achieves
competitive
performance
for
fair
learning-to-rank
information
retrieval
system
while
simultaneously
being
clearly
manipulative
in
setup
acknowledgments
this
work
is
funded
in
part
by
the
eu
project
openwebsearch
eu
under
ga
101070014
we
would
like
to
thank
frederik
zuiderveen
borgesius
marvin
van
bekkum
richard
felius
and
the
adm
club
for
their
valuable
input
throughout
this
research
ethical
concerns
this
paper
establishes
an
algorithm
to
circumvent
fairness
measures
to
still
achieve
electoral
manipulation
this
method
only
becomes
relevant
if
adhering
to
amortized
equity
were
to
become
manda
tory
without
further
qualititative
measures
we
deem
it
unlikely
that
this
work
will
be
used
for
nefarious
purposes
relevance
assessment
to
start
we
can
see
whether
users
are
shown
documents
that
are
relevant
to
their
interests
this
would
be
performance
of
model
in
classical
sense
to
assess
performance
according
to
relevance
we
use
the
normalized
discounted
cumulative
gain
or
ndcg
18
ndcg
is
derived
metric
from
dcg
which
is
defined
by
su
lin
blodgett
solon
barocas
hal
daumé
iii
and
hanna
wallach
2020
lan
guage
technology
is
power
critical
survey
of
bias
in
nlp
in
proceed
ings
of
the
58th
annual
meeting
of
the
association
for
computational
linguis
tics
association
for
computational
linguistics
online
5454
5476
https
doi
org
10.18653
v1
2020
acl-main
485
alexandra
chouldechova
2017
fair
prediction
with
disparate
impact
study
of
bias
in
recidivism
prediction
instruments
big
data
2017
153
163
feder
cooper
and
ellen
abrams
2021
emergent
unfairness
in
algorithmic
fairness-accuracy
trade-off
research
in
proceedings
of
the
2021
aaai
acm
con
ference
on
ai
ethics
and
society
46
54
yashar
deldjoo
dietmar
jannach
alejandro
bellogin
alessandro
difonzo
and
dario
zanzonelli
2022
survey
of
research
on
fair
recommender
systems
arxiv
preprint
arxiv
2205.11127
2022
fernando
diaz
bhaskar
mitra
michael
ekstrand
asia
biega
and
ben
carterette
2020
evaluating
stochastic
rankings
with
expected
exposure
in
proceedings
of
the
29th
acm
international
conference
on
information
knowledge
management
275
284
10
virginie
do
sam
corbett-davies
jamal
atif
and
nicolas
usunier
2022
online
certification
of
preference-based
fairness
for
personalized
recommender
systems
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
vol
36
6532
6540
dcg
log
11
tim
draws
nava
tintarev
ujwal
gadiraju
alessandro
bozzon
and
benjamin
timmermans
2021
this
is
not
what
we
ordered
exploring
why
biased
search
result
rankings
affect
user
attitudes
on
debated
topics
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
295
305
here
is
the
position
of
document
for
user
when
ranked
under
policy
dcg
in
itself
captures
the
intuition
that
relevant
documents
need
to
be
placed
near
the
top
of
ranking
but
since
the
range
of
dcg
can
vary
between
queries
comparing
systems
across
queries
is
cumbersome
to
ease
comparison
across
queries
dcg
is
normalized
by
di
viding
by
the
ideal
dcg
obtainable
on
the
query
we
evaluate
the
ideal
ranking
policy
which
orders
all
documents
by
relevance
the
ideal
ranker
is
not
practically
obtainable
but
provides
useful
12
sanghamitra
dutta
dennis
wei
hazar
yueksel
pin-yu
chen
sijia
liu
and
kush
varshney
2020
is
there
trade-off
between
fairness
and
accuracy
perspective
using
mismatched
hypothesis
testing
in
international
conference
on
machine
learning
pmlr
2803
2813
13
robert
epstein
and
ronald
robertson
2015
the
search
engine
manipulation
effect
seme
and
its
possible
impact
on
the
outcomes
of
elections
proceedings
of
the
national
academy
of
sciences
112
33
2015
e4512-e4521
14
sina
fazelpour
and
zachary
lipton
2020
algorithmic
fairness
from
non-ideal
perspective
in
proceedings
of
the
aaai
acm
conference
on
ai
ethics
and
society
57
63
15
ben
green
2019
good
isn
good
enough
in
proceedings
of
the
ai
for
social
good
workshop
at
neurips
vol
17
benchmark
we
denote
the
ideal
dcg
as
idcg
dcg
and
the
normalized
dcg
ndcg
as
ndcg
dcg
as
result
ndcg
is
always
between
and
allowing
for
comparison
across
queries
one
nuance
here
is
that
it
might
not
be
appropriate
to
assume
the
user
observes
all
documents
pre
sented
the
dcg
formula
can
be
adapted
to
meet
this
need
we
index
the
documents
based
on
their
position
on
the
results
page
dcg
evaluated
at
the
th
position
is
calculated
as
dcg
16
anat
hashavit
hongning
wang
raz
lin
tamar
stern
and
sarit
kraus
2021
un
derstanding
and
mitigating
bias
in
online
health
search
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
virtual
event
canada
sigir
21
association
for
computing
machin
ery
new
york
ny
usa
265
274
https://doi.org/10.1145/3404835.3462930
17
alex
hern
2018
cambridge
analytica
how
did
it
turn
clicks
into
votes
the
guardian
2018
18
kalervo
järvelin
and
jaana
kekäläinen
2002
cumulated
gain-based
evaluation
of
ir
techniques
acm
transactions
on
information
systems
tois
20
2002
422
446
19
thorsten
joachims
laura
granka
bing
pan
helene
hembrooke
filip
radlinski
and
geri
gay
2007
evaluating
the
accuracy
of
implicit
feedback
from
clicks
and
and
ndcg
can
similarly
be
adapted
query
reformulations
in
web
search
acm
transactions
on
information
systems
to
log
ndcg
dcg
as
result
we
can
tune
our
performance
measure
to
observa
tions
made
if
users
typically
investigate
no
more
than
10
docu
ments
ndcg
10
is
suitable
to
assess
whether
any
given
system
is
delivering
relevant
results
to
the
users
if
we
vary
the
order
of
preference
of
ranking
systems
can
also
vary
references
agathe
balayn
and
seda
gürses
2021
beyond
debiasing
regulating
ai
and
its
in
equalities
edri
report
https://edri.
org
wp-content
uploads
2021
09
edri_beyond
debiasing-report_online
pdf
2021
solon
barocas
moritz
hardt
and
arvind
narayanan
2019
fairness
and
machine
learning
limitations
and
opportunities
fairmlbook
org
http://www.fairmlbook.
org
hal
berghel
2018
malice
domestic
the
cambridge
analytica
dystopia
computer
51
2018
84
89
asia
biega
krishna
gummadi
and
gerhard
weikum
2018
equity
of
attention
amortizing
individual
fairness
in
rankings
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
405
414
20
joshua
kalla
and
david
broockman
2018
the
minimal
persuasive
effects
of
campaign
contact
in
general
elections
evidence
from
49
field
experiments
american
political
science
review
112
2018
148
166
21
jon
kleinberg
sendhil
mullainathan
and
manish
raghavan
2016
inherent
trade-offs
in
the
fair
determination
of
risk
scores
corr
abs
1609.05807
2016
arxiv
1609.05807
http://arxiv.org/abs/1609.05807
22
yunqi
li
hanxiong
chen
shuyuan
xu
yingqiang
ge
juntao
tan
shuchang
liu
and
yongfeng
zhang
2022
fairness
in
recommendation
survey
arxiv
preprint
arxiv
2205.13619
2022
23
tie-yan
liu
et
al
2009
learning
to
rank
for
information
retrieval
foundations
and
trends
in
information
retrieval
2009
225
331
24
david
losada
javier
parapar
and
alvaro
barreiro
2019
when
to
stop
making
relevance
judgments
study
of
stopping
methods
for
building
information
retrieval
test
collections
journal
of
the
association
for
information
science
and
technology
70
2019
49
60
25
alessandro
melchiorre
navid
rekabsaz
emilia
parada-cabaleiro
stefan
brandl
oleg
lesota
and
markus
schedl
2021
investigating
gender
fairness
of
recommen
dation
algorithms
in
the
music
domain
information
processing
management
58
2021
102666
26
jakob
mökander
jessica
morley
mariarosaria
taddeo
and
luciano
floridi
2021
ethics-based
auditing
of
automated
decision-making
systems
nature
scope
and
limitations
science
and
engineering
ethics
27
2021
30
27
marco
morik
ashudeep
singh
jessica
hong
and
thorsten
joachims
2020
con
trolling
fairness
and
bias
in
dynamic
learning-to-rank
in
proceedings
of
the
43rd
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
429
438
28
arvind
narayanan
2019
how
to
recognize
ai
snake
oil
arthur
miller
lecture
on
science
and
ethics
2019
29
harrie
oosterhuis
2021
computationally
efficient
optimization
of
plackett-luce
ranking
models
for
relevance
and
fairness
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
1023
1032
30
zohreh
ovaisi
kathryn
vasilaky
and
elena
zheleva
2021
propensity
independent
bias
recovery
in
offline
learning-to-rank
systems
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
1763
1767
31
amifa
raj
connor
wood
ananda
montoly
and
michael
ekstrand
2020
com
paring
fair
ranking
metrics
arxiv
preprint
arxiv
2009.01311
2020
32
andrew
selbst
danah
boyd
sorelle
friedler
suresh
venkatasubramanian
and
janet
vertesi
2019
fairness
and
abstraction
in
sociotechnical
systems
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
59
68
33
ashudeep
singh
and
thorsten
joachims
2018
fairness
of
exposure
in
rankings
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
2219
2228
34
ashudeep
singh
and
thorsten
joachims
2019
policy
learning
for
fairness
in
ranking
advances
in
neural
information
processing
systems
32
2019
35
eero
sormunen
2002
liberal
relevance
criteria
of
trec
counting
on
negligible
documents
in
proceedings
of
the
25th
annual
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
324
330
36
david
sumpter
2018
why
the
facebook
data
available
to
cambridge
analytica
could
not
be
used
to
target
personalities
in
the
us
presidential
election
2018
https://soccermatics.medium.com/why-the-facebook-data-
available-to-cambridge-analytica-could-not-be-used-to-target-personalities
in-2904fa0571bd
37
jared
sylvester
and
edward
raff
2018
what
about
applied
fairness
arxiv
preprint
arxiv
1806.05250
2018
38
channel
news
investigations
team
2018
exposed
undercover
secrets
of
trump
data
firm
2018
https://www.channel4.com/news/exposed-
undercover-secrets-of-donald-trump-data-firm-cambridge-analytica
39
nguyen
vo
and
kyumin
lee
2019
learning
from
fact-checkers
analysis
and
generation
of
fact-checking
language
in
proceedings
of
the
42nd
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
paris
france
sigir
19
association
for
computing
machinery
new
york
ny
usa
335
344
https://doi.org/10.1145/3331184.3331248
40
ellen
voorhees
ian
soboroff
and
jimmy
lin
2022
can
old
trec
col
lections
reliably
evaluate
modern
neural
retrieval
models
arxiv
preprint
arxiv
2201.11086
2022
41
sandra
wachter
brent
mittelstadt
and
chris
russell
2021
why
fairness
cannot
be
automated
bridging
the
gap
between
eu
non-discrimination
law
and
ai
computer
law
security
review
41
2021
105567
42
lequn
wang
yiwei
bai
wen
sun
and
thorsten
joachims
2021
fairness
of
exposure
in
stochastic
bandits
in
international
conference
on
machine
learning
pmlr
10686
10696
43
xuanhui
wang
michael
bendersky
donald
metzler
and
marc
najork
2016
learning
to
rank
with
selection
bias
in
personal
search
in
proceedings
of
the
39th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
115
124
44
hilde
weerts
lambèr
royakkers
and
mykola
pechenizkiy
2022
does
the
end
justify
the
means
on
the
moral
justification
of
fairness-aware
machine
learning
arxiv
preprint
arxiv
2202.08536
2022
45
tao
yang
and
qingyao
ai
2021
maximizing
marginal
fairness
for
dynamic
learning
to
rank
in
proceedings
of
the
web
conference
2021
137
145
46
xun
yang
xiangnan
he
xiang
wang
yunshan
ma
fuli
feng
meng
wang
and
tat-seng
chua
2019
interpretable
fashion
matching
with
rich
attributes
in
proceedings
of
the
42nd
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
paris
france
sigir
19
association
for
computing
machinery
new
york
ny
usa
775
784
https://doi.org/10.1145/
3331184.3331242
47
shoshana
zuboff
2019
the
age
of
surveillance
capitalism
the
fight
for
human
future
at
the
new
frontier
of
power
barack
obama
books
of
2019
profile
books
121
125
pages
received
may
10
2023