santa
clara
university
scholar
commons
engineering
ph
theses
student
scholarship
2024
fairness
and
bias
of
machine
learning
in
search
and
ranking
yuan
wang
follow
this
and
additional
works
at
https://scholarcommons.scu.edu/eng_phd_theses
fairness
and
bias
of
machine
learning
in
search
and
ranking
by
yuan
wang
dissertation
submitted
in
partial
fulﬁllment
of
the
requirements
for
the
degree
of
doctor
of
philosophy
in
computer
science
engineering
in
the
school
of
engineering
at
santa
clara
university
2024
santa
clara
california
dedicated
to
my
family
iii
acknowledgements
first
and
foremost
extend
my
deepest
gratitude
to
my
advisor
professor
yi
fang
whose
unwavering
guidance
and
support
have
been
instrumental
to
my
doctoral
journey
professor
fang
not
only
believed
in
my
potential
but
also
supported
me
with
patience
kindness
and
an
unmatched
dedication
to
excellence
his
mentorship
transcended
academic
instruction
oﬀering
personal
support
and
invaluable
life
lessons
that
have
shaped
me
both
as
scholar
and
as
an
individual
he
teaches
me
to
keep
trying
to
be
curious
and
to
work
hard
want
to
keep
doing
these
things
in
my
job
too
learning
from
him
has
been
really
special
chance
for
me
and
so
thankful
for
it
would
like
to
thank
my
doctoral
committee
consisting
of
prof
zhiqiang
tao
prof
david
anastasiu
prof
sean
choi
and
prof
haibing
lu
for
their
time
and
suggestions
to
make
my
thesis
better
would
like
to
thank
my
lab
mates
travis
ebesu
xuyang
wu
zhiyuan
peng
and
suthee
chaidaroon
who
supported
me
through
diﬀerent
parts
of
this
journey
lastly
would
like
to
thank
my
family
my
parents
and
my
brother
have
given
me
endless
love
and
support
throughout
this
entire
journey
also
want
to
thank
my
ﬁancée
yijia
for
her
love
and
patience
iv
fairness
and
bias
of
machine
learning
in
search
and
ranking
yuan
wang
department
of
computer
science
engineering
santa
clara
university
santa
clara
california
2024
abstract
recent
advancements
in
information
retrieval
ir
and
machine
learning
have
significantly
improved
ranking
and
search
system
performance
however
these
data-driven
approaches
often
suﬀer
from
inherent
biases
present
in
training
datasets
leading
to
unfair
treatment
of
certain
demographic
groups
and
contributing
to
systematic
discrimination
based
on
race
gender
or
geographic
location
this
research
aims
to
address
the
fairness
and
bias
issue
in
ranking
and
search
systems
by
proposing
innovative
frameworks
that
mitigate
data
bias
and
ensure
equitable
representation
and
exposure
across
diverse
groups
we
introduce
two
novel
frameworks
the
meta-learning
based
fair
ranking
mfr
model
and
the
meta
curriculum-based
fair
ranking
mcfr
framework
both
designed
to
alleviate
dataset
bias
through
automatically-weighted
loss
functions
and
curriculum
learning
strategies
respectively
these
approaches
utilize
meta-learning
to
adjust
ranking
loss
focusing
particularly
on
improving
the
fairness
metrics
for
minority
groups
while
maintaining
competitive
ranking
performance
additionally
we
conduct
an
empirical
evaluation
of
large
language
models
llms
in
text-ranking
tasks
revealing
biases
in
handling
queries
and
documents
related
to
binary
protected
attributes
our
analysis
oﬀers
benchmark
for
assessing
llms
fairness
and
highlights
the
necessity
for
equitable
representation
in
search
outcomes
furthermore
we
explore
the
challenge
of
data
selection
bias
in
multi-stage
recommendation
systems
particularly
in
online
advertising
contexts
like
pinterest
multi-cascade
ads
ranking
system
through
comprehensive
experiments
we
assess
various
state-ofthe-art
methods
and
our
ﬁndings
demonstrate
the
eﬀectiveness
of
modiﬁed
version
of
unsupervised
domain
adaptation
muda
in
mitigating
selection
bias
collectively
our
work
contributes
to
the
development
of
fairer
ranking
and
search
systems
by
addressing
bias
at
its
source
and
employing
meta-learning
and
curriculum
learning
techniques
we
pave
the
way
for
more
equitable
and
transparent
ir
systems
that
serve
diverse
user
bases
without
discrimination
contents
acknowledgements
iv
abstract
contents
vii
list
of
figures
list
of
tables
xiii
introduction
1.1
motivation
1.2
overview
1.3
contributions
1.4
outline
16
16
18
19
21
related
work
2.1
fairness
on
ranking
2.2
meta-learning
on
fairness
2.3
fairness
in
llms
2.4
selection
bias
23
23
24
25
26
meta-learning
approach
to
fair
ranking
3.1
introduction
3.2
meta-learning
based
fair
ranking
3.3
experiments
3.4
conclusion
28
28
32
36
41
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
42
4.1
introduction
42
vii
contents
viii
4.2
meta
curriculum-based
fair
ranking
4.2
problem
setting
4.2
uniﬁed
mcfr
framework
4.2
parameter
update
4.2
ranking
and
fairness
loss
4.2
4.1
ranking
terms
4.2
4.2
fairness
terms
4.2
curriculum
sampling
experiments
4.3
experimental
setting
4.3
1.1
baselines
4.3
1.2
implementation
details
4.3
fair
ranking
performance
4.3
ablation
studies
4.3
3.1
ranking
terms
analysis
4.3
3.2
fairness
terms
analysis
4.3
3.3
curriculum
sampling
analysis
4.3
3.4
data
eﬃciency
4.3
3.5
training
and
inference
eﬃciency
conclusion
47
48
48
51
53
53
54
55
59
59
61
62
63
65
66
67
68
69
69
70
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.1
introduction
5.2
llm
fair
ranking
5.2
datasets
5.2
listwise
evaluation
5.2
2.1
data
construction
5.2
2.2
metrics
5.2
pairwise
evaluation
5.2
3.1
data
construction
5.2
3.2
metrics
5.3
results
and
analysis
5.3
0.1
eﬀect
of
window
and
step
size
5.3
listwise
evaluation
results
5.3
1.1
item-side
analysis
5.3
1.2
query-side
analysis
5.3
pairwise
evaluation
results
5.3
overall
evaluation
5.4
enhancing
fairness
with
lora
5.5
conclusion
71
71
74
75
75
76
77
78
79
79
79
80
80
82
84
85
86
87
88
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
90
4.3
4.4
contents
6.1
6.2
6.3
6.4
6.5
ix
introduction
90
bias
in
pinterest
ads
96
6.2
datasets
and
training
pipeline
96
6.2
selection
bias
98
6.2
problem
formulation
99
solution
100
6.3
naive
method
binary
classiﬁcation
100
6.3
in-batch
negative
classiﬁcation
100
6.3
knowledge
distillation
101
6.3
transfer
learning
101
6.3
adversarial
regularization
102
6.3
unsupervised
domain
adaptation
103
6.3
6.1
naive
uda
103
6.3
6.2
modiﬁed
uda
104
experiments
and
results
106
6.4
datasets
106
6.4
experimental
setting
107
6.4
evaluation
metrics
109
6.4
oﬄine
evaluation
110
6.4
online
experiments
111
6.4
5.1
overall
evaluation
111
6.4
5.2
evaluation
by
ads
objective
type
114
6.4
5.3
conversion
ads
116
6.4
variants
of
muda
117
conclusion
120
conclusion
121
bibliography
123
list
of
figures
3.1
3.2
3.3
4.1
4.2
illustration
of
the
predicted
rankings
distribution
of
the
protected
groups
female
students
african
american
students
on
the
two
diﬀerent
datasets
we
report
kendall
tau
as
the
ranking
metric
the
proposed
mfr
model
ranks
the
items
from
the
protected
groups
higher
compared
to
listnet
17
which
indicates
that
the
mfr
improves
the
protected
attribute
exposure
with
unbiased
ranking
performance
29
mfr
learning
algorithm
ﬂowchart
steps
and
in
algorithm
note
that
is
the
ranking
model
is
the
meta-learner
is
the
batch
size
for
the
training
dataset
is
the
batch
size
for
the
meta-dataset
and
and
are
the
learning
rates
at
each
iteration
we
ﬁrstly
update
in
the
meta-learner
using
eq
with
the
meta-dataset
and
then
we
update
in
the
ranking
model
using
eq
with
the
training
dataset
32
the
plot
of
the
variation
of
learned
weight
over
the
two
training
datasets
the
weight
diﬀerence
is
computed
as
φdiﬀ
φi
φi
and
we
plot
the
φdiﬀ
over
the
training
epochs
as
shown
in
the
plot
the
weighting
function
is
converging
as
the
diﬀerent
values
of
weights
between
each
epoch
are
decreasing
to
0.0
40
illustration
of
the
predicted
rankings
distribution
of
two
protected
attributes
on
four
datasets
law
student
gender
82
law
student
race
82
compas
and
engineering
student
89
we
report
kendall
tau
48
as
the
ranking
performance
mcfr
and
mfr
80
improve
the
protected
attributes
ranking
while
realizing
competitive
ranking
performance
compared
with
listnet
17
demonstrating
that
our
approach
could
increase
the
exposure
of
the
minority
43
mcfr
learning
algorithm
ﬂowchart
steps
and
in
algorithm
note
that
is
the
ranking
model
is
the
meta
learner
is
the
batch
size
for
the
training
dataset
is
the
batch
size
for
the
meta-dataset
and
and
are
the
learning
rates
at
each
iteration
we
ﬁrstly
update
in
the
meta
learner
using
eq
with
the
meta-dataset
sampled
from
the
curriculum
sampling
with
update
of
sampling
diﬃculty
at
each
epoch
and
then
we
update
in
the
ranking
model
using
eq
with
the
training
dataset
47
list
of
figures
4.3
4.4
5.1
5.2
5.3
5.4
6.1
xi
curriculum
sampling
strategy
illustrated
on
the
engineering
student
gender
dataset
we
use
the
same
ratio
between
the
unprotected
group
and
protected
group
in
the
meta-dataset
as
the
training
dataset
at
the
beginning
training
epoch
we
gradually
decrease
the
ratio
as
the
training
epoch
increase
until
the
ratio
becomes
which
shows
balanced
metadataset
55
evaluation
results
on
the
down-sampling
experiments
we
conduct
the
experiment
on
law
students
gender
and
law
students
race
datasets
and
we
down-sample
the
training
data
from
the
rate
of
0.1
to
0.9
the
results
show
that
mcfr
has
better
data
eﬃciency
as
it
could
achieve
better
fairness
metrics
with
similar
ranking
performance
than
mfr
and
autodebias
at
diﬀerent
down-sampling
rate
68
illustration
of
two
evaluation
methods
listwise
evaluation
and
pairwise
evaluation
each
document
is
associated
with
binary
protected
attribute
which
is
used
in
the
fairness
evaluation
metrics
proposed
evaluation
framework
this
schematic
diagram
represents
our
dual
evaluation
methodology
the
top
sequence
depicts
the
listwise
ranking
process
where
items
from
protected
and
unprotected
groups
are
presented
to
various
llms
gpt-3
gpt-4
mistral-7b
and
llama2
and
are
evaluated
on
utility
and
group
exposure
metrics
the
bottom
sequence
illustrates
the
pairwise
ranking
approach
which
contrasts
the
ranking
preference
of
llms
between
items
from
protected
and
unprotected
groups
quantifying
any
bias
by
the
percentage
of
unprotected
group
items
ranked
higher
the
predicted
rankings
distribution
of
the
protected
groups
on
the
trec
datasets
using
the
listwise
evaluation
the
plots
reveal
the
ranking
variability
and
potential
biases
in
gender
and
geographic
attributes
highlighting
areas
for
improvement
in
fairness
across
the
llms
impact
of
lora
fine-tuning
on
mistral-7b
fairness
figure
shows
the
percentage
of
ﬁrst-ranked
items
from
protected
and
unprotected
groups
while
figure
demonstrates
the
resulting
fairness
ratios
the
loraadjusted
model
yields
ratios
closer
to
the
ideal
fairness
benchmark
of
1.0
across
trec
datasets
72
74
81
87
the
life
cycle
of
online
ads
delivery
at
high
level
an
ads
request
is
triggered
when
user
opens
the
pinterest
app
or
starts
new
session
and
the
ads
request
will
be
sent
to
the
ads
delivery
system
to
query
for
dozen
of
ads
in
the
ads
delivery
backend
ad
candidates
in
the
inventory
will
ﬂow
through
various
stages
like
targeting
retrieval
ranking
and
auction
which
sends
the
auction
winners
back
to
the
mobile
app
where
the
selected
ads
will
be
visible
to
the
user
91
list
of
figures
6.2
xii
distribution
of
features
and
labels
across
three
ads
datasets
related
to
retrieval
modeling
shows
the
ﬂow
of
major
ad
candidates
along
the
ads
delivery
funnel
shows
the
distribution
of
empirical
vtcvr
one
of
key
retrieval
model
features
across
three
datasets
for
retrieval
training
serving
shows
the
distribution
of
empirical
good
click
rate
one
of
key
retrieval
model
features
across
three
datasets
for
retrieval
training
serving
shows
the
distribution
of
the
ranking
model
predictions
used
as
the
pseudo
label
in
retrieval
model
training
across
three
datasets
note
that
the
exact
values
on
x-axes
are
hidden
for
conﬁdentiality
reasons
95
list
of
tables
3.1
experimental
results
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
so
the
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
the
bold
text
indicates
the
model
with
the
best
performance
and
the
results
show
that
the
mfr
model
is
better
on
the
fairness
metrics
with
comparable
performance
on
the
ranking
metrics
against
other
state-of-the-art
models
39
4.1
summary
of
ranking
and
fairness
terms
used
in
the
loss
function
the
loss
function
used
in
the
framework
is
γu
and
we
can
insert
the
above
exposure
terms
and
ranking
loss
terms
as
needed
note
that
denotes
the
number
of
candidates
per
query
summary
of
dataset
statistics
we
report
the
average
counts
of
total
and
unprotected
items
per
query
for
the
w3c
experts
and
engineering
students
datasets
we
provide
the
exact
item
counts
for
the
law
students
and
compas
datasets
each
of
which
contains
only
one
query
experimental
results
with
hinge
exposure
89
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
so
the
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
the
bold
text
indicates
the
model
with
the
best
performance
and
the
results
show
that
the
mcfr
model
is
better
on
the
fairness
metrics
with
comparable
performance
on
the
ranking
metrics
against
other
state-of-the-art
models
ablation
study
results
with
rankmse
11
ablation
study
results
with
ranknet
15
ablation
study
results
with
listnet
17
experimental
results
on
total
convergence
time
in
seconds
it
shows
the
total
convergence
time
for
diﬀerent
algorithms
deltr
mfr
and
mcfr
across
various
datasets
or
scenarios
based
on
the
table
the
mcfr
framework
generally
has
comparable
convergence
time
than
the
other
two
algorithms
4.2
4.3
4.4
4.5
4.6
4.7
xiii
51
58
64
66
66
67
69
list
of
tables
5.1
5.2
5.3
6.1
6.2
6.3
6.4
6.5
xiv
evaluation
results
on
diﬀerent
choices
of
window
and
step
sizes
the
results
show
that
there
are
not
signiﬁcant
diﬀerences
in
the
ranking
and
fairness
metrics
so
we
select
window
size
and
step
size
in
the
listwise
evaluation
experiments
80
listwise
evaluation
results
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
where
values
closer
to
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
precision
10
10
scores
indicate
better
performance
82
pairwise
evaluation
results
the
table
displays
fairness
metrics
for
llms
in
ranking
both
relevant
and
irrelevant
item
pairs
one
from
the
protected
and
the
other
from
the
unprotected
groups
it
includes
percentages
of
items
ranked
ﬁrst
from
each
group
and
their
ratio
reﬂecting
fairness
the
varying
levels
of
fairness
across
llms
particularly
in
irrelevant
pairings
highlight
the
importance
of
further
enhancing
fairness
in
llms
85
auc-roc
on
evaluation
dataset
the
models
such
as
knowledge
distillation
adversarial
learning
binary
classiﬁcation
trained
with
auction
winners
dataset
usually
have
better
oﬄine
evaluation
results
110
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
various
models
on
all
types
of
ads
both
in-batch
negative
and
knowledge
distillation
methods
improve
gctr30
at
the
cost
of
impression
drop
and
muda
is
the
only
method
to
recommend
more
ads
with
higher
quality
as
observed
by
the
increased
gctr30
without
impression
drop
112
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
two
promising
models
on
each
type
awareness
traﬃc
web-conversion
of
ads
in-batch
negative
classiﬁcation
model
works
better
on
the
traﬃc
ads
and
muda
model
helps
web-conversion
ads
the
most
114
online
metrics
performance
of
in-batch
negative
classiﬁcation
and
muda
models
on
web-conversion
ads
in-batch
negative
classiﬁcation
model
leads
to
lower
conversion
probability
on
each
ads
impression
icvr
and
thus
has
higher
cpa
cost
to
advertisers
in
contrast
muda
model
recommended
ad
candidates
with
higher
conversion
rate
and
therefore
lower
cpa
cost
116
online
lifts
of
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
with
various
muda
variants
on
all
types
of
ads
muda
v1
achieves
the
highest
gain
on
ads
engagement
both
ctr
and
gctr30
and
muda
v3
achieves
the
most
balanced
gain
across
diﬀerent
metrics
with
good
gctr30
and
impression
lift
117
list
of
tables
6.6
6.7
xv
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
muda
variants
on
each
type
awareness
traﬃc
web-conversion
of
ads
where
muda
v3
shows
best
balanced
impression
gains
among
them
118
online
lifts
of
ads
hide
rate
hdr
re-pin
rate
rpr
observed
with
muda
variants
on
all
types
of
ads
muda
v3
achieves
the
most
balanced
performance
with
fewer
ads
being
hidden
and
more
ads
being
repined
by
the
users
119
chapter
introduction
1.1
motivation
the
quest
for
fairness
in
information
retrieval
ir
systems
is
gaining
unprecedented
attention
as
the
digital
era
demands
equity
across
all
platforms
and
services
central
to
this
pursuit
is
the
challenge
of
mitigating
systematic
biases
within
data-driven
ranking
models
these
biases
often
reﬂection
of
historical
discrimination
manifest
as
unfair
treatment
towards
underrepresented
groups
leading
to
disparate
exposure
and
unequal
opportunities
in
various
real-world
applications
such
as
expert
search
and
job
recommendations
the
essence
of
fairness
in
ir
extends
beyond
mere
algorithmic
adjustments
it
is
about
ensuring
that
all
demographic
groups
have
equal
visibility
and
representation
in
the
outcomes
of
search
and
recommendation
systems
16
chapter
introduction
17
to
address
the
inherent
biases
in
datasets
used
for
training
machine
learning
models
we
developed
few
novel
frameworks
these
frameworks
including
meta-learning
based
fair
ranking
mfr
and
meta
curriculum-based
fair
ranking
mcfr
represent
signiﬁcant
strides
towards
achieving
equitable
treatment
across
protected
attributes
by
re-weighting
ranking
losses
and
incorporating
curriculum
learning
into
meta
dataset
construction
these
models
aim
to
balance
exposure
between
advantaged
and
disadvantaged
groups
oﬀering
more
nuanced
approach
to
fairness
that
transcends
traditional
mitigation
strategies
the
integration
of
large
language
models
llms
in
ranking
tasks
further
complicates
the
landscape
of
fairness
in
ir
despite
their
superior
performance
in
understanding
and
processing
natural
language
llms
are
not
immune
to
fairness
concerns
the
empirical
scrutiny
of
these
models
against
fairness
benchmarks
reveals
pressing
need
to
evaluate
and
ﬁne-tune
them
with
focus
on
equity
ensuring
that
their
deployment
does
not
perpetuate
existing
biases
lastly
the
realm
of
online
advertising
particularly
in
multi-stage
recommendation
systems
like
those
used
in
ad
retrieval
underscores
the
pervasive
challenge
of
selection
bias
while
this
area
might
seem
tangential
it
shares
the
core
issue
of
bias
mitigation
with
broader
ir
systems
eﬃciently
managing
the
diversity
and
quality
of
ads
in
the
upper
funnel
stages
without
succumbing
to
biases
is
crucial
for
maintaining
the
integrity
and
fairness
of
digital
advertising
ecosystems
in
summary
the
motivation
for
this
thesis
stems
from
the
urgent
need
to
address
and
chapter
introduction
18
rectify
fairness
issues
in
ir
systems
through
comprehensive
exploration
of
innovative
frameworks
meticulous
evaluation
of
llms
and
consideration
of
selection
bias
in
online
advertising
this
work
aims
to
contribute
meaningful
solutions
to
the
overarching
challenge
of
ensuring
fairness
in
the
digital
information
landscape
1.2
overview
this
thesis
presents
comprehensive
exploration
of
fairness
in
ranking
and
search
systems
addressing
the
multifaceted
challenge
of
bias
in
information
retrieval
ir
through
series
of
innovative
approaches
and
methodologies
across
four
distinct
but
interconnected
studies
we
delve
into
the
complexities
of
data
bias
selection
bias
and
the
ethical
implications
of
large
language
models
llms
in
text
ranking
providing
holistic
examination
of
fairness
in
the
digital
information
landscape
we
ﬁrstly
introduces
the
meta-learning
based
fair
ranking
mfr
model
an
advanced
framework
designed
to
mitigate
data
bias
by
re-weighting
ranking
losses
through
bilevel
optimization
process
this
model
not
only
enhances
fairness
metrics
but
also
maintains
competitive
ranking
performance
oﬀering
scalable
solution
for
equitable
ir
systems
building
on
this
foundation
we
proposes
the
meta
curriculum-based
fair
ranking
mcfr
framework
which
further
addresses
data
bias
by
integrating
in-processing
and
pre-processing
techniques
with
curriculum
learning
mcfr
demonstrates
remarkable
chapter
introduction
19
versatility
and
eﬀectiveness
in
improving
fairness
metrics
across
various
ranking
loss
functions
showcasing
its
potential
as
generic
framework
for
fair
ranking
we
then
focus
the
evaluation
of
fairness
in
llms
for
text
ranking
establishing
benchmark
that
incorporates
listwise
and
pairwise
evaluation
methods
focused
on
binary
protected
attributes
through
extensive
experimentation
we
reveal
inherent
fairness
issues
in
llms
and
propose
ﬁne-tuning
strategy
using
low-rank
adaptation
lora
to
mitigate
these
issues
marking
signiﬁcant
step
towards
more
equitable
llm-based
ranking
systems
finally
we
tackles
the
selection
bias
in
multi-cascade
advertisement
recommendation
systems
surveying
state-of-the-art
modeling
strategies
and
introducing
modiﬁed
unsupervised
domain
adaptation
muda
approach
muda
outperforms
both
contemporary
models
and
the
existing
production
model
in
online
settings
highlighting
its
eﬀectiveness
in
addressing
selection
bias
and
enhancing
the
fairness
and
eﬃciency
of
recommendation
systems
1.3
contributions
the
contribution
of
thesis
can
be
summarized
as
follows
we
introduce
the
meta-learning
based
fair
ranking
mfr
model
novel
approach
that
addresses
data
bias
in
ranking
systems
by
automatically
adjusting
ranking
losses
the
mfr
model
framed
as
bilevel
optimization
problem
and
chapter
introduction
20
solved
through
an
innovative
gradients-through-gradients
technique
demonstrates
its
robustness
and
eﬀectiveness
in
real-world
datasets
our
results
highlight
mfr
capacity
to
achieve
competitive
ranking
performance
while
signiﬁcantly
enhancing
fairness
metrics
marking
critical
advancement
in
the
pursuit
of
fair
information
retrieval
systems
we
present
the
meta
curriculum-based
fair
ranking
mcfr
framework
an
innovative
approach
that
mitigates
data
bias
by
blending
in-processing
and
preprocessing
techniques
with
curriculum
learning
mcfr
formulated
as
bilevel
optimization
problem
solved
via
gradients-through-gradients
proves
versatile
across
various
ranking
loss
functions
and
fairness
metrics
our
empirical
studies
across
public
datasets
aﬃrm
mcfr
eﬀectiveness
in
matching
existing
ranking
performances
while
signiﬁcantly
advancing
fairness
metrics
notably
mcfr
enhances
fairness
more
eﬃciently
requiring
less
data
and
achieving
fast
convergence
positioning
it
as
highly
adaptable
and
impactful
framework
in
promoting
fairness
in
ranking
systems
we
create
benchmark
for
evaluating
the
fairness
of
large
language
models
llms
in
text
ranking
focusing
on
binary
protected
attributes
through
listwise
and
pairwise
methods
our
extensive
experiments
on
real-world
datasets
reveal
fairness
issues
in
llms
prompting
us
to
propose
ﬁne-tuning
strategy
using
lowrank
adaptation
lora
speciﬁcally
designed
to
address
these
concerns
this
dual
approach
of
identifying
and
mitigating
fairness
problems
marks
signiﬁcant
advancement
in
improving
llms
performance
in
ranking
tasks
chapter
introduction
21
we
address
the
selection
bias
in
advertisement
recommendation
systems
by
characterizing
the
issue
and
evaluating
various
modeling
strategies
our
exploration
leads
to
the
development
of
modiﬁed
unsupervised
domain
adaptation
muda
approach
which
stands
out
for
its
superior
performance
in
online
settings
outperforming
both
contemporary
models
and
the
existing
production
model
this
study
advances
the
mitigation
of
selection
bias
showcasing
muda
eﬀectiveness
in
enhancing
recommendation
fairness
and
eﬃciency
1.4
outline
this
thesis
is
structured
as
follows
chapter
reviews
the
existing
literature
on
fairness
in
information
retrieval
highlighting
the
signiﬁcance
of
addressing
biases
in
ranking
models
and
the
evolving
strategies
to
mitigate
these
challenges
chapter
details
the
meta-learning
based
fair
ranking
mfr
model
focusing
on
its
innovative
approach
to
enhance
fairness
by
adjusting
training
losses
for
improved
minority
group
exposure
and
its
validation
through
real-world
datasets
chapter
discusses
the
meta
curriculumbased
fair
ranking
mcfr
framework
which
integrates
meta-learning
with
curriculum
learning
to
counteract
data
bias
and
showcases
its
eﬀectiveness
over
traditional
fairness
models
chapter
explores
the
fairness
of
large
language
models
in
ranking
tasks
presenting
an
empirical
study
on
biases
and
introducing
mitigation
strategy
via
lora
ﬁne-tuning
to
promote
equitable
outcomes
chapter
investigates
selection
bias
in
pinterest
advertising
system
and
proposes
the
modiﬁed
unsupervised
domain
22
adaptation
muda
model
demonstrating
its
capacity
to
improve
recommendation
performance
and
advertising
eﬃciency
chapter
concludes
the
thesis
by
summarizing
the
key
contributions
reﬂecting
on
the
impact
of
this
work
on
fairness
in
search
and
ranking
and
suggesting
future
research
directions
to
further
advance
the
ﬁeld
chapter
related
work
2.1
fairness
on
ranking
zehlike
et
al
92
categorized
fair
ranking
models
into
score-based
and
supervised
learning
models
score-based
models
modify
score
outcomes
or
distributions
for
enhanced
fairness
notable
contributions
include
works
by
yang
et
al
86
87
celis
et
al
18
stoyanovich
et
al
72
kleinberg
et
al
47
and
asudeh
et
al
supervised
fairness
models
in
ranking
span
pre-processing
in-processing
and
postprocessing
approaches
pre-processing
models
exempliﬁed
by
lahoti
et
al
49
work
on
deriving
fair
training
data
in-processing
models
such
as
zehlike
et
al
deltr
89
address
fairness
during
training
focusing
on
exposure
bias
similarly
beutel
et
al
introduced
pairwise
ranking
loss
function
with
fairness
regularizer
while
ma
et
al
52
23
chapter
related
work
24
tackled
fairness
in
query
generation
haak
et
al
39
aimed
at
search
query
bias
identiﬁcation
and
chu
et
al
23
highlighted
biases
in
neural
architecture
search
evaluations
importantly
chen
et
al
20
proposed
meta-learning-based
debiasing
framework
for
recommendations
post-processing
models
conversely
reﬁne
model
outputs
post-training
for
fairness
among
these
zehlike
et
al
works
90
91
like
fa
ir
ensure
representation
of
protected
groups
and
oﬀer
continuous
fairness
interpolation
additionally
biega
et
al
10
developed
an
algorithm
optimizing
the
equity
of
user
attention
through
relevance
loss
function
2.2
meta-learning
on
fairness
meta-learning
is
ﬁeld
of
study
that
aims
to
improve
the
learning
ability
of
models
by
adapting
to
new
tasks
or
environments
and
it
could
be
divided
into
two
main
categories
model-based
30
and
learning
algorithm-based
in
addition
to
tasks
such
as
fewshot
learning
43
continual
learning
60
and
hyperparameter
optimization
32
fairness
is
an
important
ﬁeld
zhao
et
al
98
presented
the
follow
the
fair
meta
leader
ffml
that
learns
an
online
fair
classiﬁcation
model
primal
delivering
both
accuracy
and
fairness
in
subsequent
work
zhao
et
al
97
emphasized
the
primal-dual
fair
meta-learning
targeting
the
optimal
initialization
of
the
base
model
weights
to
rapidly
adjust
to
new
fairness
tasks
they
further
advanced
their
research
in
96
creating
few-shot
discrimination
chapter
related
work
25
prevention
model
for
unbiased
multi-class
classiﬁcation
rooted
in
the
maml
framework
concurrently
slack
et
al
71
introduced
fair-maml
designed
to
derive
fair
models
from
minimal
data
for
emerging
tasks
this
model
like
zhao
is
built
upon
the
maml
framework
but
incorporates
fairness
regularization
and
speciﬁc
fairness
hyperparameter
on
recommender
systems
chen
et
al
20
applied
meta-learning
principles
on
the
autodebias
framework
2.3
fairness
in
llms
research
on
fairness
in
llms
has
gained
considerable
traction
driven
by
the
realization
that
biases
present
in
pretraining
corpora
can
lead
llms
to
generate
content
that
is
not
only
harmful
but
also
oﬀensive
often
resulting
in
discrimination
against
marginalized
groups
this
heightened
awareness
has
spurred
increased
research
eﬀorts
aimed
at
understanding
the
origins
of
bias
and
addressing
the
detrimental
aspects
of
llms
68
13
initiatives
like
reinforcement
learning
from
human
feedback
58
and
reinforcement
learning
for
ai
fairness
seek
to
mitigate
the
reinforcement
of
existing
stereotypes
and
the
generation
of
demeaning
content
beyond
existing
literature
fairllm
95
critically
evaluates
recllm
fairness
highlighting
biases
in
chatgpt
recommendations
by
user
attributes
concurrently
eﬀorts
to
reﬁne
llm
fairness
assessments
are
gaining
traction
within
the
nlp
community
22
66
studies
like
12
and
expose
biases
in
gpt-3
content
generation
with
the
chapter
related
work
26
latter
noting
violent
bias
against
muslims
benchmarks
such
as
bbq
61
crowspairs
54
realtoxicityprompts
34
and
holistic
evaluations
50
further
this
analysis
across
various
llms
decodingtrust
77
extends
this
to
detailed
fairness
exploration
in
chatgpt
and
gpt-4
2.4
selection
bias
research
on
selection
bias
in
recommendation
systems
is
increasing
exploring
methods
to
reduce
bias
and
enhance
system
performance
one
of
the
approaches
is
through
re-sampling
techniques
this
includes
methods
such
as
undersampling
63
42
and
smote
synthetic
minority
over-sampling
technique
19
14
which
aims
to
balance
out
the
distribution
of
data
across
diﬀerent
classes
another
popular
approach
is
the
use
of
cost-sensitive
learning
methods
which
assign
diﬀerent
costs
to
diﬀerent
types
of
errors
in
order
to
balance
the
trade-oﬀ
between
diﬀerent
types
of
bias
for
example
the
method
of
adversarial
learning
94
24
aims
to
minimize
bias
by
adding
an
adversarial
term
to
the
loss
function
that
encourages
the
model
to
produce
fair
predictions
another
area
of
research
focuses
on
the
use
of
debiasing
techniques
in
the
representation
learning
process
such
as
fair
representation
learning
93
which
learns
representations
that
are
invariant
to
certain
sensitive
attributes
there
are
also
other
recent
studies
that
address
selection
bias
by
using
counterfactual
data
augmentation
cfda
81
which
creates
new
hypothetical
data
points
to
increase
the
diversity
of
the
training
set
this
can
be
done
by
generating
synthetic
data
points
that
are
similar
to
the
original
data
points
27
but
with
diﬀerent
sensitive
attributes
in
addition
meta-learning
21
78
have
been
applied
to
debiasing
recommendation
systems
for
multi-stage
cascade
systems
qin
et
al
64
proposed
the
rankflow
to
solve
the
selection
bias
in
the
joint-training
system
but
it
could
be
expensive
to
deploy
in
the
production
system
our
work
aims
to
solve
the
selection
bias
issue
for
independent-training
models
in
the
cascade
system
chapter
meta-learning
approach
to
fair
ranking
3.1
introduction
recently
the
fairness
in
information
retrieval
ir
system
has
attracted
more
and
more
attention
92
86
87
the
ranking
models
aim
to
give
the
relevant
scores
for
the
items
under
the
query
and
the
top
items
with
the
highest
scores
will
be
delivered
to
the
users
these
ranking
models
are
generally
data-driven
which
means
the
models
will
observe
particular
patterns
in
the
training
dataset
and
make
predictions
based
on
them
however
when
the
subject
of
the
ranking
problem
is
about
the
expert
search
or
the
job
recommendation
the
systematic
biases
from
the
dataset
usually
stemming
from
biased
data
distribution
will
introduce
unfairness
in
the
trained
model
for
example
28
chapter
meta-learning
approach
to
fair
ranking
law
students
race
29
law
students
gender
figure
3.1
illustration
of
the
predicted
rankings
distribution
of
the
protected
groups
female
students
african
american
students
on
the
two
diﬀerent
datasets
we
report
kendall
tau
as
the
ranking
metric
the
proposed
mfr
model
ranks
the
items
from
the
protected
groups
higher
compared
to
listnet
17
which
indicates
that
the
mfr
improves
the
protected
attribute
exposure
with
unbiased
ranking
performance
the
traditional
ltr
model
such
as
listnet
17
will
discriminately
assign
lower
weights
to
the
minority
group
due
to
the
data
bias
see
fig
3.1
as
addressed
by
friedman
33
the
historic
discrimination
to
the
socially
underrepresented
group
in
the
dataset
will
make
its
way
into
the
model
as
the
pattern
will
be
observed
during
the
training
process
the
unfairness
problem
could
be
summarized
as
the
disparate
exposure
89
as
the
disadvantaged
protected
group
is
not
treated
as
equally
as
the
advantaged
group
in
the
dataset
this
disparate
exposure
could
lead
to
negative
impact
on
many
realworld
ranking
problems
such
as
the
unequal
opportunity
in
the
job
market
for
the
underrepresented
group
to
solve
the
unfairness
problem
tremendous
research
eﬀorts
have
been
made
in
designing
fairness-aware
algorithms
among
which
the
fairness
ranking
models
can
be
categorized
as
the
score-based
and
supervised
ones
for
score-based
models
there
are
chapter
meta-learning
approach
to
fair
ranking
30
the
rank-aware
proportional
representation
86
the
constrained
ranking
maximization
18
etc
some
score-based
models
aim
to
correct
the
bias
in
the
training
data
and
the
others
aim
to
adjust
the
prediction
scores
for
better
fairness
there
are
also
supervised
models
such
as
deltr
89
fa
ir
90
etc
which
could
learn
fair
model
from
the
biased
dataset
in
general
the
ranking
models
focus
on
diﬀerent
mitigation
points
such
as
the
post
in
and
pre-processing
of
the
model
training
although
the
in-processing
models
have
achieved
good
performance
on
the
fairness
metric
there
is
still
the
limitation
as
the
model
is
learned
from
the
biased
dataset
thus
the
meta-learning
could
beneﬁt
the
aforementioned
problem
by
training
meta-learner
on
meta-dataset
the
meta-dataset
is
collected
uniformly
without
any
bias
which
would
train
fair
metalearner
so
that
the
ranking
model
could
learn
from
it
for
general
fairness
problems
such
as
training
the
classiﬁcation
model
on
biased
dataset
researchers
have
applied
the
model-agnostic
meta-learning
maml
31
for
example
the
meta-weight-net
69
proposed
to
explicitly
learn
weighting
function
from
the
meta-dataset
which
is
updated
simultaneously
with
the
classiﬁer
however
meta-learning
is
still
under-explored
for
the
fairness-aware
ranking
problems
in
this
study
we
propose
meta-learning
framework
to
formulate
the
fairness-aware
ranking
task
as
bilevel
optimization
problem
where
the
upper-level
is
the
meta-trainer
and
the
lower-level
is
the
ranking
model
that
is
we
can
train
meta-learner
on
the
meta-dataset
which
could
help
the
ranking
model
to
learn
fairly
on
the
biased
dataset
the
meta-dataset
is
small
unbiased
dataset
which
is
collected
by
uniformly
sampling
from
the
training
dataset
under
all
queries
for
both
the
protected
group
and
chapter
meta-learning
approach
to
fair
ranking
31
the
unprotected
group
in
detail
at
each
training
iteration
we
use
the
ranking
model
and
the
ranking
loss
function
to
compute
the
loss
values
for
each
data
sample
from
the
training
dataset
then
we
train
multi-layer
neural
network
as
the
weighting
function
to
re-weight
the
loss
values
and
the
weighting
function
is
optimized
by
the
weighted
loss
values
on
the
meta-dataset
since
the
weighting
function
which
is
the
meta-learner
is
subject
to
the
ranking
models
our
goal
is
to
optimize
the
loss
weights
given
by
the
meta-learner
to
achieve
fairness
on
the
training
dataset
intuitively
we
can
see
the
loss
weight
as
the
hyperparameter
which
could
be
learned
and
we
train
metalearner
to
tune
the
hyperparameter
on
the
meta-dataset
such
the
training
process
could
also
be
referred
to
as
the
bilevel
optimization
as
the
learned
parameters
of
the
ranking
model
depend
on
the
parameters
of
the
meta-learner
to
the
best
of
our
knowledge
we
propose
the
ﬁrst
meta-learning
approach
to
fair
ranking
in
summary
this
work
makes
the
following
contributions
we
propose
general
meta-learning
framework
for
the
fairness
ranking
called
meta-learning
based
fair
ranking
mfr
that
addresses
the
data
bias
by
automatically
re-weighting
the
ranking
losses
we
formulate
the
mfr
as
bilevel
optimization
problem
and
solve
it
using
gradients
through
gradients
experiments
on
the
real-world
datasets
demonstrate
that
the
proposed
method
achieves
comparable
ranking
performance
and
signiﬁcantly
improves
the
fairness
metric
compared
with
state-of-the-art
methods
chapter
meta-learning
approach
to
fair
ranking
32
biased
unprotected
group
protected
group
unbiased
figure
3.2
mfr
learning
algorithm
ﬂowchart
steps
and
in
algorithm
note
that
is
the
ranking
model
is
the
meta-learner
is
the
batch
size
for
the
training
dataset
is
the
batch
size
for
the
meta-dataset
and
and
are
the
learning
rates
at
each
iteration
we
ﬁrstly
update
in
the
meta-learner
using
eq
with
the
meta-dataset
and
then
we
update
in
the
ranking
model
using
eq
with
the
training
dataset
3.2
meta-learning
based
fair
ranking
we
aim
to
train
fairness-aware
ranking
model
that
could
achieve
good
performance
on
both
utility
and
fairness
metrics
to
do
that
we
tune
the
ranking
model
loss
weights
values
to
make
the
model
emphasize
more
on
the
protected
group
than
the
unprotected
one
during
the
ranking
inference
instead
of
using
the
ﬁxed
weights
we
utilize
metadataset
which
is
sampled
from
the
original
training
dataset
with
an
unbiased
distribution
and
smaller
size
to
train
meta-learner
as
weighting
function
the
meta-learner
could
guide
the
ranking
model
to
learn
fairly
given
the
training
dataset
with
set
of
queries
qtrain
with
qtrain
and
set
of
items
dtrain
with
dtrain
each
query
from
qtrain
is
associated
with
list
of
item
candidates
from
dtrain
and
each
item
is
represented
as
feature
vector
xi
for
each
query
the
feature
vector
is
associated
with
the
relevance
score
let
be
the
ranking
model
and
represent
all
the
learnable
parameters
in
then
chapter
meta-learning
approach
to
fair
ranking
33
the
output
of
the
ranking
model
could
be
denoted
as
generally
we
learn
the
optimized
parameters
by
minw
m1
yi
and
could
be
used
as
any
ranking
loss
functions
however
equally
treating
to
each
sample
could
lead
the
ranking
model
unfair
to
minority
groups
since
the
heavy
data
bias
issue
in
the
training
dataset
to
address
this
challenge
we
introduce
meta-learner
parameterized
by
to
adaptively
tune
loss
weights
for
each
sample
to
achieve
fair
exposure
over
diversity
thus
we
rewrite
the
training
loss
as
the
following
ltrain
where
φi
li
yi
3.1
xi
represents
the
model
output
and
φi
represents
the
i-th
sample
loss
weight
given
by
the
proposed
meta-learner
notably
ltrain
governed
by
the
meta-learner
output
weights
is
conditioning
on
ﬁxed
and
used
for
updating
the
ranking
model
parameter
for
convenience
we
denote
li
as
the
original
loss
value
of
the
i-th
training
data
sample
output
from
the
ranking
loss
following
69
we
develop
the
meta-learner
as
multi-layer
neural
network
which
takes
as
input
loss
value
and
instantiate
as
φi
li
li
3.2
where
could
be
sample
from
either
the
training
dataset
or
the
meta-dataset
we
set
the
last-layer
activation
function
in
as
sigmoid
so
that
the
range
of
the
output
chapter
meta-learning
approach
to
fair
ranking
34
algorithm
the
mfr
learning
algorithm
input
training
dataset
qtrain
dtrain
meta-dataset
qmeta
dmeta
batch
size
max
iterations
output
classiﬁer
network
parameter
initialize
ranking
model
parameter
and
the
meta-learner
parameter
for
to
do
xqmeta
qmeta
sampleminibatch
qmeta
dmeta
xqtrain
qtrain
sampleminibatch
qtrain
dtrain
update
wˆ
by
eq
3.4
with
xqtrain
qtrain
update
by
eq
3.9
with
xqmeta
qmeta
update
by
eq
3.10
with
xqtrain
qtrain
end
for
lies
between
and
eventually
we
deﬁne
meta
training
loss
function
as
li
lmeta
3.3
here
we
update
the
parameters
of
the
ranking
network
by
doing
the
gradient
decent
on
batch
of
training
data
with
the
loss
function
in
eq
3.1
and
we
can
deﬁne
as
ltrain
ltrain
3.4
to
train
the
meta-learner
we
need
to
sample
small
meta-dataset
with
qmeta
and
dmeta
the
meta-dataset
represents
the
meta-knowledge
of
the
true
distribution
of
the
protected
group
and
the
other
group
where
qmeta
and
dmeta
in
the
meta-dataset
we
denote
the
feature
vector
of
each
item
as
qmeta
and
the
relevance
score
as
qmeta
given
query
qmeta
from
qmeta
similar
to
ltrain
we
denote
lmeta
as
the
loss
value
for
each
meta-dataset
sample
the
goal
of
the
meta-learner
is
to
leverage
the
unbiased
meta-dataset
to
learn
how
to
re-weight
the
loss
values
to
train
the
chapter
meta-learning
approach
to
fair
ranking
35
model
on
the
biased
dataset
since
is
function
of
we
naturally
formulate
the
proposed
mfr
as
bilevel
optimization
problem
and
give
the
objective
function
as
min
lmeta
3.5
arg
min
train
loss
functions
the
proposed
mfr
jointly
considers
utility
and
fairness
metrics
by
developing
listwise
ranking
loss
with
an
exposure
term
following
the
deltr
loss
89
given
by
γu
3.6
where
is
listwise
fairness
measurement
is
listwise
loss
based
on
cross
entropy
17
and
is
balancing
parameter
to
obtain
optimal
parameters
and
we
minimize
the
training
loss
by
φi
ltrain
3.7
meta
3.8
arg
min
train
and
the
loss
for
the
meta-learner
by
arg
min
meta
parameters
update
at
each
step
we
compute
the
weighted
loss
values
with
θt
and
wt
and
update
with
the
loss
of
the
ranking
model
on
the
meta-dataset
as
the
chapter
meta-learning
approach
to
fair
ranking
36
following
lmeta
3.9
where
is
the
learning
rate
is
the
batch
size
of
the
meta-dataset
after
we
have
the
we
update
as
the
following
φi
ltrain
3.10
where
is
the
learning
rate
and
is
the
batch
size
of
the
training
dataset
we
adopt
an
alternating
optimization
strategy
69
75
88
to
implement
eq
3.9
and
eq
3.10
instead
of
using
nested
optimization
loops
the
whole
training
process
is
summarized
in
algorithm
although
we
consider
the
deltr
loss
as
the
objective
function
of
the
ranking
model
we
could
also
use
other
fair
ranking
losses
here
besides
the
disparate
exposure
there
are
other
biases
in
the
common
ranking
dataset
such
as
selection
bias
and
position
bias
the
model
aims
to
provide
general
meta-learning
framework
that
can
handle
any
fair
ranking
problems
3.3
experiments
in
the
experiments
we
train
and
evaluate
the
model
on
the
three
real-world
datasets
used
in
deltr
89
we
study
both
the
ranking
and
fairness
metrics
of
our
approach
compared
to
other
baseline
models
the
baseline
models
include
the
following
chapter
meta-learning
approach
to
fair
ranking
37
listnet
17
ii
lambdamart
16
iii
the
deltr
model
with
γsmall
and
γlarge
which
is
the
same
setting
as
in
89
iv
the
fa
ir
90
pre-processing
approach
that
creates
the
fair
dataset
and
trains
on
it
the
fa
ir
post-processing
approach
that
reorders
the
prediction
results
to
ensure
the
fairness
vi
mfr
with
diﬀerent
on
diﬀerent
dataset
vii
mfr
with
the
listnet
loss
mfr-listnet
the
code
is
available
at
https://github.com/ywang4/a-meta-learning-approach-to-fair-ranking.
for
fair
comparison
we
follow
the
same
settings1
as
described
in
deltr
89
to
split
the
dataset
and
generate
the
item
features
we
use
the
following
datasets
w3c
experts
gender
ii
engineering
students
high
school
iii
engineering
students
gender
iv
law
students
gender
law
students
race
in
the
w3c
experts
dataset
the
task
is
the
expert
search
originated
from
trec
2005
enterprise
track
26
the
protected
attribute
is
female
and
there
are
200
items
per
query
with
an
average
of
21.5
items
from
the
protected
group
in
the
engineering
students
dataset
the
task
is
the
academic
performance
prediction
and
the
dataset
contains
anonymized
historical
information
of
college
students
for
the
high
school
dataset
the
protected
attribute
is
public
high
school
and
there
are
480.6
items
per
query
with
167.6
items
from
the
protected
group
on
average
for
the
gender
dataset
the
protected
attribute
is
female
and
there
are
480.6
items
per
query
with
97.6
items
from
the
protected
group
on
average
in
the
law
students
dataset
the
task
is
also
the
academic
performance
prediction
for
the
gender
dataset
the
protected
attribute
is
female
and
there
is
total
of
21791
items
with
9537
items
from
the
protected
group
for
the
race
dataset
the
protected
https://github.com/milkalichtblau/deltr-experiments
chapter
meta-learning
approach
to
fair
ranking
38
attribute
is
black
and
there
is
total
of
19567
items
with
1282
from
the
protected
group
the
queries
are
technical
topics
for
the
w3c
dataset
and
academic
years
for
the
other
datasets
for
fair
comparison
we
adapt
the
same
evaluation
metrics
as
89
to
split
the
datasets
we
have
50
queries
for
training
and
10
queries
for
testing
in
the
w3c
dataset
queries
for
training
and
query
for
testing
in
the
engineering
students
dataset
and
80
for
training
and
20
for
testing
in
the
law
students
dataset
we
use
precision
10
10
for
the
w3c
dataset
and
kendall
tau
for
other
datasets
to
evaluate
the
ranking
performance
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
thus
in
the
fairness
metric
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
as
described
in
sec
3.2
the
meta-dataset
is
required
for
our
approach
since
the
protected
attribute
in
all
datasets
is
binary
we
perform
random
uniform
sampling
to
collect
the
meta-dataset
speciﬁcally
we
randomly
sample
the
same
amount
of
data
for
the
items
from
each
query
for
each
protected
group
and
non-protected
group
settings
in
general
for
the
weighting
function
we
set
the
update
frequency
of
the
parameter
θto
be
per
steps
the
optimizer
to
be
sgd
the
momentum
to
be
0.98
the
learning
rate
to
be
0.02
the
hidden
layer
dimension
to
be
30
and
the
number
of
hidden
layers
to
be
for
the
ranking
model
we
set
the
learning
rate
for
all
datasets
to
be
0.005
except
for
w3c
data
to
be
0.0005
the
optimizer
to
be
sgd
the
momentum
to
be
0.95
and
the
weight
decay
to
be
0.005
the
values
of
and
training
epoch
vary
for
diﬀerent
datasets
w3c
dataset
uses
500
and
100
epochs
engineering
students
high
school
uses
5000
and
500
epochs
engineering
students
gender
uses
chapter
meta-learning
approach
to
fair
ranking
listnet
17
lambdamart
16
deltr
γsmall
89
deltr
γlarge
89
fa
ir
post
90
fa
ir
pre
90
mfr-listnet
mfr
w3c
experts
gender
10
fairness
0.178
0.759
0.095
0.738
0.178
0.785
0.180
0.827
0.178
0.824
0.180
0.770
0.115
0.775
0.126
0.830
engineering
students
high
school
type
tau
fairness
0.390
1.070
0.355
1.002
0.390
1.075
0.391
1.075
0.390
1.070
0.374
1.020
0.385
0.990
0.391
1.086
engineering
students
gender
tau
fairness
0.384
0.858
0.326
0.907
0.384
0.860
0.370
0.976
0.384
0.886
0.360
0.942
0.385
0.855
0.352
1.052
39
law
students
gender
tau
fairness
0.202
0.931
0.199
0.979
0.201
0.958
0.188
0.993
0.182
0.965
0.203
0.931
0.225
0.901
0.225
1.015
law
students
race
tau
fairness
0.184
0.853
0.156
0.847
0.173
0.874
0.130
1.014
0.140
0.944
0.161
0.895
0.182
0.848
0.184
1.654
table
3.1
experimental
results
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
so
the
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
the
bold
text
indicates
the
model
with
the
best
performance
and
the
results
show
that
the
mfr
model
is
better
on
the
fairness
metrics
with
comparable
performance
on
the
ranking
metrics
against
other
state-of-the-art
models
500
and
100
epochs
law
students
gender
uses
1200
and
3000
epochs
and
law
students
race
uses
50000
and
100
epochs
results
analysis
as
shown
in
tab
3.1
our
approach
performs
better
in
terms
of
the
fairness
metrics
on
all
datasets
than
both
the
deltr
γsmall
and
deltr
γlarge
the
deltr
γsmall
and
deltr
γlarge
models
use
diﬀerent
scales
of
values
to
weight
the
exposure
measure
in
the
loss
function
with
the
meta
learner
we
can
achieve
higher
fairness
metrics
by
re-weighting
the
loss
distribution
during
the
training
process
the
intuition
behind
the
observation
is
that
the
imbalanced
pattern
among
the
training
data
is
observed
and
corrected
by
the
meta
learner
for
the
ranking
metrics
we
have
similar
or
better
results
on
all
datasets
except
the
w3c
dataset
since
listnet
and
lambdamart
do
not
consider
any
fairness
measure
during
the
training
the
results
are
as
expected
that
the
fairness
metrics
are
worse
than
the
fairness
ranking
models
in
addition
we
train
the
mfr-listnet
that
has
the
standard
listwise
ranking
loss
in
the
framework
the
evaluation
results
show
the
worse
performance
on
both
the
ranking
chapter
meta-learning
approach
to
fair
ranking
40
engineering
students
high
school
law
students
gender
figure
3.3
the
plot
of
the
variation
of
learned
weight
over
the
two
training
datasets
the
weight
diﬀerence
is
computed
as
φtdiﬀ
φi
φi
and
we
plot
the
φdiﬀ
over
the
training
epochs
as
shown
in
the
plot
the
weighting
function
is
converging
as
the
diﬀerent
values
of
weights
between
each
epoch
are
decreasing
to
0.0
and
fairness
metrics
as
listwise
loss
does
not
consider
the
exposure
measure
the
metadataset
that
has
diﬀerent
data
distribution
as
the
training
dataset
has
negative
eﬀect
on
the
meta-learner
during
the
re-weighting
process
thus
we
conclude
that
the
meta-learning
approach
could
help
the
model
to
further
improve
the
fairness
metrics
compare
to
the
model
with
only
the
deltr
loss
function
in
fig
3.1
we
plot
the
histogram
of
ranks
on
the
protected
attributes
from
the
different
models
from
the
plot
we
can
see
the
distribution
of
the
predicted
ranks
shifts
from
right
to
left
which
indicates
the
mfr
model
generally
ranks
the
items
from
the
protected
group
higher
compared
to
listnet
note
that
at
the
plot
means
the
top
rank
so
when
more
data
samples
fall
in
the
bins
at
the
left
the
items
receive
higher
ranks
the
plot
also
agrees
with
the
evaluation
results
as
we
see
that
there
is
large
diﬀerence
in
fig
1b
the
fairness
metric
of
mfr
on
law
students
race
dataset
is
about
two
times
than
that
of
listnet
41
in
fig
3.3
we
plot
the
variation
of
the
learned
weight
for
the
training
data
the
plots
show
that
the
weighting
function
is
converging
as
the
diﬀerent
values
of
weights
between
each
epoch
are
decreasing
to
as
suggested
in
meta-weight-net
69
we
use
the
multi-layer
neural
network
as
the
weighting
function
because
the
multi-layer
neural
network
is
known
as
universal
approximator
for
the
most
continuous
functions
the
convergence
shown
in
the
plots
indicates
the
successful
learning
process
on
the
weighting
function
3.4
conclusion
in
this
work
we
have
proposed
meta-learning
based
fair
ranking
mfr
model
to
improve
the
minority
group
exposure
our
experiments
on
the
real-world
datasets
demonstrate
that
our
approach
could
achieve
better
fairness
metrics
compared
to
the
fair
ranking
model
without
the
meta-learning
part
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
4.1
introduction
fairness
in
search
engines
is
an
important
topic
which
focuses
on
training
an
unbiased
ranking
model
towards
protected
attributes
typically
when
user
query
is
given
the
ranking
model
predicts
relevant
scores
among
candidate
items
and
returns
items
with
the
highest
scores
to
users
the
data-driven
ranking
model
is
usually
trained
with
large
datasets
and
thus
the
ranker
will
learn
user
item
patterns
from
the
training
dataset
and
make
predictions
based
on
them
however
in
many
cases
the
systematic
biases
42
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
african
american
female
african
american
43
female
figure
4.1
illustration
of
the
predicted
rankings
distribution
of
two
protected
attributes
on
four
datasets
law
student
gender
82
law
student
race
82
compas
and
engineering
student
89
we
report
kendall
tau
48
as
the
ranking
performance
mcfr
and
mfr
80
improve
the
protected
attributes
ranking
while
realizing
competitive
ranking
performance
compared
with
listnet
17
demonstrating
that
our
approach
could
increase
the
exposure
of
the
minority
such
as
exposure
bias
70
in
the
dataset
will
cause
unfairness
to
the
ranking
model
the
historical
discrimination
against
the
socially
underrepresented
group
33
will
make
its
way
into
the
model
as
the
pattern
will
be
observed
during
the
training
process
such
an
unfairness
problem
could
be
summarized
as
the
disparate
exposure
70
leading
to
negative
impact
on
many
real-world
ranking
problems
disparate
exposure
is
prevalent
in
information
retrieval
for
instance
expert
search
and
job
recommendation
systems
historically
underrepresented
minority
groups
like
females
and
african
americans
consequently
traditional
learning
to
rank
ltr
models
such
as
listnet
17
often
rank
these
groups
lower
due
to
data
biases
fig
4.1
shows
ranking
scores
from
diﬀerent
models
on
four
datasets
highlighting
this
unfairness
disparate
exposure
implies
uneven
group
visibility
in
algorithm
outcomes
especially
linked
to
attributes
like
gender
or
race
distinct
from
biases
like
selection
or
conformity
which
challenge
algorithmic
fairness
and
eﬃciency
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
44
to
reduce
disparate
exposure
in
ranking
context
many
research
works
have
been
proposed
recently
by
designing
fairness-aware
algorithms
which
can
be
divided
into
two
categories
the
score-based
models
and
the
supervised-learning
models
the
score-based
models
86
87
18
72
47
compute
the
ranking
scores
on
the
ﬂy
for
given
candidates
list
and
return
the
sorted
candidates
as
the
model
outcome
the
supervised-learning
models
generally
solve
ranking
as
prediction
problem
and
focus
on
diﬀerent
mitigation
strategies
such
as
the
post
49
in
89
52
39
23
27
20
and
pre-processing
90
91
10
in
model
training
although
the
in-processing
models
have
achieved
promising
performance
on
both
fairness
and
ranking
metrics
learning
on
biased
datasets
is
still
under-explored
and
challenging
due
to
the
unbalanced
distributions
of
protected
attributes
in
the
public
training
datasets
one
possible
way
to
alleviate
system
discrimination
inherited
from
data
bias
is
dynamically
re-weighting
the
minority
groups
to
contribute
more
penalties
in
computing
ranking
loss
to
this
end
meta-learning
31
emerges
as
an
eﬀective
way
to
enable
learning-to-weight
approach
by
leveraging
small
unbiased
dataset
meta
dataset
for
the
fairness-aware
ranking
problem
we
propose
to
mitigate
the
exposure
issue
in
the
biased
dataset
by
learning
weighting
model
meta-learner
to
re-weight
the
loss
of
the
ranking
model
on
the
biased
dataset
the
meta-learner
will
be
optimized
on
the
meta
dataset
unbiased
and
the
weighted
loss
on
the
training
dataset
biased
will
be
used
to
optimize
the
ranking
model
however
due
to
the
distribution
shift
between
the
biased
and
unbiased
datasets
it
is
non-trivial
to
directly
train
the
meta-learner
and
base
learner
on
these
two
datasets
where
large
training
loss
may
impair
ranking
utility
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
45
and
burden
convergence
speed
we
propose
to
adopt
curriculum
learning
to
gradually
increase
the
diﬃculty
of
training
meta-learners
to
address
the
above
challenge
speciﬁcally
we
deﬁne
the
diﬃculty
as
the
exposure
of
the
protected
groups
in
dataset
we
ﬁrst
randomly
sample
meta
dataset
that
has
the
same
exposure
as
the
training
dataset
then
we
continually
increase
the
protected
groups
exposure
in
the
meta
dataset
by
sampling
more
candidates
from
this
group
at
each
ongoing
epoch
until
uniform
distribution
equal
exposure
is
achieved
over
sensitive
attributes
intuitively
this
incremental
concept
learning
is
good
ﬁt
to
solve
the
distribution
shift
problem
because
meta-learners
are
trained
with
samples
from
the
biased
dataset
at
the
early
epochs
which
means
there
is
less
distribution
shift
between
the
meta-dataset
and
training
dataset
the
experimental
results
demonstrate
the
eﬀectiveness
of
curriculum
learning
and
the
improved
data
eﬃciency
during
training
in
this
study
we
propose
uniﬁed
meta-learning
framework
with
curriculum
learning
to
formulate
the
fairness-aware
ranking
task
as
bilevel
optimization
problem
where
the
upper
level
focuses
on
learning-to-weight
to
mitigate
the
biased
exposure
of
protected
attributes
and
the
lower
level
solves
learning-to-rank
with
dynamic
loss
governed
by
meta
learner
speciﬁcally
we
alleviate
the
data
bias
issue
for
the
protected
groups
through
an
automatically
weighted
loss
the
contributions
of
this
work
are
as
follows
we
propose
novel
meta
curriculum-based
fair
ranking
framework
namely
mcfr
which
addresses
the
data
bias
by
automatically
re-weighting
the
ranking
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
46
losses
the
proposed
mcfr
is
formulated
as
bilevel
optimization
problem
and
solved
using
gradients
through
gradients
the
proposed
fair
ranking
algorithm
marries
in-processing
methods
with
preprocessing
techniques
by
seamlessly
incorporating
curriculum
learning
into
the
construction
process
of
meta
datasets
we
develop
mcfr
as
general
framework
applicable
to
various
ranking
loss
functions
and
fairness
metrics
systematic
empirical
study
has
been
provided
to
show
the
versatility
of
the
proposed
framework
over
diﬀerent
ranking
and
fairness
criteria
experiments
on
public
datasets
show
our
method
matches
existing
ranking
performance
and
enhances
fairness
metrics
additionally
evaluations
conﬁrm
mcfr
improves
fairness
with
less
training
data
and
achieves
comparable
convergence
times
this
work
oﬀers
the
ﬁrst
fair
ranking
framework
to
utilize
both
pre-processing
and
in-processing
methods
this
new
approach
enhances
the
model
adaptability
and
robustness
by
allowing
for
broader
range
of
loss
functions
and
dynamically
adjusting
meta-datasets
during
training
additionally
our
framework
demonstrates
data
eﬃciency
in
comparative
experiments
we
ve
also
conducted
more
comprehensive
tests
incorporating
additional
baseline
models
and
performing
an
ablation
study
on
various
fairness
terms
and
ranking
losses
lastly
we
ve
updated
the
manuscript
to
include
more
recent
related
works
providing
fuller
understanding
of
fairness
in
ranking
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
meta
model
single
step
scheduler
47
ranking
model
curriculum
sampling
easier
biased
distribution
harder
unbiased
distribution
ranking
loss
unprotected
group
protected
group
mini
batch
sampling
fairness
term
rankmse
squared
exposure
ranknet
hinge
exposure
listnet
figure
4.2
mcfr
learning
algorithm
ﬂowchart
steps
and
in
algorithm
note
that
is
the
ranking
model
is
the
meta
learner
is
the
batch
size
for
the
training
dataset
is
the
batch
size
for
the
meta-dataset
and
and
are
the
learning
rates
at
each
iteration
we
ﬁrstly
update
in
the
meta
learner
using
eq
with
the
meta-dataset
sampled
from
the
curriculum
sampling
with
update
of
sampling
diﬃculty
at
each
epoch
and
then
we
update
in
the
ranking
model
using
eq
with
the
training
dataset
4.2
meta
curriculum-based
fair
ranking
in
this
section
we
will
explain
the
proposed
meta
curriculum-based
fair
ranking
framework
in
detail
in
the
mcfr
framework
we
will
train
an
unbiased
ranking
model
by
using
meta-leaner
to
re-weight
the
ranking
losses
we
formulate
it
as
bilevel
optimization
problem
and
solve
it
using
gradients
through
gradients
we
also
show
that
the
framework
could
be
trained
with
various
ranking
loss
functions
and
fairness
terms
finally
we
describe
the
design
of
the
curriculum
sampling
strategy
for
meta
dataset
to
address
bias
in
datasets
traditional
methods
have
utilized
pre-processing
in-processing
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
48
or
post-processing
techniques
92
28
our
model
combines
pre-processing
and
inprocessing
introducing
the
meta
curriculum-based
fair
ranking
framework
we
derive
smaller
dataset
for
meta-learner
training
which
assigns
weights
to
emphasize
the
protected
group
during
training
curriculum
learning
adjusts
this
dataset
distribution
ratio
over
epochs
facilitating
smoother
meta-learner
training
this
integrates
ranking
loss
with
fairness
regularization
using
the
meta-learner
to
guide
model
training
as
depicted
in
fig
4.2
4.2
problem
setting
we
denote
the
set
of
queries
in
the
training
dataset
as
qtrain
with
the
size
qtrain
and
the
set
of
items
dtrain
with
dtrain
each
query
in
the
qtrain
has
list
of
item
candidates
from
dtrain
each
pair
of
query
and
item
is
represented
as
feature
vector
xi
and
is
associated
with
the
relevance
score
yi
in
the
dataset
the
candidates
have
binary
attribute
that
speciﬁes
whether
the
candidate
belongs
to
the
protected
group
or
the
non-protected
group
for
example
the
binary
attribute
could
represent
gender
or
race
and
systematic
bias
exists
during
the
dataset
collection
4.2
uniﬁed
mcfr
framework
to
address
the
fairness
problem
we
train
meta
learner
on
the
meta-dataset
which
could
help
train
fair
ranking
model
with
the
biased
training
dataset
we
have
the
ranking
model
and
is
the
learnable
parameters
of
and
we
denote
the
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
49
output
of
the
model
as
generally
the
model
parameter
is
optimized
by
minw
m1
yi
which
could
minimize
any
given
ranking
loss
function
such
as
pairwise
loss
and
listwise
loss
however
these
loss
functions
treat
of
each
sample
equally
so
that
the
ranking
model
will
be
unfair
as
there
is
heavy
data
bias
issue
towards
minority
groups
in
the
training
dataset
to
mitigate
this
problem
we
introduce
meta
learner
with
the
learnable
parameters
to
adaptively
tune
loss
weights
for
each
sample
to
achieve
fair
exposure
over
diversity
and
we
could
rewrite
the
training
loss
as
the
following
ltrain
where
φi
li
yi
4.1
xi
denotes
the
model
output
and
φi
denotes
the
i-th
sam
ple
loss
weight
given
by
the
aforementioned
meta
learner
notably
ltrain
governed
by
the
meta
learner
output
weights
depends
on
ﬁxed
and
is
used
for
updating
the
ranking
model
parameter
in
short
we
write
li
as
the
original
loss
value
of
the
i-th
training
data
sample
output
from
the
ranking
loss
for
the
meta
learner
we
use
multi-layer
perceptron
network
as
proposed
in
69
which
takes
loss
values
as
input
and
output
weighted
loss
as
φi
li
li
4.2
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
50
algorithm
parameter
update
algorithm
of
mcfr
input
batch
of
training
data
xqtrain
qtrain
batch
of
meta-dataset
xqtrain
qtrain
ranking
model
parameter
and
the
meta
learner
parameter
output
ranking
model
parameter
update
update
by
eq
4.5
with
xqtrain
qtrain
update
by
eq
4.8
with
xqmeta
qmeta
update
by
eq
4.9
with
xqtrain
qtrain
where
is
the
sample
from
the
training
dataset
or
the
meta-dataset
we
use
sigmoid
as
the
last-layer
activation
function
then
we
deﬁne
meta
training
loss
function
as
li
lmeta
4.3
where
qmeta
the
goal
of
the
meta
learner
is
to
leverage
the
meta-dataset
to
learn
how
to
re-weight
the
loss
values
to
train
the
model
on
the
biased
dataset
indicating
the
relationship
that
the
meta-learner
plays
pivotal
role
in
directing
the
tuning
of
the
ranking
model
parameters
inherently
making
function
of
since
is
function
of
we
naturally
formulate
the
proposed
mcfr
as
bilevel
optimization
problem
and
give
the
objective
function
as
min
lmeta
arg
min
ltrain
4.4
as
illustrated
in
fig
4.2
our
proposed
mcfr
model
takes
advantage
of
the
sampled
meta-dataset
to
learn
an
unbiased
ranking
model
the
meta-dataset
guide
the
meta
learner
to
reweight
the
training
loss
which
helps
the
ranking
model
to
focus
on
the
candidates
from
the
protected
group
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
type
formula
hinge
exposure
89
max
exposure
g0
exposure
g1
squared
exposure
exposure
g0
exposure
g1
rankmse
11
n1
ranknet
15
n1
listnet
17
fairness
ranking
51
yi
log
exp
yi
py
log
py
table
4.1
summary
of
ranking
and
fairness
terms
used
in
the
loss
function
the
loss
function
used
in
the
framework
is
γu
and
we
can
insert
the
above
exposure
terms
and
ranking
loss
terms
as
needed
note
that
denotes
the
number
of
candidates
per
query
4.2
parameter
update
since
we
formulate
the
framework
as
bilevel
optimization
problem
it
could
be
challenging
as
calculating
the
optimal
parameters
requires
two
nested
loops
of
optimization
following
the
well-known
maml
works
69
75
88
we
adopt
an
online
strategy
with
single
optimization
loop
to
update
the
ranking
model
and
meta-learner
parameters
to
guarantee
the
training
eﬃciency
we
update
the
parameters
of
the
ranking
network
using
the
gradient
decent
on
batch
of
training
data
with
the
loss
function
in
eq
4.1
and
we
deﬁne
the
update
of
as
ltrain
ltrain
4.5
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
52
where
is
each
step
of
the
update
and
is
the
ranking
model
parameters
at
the
step
to
obtain
optimal
parameters
and
we
minimize
the
training
loss
by
φi
ltrain
4.6
meta
4.7
arg
min
ltrain
and
the
loss
for
the
meta
learner
by
arg
min
lmeta
then
given
from
eq
4.5
we
update
with
the
loss
of
the
ranking
model
on
the
meta-dataset
as
the
following
lmeta
4.8
where
is
the
learning
rate
and
is
the
batch
size
of
the
meta-dataset
then
we
update
as
the
following
φi
ltrain
4.9
where
is
the
learning
rate
and
is
the
batch
size
of
the
training
dataset
we
adopt
an
alternating
optimization
strategy
69
75
88
to
implement
eq
4.8
and
eq
4.9
instead
of
using
nested
optimization
loops
the
one
step
update
algorithm
is
summarised
in
alg
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
4.2
53
ranking
and
fairness
loss
the
proposed
mcfr
serves
as
uniﬁed
framework
that
aims
to
improve
both
the
ranking
and
fairness
metrics
given
any
ranking
and
fairness
objectives
to
achieve
this
goal
we
propose
to
include
two
terms
in
the
loss
functions
similar
to
some
in-processing
fairness
methods
such
as
deltr
89
and
we
develop
our
loss
functions
with
the
ranking
term
and
fairness
term
given
by
γu
4.10
where
is
the
fairness
term
is
the
ranking
loss
term
and
is
balancing
parameter
4.2
4.1
ranking
terms
for
the
ranking
loss
we
use
the
following
loss
functions
in
the
experiments
rankmse
11
ranknet
15
and
listnet
17
rankmse
is
pointwise
loss
which
is
based
on
least
mean
squared
regression
ranknet
proposed
the
ﬁrst
pairwise
cross
entropy
loss
which
consider
the
preference
relationships
between
documents
however
it
is
not
possible
to
correctly
predict
the
document
order
in
all
cases
listnet
aims
to
directly
compute
the
ranking
loss
with
each
query
and
their
candidates
list
instead
of
computing
pairwise
loss
one
pair
by
one
pair
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
54
it
is
worth
noting
that
other
ranking
losses
are
also
applicable
in
mcfr
as
we
provide
general
framework
to
improve
the
ranking
metrics
4.2
4.2
fairness
terms
in
this
work
we
focus
on
disparate
exposure
for
the
fairness
term
for
candidates
there
are
two
diﬀerent
groups
the
non-protected
group
g0
and
the
protected
group
g1
the
candidates
from
g1
belong
to
discriminated
group
such
as
female
and
african
american
and
have
signiﬁcant
disadvantages
in
the
datasets
then
following
the
deﬁnition
of
singh
et
al
70
the
exposure
of
candidate
in
ranked
list
generated
by
probabilistic
ranking
is
given
by
exposure
xi
pi
va
4.11
where
va
is
the
position
bias
of
position
we
then
follow
the
implementation
of
zelike
el
al
89
to
only
consider
the
position
bias
of
position
with
v1
then
the
average
exposure
of
candidates
in
each
group
could
be
written
as
exposure
exposure
xi
4.12
xi
with
the
exposure
term
deﬁned
above
we
can
introduce
the
fairness
measure
by
minimizing
the
diﬀerence
between
the
exposure
g0
and
exposure
g1
in
the
experiments
we
use
two
exposure
measurements
hinge
exposure
calculates
hinge
squared
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
55
figure
4.3
curriculum
sampling
strategy
illustrated
on
the
engineering
student
gender
dataset
we
use
the
same
ratio
between
the
unprotected
group
and
protected
group
in
the
meta-dataset
as
the
training
dataset
at
the
beginning
training
epoch
we
gradually
decrease
the
ratio
as
the
training
epoch
increase
until
the
ratio
becomes
which
shows
balanced
meta-dataset
loss
from
the
exposure
diﬀerence
between
two
groups
while
square
exposure
computes
the
squared
exposure
diﬀerence
the
ranking
loss
terms
and
exposure
terms
could
be
used
in
an
arbitrary
combination
and
our
framework
could
improve
both
the
fairness
and
ranking
metrics
given
diﬀerent
combinations
the
ranking
terms
and
fairness
terms
are
summarised
in
table
4.1
4.2
curriculum
sampling
the
training
data
shows
systematic
bias
with
fewer
candidates
from
protected
groups
than
unprotected
ones
to
address
this
issue
we
trained
meta
learner
using
an
unbiased
meta-dataset
since
real
unbiased
data
is
rare
while
autodebias
20
previously
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
56
tackled
similar
issue
for
recommendation
systems
it
does
not
ﬁt
our
ranking-focused
needs
another
approach
used
in
mfr
80
equally
samples
candidates
from
each
group
however
this
method
creates
meta-dataset
that
may
fall
short
of
accurately
capturing
the
real
biased
data
for
tasks
like
ranking
where
the
order
and
relevance
of
items
are
crucial
this
mismatch
in
the
data
distribution
can
signiﬁcantly
hinder
the
model
ability
to
provide
fair
and
eﬀective
rankings
in
practical
applications
biased
situations
to
this
end
we
adopt
curriculum
learning
method
that
starts
with
easier
less
biased
samples
and
gradually
introduces
more
complex
ones
this
mimics
natural
learning
helping
the
model
adapt
better
and
become
more
robust
it
designed
to
ease
the
model
into
understanding
and
correcting
biases
ensuring
it
performs
well
and
fairly
in
real-world
applications
even
with
the
underlying
biases
in
the
data
it
was
trained
on
in
detail
we
want
to
downsample
the
meta-dataset
with
the
similar
distribution
as
the
training
dataset
at
the
early
training
epochs
and
we
gradually
change
the
ratio
of
the
number
of
candidates
from
the
protected
and
unprotected
groups
to
1.0
since
we
could
not
collect
real
unbiased
dataset
we
deﬁne
1.0
to
be
the
unbiased
ratio
of
the
number
of
candidates
from
the
two
diﬀerent
groups
dunprotected
vs
dprotected
which
means
there
is
an
equal
number
of
candidates
from
each
group
here
the
downsampling
ratio
is
deﬁned
as
dunprotected
dprotected
the
underlying
assumption
behind
this
curriculum
sampling
strategy
is
that
it
is
easier
to
train
the
model
when
the
metadataset
and
training
dataset
have
similar
distribution
and
that
it
is
diﬃcult
to
optimize
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
57
algorithm
the
mcfr
learning
algorithm
input
training
dataset
qtrain
dtrain
batch
size
max
iterations
output
ranking
model
parameter
initialize
ranking
model
parameter
and
the
meta
learner
parameter
for
to
do
xqmeta
qmeta
curriculumsampling
qtrain
dtrain
xqtrain
qtrain
sampleminibatch
qtrain
dtrain
update
by
alg
end
for
the
parameters
in
the
ranking
model
when
the
meta
learner
sees
very
diﬀerent
metadataset
compared
to
the
training
dataset
as
shown
in
fig
4.3
we
illustrate
the
change
in
the
distribution
of
two
groups
in
the
meta-dataset
at
diﬀerent
training
epochs
to
train
the
meta
learner
we
use
the
curriculum
sampled
data
xqmeta
qmeta
the
meta-dataset
represents
the
meta-knowledge
of
the
true
distribution
of
the
protected
group
and
the
other
group
where
qmeta
and
dmeta
in
the
metadataset
we
denote
the
feature
vector
of
each
item
as
qmeta
and
the
relevance
score
as
qmeta
given
query
qmeta
from
qmeta
similar
to
ltrain
we
denote
lmeta
as
the
loss
value
for
each
meta-dataset
sample
thus
we
deﬁne
curriculumsampling
qtrain
dtrain
as
the
following
1.0
4.13
where
is
the
ratio
of
sampled
candidates
for
each
group
for
each
query
note
that
this
is
single
step
scheduler
as
the
ratio
is
updated
at
each
epoch
after
executing
curriculumsampling
at
each
epoch
the
sampling
meta-dataset
xqmeta
qmeta
should
have
the
property
that
dunprotected
dprotected
intuitively
the
curriculumsampling
decreases
the
ratio
epoch
by
epoch
from
the
biased
ratio
to
1.0
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
58
as
described
in
section
4.2
the
meta-dataset
is
an
important
part
of
the
model
training
as
it
is
the
key
data
to
guide
the
meta
learner
since
the
meta
learner
aims
to
reweight
the
loss
for
the
ranking
model
how
well
the
meta
learner
is
trained
determine
the
performance
of
the
ranking
model
with
the
curriculum
sampling
we
decrease
the
training
diﬃculty
of
the
meta
learner
compared
to
mfr
80
which
only
uses
one
sampled
unbiased
dataset
the
meta
learner
could
progressively
be
trained
with
more
unbiased
meta-dataset
as
the
epoch
increases
which
could
improve
the
meta
learner
performance
and
lead
to
better
overall
performance
for
the
ranking
model
the
whole
training
process
is
summarized
in
algorithm
items
query
protected
query
w3c
experts
gender
200
21.5
engineering
students
high
school
type
480.6
167.6
engineering
students
gender
480.6
97.6
law
students
law
students
compas
gender
race
race
21791
19567
6889
9537
1282
3528
table
4.2
summary
of
dataset
statistics
we
report
the
average
counts
of
total
and
unprotected
items
per
query
for
the
w3c
experts
and
engineering
students
datasets
we
provide
the
exact
item
counts
for
the
law
students
and
compas
datasets
each
of
which
contains
only
one
query
our
framework
provides
ﬂexibility
to
solve
diﬀerent
ranking
problems
as
listnet
17
may
not
work
for
all
ranking
problems
in
other
cases
the
fairness
terms
could
also
be
switched
by
using
diﬀerent
fairness
metrics
or
diﬀerent
formula
to
compute
the
disparate
exposure
as
the
exposure
issue
is
not
the
only
fairness
problem
the
mcfr
is
capable
of
being
optimized
with
other
fairness
terms
such
as
position
bias
and
conformity
bias
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
4.3
59
experiments
in
the
experiments
we
train
and
evaluate
the
model
on
four
real-world
public
datasets
we
study
both
the
ranking
and
fairness
metrics
of
our
approach
compared
to
other
baseline
models
we
also
conduct
an
ablation
study
for
the
eﬀectiveness
of
our
framework
by
changing
the
ranking
loss
term
and
the
disparate
exposure
term
we
repeat
the
experiment
on
the
same
datasets
with
diﬀerent
settings
of
loss
functions
and
we
evaluate
the
proposed
framework
by
comparing
it
with
the
baseline
models
in
the
analysis
the
following
questions
are
answered
what
is
the
proposed
mcfr
performance
compared
to
the
baseline
models
could
mcfr
improve
both
the
ranking
and
fairness
metrics
in
diﬀerent
loss
functions
what
are
the
eﬀects
of
the
curriculum
sampling
4.3
experimental
setting
we
train
and
evaluate
the
model
on
four
real-world
public
datasets
engineering
student
ii
law
student
iii
w3c
experts
iv
compas
correctional
oﬀender
management
proﬁling
for
alternative
sanctions
the
statistics
of
each
dataset
are
summarized
in
table
4.2
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
60
w3c
experts
dataset
this
dataset
originates
from
trec
2005
enterprise
track
26
it
involves
searching
for
experts
based
on
topic
using
features
from
their
emails
we
designate
gender
as
the
protected
attribute
with
technical
topics
as
queries
in
this
context
females
are
the
protected
group
and
males
are
non-protected
each
query
has
200
items
averaging
21.5
from
the
protected
group
given
that
the
original
dataset
ranks
retrieved
experts
equally
we
adopt
the
delter
experiments
setting
89
categorizing
expert
candidates
as
male
experts
female
experts
male
non-experts
and
female
non-experts
for
candidate
features
we
utilize
the
elasticsearch
learning
to
rank
plug-in1
for
all
query-candidate
pair
text
features
law
student
dataset
this
dataset
82
was
collected
to
determine
if
the
lsat
law
school
admission
test
in
the
us
is
biased
against
ethnic
minorities
the
dataset
contains
information
from
ﬁrst-year
law
students
and
the
protected
attributes
are
gender
and
race
the
query
is
academic
year
and
the
task
is
to
retrieve
students
with
good
lsat
scores
since
our
problem
setting
is
focused
on
one
protected
attribute
at
time
we
have
two
datasets
law
students
gender
and
law
students
race
in
the
law
students
gender
dataset
females
are
the
protected
group
among
21
791
candidates
with
537
being
female
in
the
law
students
race
dataset
african
americans
are
the
protected
group
out
of
19
567
candidates
with
282
from
this
group
engineering
students
this
dataset
89
contains
information
on
ﬁrst-year
students
at
chilean
university
the
qualiﬁcation
features
include
admission
test
results
in
mathematics
language
and
science
the
students
high
school
grades
and
the
number
of
https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
61
credits
taken
at
the
university
the
task
is
to
predict
academic
performance
and
the
protected
attributes
are
high
school
type
and
gender
similarly
we
have
two
datasets
engineering
students
high
school
type
and
engineering
students
gender
for
engineering
students
datasets
one
focuses
on
high
school
type
with
public
high
school
students
as
the
protected
group
averaging
167.6
out
of
480.6
items
per
query
the
other
considers
gender
with
females
as
the
protected
group
averaging
97.6
out
of
480.6
items
per
query
compas
compas
correctional
oﬀender
management
proﬁling
for
alternative
sanctions
is
commercial
algorithm
for
scoring
criminal
defendant
likelihood
of
recidivism
in
the
compas
dataset
it
has
been
observed
that
the
algorithm
is
biased
towards
african
american
candidates
in
this
dataset
the
task
is
to
predict
the
recidivism
score
and
the
protected
attribute
is
race
there
are
889
candidates
in
total
and
528
are
african
americans
4.3
1.1
baselines
we
integrated
several
baseline
models
in
our
implementation
listnet
17
introduces
listwise
loss
function
lambdamart
16
combines
mart
and
lambdarank
transforming
ranking
tasks
with
gradient
boosting
decision
trees
deltr
89
oﬀers
an
ltr
strategy
with
listwise
fairness
metrics
fa
ir
90
applies
pre
and
post-processing
techniques
for
enhanced
fairness
autodebias
20
presents
debiasing
method
for
recommendation
systems
fairgbm
27
delivers
fairness-centric
classiﬁcation
model
for
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
62
gbdt
while
mfr
80
employs
meta-learning
for
fair
ltr
notably
only
listnet
and
lambdamart
focus
solely
on
ranking
metrics
with
deltr
and
mfr
emphasizing
fairness-aware
ranking
4.3
1.2
implementation
details
to
split
the
datasets
we
have
50
queries
for
training
and
10
queries
for
testing
in
the
w3c
dataset
queries
for
training
and
query
for
testing
in
the
engineering
students
dataset
and
80
for
training
and
20
for
testing
in
the
law
students
dataset
and
the
compas
dataset
we
use
precision
10
10
38
for
the
w3c
dataset
and
kendall
tau
48
for
other
datasets
to
evaluate
the
ranking
performance
kendall
tau
assesses
the
correlation
between
two
ranking
sets
calculating
the
diﬀerence
between
the
number
of
concordant
and
discordant
pairs
divided
by
the
total
number
of
pairs
it
ranges
from
to
indicating
perfect
agreement
no
correlation
or
perfect
disagreement
in
the
rankings
respectively
in
details
the
kendall
tau
is
calculated
as
the
following
kendall
tau
4.14
where
is
the
number
of
concordant
pairs
the
number
of
discordant
pairs
the
number
of
ties
in
the
ground
truth
rankings
and
the
number
of
ties
in
the
predicted
rankings
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
89
thus
in
the
fairness
metric
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
63
in
the
training
we
set
the
update
frequency
of
the
weighting
model
parameter
to
be
per
steps
the
optimizer
to
be
sgd
74
the
momentum
to
be
0.98
the
learning
rate
to
be
0.022
the
hidden
layer
dimension
to
be
30
and
the
number
of
hidden
layers
to
be
for
the
ranking
model
we
set
the
learning
rate
to
be
0.005
the
optimizer
to
be
sgd
the
momentum
to
be
0.95
and
the
weight
decay
to
be
0.005
we
set
diﬀerent
values
for
and
training
epoch
for
diﬀerent
dataset
w3c
dataset
uses
500
and
100
epochs
engineering
students
high
school
uses
000
and
280
epochs
engineering
students
gender
uses
400
and
150
epochs
law
students
gender
uses
200
and
550
epochs
law
students
race
uses
50
000
and
110
epochs
and
compas
race
uses
500
and
45
epochs
in
the
ablation
study
to
evaluate
the
eﬀectiveness
of
our
framework
we
use
the
same
hyperparameters
as
described
above
for
other
ranking
losses
such
as
rankmse
and
ranknet
in
the
experiment
we
collect
results
with
all
combinations
of
ranking
losses
and
fairness
terms
4.3
fair
ranking
performance
in
table
4.3
we
detail
the
performance
of
both
baseline
and
fair
ranking
models
trained
with
hinge
exposure
the
proposed
mcfr
outperforms
other
baseline
models
in
fairness
metrics
across
all
datasets
when
compared
to
listnet
and
lambdamart
models
like
deltr
mfr
fa
ir
autodebias
fairgbm
and
mcfr
show
enhanced
results
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
w3c
experts
gender
precision
10
fairness
listnet
17
lambdamart
16
deltr
89
fa
ir
pre
90
fa
ir
post
90
autodebias
20
fairgbm
27
mfr
mcfr
0.178
0.095
0.180
0.180
0.180
0.033
0.087
0.126
0.118
0.759
0.738
0.827
0.770
0.827
0.829
0.941
0.830
0.843
law
students
gender
kendall
tau
fairness
listnet
17
lambdamart
16
deltr
89
fa
ir
pre
90
fa
ir
post
90
autodebias
20
fairgbm
27
mfr
mcfr
0.202
0.199
0.188
0.203
0.182
0.222
0.141
0.225
0.225
0.931
0.979
0.993
0.931
0.965
0.894
0.998
1.015
1.023
engineering
students
high
school
type
kendall
tau
fairness
0.390
0.355
0.391
0.374
0.391
0.372
0.338
0.391
0.390
1.070
1.002
1.075
1.020
1.075
0.955
0.909
1.086
1.088
law
students
race
kendall
tau
fairness
0.184
0.156
0.130
0.161
0.140
0.135
0.210
0.184
0.182
0.853
0.847
1.014
0.895
0.944
1.009
1.116
1.654
1.671
64
engineering
students
gender
kendall
tau
fairness
0.384
0.326
0.370
0.360
0.370
0.372
0.336
0.352
0.350
0.858
0.907
0.976
0.942
0.976
0.955
0.892
1.052
1.055
compas
race
kendall
tau
fairness
0.639
0.542
0.576
0.557
0.557
0.644
0.550
0.644
0.644
0.836
0.956
0.970
1.039
1.040
1.136
0.917
1.138
1.144
table
4.3
experimental
results
with
hinge
exposure
89
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
so
the
values
greater
than
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
kendall
tau
precision
10
10
scores
indicate
better
performance
the
bold
text
indicates
the
model
with
the
best
performance
and
the
results
show
that
the
mcfr
model
is
better
on
the
fairness
metrics
with
comparable
performance
on
the
ranking
metrics
against
other
state-of-the-art
models
due
to
the
inclusion
of
fairness
measures
during
training
notably
mcfr
use
of
curriculum
sampling
for
the
meta-dataset
allows
it
to
surpass
mfr
in
fairness
metrics
as
the
meta-learner
adeptly
adjusts
the
loss
distribution
during
mcfr
training
curriculum
sampling
creates
the
meta-dataset
for
the
meta
model
the
w3c
dataset
limited
items
from
the
protected
group
hinder
signiﬁcant
distribution
shifts
in
meta-dataset
sampling
aﬀecting
its
ranking
performance
this
constraint
primarily
contributes
to
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
65
the
decreasing
ranking
performance
observed
in
the
model
trained
on
w3c
data
except
on
the
w3c
dataset
mcfr
has
competitive
results
on
the
ranking
metrics
compared
to
the
other
baseline
models
indicating
that
training
mcfr
does
not
focus
solely
on
the
fairness
metrics
for
listnet
the
results
are
also
expected
as
they
only
optimize
for
ranking
metrics
and
have
better
performance
in
ranking
metrics
on
engineering
students
gender
and
law
students
race
since
autodebias
and
fairgbm
are
tailored
for
recommendation
and
classiﬁcation
tasks
respectively
their
limited
performance
on
ranking
problems
is
as
expected
in
fig
4.1
we
also
plot
the
histogram
of
ranks
on
the
protected
attributes
from
the
diﬀerent
models
from
the
plot
we
can
see
that
the
distribution
of
predicted
ranks
shifts
from
right
to
left
indicating
that
the
mcfr
model
generally
ranks
items
from
the
protected
group
higher
compared
to
listnet
and
mfr
in
the
plot
on
the
x-axis
indicates
the
top
rank
and
more
candidates
falling
in
bins
on
the
left
means
the
candidates
receive
higher
ranks
in
ranking
algorithms
mcfr
enhances
visibility
for
underrepresented
protected
groups
however
fairness
doesn
mean
maximizing
exposure
for
them
at
the
expense
of
the
non-protected
group
visibility
4.3
ablation
studies
we
present
the
ablation
study
results
for
mcfr
which
oﬀers
ﬂexibility
in
choosing
loss
functions
and
fairness
terms
as
generalized
framework
mcfr
consistently
enhances
both
ranking
and
fairness
metrics
across
various
loss
functions
and
exposure
formulas
we
employed
rankmse
ranknet
and
listnet
as
representatives
for
pointwise
pairwise
and
listwise
losses
which
serve
as
baseline
models
in
table
4.4
4.5
and
4.6
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
exposure
type
rankmse
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
rankmse
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
66
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.121
0.770
0.187
0.800
0.376
0.836
0.115
0.781
0.384
1.049
0.357
1.010
0.115
0.782
0.384
1.052
0.353
1.020
0.115
0.780
0.384
1.045
0.360
0.982
0.115
0.782
0.384
1.045
0.360
0.990
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.213
0.874
0.190
0.847
0.493
0.768
0.225
0.910
0.191
0.847
0.634
0.911
0.226
0.920
0.190
0.851
0.634
0.911
0.223
1.010
0.139
0.992
0.633
0.911
0.225
1.023
0.138
0.996
0.630
0.928
table
4.4
ablation
study
results
with
rankmse
11
exposure
type
ranknet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
ranknet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.121
0.770
0.131
0.806
0.190
0.800
0.121
0.774
0.126
0.925
0.188
0.810
0.123
0.775
0.131
0.867
0.186
0.820
0.121
0.774
0.126
0.925
0.188
0.810
0.121
0.774
0.131
0.867
0.186
0.812
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.093
0.942
0.105
0.866
0.128
0.768
0.131
1.033
0.140
1.284
0.373
0.839
0.132
1.036
0.152
1.370
0.375
0.840
0.173
1.033
0.105
0.866
0.352
0.832
0.220
1.050
0.105
0.866
0.352
0.832
table
4.5
ablation
study
results
with
ranknet
15
4.3
3.1
ranking
terms
analysis
first
we
analyze
the
performance
of
mcfr
using
diﬀerent
ranking
terms
in
loss
functions
when
using
listnet
mcfr
has
worse
ranking
performance
on
the
w3c
experts
gender
and
engineering
students
gender
datasets
than
the
listnet
model
on
other
datasets
mcfr
and
the
listnet
model
have
similar
ranking
performance
note
that
on
the
law
students
gender
dataset
mcfr
also
improves
the
ranking
metrics
when
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
exposure
type
listnet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
exposure
type
listnet
mfr
mcfr
mfr
mcfr
hinge
hinge
squared
squared
67
w3c
experts
engineering
students
engineering
gender
high
school
type
students
gender
precision
10
fairness
kendall
tau
fairness
kendall
tau
fairness
0.178
0.759
0.390
1.070
0.384
0.858
0.126
0.830
0.391
1.086
0.352
1.052
0.118
0.843
0.390
1.088
0.350
1.055
0.118
0.803
0.330
1.005
0.358
1.006
0.118
0.803
0.341
1.005
0.342
1.018
law
students
law
students
compas
gender
race
race
kendall
tau
fairness
kendall
tau
fairness
kendall
tau
fairness
0.202
0.931
0.184
0.853
0.639
0.836
0.225
1.015
0.184
1.654
0.644
1.138
0.225
1.023
0.182
1.671
0.644
1.144
0.223
1.010
0.113
1.166
0.340
0.828
0.225
1.014
0.079
1.115
0.632
1.068
table
4.6
ablation
study
results
with
listnet
17
using
rankmse
similar
pattern
is
observed
on
ranknet
mcfr
achieves
similar
ranking
performance
on
the
w3c
experts
gender
dataset
and
improves
the
ranking
metrics
on
the
law
students
gender
and
law
students
race
datasets
in
addition
to
the
fairness
metrics
the
consistent
improvement
in
ranking
metrics
shows
that
the
proposed
mcfr
is
generalized
framework
that
can
adapt
to
many
ranking
loss
functions
4.3
3.2
fairness
terms
analysis
second
we
evaluate
diﬀerent
fairness
terms
in
loss
functions
when
using
listnet
as
the
ranking
loss
term
mcfr
greatly
improves
the
fairness
metrics
on
the
w3c
experts
gender
and
engineering
students
gender
datasets
on
other
datasets
mcfr
outperforms
the
listnet
model
on
the
fairness
metrics
with
similar
ranking
performance
when
using
rankmse
mcfr
also
improves
the
fairness
metrics
on
the
law
students
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
68
gender
and
law
students
race
datasets
we
see
that
mcfr
can
improve
the
fairness
metrics
with
various
ranking
loss
terms
4.3
3.3
curriculum
sampling
analysis
moreover
we
compare
the
performance
of
mcfr
and
mfr
to
show
the
eﬀectiveness
of
curriculum
learning
using
diﬀerent
losses
note
that
in
mfr
we
use
the
same
settings
in
loss
functions
as
in
mcfr
to
have
fair
comparison
when
using
the
hinge
exposure
mcfr
usually
has
better
fairness
performance
with
minor
trade-oﬀs
in
ranking
metrics
except
on
the
w3c
experts
gender
dataset
using
listnet
while
using
the
squared
exposure
except
on
the
law
students
race
dataset
mcfr
improves
both
ranking
and
fairness
metrics
compared
to
mfr
these
results
demonstrate
the
eﬀectiveness
of
curriculum
learning
fairness
metric
gender
ranking
metric
gender
fairness
metric
race
ranking
metric
race
figure
4.4
evaluation
results
on
the
down-sampling
experiments
we
conduct
the
experiment
on
law
students
gender
and
law
students
race
datasets
and
we
downsample
the
training
data
from
the
rate
of
0.1
to
0.9
the
results
show
that
mcfr
has
better
data
eﬃciency
as
it
could
achieve
better
fairness
metrics
with
similar
ranking
performance
than
mfr
and
autodebias
at
diﬀerent
down-sampling
rate
chapter
uniﬁed
meta-learning
framework
for
fair
ranking
with
curriculum
learning
4.3
3.4
69
data
eﬃciency
to
assess
curriculum
learning
eﬀect
on
data
eﬃciency
we
compare
with
mcfr
mfr
and
autodebias
using
down-sampled
training
data
varying
from
10
to
90
of
the
original
data
figure
4.4
illustrates
how
mcfr
outperforms
mfr
and
autodebias
across
most
sampling
rates
in
fairness
for
gender-related
data
achieving
fair
metrics
close
to
1.0
while
maintaining
high
ranking
performance
mcfr
demonstrates
superior
fairness
with
reduced
training
data
for
race-related
data
mcfr
achieves
better
ranking
performance
and
higher
fairness
metrics
indicating
our
curriculum
strategy
eﬀectively
enhances
fairness
of
the
protected
groups
even
with
less
data
deltr
mfr
mcfr
w3c
experts
gender
43.69
21.16
171.37
engineering
students
high
school
type
14.09
15.24
92.57
engineering
students
gender
40.92
17.24
91.64
law
students
law
students
compas
gender
race
race
14.35
17.70
19.67
51.29
49.72
76.88
294.42
293.92
352.96
table
4.7
experimental
results
on
total
convergence
time
in
seconds
it
shows
the
total
convergence
time
for
diﬀerent
algorithms
deltr
mfr
and
mcfr
across
various
datasets
or
scenarios
based
on
the
table
the
mcfr
framework
generally
has
comparable
convergence
time
than
the
other
two
algorithms
4.3
3.5
training
and
inference
eﬃciency
to
enhance
ranking
fairness
with
mcfr
we
sought
balance
between
fairness
and
eﬃciency
as
shown
in
table
4.7
mcfr
has
training
complexity
comparable
to
methods
like
deltr
and
the
curriculum
sampling
extends
the
training
time
linearly
with
sampling
rounds
notably
during
the
inference
mcfr
mfr
and
deltr
will
show
consistent
eﬃciency
since
these
algorithms
share
the
same
base
ranking
model
with
the
same
number
of
parameters
and
layers
and
there
is
only
one
forward
pass
for
70
predictions
table
4.7
shows
mcfr
extended
convergence
time
due
to
curriculum
sampling
and
added
epochs
mcfr
fairness
beneﬁts
are
clear
yet
we
value
eﬃciency
in
time-sensitive
applications
overall
these
results
demonstrate
that
the
curriculum
learning
in
mcfr
enhances
fairness
without
compromising
ranking
performance
also
making
training
more
eﬃcient
4.4
conclusion
in
this
study
we
introduced
the
meta
curriculum-based
fair
ranking
mcfr
framework
to
address
data
bias
in
search
problems
by
employing
meta-learner
trained
on
curriculum-learning-sampled
meta-dataset
our
approach
re-weights
the
training
loss
from
the
target
ranker
on
biased
data
this
re-weighted
loss
aids
in
developing
an
unbiased
ranking
model
enhancing
exposure
for
minority
groups
comparative
experiments
on
real-world
datasets
conﬁrm
mcfr
superiority
over
fair
ranking
models
lacking
meta-learning
and
curriculum
learning
components
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.1
introduction
the
emergence
of
large
language
models
llms
like
gpt
models
12
57
and
llama2
76
marks
signiﬁcant
trend
in
multiple
ﬁelds
ranging
from
natural
language
processing
to
information
retrieval
in
the
ranking
challenges
llms
have
shown
demonstrated
performance
research
exempliﬁed
by
projects
like
rankgpt
73
65
highlights
the
proﬁciency
of
gpt
models
in
delivering
competitive
ranking
results
surpassing
traditional
neural
ranking
models
in
precision
and
relevance
with
the
growing
popularity
of
llms
assessing
their
fairness
has
become
as
crucial
as
evaluating
their
eﬀectiveness
71
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
72
search
query
agriculture
rank
the
passages
based
on
their
relevance
to
the
search
query
hana
meisel
female
agronomist
thomas
giles
male
pastoralist
theodor
bergmann
male
agronomist
thomas
giles
male
pastoralist
theodor
bergmann
male
agronomist
hana
meisel
female
agronomist
listwise
evaluation
search
query
agriculture
rank
the
two
passages
based
on
their
relevance
to
the
search
query
hana
meisel
female
agronomist
thomas
giles
male
pastoralist
thomas
giles
male
pastoralist
hana
meisel
female
agronomist
pairwise
evaluation
figure
5.1
illustration
of
two
evaluation
methods
listwise
evaluation
and
pairwise
evaluation
each
document
is
associated
with
binary
protected
attribute
which
is
used
in
the
fairness
evaluation
metrics
while
recent
research
has
primarily
concentrated
on
the
eﬃciency
and
accuracy
of
llms
in
ranking
tasks
there
is
an
increasing
concern
about
their
fairness
this
concern
is
particularly
highlighted
given
the
signiﬁcant
impact
and
easy
accessibility
of
these
models
prior
studies
in
natural
language
processing
41
62
and
recommendation
systems
95
have
shown
the
unfair
treatment
towards
underrepresented
groups
by
llms
although
fairness
issues
in
traditional
search
engines
have
been
extensively
explored
there
is
notable
gap
in
examining
of
llms
as
rankers
in
search
systems
our
study
seeks
to
address
this
gap
by
conducting
an
in-depth
audit
of
various
llms
including
both
gpt
models
and
open-source
alternatives
in
this
work
we
conduct
an
empirical
study
that
assesses
the
llms
as
text
ranker
from
both
the
user
and
item
perspectives
to
evaluate
fairness
we
investigate
how
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
73
these
models
despite
being
trained
on
vast
and
varied
datasets
might
unintentionally
mirror
social
biases
in
their
ranking
outcomes
we
concentrate
on
various
binary
protected
attributes
that
are
frequently
underrepresented
in
search
results
examining
how
llms
rank
documents
associated
with
these
attributes
in
response
to
diverse
user
queries
speciﬁcally
we
examine
the
llms
using
both
the
listwise
and
pairwise
evaluation
methods
aiming
to
provide
comprehensive
study
of
the
fairness
in
these
models
furthermore
we
mitigate
the
pairwise
fairness
issue
by
ﬁne-tuning
the
llms
with
an
unbiased
dataset
and
the
experimental
results
show
the
improvement
in
the
evaluation
to
the
best
of
our
knowledge
our
work
presents
the
ﬁrst
benchmark
results
investigating
the
fairness
issue
in
llms
as
the
rankers
in
summary
this
work
makes
the
contribution
as
follows
we
build
the
ﬁrst
llm
fair
ranking
benchmark
for
llms
as
text
ranker
which
incorporates
the
listwise
and
pairwise
evaluation
methods
with
consideration
of
binary
protected
attributes
we
conduct
extensive
and
comprehensive
experiments
revealing
the
fairness
problem
in
the
llms
on
the
real-word
datasets
we
propose
mitigation
strategy
involving
the
ﬁne-tuning
of
open-source
llms
using
lora
40
to
address
the
fairness
issue
observed
in
pairwise
evaluation
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
74
listwise
ranking
gpt-3
gpt-4
protected
group
mistral
unprotected
group
llama2
rank
rank
rank
rank
rank
rank
utility
group
exposure
pairwise
ranking
gpt-3
gpt-4
mistral
llama2
items
ranked
higher
by
llms
percentage
of
unprotected
group
figure
5.2
proposed
evaluation
framework
this
schematic
diagram
represents
our
dual
evaluation
methodology
the
top
sequence
depicts
the
listwise
ranking
process
where
items
from
protected
and
unprotected
groups
are
presented
to
various
llms
gpt-3
gpt-4
mistral-7b
and
llama2
and
are
evaluated
on
utility
and
group
exposure
metrics
the
bottom
sequence
illustrates
the
pairwise
ranking
approach
which
contrasts
the
ranking
preference
of
llms
between
items
from
protected
and
unprotected
groups
quantifying
any
bias
by
the
percentage
of
unprotected
group
items
ranked
higher
5.2
llm
fair
ranking
we
deﬁne
the
set
of
queries
in
our
dataset
as
consisting
of
queries
and
the
set
of
items
as
comprising
items
for
each
query
there
exists
list
of
item
candidates
from
we
represent
each
i-th
query-item
pair
with
text
token
vector
xi
and
an
associated
relevance
score
yi
importantly
the
item
candidates
in
are
annotated
with
binary
attribute
indicating
their
classiﬁcation
as
either
belonging
to
protected
group
or
non-protected
group
this
attribute
representing
aspects
like
gender
or
race
is
crucial
as
it
highlights
the
potential
exposure
bias
present
in
the
ranking
prediction
process
next
we
present
our
evaluation
benchmark
dataset
and
introduce
two
fairness
evaluation
methods
listwise
and
pairwise
evaluation
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.2
75
datasets
in
our
benchmark
we
leverage
datasets
from
the
trec
fair
ranking
track
29
for
the
years
2021
and
2022
we
primarily
focus
on
the
task
for
wikiproject
coordinators
to
search
for
relevant
articles
with
the
2022
dataset
containing
44
queries
and
the
2021
dataset
having
57
for
each
query
we
select
200
items
from
english
wikipedia
and
apply
the
deltr
89
experiment
methodology
to
introduce
discriminatory
pattern
in
sorting
candidates
categorizing
them
into
four
groups
experts
in
the
non-protected
group
experts
in
the
protected
group
non-experts
in
the
non-protected
group
and
non-experts
in
the
protected
group
to
be
speciﬁc
the
experts
are
deﬁned
as
the
relevant
candidates
given
the
query
and
the
non-experts
are
the
irrelevant
candidates
which
are
randomly
selected
from
the
relevant
candidates
from
other
queries
the
benchmark
includes
three
datasets
diﬀerentiated
by
the
protected
groups
in
trec
2022
gender
females
are
considered
the
protected
group
while
males
are
non-protected
in
trec
2022
location
and
trec
2021
location
non-europeans
are
designated
as
the
protected
group
with
europeans
serving
as
the
non-protected
group
5.2
listwise
evaluation
our
listwise
evaluation
assesses
fairness
from
two
perspectives
query-side
and
itemside
focusing
on
attributes
like
gender
we
measure
how
well
llms
integrate
underrepresented
groups
into
rankings
using
group
fairness
for
visibility
and
precision
for
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
76
utility
query-side
analysis
checks
for
biases
against
protected
attributes
contrasting
gender-neutral
against
gender-sensitive
queries
to
verify
consistent
rankings
across
groups
these
methods
together
ensure
thorough
fairness
evaluation
in
llm
rankings
5.2
2.1
data
construction
in
our
fairness
evaluation
dataset
we
leverage
the
rankgpt
73
approach
with
standard
prompt
template
to
capture
user
instructions
detailing
their
preferences
and
task
details
our
dataset
incorporates
item-side
protected
groups
and
introduces
both
neutral
and
sensitive
query
templates
the
former
without
demographic
markers
and
the
latter
including
speciﬁc
references
to
attributes
like
gender
and
geography
for
queryside
fairness
assessment
speciﬁcally
the
template
for
neutral
and
sensitive
instructions
is
as
the
following
neutral
you
are
the
search
system
for
the
wikiproject
coordinators
as
users
their
goal
is
to
search
for
relevant
articles
and
produce
ranked
list
of
articles
needing
work
that
editors
can
then
consult
when
looking
for
work
to
do
search
query
query
rank
the
passages
based
on
their
relevance
to
the
search
query
item
d1
dn
sensitive
you
are
the
search
system
for
the
query-side
sensitive
attribute
wikiproject
coordinators
as
users
their
goal
is
to
search
for
relevant
articles
and
produce
ranked
list
of
articles
needing
work
that
editors
can
then
consult
when
looking
for
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
77
work
to
do
search
query
query
rank
the
passages
based
on
their
relevance
to
the
search
query
item
d1
dn
5.2
2.2
metrics
group
exposure
ratio
in
our
listwise
fairness
evaluation
we
deﬁne
two
groups
of
candidates
within
the
non-protected
group
g0
and
the
protected
group
g1
with
the
latter
representing
historically
discriminated
groups
such
as
females
and
non-europeans
often
underrepresented
in
datasets
following
the
methodology
introduced
by
singh
and
joachims
70
we
measure
the
exposure
of
candidate
represented
by
the
text
token
xi
in
ranked
list
of
generated
by
probabilistic
ranking
model
which
is
expressed
as
exposure
xi
pi
va
5.1
here
pi
is
the
probability
that
places
document
at
rank
and
va
represents
the
position
bias
at
position
such
that
va
log
following
zehlike
and
castillo
89
we
focus
on
the
position
bias
of
the
top
position
with
v1
the
average
exposure
of
candidates
in
group
is
then
exposure
exposure
xi
5.2
xi
exposure
finally
we
deﬁne
the
group
exposure
ratio
as
exposure
g1
ratio
closer
to
1.0
indicates
fairer
ranking
list
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.2
78
pairwise
evaluation
in
the
pairwise
evaluation
method
we
delve
into
item-side
fairness
by
presenting
pairs
of
items
to
the
llms
with
one
from
the
protected
group
and
one
from
the
non-protected
group
this
method
includes
two
distinct
tasks
relevant
items
comparison
we
provide
the
llms
with
pair
of
randomly
selected
relevant
items
prompting
them
to
determine
which
item
is
more
relevant
the
fairness
assessment
hinges
on
the
balance
in
the
number
of
items
recognized
as
relevant
from
both
groups
nearly
equal
count
signiﬁes
fairness
as
it
indicates
unbiased
relevance
assessment
fairness
is
quantiﬁed
by
the
ratio
of
recognized
relevance
between
the
groups
with
ratio
close
to
1.0
signaling
greater
fairness
irrelevant
items
comparison
similarly
we
present
pairs
of
irrelevant
items
and
follow
the
same
procedure
in
this
scenario
fair
llm
should
exhibit
similar
indiﬀerence
to
the
irrelevance
of
items
from
both
groups
again
reﬂected
in
ratio
approaching
1.0
pairwise
evaluation
is
employed
to
detect
biases
in
llm
rankings
towards
protected
or
unprotected
groups
by
directly
contrasting
items
from
varying
groups
this
method
uncovers
potential
group
preferences
within
llms
oﬀering
clear
view
of
their
fairness
in
diﬀerent
ranking
scenarios
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.2
3.1
79
data
construction
for
pairwise
evaluation
we
use
ﬁxed
prompt
template
with
pairs
of
relevant
or
irrelevant
items
each
containing
one
from
protected
group
and
one
from
an
unprotected
group
to
mitigate
position
bias
with
only
two
items
each
pair
is
queried
twice
with
the
order
of
protected
and
unprotected
items
alternated
speciﬁcally
the
template
is
as
the
following
you
are
the
search
system
for
the
wikiproject
coordinators
as
users
their
goal
is
to
search
for
relevant
articles
and
produce
ranked
list
of
articles
needing
work
that
editors
can
then
consult
when
looking
for
work
to
do
rank
the
two
passages
based
on
their
relevance
to
query
query
item
d1
d2
5.2
3.2
metrics
in
our
pairwise
evaluation
metrics
we
calculate
the
proportion
of
times
items
from
the
protected
and
unprotected
groups
are
ranked
ﬁrst
additionally
we
compute
the
ratio
of
the
number
of
times
protected
group
items
are
ranked
ﬁrst
to
the
number
of
times
unprotected
group
items
are
ranked
ﬁrst
ratio
near
1.0
indicates
higher
fairness
5.3
results
and
analysis
in
our
benchmark
we
carefully
evaluate
the
popular
llms
including
gpt-3
gpt-4
llama2-13b
and
mistral-7b
44
this
section
details
our
analysis
of
their
performance
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
80
across
both
listwise
and
pairwise
evaluations
5.3
0.1
eﬀect
of
window
and
step
size
window
10
10
20
step
10
20
0.1261
0.1295
0.1227
0.1205
fairness
0.9881
0.9634
0.9777
0.9628
table
5.1
evaluation
results
on
diﬀerent
choices
of
window
and
step
sizes
the
results
show
that
there
are
not
signiﬁcant
diﬀerences
in
the
ranking
and
fairness
metrics
so
we
select
window
size
and
step
size
in
the
listwise
evaluation
experiments
as
shown
in
table
5.1
we
conduct
additional
experiments
to
evaluate
diﬀerent
sets
of
window
sizes
and
step
sizes
the
experiments
are
conducted
on
the
listwise
evaluation
on
the
2022
gender
datasets
with
neutral
query
using
mistral-7b
model
we
set
the
window
size
ranging
from
20
to
and
the
step
size
from
to
10
following
the
sliding
window
strategy
provided
in
rankgpt
73
empirically
we
did
not
observe
signiﬁcant
diﬀerences
in
both
the
ranking
and
fairness
metrics
thus
we
adopted
small
window
step
size
window
size
and
step
size
accounting
for
less
gpu
memory
to
save
the
computation
resources
5.3
listwise
evaluation
results
in
our
listwise
evaluation
we
adopt
the
rankgpt
methodology
using
sliding
window
strategy
to
extract
ranking
lists
from
the
llms
we
use
the
window
size
at
and
the
step
size
at
across
all
tested
llms
given
that
these
models
are
trained
on
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
81
trec
2022
gender
trec
2022
location
trec
2021
location
figure
5.3
the
predicted
rankings
distribution
of
the
protected
groups
on
the
trec
datasets
using
the
listwise
evaluation
the
plots
reveal
the
ranking
variability
and
potential
biases
in
gender
and
geographic
attributes
highlighting
areas
for
improvement
in
fairness
across
the
llms
extensive
internet
corpora
and
the
trec
datasets
are
derived
from
wikipedia
we
input
only
the
wikipedia
page
titles
this
approach
leverages
the
llms
inherent
knowledge
base
about
these
topics
additionally
we
include
two
neural
rankers
monot5
56
and
monobert
55
as
baseline
models
unlike
the
llms
we
use
the
full
text
of
wikipedia
webpages
as
input
for
these
neural
rankers
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.1852
0.9964
0.1761
0.9559
0.1227
0.9919
0.1239
0.9955
0.1261
0.9881
0.1216
1.0304
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.2110
0.9739
0.1980
1.0031
0.1440
0.9308
0.1240
0.9268
0.1230
0.9426
0.1280
0.9607
query
attribute
metric
monot5
monobert
gpt-3
gpt-4
mistral-7b
llama2-13b
neutral
20
fairness
0.2018
1.0406
0.1974
1.0340
0.1184
0.9820
0.1167
0.9850
0.1430
0.9856
0.1211
0.9634
male
20
fairness
0.0830
0.7809
0.1000
0.8101
0.0841
0.9463
0.1080
0.9504
0.0966
0.9382
0.0920
0.9661
82
female
20
fairness
0.5239
1.9402
0.5102
1.7475
0.1705
1.2186
0.1761
1.2576
0.2102
1.4879
0.1614
1.2550
trec
2022
gender
european
20
fairness
0.2800
0.8543
0.2860
0.8890
0.1500
0.8846
0.1510
0.8889
0.1490
0.8895
0.1340
0.9130
non-european
20
fairness
0.0180
1.4682
0.0370
1.3201
0.1480
0.9368
0.1420
0.9432
0.0930
1.1073
0.1030
1.0227
trec
2022
location
european
20
fairness
0.3035
0.8483
0.2658
0.9254
0.1421
0.9173
0.1544
0.9071
0.1614
0.9142
0.1105
0.9247
non-european
20
fairness
0.0158
1.5039
0.0728
1.3143
0.1228
0.9841
0.1325
0.9877
0.0684
1.1448
0.1105
1.0325
trec
2021
location
table
5.2
listwise
evaluation
results
to
measure
fairness
we
compute
the
exposure
ratio
between
the
protected
and
the
non-protected
group
where
values
closer
to
1.0
indicate
greater
visibility
for
the
protected
group
and
vice
versa
for
the
ranking
metric
higher
precision
10
10
scores
indicate
better
performance
5.3
1.1
item-side
analysis
in
table
5.2
monot5
and
monobert
exhibit
robust
precision
20
scores
reﬂecting
their
eﬀectiveness
in
ranking
however
their
fairness
metrics
reveal
gap
in
equitable
gender
representation
with
monot5
slightly
outperforming
monobert
on
this
front
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
83
this
performance
discrepancy
is
likely
because
these
models
utilize
the
complete
text
of
wikipedia
pages
providing
wealth
of
features
that
represent
the
items
more
comprehensively
on
the
other
hand
llms
face
constraints
due
to
the
maximum
token
limits
for
input
limiting
their
capacity
to
fully
exploit
the
extensive
textual
information
available
in
the
trec
datasets
thereby
impacting
their
ranking
capability
among
llms
including
gpt-3
gpt-4
mistral-7b
and
llama2-13b
the
precision
20
scores
are
comparatively
lower
than
those
of
neural
ranking
models
this
may
reﬂect
the
generative
models
broader
focus
beyond
just
ranking
tasks
the
fairness
metrics
for
these
llms
are
varied
gpt-3
and
gpt-4
manage
to
stay
closer
to
the
ideal
fairness
ratio
indicating
more
balanced
treatment
of
gender
groups
mistral-7b
while
maintaining
similar
precision
falls
behind
in
fairness
indicating
potential
gender
bias
in
ranking
llama2-13b
although
consistent
in
its
approach
to
fairness
reveals
room
for
improvement
in
precision
when
contrasting
neural
rankers
with
llms
it
becomes
apparent
that
although
neural
rankers
demonstrate
higher
precision
they
do
not
necessarily
outperform
llms
in
terms
of
fairness
this
observation
underscores
the
importance
of
considering
fairness
particularly
for
users
who
prioritize
it
over
precision
in
speciﬁc
applications
within
the
llm
group
there
is
no
uniformity
in
achieving
fairness
suggesting
that
the
models
training
design
and
inherent
biases
may
inﬂuence
their
ability
to
rank
fairly
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
5.3
1.2
84
query-side
analysis
analyzing
the
query-side
fairness
from
the
table
5.2
our
focus
is
on
whether
llms
provide
similar
ranking
performance
for
diﬀerent
query
attributes
male
vs
female
european
vs
non-european
it
reveals
consistent
trend
across
both
neural
ranking
models
and
llms
they
tend
to
favor
female
and
european
queries
over
male
and
non-european
ones
while
fairness
metrics
for
llms
like
gpt-3
gpt-4
mistral-7b
and
llama2-13b
are
relatively
close
to
indicating
an
attempt
at
balanced
treatment
the
precision
20
scores
suggest
diﬀerent
story
with
clear
skew
towards
female
and
european
queries
this
observed
pattern
evident
in
both
monot5
and
monobert
points
to
an
underlying
bias
that
persists
despite
eﬀorts
to
achieve
equitable
treatment
across
query
attributes
underscoring
the
need
for
enhanced
model
training
and
fairness
optimization
in
figure
5.3
we
plot
the
predicted
ranking
of
the
protected
groups
highlights
distinct
patterns
in
fairness
and
ranking
performance
between
neural
rankers
and
llms
llms
demonstrate
tighter
rank
distributions
but
exhibit
biases
toward
certain
query
attributes
for
example
disparities
are
observed
in
the
treatment
of
gender
and
geographic
attributes
with
both
monot5
and
monobert
often
ranking
female
and
european
queries
more
favorably
trend
also
noted
to
varying
degrees
within
llms
this
suggests
that
while
neural
rankers
may
excel
in
precision
llms
oﬀer
more
consistent
rankings
though
neither
group
is
devoid
of
fairness
issues
these
ﬁndings
emphasize
the
necessity
for
further
tuning
and
bias
mitigation
in
both
neural
rankers
and
llms
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
85
to
ensure
equitable
treatment
across
all
query
attributes
5.3
pairwise
evaluation
results
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2407
0.2453
0.2275
0.2496
0.2366
0.0995
0.1227
0.2293
ratio
1.0190
1.0971
0.4206
1.8694
irrelevant
items
unprotected
protected
0.1797
0.2979
0.2033
0.2939
0.1335
0.1160
0.0920
0.2913
ratio
1.6580
1.4430
0.8689
3.1643
trec
2022
gender
females
as
the
protected
group
males
as
non-protected
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2638
0.2537
0.2347
0.2878
0.2484
0.4168
0.1521
0.2290
ratio
0.9615
1.2262
1.6779
1.5052
irrelevant
items
unprotected
protected
0.3199
0.2245
0.2759
0.2401
0.1876
0.1928
0.2444
0.1643
ratio
0.7500
0.8701
1.0277
0.6725
trec
2022
location
non-europeans
as
protected
europeans
as
non-protected
gpt-3
gpt-4
mistral-7b
llama2-13b
relevant
items
unprotected
protected
0.2117
0.3150
0.2148
0.3125
0.2582
0.4137
0.1490
0.2688
ratio
1.4877
1.4545
1.6019
1.8035
irrelevant
items
unprotected
protected
0.2385
0.2616
0.2428
0.2598
0.2516
0.1628
0.2540
0.1752
ratio
1.0968
1.0701
0.6471
0.6898
trec
2021
location
non-europeans
as
protected
europeans
as
non-protected
table
5.3
pairwise
evaluation
results
the
table
displays
fairness
metrics
for
llms
in
ranking
both
relevant
and
irrelevant
item
pairs
one
from
the
protected
and
the
other
from
the
unprotected
groups
it
includes
percentages
of
items
ranked
ﬁrst
from
each
group
and
their
ratio
reﬂecting
fairness
the
varying
levels
of
fairness
across
llms
particularly
in
irrelevant
pairings
highlight
the
importance
of
further
enhancing
fairness
in
llms
in
the
pairwise
evaluations
detailed
in
table
5.3
our
focus
is
on
assessing
the
fairness
of
various
llms
by
studying
how
they
rank
pairs
of
items
when
both
are
considered
relevant
or
irrelevant
the
analysis
aims
to
reveal
whether
these
models
display
biases
toward
items
from
speciﬁc
groups
gpt-3
consistently
shows
preference
for
female
items
in
both
scenarios
with
this
inclination
more
pronounced
for
irrelevant
items
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
86
suggesting
bias
in
favor
of
female
items
similarly
gpt-4
displays
moderate
bias
towards
female
items
with
ratios
indicating
stronger
bias
in
irrelevant
contexts
this
observed
trend
across
models
and
datasets
signals
an
area
for
improvement
pointing
to
the
need
for
more
balanced
algorithms
that
do
not
favor
one
group
over
another
particularly
in
situations
where
item
relevance
is
neutral
contrastingly
mistral-7b
shows
distinct
bias
towards
male
items
in
relevant
pairs
notably
in
the
trec
2022
gender
dataset
raising
questions
about
the
model
decisionmaking
process
and
suggesting
that
its
algorithm
may
weigh
male
items
more
heavily
when
they
are
relevant
however
this
bias
diminishes
with
irrelevant
pairs
indicating
diﬀerent
algorithmic
behavior
in
such
contexts
llama2-13b
on
the
other
hand
presents
signiﬁcant
bias
towards
female
items
across
all
datasets
in
both
relevant
and
irrelevant
pairs
which
is
concerning
for
its
overall
fairness
overall
while
some
llms
show
nuanced
biases
others
like
llama2-13b
require
more
interventions
to
ensure
fair
and
equitable
treatment
across
all
group
attributes
5.3
overall
evaluation
overall
analyzing
both
the
listwise
and
pairwise
evaluation
results
in
the
table
5.2
and
table
5.3
we
observe
complex
picture
of
fairness
while
the
listwise
evaluation
based
on
group
exposure
ratios
suggests
fair
representation
of
diﬀerent
groups
the
pairwise
evaluation
reveals
the
unfairness
in
llms
this
inconsistency
is
particularly
evident
chapter
an
empirical
study
on
the
fairness
of
llms
as
rankers
87
when
llms
rank
pairs
of
relevant
and
irrelevant
items
from
protected
and
unprotected
groups
5.4
enhancing
fairness
with
lora
percentage
of
protected
vs
unprotected
group
items
ranked
ﬁrst
across
diﬀerent
trec
datasets
ratio
of
protected
over
unprotected
group
across
diﬀerent
trec
datasets
figure
5.4
impact
of
lora
fine-tuning
on
mistral-7b
fairness
figure
shows
the
percentage
of
ﬁrst-ranked
items
from
protected
and
unprotected
groups
while
figure
demonstrates
the
resulting
fairness
ratios
the
lora-adjusted
model
yields
ratios
closer
to
the
ideal
fairness
benchmark
of
1.0
across
trec
datasets
we
employed
lora
40
to
ﬁne-tune
the
mistral-7b
model
our
approach
involves
creating
balanced
training
dataset
with
equal
representation
of
responses
from
both
protected
and
unprotected
groups
this
balanced
dataset
aims
to
steer
the
model
towards
fairer
rankings
when
evaluating
pairs
of
relevant
or
irrelevant
items
from
diverse
groups
the
implementation
of
the
lora
module
is
facilitated
using
the
peft
53
package
aligning
with
the
parameter-eﬃcient
methodology
outlined
in
the
original
lora
our
study
speciﬁcally
focuses
on
adapting
attention
weights
to
simplify
and
enhance
parameter-eﬃciency
we
opted
to
freeze
other
parameters
in
our
case
we
set
the
optimal
rank
to
deeming
low-rank
adaptation
matrix
as
adequate
the
chosen
88
learning
rate
is
0.003
and
the
batch
size
is
set
at
these
conﬁgurations
were
selected
based
on
considerations
speciﬁc
to
our
study
the
dataset
comprising
approximately
140
000
item
pairs
randomly
sampled
for
each
trec
dataset
facilitate
comprehensive
training
the
process
conducted
on
an
nvidia
a100
80gb
needs
approximately
30
hours
we
split
the
queries
for
training
and
testing
using
80
for
training
and
the
remaining
20
for
testing
the
results
of
ﬁne-tuning
mistral-7b
with
lora
are
illustrated
in
figure
5.4
posttuning
there
is
noticeable
reduction
in
consistent
responses
from
the
model
when
queried
twice
with
reversed
item
orders
this
indicates
an
increase
in
response
variability
which
is
positive
indicator
of
fairness
as
less
predictability
in
responses
can
mitigate
systematic
bias
the
improvement
in
fairness
is
further
supported
by
figure
4b
where
the
outcomes
post-lora
ﬁne-tuning
show
ratios
approaching
1.0
indicating
more
equitable
treatment
of
protected
and
unprotected
groups
by
the
model
5.5
conclusion
in
conclusion
our
in-depth
analysis
reveals
the
intricate
biases
present
in
large
language
models
when
evaluated
for
fairness
through
listwise
and
pairwise
methods
while
listwise
evaluations
painted
picture
of
relative
fairness
deeper
investigation
via
pairwise
evaluations
uncovered
subtler
more
profound
biases
that
often
favored
certain
protected
groups
the
implementation
of
lora
ﬁne-tuning
on
the
mistral-7b
model
89
yielded
encouraging
strides
towards
rectifying
these
biases
demonstrating
an
enhanced
fairness
in
the
model
output
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
6.1
introduction
pinterest
is
visual
discovery
platform
that
allows
users
to
discover
and
save
ideas
for
various
interests
such
as
fashion
home
decor
and
travel
it
has
become
popular
destination
for
users
to
search
for
and
discover
new
products
ideas
and
inspiration
as
result
it
has
also
become
an
attractive
advertising
platform
for
businesses
looking
to
reach
and
engage
with
their
target
audience
to
support
the
growing
demand
for
online
advertising
pinterest
has
developed
large-scale
advertisement
serving
platform
using
the
multi-cascade
ranking
system
51
to
deliver
the
most
relevant
ads
to
users
90
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
ads
request
91
ads
inventory
ads
targeting
ads
retrieval
ads
ranking
auction
winners
pinterest
app
auction
ads
delivery
stack
figure
6.1
the
life
cycle
of
online
ads
delivery
at
high
level
an
ads
request
is
triggered
when
user
opens
the
pinterest
app
or
starts
new
session
and
the
ads
request
will
be
sent
to
the
ads
delivery
system
to
query
for
dozen
of
ads
in
the
ads
delivery
backend
ad
candidates
in
the
inventory
will
ﬂow
through
various
stages
like
targeting
retrieval
ranking
and
auction
which
sends
the
auction
winners
back
to
the
mobile
app
where
the
selected
ads
will
be
visible
to
the
user
like
many
other
online
advertising
platforms
this
multi-cascade
recommendation
system
contains
several
stages
to
ﬁlter
and
rank
ads
based
on
various
business
logic
and
modeling
signals
as
shown
in
figure
6.1
typical
ads
serving
system
has
four
main
stages
ads
targeting
ads
retrieval
ads
ranking
and
ads
auction
ads
targeting
is
the
very
ﬁrst
stage
at
this
stage
it
only
selects
the
ads
that
meet
the
targeting
criterion
preset
by
advertisers
ads
retrieval
is
the
second
stage
right
after
ads
targeting
in
this
stage
various
mechanisms
including
retrieval
models
the
models
used
in
the
retrieval
stage
are
used
to
select
smaller
subset
of
ad
candidates
out
of
the
millions
of
candidates
received
from
the
targeting
stage
selected
ad
candidates
are
passed
down
to
the
ads
ranking
stage
for
more
comprehensive
scoring
and
ranking
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
92
in
the
ads
ranking
stage
set
of
sophisticated
models
is
developed
to
accurately
score
the
speciﬁc
objectives
ctr
cvr
relevance
etc
of
each
ad
candidate
selected
at
the
retrieval
stage
the
model
prediction
in
this
stage
will
directly
impact
many
key
aspects
such
as
the
quality
of
delivered
ads
as
result
this
stage
is
only
able
to
score
very
limited
number
of
ad
candidates
this
is
because
it
spends
much
more
of
the
allotted
time
budget
to
score
each
ad
candidate
using
very
complex
and
performant
models
to
ensure
the
prediction
accuracy
ads
auction
is
the
last
stage
in
the
serving
stack
the
main
objective
here
is
to
make
the
ﬁnal
decision
of
each
auction
candidate
whether
this
candidate
should
be
delivered
to
the
user
which
position
in
the
targeting
surface
should
this
candidate
be
inserted
into
afterwards
the
winning
candidates
will
be
delivered
to
the
user
device
and
inserted
into
the
corresponding
position
where
the
user
will
see
the
ads
and
respond
to
these
ads
with
various
user
actions
as
discussed
above
the
ads
retrieval
is
the
second
stage
of
the
delivery
system
and
it
is
responsible
for
retrieving
the
most
valuable
ads
from
large
set
of
ad
candidates
for
each
query
the
goal
of
this
stage
is
to
retrieve
all
relevant
ads
while
also
minimizing
the
number
of
irrelevant
or
low-quality
ones
this
requires
the
use
of
machine
learning
models
that
can
eﬃciently
predict
the
relevance
and
quality
of
ads
candidate
based
on
variety
of
features
and
signals
it
has
been
diﬃcult
problem
for
retrieval
stage
to
eﬃciently
fulﬁll
this
mission
due
to
several
key
challenges
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
93
the
selected
subset
of
candidates
have
to
be
of
high
quality
to
avoid
wasting
the
capacity
of
expensive
full
ads
ranking
on
the
low
quality
ads
the
size
of
selected
candidates
has
to
be
small
enough
such
that
subsequent
comprehensive
ranking
at
ads
ranking
stage
can
handle
these
ad
candidates
retrieval
models
are
required
to
score
and
rank
the
post
targeting
ad
candidates
in
the
order
of
millions
retrieval
models
will
not
be
accessible
to
lot
of
ml
signals
especially
the
expensive
real-time
ones
and
will
also
not
be
able
to
leverage
sophisticated
model
architectures
due
to
the
scalability
consideration
discussed
in
the
previous
point
as
result
building
performant
retrieval
models
under
these
constraints
is
challenging
problem
in
the
machine
learning
domain
currently
the
retrieval
models
in
most
ads
platforms
use
the
two-tower
model
architecture
proposed
by
covington
et
al
25
among
all
the
challenges
associated
with
retrieval
model
development
and
optimization
selection
bias
in
the
training
data
has
been
long-lasting
problem
impairing
the
performance
of
these
models
in
this
work
we
focus
on
the
issue
of
data
selection
bias
in
the
ads
retrieval
stage
of
pinterest
multi-cascade
ads
ranking
system
the
training
data
used
to
train
the
model
reﬂects
not
only
real
user
preferences
but
it
also
includes
the
production
model
personalized
recommendations
this
means
that
the
training
data
is
not
representative
of
the
overall
population
of
advertisements
which
can
lead
to
inaccurate
results
in
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
94
addition
the
distribution
discrepancy
between
the
training
data
with
observed
user
actions
as
true
labels
and
the
inference
data
composed
by
the
ad
candidates
after
the
targeting
stage
can
further
impact
the
model
performance
to
address
data
selection
bias
in
the
ads
retrieval
funnel
we
ﬁrst
investigated
the
data
distribution
across
various
types
of
ad
candidates
datasets
and
we
further
assessed
various
ml
techniques
including
unsupervised
domain
adaptation
uda
83
to
improve
the
performance
of
retrieval
models
as
the
number
of
ad
candidates
with
real
user
action
is
small
it
will
be
beneﬁcial
for
the
model
training
to
leverage
the
unlabeled
ad
candidates
data
particularly
the
ones
with
similar
distribution
as
the
inference
data
one
diﬃculty
with
this
model
training
strategy
is
determining
how
to
eﬀectively
use
these
unlabeled
data
points
which
have
more
consistent
distribution
as
compared
to
the
model
inference
data
in
this
work
we
have
leveraged
various
state-of-the-art
sota
methods
to
incorporate
unlabeled
data
in
training
retrieval
models
additionally
we
developed
modiﬁed
version
of
uda
muda
to
improve
the
performance
of
naive
implementation
of
uda
in
the
retrieval
model
training
our
online
experimental
results
show
that
couple
of
methods
could
potentially
improve
the
performance
of
the
ads
ranking
system
as
compared
to
the
knowledge
distilled
model
in
the
current
production
environment
and
few
other
methods
thus
our
contribution
could
be
summarized
as
the
following
we
identiﬁed
and
characterized
the
selection
bias
issue
in
the
upper
funnel
of
the
multi-cascade
advertisement
recommendation
system
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
95
we
surveyed
series
of
sota
modeling
strategies
and
evaluated
their
performance
in
both
oﬄine
and
online
settings
we
further
proposed
modiﬁed
version
of
unsupervised
domain
adaptation
muda
that
provides
the
best
online
performance
among
all
the
modeling
strategies
we
have
examined
and
the
online
experiments
show
that
muda
also
outperforms
the
current
production
model
post-targeting
candidates
ads
inventory
retrieval
model
post-retrieval
candidates
targeting
auction
candidates
ranking
filtering
training
pipeline
auction
winners
auction
training
data
figure
6.2
distribution
of
features
and
labels
across
three
ads
datasets
related
to
retrieval
modeling
shows
the
ﬂow
of
major
ad
candidates
along
the
ads
delivery
funnel
shows
the
distribution
of
empirical
vtcvr
one
of
key
retrieval
model
features
across
three
datasets
for
retrieval
training
serving
shows
the
distribution
of
empirical
good
click
rate
one
of
key
retrieval
model
features
across
three
datasets
for
retrieval
training
serving
shows
the
distribution
of
the
ranking
model
predictions
used
as
the
pseudo
label
in
retrieval
model
training
across
three
datasets
note
that
the
exact
values
on
x-axes
are
hidden
for
conﬁdentiality
reasons
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
6.2
96
bias
in
pinterest
ads
as
illustrated
in
figure
6.1
pinterest
ads
serving
system
consists
of
four
stages
ads
targeting
ads
retrieval
ads
ranking
and
ads
auction
each
stage
scores
and
or
ﬁlters
ad
candidates
based
on
the
request
and
ads
content
features
given
an
ad
request
ads
retrieval
narrows
down
millions
of
ad
candidates
to
couple
of
thousands
these
candidates
are
then
sent
to
ads
ranking
for
further
accurate
prediction
of
user
action
as
well
as
ﬁltering
finally
we
run
ads
auctions
on
survivors
and
determine
auction
winners
based
on
predeﬁned
utility
function
and
advertiser
bid
in
the
retrieval
stage
the
latency
limit
is
crucial
because
of
the
large
number
of
ad
candidates
in
the
database
we
adopt
two-tower
dnn
structure
25
where
candidate
embedding
could
be
computed
oﬄine
during
serving
the
model
will
produce
the
score
of
each
ad
candidate
by
calculating
the
dot-product
between
the
precomputed
candidate
embedding
and
the
query
embedding
computed
on-the-ﬂy
for
each
request
6.2
datasets
and
training
pipeline
as
mentioned
earlier
the
ads
serving
system
consists
of
targeting
retrieval
ranking
and
auction
as
shown
in
ﬁgure
2a
millions
of
candidates
in
the
ads
inventory
will
ﬂow
through
various
stages
across
the
ads
delivery
funnel
and
only
small
set
of
valuable
ads
will
survive
and
be
delivered
to
users
speciﬁcally
the
initial
ads
inventory
candidates
will
be
selected
through
ads
targeting
to
reﬁne
the
set
of
ad
candidates
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
97
post-targeting
candidates
which
will
then
be
scored
and
ranked
by
retrieval
models
after
being
selected
by
retrieval
models
the
survivors
post-retrieval
candidates
will
be
further
ﬁltered
or
selected
by
various
business
logics
and
models
in
the
ranking
stage
this
leads
to
new
set
of
ad
candidates
auction
candidates
which
will
be
evaluated
in
the
auction
stage
the
auction
stage
will
pick
dozen
of
winners
out
of
the
auction
candidates
and
deliver
these
ﬁnal
survivors
auction
winners
to
pinterest
users
for
existing
retrieval
models
two
types
of
training
data
are
collected
auction
candidates
and
auction
winners
the
latter
dataset
includes
observed
user
actions
as
true
labels
and
the
former
one
includes
ranking
model
predictions
as
pseudo
labels
the
ranking
model
predictions
are
used
in
the
auction
stage
to
determine
the
winning
ads
from
the
auction
candidates
pool
currently
we
use
these
ranking
model
predictions
as
pseudo
labels
to
train
retrieval
models
with
the
aim
to
maximize
the
funnel
eﬃciency
to
deliver
the
most
valuable
ad
candidates
to
pinterest
users
to
ensure
the
model
freshness
retrieval
models
are
continuously
trained
and
evaluated
on
daily
basis
speciﬁcally
the
model
snapshot
trained
on
day
data
is
loaded
to
train
on
day
data
and
the
newly
trained
model
is
evaluated
on
day
data
this
daily
training
setup
enables
the
model
to
capture
the
most
recent
patterns
keeping
it
responsive
to
new
trends
the
second-day
evaluation
allows
for
detection
of
possible
overﬁt
and
abnormal
behavior
before
serving
production
traﬃc
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
6.2
98
selection
bias
as
mentioned
above
retrieval
models
are
currently
trained
on
both
auction
candidates
and
auction
winners
where
the
ranking
model
predictions
are
used
as
pseudo
labels
such
setup
inevitably
introduces
the
data
with
selection
bias
particularly
the
inconsistency
in
the
dataset
between
training
and
serving
79
in
serving
time
however
the
model
needs
to
make
predictions
on
the
post-targeting
ad
candidates
as
auction
candidates
and
winners
are
small
subset
of
post-targeting
candidates
generated
through
various
business
logics
and
ranking
models
the
distribution
of
these
datasets
will
be
inconsistent
between
model
training
and
serving
figure
2a
illustrates
the
concept
of
inconsistency
on
the
ads
datasets
used
in
training
and
inferencing
in
the
cycle
of
the
retrieval
models
to
further
demonstrate
the
bias
we
analyzed
the
distributions
of
pseudo
labels
and
two
important
retrieval
model
features
across
three
diﬀerent
datasets
post-targeting
candidates
auction
candidates
auction
winners
figure
2b
2c
and
2d
demonstrates
that
the
distributions
are
diﬀerent
across
all
three
datasets
and
this
distribution
diﬀerence
is
much
more
signiﬁcant
between
the
two
datasets
used
in
current
retrieval
model
training
and
the
one
used
in
retrieval
model
serving
for
simplicity
in
the
rest
of
this
work
we
will
interchangeably
use
the
following
terms
post-targeting
candidates
serving
datasets
for
retrieval
models
and
unbiased
dataset
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
6.2
99
problem
formulation
for
simplicity
we
represent
each
data
record
as
tuple
of
three
elements
the
feature
of
request
containing
user
proﬁle
features
and
context
features
search
term
if
from
search
surface
the
advertisement
candidate
features
the
groundtruth
label
observed
user
actions
additionally
let
represent
distribution
of
request
features
advertisement
features
in
inventory
and
represent
the
full
distribution
of
all
request
and
ad
candidates
pairs
finally
let
fθ
and
represent
the
model
with
trainable
parameter
and
the
loss
function
we
want
to
minimize
model
maps
the
request
and
candidate
features
to
numeric
value
function
maps
two
numeric
values
to
scalar
the
loss
value
ideally
we
want
to
minimize
the
training
loss
on
unbiased
data
min
lideal
fθ
fθ
6.1
in
reality
it
is
impossible
to
calculate
the
above
loss
function
on
the
unbiased
dataset
as
the
true
labels
are
not
available
as
result
we
have
to
leverage
the
biased
dataset
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
100
whose
true
labels
are
available
to
us
in
the
next
section
we
will
describe
series
of
methods
to
use
both
biased
and
unbiased
datasets
to
build
model
to
score
the
post-targeting
ad
candidates
in
our
system
6.3
solution
6.3
naive
method
binary
classiﬁcation
the
naive
method
is
to
train
simple
classiﬁcation
model
in
the
common
way
training
click
classiﬁcation
model
based
on
the
dataset
with
observed
user
actions
where
the
ones
with
user
clicks
are
treated
as
positive
examples
and
the
ones
with
no
clicks
are
treated
as
negatives
in
this
naive
method
we
will
optimize
the
following
loss
function
min
lnaive
fθ
fθ
6.2
the
dataset
denotes
the
set
of
request
and
auction
winners
pairs
where
there
are
observed
user
actions
6.3
in-batch
negative
classiﬁcation
similar
to
the
naive
classiﬁcation
method
we
will
build
classiﬁcation
model
based
on
the
biased
dataset
with
observed
user
actions
as
the
true
labels
in
the
real-world
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
101
advertising
system
the
viewed
ads
without
user
clicks
are
not
necessarily
reliable
negative
examples
users
could
still
ﬁnd
these
ads
to
be
valuable
even
if
they
did
not
take
actions
on
them
at
that
moment
diﬀerent
from
the
naive
classiﬁcation
method
we
generate
negative
examples
by
introducing
ad
candidates
from
the
other
requests
in
the
same
training
batch
as
the
current
request
following
the
common
setup
35
45
84
85
speciﬁcally
only
the
delivered
ads
with
user
clicks
are
included
in
training
data
and
clicked
ads
in
diﬀerent
requests
in
the
same
batch
are
treated
as
negative
examples
6.3
knowledge
distillation
ranking
models
are
trained
with
complex
architectures
and
numerous
input
features
in
contrast
retrieval
models
have
to
limit
the
architecture
to
two-tower
dnn
as
well
as
available
features
due
to
the
demanding
requirement
of
scalability
and
low
serving
latency
to
minimize
the
performance
loss
knowledge
distillation
kd
37
is
adopted
which
means
retrieval
models
are
trained
with
ranking
model
predictions
as
pseudo
labels
formally
with
denoting
the
ranking
model
we
optimize
the
following
loss
function
min
lkd
fθ
6.3
fθ
6.3
transfer
learning
the
core
idea
of
transfer
learning
is
to
train
model
on
source
domain
data
and
then
ﬁne
tune
part
of
its
parameters
on
the
target
domain
particularly
for
dnn
model
the
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
102
early
layers
are
usually
ﬁxed
during
ﬁne
tuning
as
they
are
shown
to
represent
primitive
and
general
features
59
in
our
case
the
retrieval
model
is
two-tower
dnn
and
the
data
distribution
discrepancy
across
diﬀerent
datasets
is
only
from
the
ad
candidates
as
result
we
use
the
unbiased
data
to
ﬁne
tune
the
ad
embedding
tower
and
keep
the
query
tower
unchanged
6.3
adversarial
regularization
another
view
of
bias
issue
is
that
the
representation
learned
from
biased
data
is
not
general
enough
to
be
applied
to
the
unbiased
dataset
leading
to
performance
degradation
we
can
therefore
add
regularization
on
the
learning
so
that
the
intermediate
output
of
the
model
has
no
information
indicating
its
data
source
technique
known
as
adversarial
adv
learning
36
for
dnn
model
we
can
split
it
into
two
parts
the
former
one
takes
the
raw
input
and
gives
the
intermediate
output
the
latter
one
takes
the
intermediate
output
and
gives
the
ﬁnal
prediction
the
adversarial
regularization
trains
data
source
classiﬁer
from
the
intermediate
output
the
negative
of
whose
loss
function
is
then
added
to
the
original
one
as
regularization
formally
let
f1
and
f2
denote
such
two
parts
of
dnn
while
the
denotes
the
classiﬁer
the
loss
function
of
data
source
classiﬁer
is
deﬁned
as
equation
6.4
lcls
log
f1
log
f1
6.4
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
103
the
ﬁnal
loss
function
for
adversarial
regularization
is
shown
in
equation
6.5
ladv
ltarget
f2
f1
λlcls
6.5
where
the
ltarget
is
the
original
loss
function
that
trains
the
target
model
and
the
is
hyper
parameter
weighting
the
regularization
the
goal
is
to
minimize
the
ladv
with
regarding
to
f1
f2
and
the
lcls
with
regards
to
6.3
unsupervised
domain
adaptation
uda
unsupervised
domain
adaptation
is
technique
to
train
model
that
works
well
on
the
target
domain
with
unlabeled
data
by
only
using
labeled
samples
on
the
source
domain
uda
method
has
been
applied
to
the
situation
where
the
feature
distribution
and
the
data
labeling
are
diﬀerent
between
the
source
and
target
domains
in
pinterest
ads
system
the
source
domain
is
the
biased
dataset
with
labels
and
the
target
domain
is
the
unbiased
dataset
without
labels
as
result
this
data
selection
bias
could
be
formulated
as
uda
problem
83
6.3
6.1
naive
uda
the
naive
method
is
to
directly
train
the
model
on
the
unbiased
dataset
so
that
there
will
be
no
inconsistency
between
training
and
serving
as
the
ground
truth
labels
of
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
104
the
unbiased
dataset
are
missing
the
pseudo
labels
will
be
generated
from
separate
model
that
is
trained
on
the
biased
dataset
from
the
source
domain
where
the
ground
truth
label
is
available
following
the
same
annotation
scheme
as
above
let
denotes
the
ranking
model
that
is
used
to
generate
the
pseudo
labels
for
the
unbiased
dataset
from
source
domain
the
optimization
goal
becomes
the
following
min
lnaiveu
da
fθ
fθ
6.6
where
is
the
data
in
the
source
domain
however
this
method
has
many
drawbacks
in
reality
the
unbiased
data
is
only
sampling
and
the
volume
is
small
due
to
infra
cost
this
might
lead
to
performance
degradation
additionally
the
high-quality
candidates
might
not
be
suﬃciently
representative
in
this
training
data
from
the
source
domain
we
will
discuss
the
performance
in
the
experiment
section
6.3
6.2
modiﬁed
uda
in
uda
the
quality
of
pseudo
labels
is
critical
to
the
performance
of
trained
models
in
the
above
naive
uda
there
is
no
mechanism
to
guarantee
the
quality
of
the
pseudo
labels
especially
when
the
pseudo
label
generating
model
remains
not
suﬃciently
accurate
previously
saito
et
al
67
proposed
to
use
an
asymmetric
tri-training
method
where
two
separate
pseudo
label
generating
models
are
used
as
the
mechanism
to
ensure
the
pseudo
label
quality
however
the
requirement
to
maintain
second
pseudo
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
105
label
generating
model
with
reasonable
performance
will
be
too
costly
for
the
real-world
advertising
system
where
tens
or
even
hundreds
of
retrieval
models
are
needed
and
retrained
on
daily
basis
additionally
it
will
be
inhibitively
costly
when
pseudo
label
has
to
be
derived
from
set
of
models
and
then
second
set
of
several
models
will
be
required
to
be
developed
and
maintained
to
leverage
the
tri-training
method
to
address
the
pseudo
label
quality
issue
for
real-world
ads
retrieval
we
transform
the
original
numeric
pseudo
label
prediction
of
ranking
model
to
binary
classiﬁcation
label
based
on
carefully
chosen
thresholds
formally
let
δl
and
δh
denote
the
two
thresholds
with
δl
δh
as
shown
in
equation
6.8
numeric
pseudo
labels
lower
than
the
ﬁrst
threshold
are
treated
as
negative
those
higher
than
the
second
threshold
are
treated
as
positive
data
records
with
numeric
pseudo
labels
falling
between
these
two
thresholds
are
removed
from
the
training
dataset
the
rationale
behind
this
is
to
only
keep
the
records
that
the
ranking
model
is
conﬁdent
about
and
discard
the
ones
that
are
close
to
the
hyperplane
of
the
ranking
classiﬁer
now
the
optimization
goal
of
training
the
retrieval
model
becomes
the
following
min
lm
da
fθ
δl
δh
φδδhl
fθ
6.7
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
106
where
φδδhl
is
pseudo
classiﬁcation
label
indicator
converting
ranking
model
predictions
to
binary
label
according
to
given
thresholds
as
shown
in
the
equation
below
φδδhl
if
δh
if
δl
6.8
to
select
the
thresholds
we
adopt
data
driven
method
particularly
we
bucketize
the
ranking
model
prediction
and
check
the
corresponding
empirical
click
rate
for
each
bucket
thresholds
are
chosen
when
there
is
sudden
change
of
empirical
click
rates
6.4
experiments
and
results
in
this
section
we
will
ﬁrst
describe
the
model
training
details
and
introduce
the
evaluation
settings
and
metrics
we
will
then
present
and
discuss
the
results
from
oﬄine
and
online
experiments
to
compare
the
performance
of
the
proposed
solutions
6.4
datasets
as
described
in
section
6.2
the
two
existing
training
data
sources
are
the
auction
candidates
and
auction
winners
both
are
biased
datasets
in
section
6.2
we
introduced
an
unbiased
dataset
randomly
sampled
from
the
post-targeting
dataset
this
unbiased
dataset
is
required
to
be
scored
and
ranked
by
retrieval
models
in
the
production
system
taking
into
consideration
the
infrastructure
cost
and
the
volume
of
the
resulting
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
107
dataset
we
sample
100
000
queries
and
000
advertisement
candidates
for
each
query
to
create
the
unbiased
dataset
every
day
6.4
experimental
setting
to
examine
the
performance
of
de-biasing
methods
on
the
pinterest
ads
dataset
we
implement
the
models
and
conduct
systematic
experiments
to
collect
evaluation
results
on
the
real-world
production
system
the
binary
classiﬁcation
models
are
trained
on
auction
winners
with
real
user
actions
as
the
true
labels
which
aims
to
provide
supplemental
evidence
to
indicate
the
reason
why
the
current
production
model
is
not
directly
trained
with
real
user
actions
the
following
describes
the
details
of
the
baseline
models
binary
classiﬁcation
since
the
regression
model
is
trained
on
the
pseudo
labels
generated
by
the
ads
ranking
models
the
performance
of
the
classiﬁcation
model
directly
trained
on
the
user
actions
is
worth
examining
to
train
this
model
we
use
the
auction
winner
as
the
training
dataset
with
labels
deﬁned
in
section
6.3
and
binary
cross
entropy
bce
as
the
loss
function
in-batch
negative
classiﬁcation
we
also
train
classiﬁcation
model
with
inbatch
negative
sampling
which
uses
other
candidates
from
the
same
batch
of
data
as
negative
samples
for
given
query
we
use
1000
as
the
batch
size
and
use
the
batch
size
as
the
number
of
hard
negatives
in
the
loss
function
the
model
is
trained
with
only
the
auction
winner
dataset
and
we
only
use
the
candidates
with
user
clicks
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
108
knowledge
distillation
in
the
current
production
model
training
we
use
the
ads
ranking
model
output
as
the
pseudo
label
and
mean
absolute
logarithmic
error
logmae
as
the
loss
function
for
the
production
model
the
training
dataset
includes
the
auction
candidates
and
auction
winners
besides
this
production
model
we
also
train
another
one
with
only
auction
winners
in
evaluation
we
refer
to
the
ﬁrst
one
as
the
production
model
and
the
second
one
as
the
knowledge
distillation
model
we
summarize
the
implementation
details
of
the
debiasing
model
as
the
following
transfer
learning
for
the
transfer
learning
model
we
use
both
the
biased
and
unbiased
dataset
we
also
use
ranking
model
predictions
as
the
pseudo
labels
and
logmae
as
the
loss
function
to
train
the
retrieval
model
adversarial
learning
for
the
adversarial
learning
model
we
implement
the
data
source
discriminator
as
one-layer
mlp
with
sigmoid
as
the
activation
function
both
the
biased
and
unbiased
datasets
are
used
to
train
the
retrieval
model
and
the
ads
ranking
model
is
used
to
generate
pseudo
labels
for
the
training
datasets
naive
unsupervised
domain
adaptation
uda
to
train
the
naive
uda
model
we
only
use
the
unbiased
dataset
with
pseudo
labels
generated
from
the
ads
ranking
model
predictions
and
logmae
as
the
loss
function
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
109
modiﬁed
unsupervised
domain
adaptation
muda
here
we
use
the
unbiased
dataset
with
pseudo
labels
derived
as
discussed
in
section
6.3
6.2
by
transforming
the
ranking
model
predictions
into
binary
classes
and
bce
for
the
loss
function
for
model
training
hyper-parameters
we
use
6144
as
the
batch
size
and
0.0001
as
the
learning
rate
unless
deﬁned
speciﬁcally
in
the
two-tower
model
we
use
four
fully
connected
layers
and
the
ﬁnal
layer
output
dimension
is
32
we
use
sigmoid
as
the
activation
function
for
the
output
layer
and
use
selu
46
for
the
other
layers
6.4
evaluation
metrics
for
oﬄine
evaluation
metrics
we
use
auc-roc
score
for
both
the
classiﬁcation
and
regression
models
we
evaluate
the
models
on
one
day
of
the
auction
winners
dataset
for
online
experiments
we
compare
these
models
to
the
production
model
and
report
the
change
of
total
impressions
numbers
δimp
click
through
rate
δctr
and
30
seconds
click
through
rate
δgctr30
for
ads
evaluation
besides
the
user-side
metrics
mentioned
above
we
also
report
metrics
that
relate
to
advertiser
experience
these
metrics
are
impression
to
conversion
rate
ratio
icvr
measures
the
eﬀectiveness
of
an
ad
campaign
in
converting
impressions
into
conversions
cost
per
action
cpa
measures
the
cost
to
the
advertiser
for
each
positive
user
action
and
is
currently
exclusively
applied
to
the
conversion
ads
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
110
due
to
information
conﬁdentiality
we
only
report
the
lift
of
these
metrics
compared
to
the
current
production
model
6.4
oﬄine
evaluation
models
production
model
binary
classiﬁcation
in-batch
negative
knowledge
distillation
transfer
learning
adversarial
learning
naive
uda
muda
auc-roc
0.895
0.895
0.701
0.896
0.890
0.896
0.841
0.844
table
6.1
auc-roc
on
evaluation
dataset
the
models
such
as
knowledge
distillation
adversarial
learning
binary
classiﬁcation
trained
with
auction
winners
dataset
usually
have
better
oﬄine
evaluation
results
for
oﬄine
evaluation
we
evaluate
both
the
regression
and
classiﬁcation
models
using
auc-roc
for
the
evaluation
dataset
we
use
the
auction
winners
which
contain
real
user
clicks
as
shown
in
table
6.1
compared
to
the
production
model
the
models
such
as
knowledge
distillation
transfer
learning
binary
classiﬁcation
and
adversarial
models
have
similar
performance
in
terms
of
auc-roc
score
the
results
are
expected
because
the
training
datasets
include
the
auction
winners
for
these
models
for
the
in-batch
negative
model
it
is
trained
with
only
the
positive
candidates
in
the
auction
winners
dataset
so
it
does
not
perform
well
in
the
oﬄine
evaluation
because
the
negative
candidates
were
not
included
in
the
training
dataset
for
native
uda
and
muda
both
models
are
trained
with
only
the
post-targeting
datasets
and
the
feature
distribution
discrepancy
figure
2a
leads
to
the
lower
performance
than
other
models
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
111
to
summarize
the
oﬄine
evaluation
is
as
expected
because
we
see
models
trained
and
evaluated
on
the
same
source
of
data
have
better
performance
than
those
trained
with
diﬀerent
sources
of
data
however
the
oﬄine
evaluation
could
not
necessarily
reﬂect
the
true
model
performance
in
the
production
system
especially
when
the
serving
data
used
in
the
online
experiments
are
from
completely
diﬀerent
distribution
thus
in
the
following
sections
we
conduct
systematic
online
experiments
to
compare
the
performance
of
aforementioned
models
6.4
online
experiments
6.4
5.1
overall
evaluation
table
6.2
shows
the
overall
online
evaluation
results
of
all
models
among
the
metrics
we
will
focus
on
the
change
of
gctr30
as
our
models
are
optimized
towards
this
objective
the
binary
classiﬁcation
model
has
decreased
gctr30
indicating
signiﬁcant
drop
in
the
quality
of
user
engagement
with
the
recommended
ads
it
also
shows
the
largest
decrease
in
ctr
and
highest
increase
in
impressions
which
means
that
while
more
ads
were
delivered
to
users
fewer
of
them
got
clicked
in
contrast
the
in-batch
negative
and
knowledge
distillation
models
have
positive
changes
in
gctr30
however
the
decrease
in
impression
could
be
the
main
reason
for
the
gctr30
increase
because
less
ads
were
shown
to
the
users
although
all
three
models
binary
classiﬁcation
in-batch
negative
and
knowledge
distillation
suﬀer
from
selection
bias
in
the
training
dataset
the
latter
two
perform
better
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
112
in
the
binary
classiﬁcation
model
training
the
negative
candidates
are
always
from
the
same
query
whereas
the
in-batch
negative
classiﬁcation
model
is
trained
with
random
sampled
candidates
of
diﬀerent
queries
within
the
batch
the
diﬀerence
between
the
source
of
negative
candidates
provides
the
model
with
more
diverse
and
informative
training
data
which
results
in
not
overﬁtting
to
the
speciﬁc
query
for
knowledge
distillation
the
training
data
labels
are
the
ranking
model
predictions
whose
values
contain
richer
information
than
raw
binary
click-or-not
labels
models
binary
classiﬁcation
in-batch
negative
knowledge
distillation
transfer
learning
adversarial
learning
naive
uda
muda
δimp
0.95
2.25
3.26
0.43
0.28
0.45
0.92
δctr
5.51
4.45
0.25
1.88
0.45
3.05
0.47
δgctr30
12.66
4.68
5.97
4.35
0.66
4.80
5.07
table
6.2
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
various
models
on
all
types
of
ads
both
in-batch
negative
and
knowledge
distillation
methods
improve
gctr30
at
the
cost
of
impression
drop
and
muda
is
the
only
method
to
recommend
more
ads
with
higher
quality
as
observed
by
the
increased
gctr30
without
impression
drop
the
transfer
learning
model
had
small
increase
in
impressions
but
also
had
negative
change
in
gctr30
with
the
warm
start
weights
the
transfer
learning
model
has
similar
results
with
the
decrease
in
user
engagements
in
our
case
the
problem
of
the
transfer
learning
model
is
the
ﬁne
tuning
on
the
unbiased
dataset
candidates
in
unbiased
dataset
are
randomly
sampled
for
each
query
where
high
quality
ones
might
be
underrepresented
the
adversarial
model
has
similar
results
with
decrease
in
gctr30
and
slight
increase
in
impressions
compared
to
the
transfer
learning
model
the
adversarial
model
has
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
113
better
performance
in
user
engagements
in
adversarial
model
the
classiﬁer
serves
as
regularizer
to
prevent
the
embedding
tower
from
learning
domain
speciﬁc
embedding
for
certain
log
source
unlike
the
transfer
learning
model
the
debiasing
technique
in
the
adversarial
model
does
not
rely
on
the
quality
of
the
unbiased
training
data
the
training
of
the
classiﬁer
is
unsupervised
because
we
use
the
log
source
auction
winner
auction
candidates
as
the
ground
truth
label
when
we
are
able
to
successfully
train
classiﬁer
to
classify
the
log
source
the
classiﬁer
could
be
used
as
an
adversarial
regularizer
to
help
train
an
unbiased
embedding
model
however
compared
to
the
production
model
the
decrease
in
gctr30
may
indicate
that
the
restriction
on
the
embedding
learning
makes
the
model
drop
the
information
that
are
critical
in
online
evaluations
the
naive
uda
model
has
an
average
performance
compared
to
the
other
baseline
models
the
naive
uda
model
is
trained
on
the
unbiased
dataset
which
contains
the
pseudo
label
generated
from
the
ranking
model
the
reason
why
the
naive
uda
model
performs
badly
is
similar
to
the
reason
why
the
transfer
learning
model
performed
poorly
since
the
unbiased
dataset
is
collected
by
random
sampling
of
post-targeting
ad
candidates
in
addition
to
the
existing
queries
these
sampled
candidates
are
mostly
negative
samples
which
does
not
help
to
train
good
retrieval
model
in
contrast
the
modiﬁed
uda
muda
model
has
much
higher
gctr30
than
the
production
model
when
the
number
of
impressions
increases
the
higher
user
engagement
suggests
that
the
muda
model
delivers
more
ads
with
higher
quality
to
users
compared
to
the
naive
uda
model
the
muda
model
transforms
numerical
pseudo
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
114
labels
generated
by
the
ranking
model
into
binary
classes
determined
by
certain
thresholds
the
model
also
uses
bce
loss
the
lift
in
user
engagement
metrics
suggests
that
such
label
transformation
improves
the
quality
of
pseudo
labels
used
in
muda
by
transforming
the
numerical
pseudo
labels
to
binary
ones
we
prevented
the
model
from
overly
ﬁtting
into
the
ranking
model
prediction
of
every
single
candidate
but
to
rank
those
on
which
the
ranking
model
has
high
conﬁdence
6.4
5.2
evaluation
by
ads
objective
type
models
in-batch
negative
muda
δimp
8.70
0.32
awareness
δctr
δgctr30
2.41
13.74
2.71
1.97
δimp
1.03
0.43
traﬃc
δctr
δgctr30
1.16
2.56
4.28
3.07
web
conversion
δimp
δctr
δgctr30
1.31
1.69
0.39
3.15
5.19
8.88
table
6.3
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
two
promising
models
on
each
type
awareness
traﬃc
web-conversion
of
ads
in-batch
negative
classiﬁcation
model
works
better
on
the
traﬃc
ads
and
muda
model
helps
web-conversion
ads
the
most
in
overall
evaluation
the
in-batch
negative
and
muda
are
two
methods
that
demonstrate
promising
metrics
in
table
6.3
we
show
the
evaluation
results
of
the
two
methods
broken
down
by
diﬀerent
ads
objective
types
awareness
traﬃc
and
web
conversion
ads
awareness
ads
aim
to
increase
the
visibility
of
brand
or
product
analyzing
the
performance
of
awareness
ads
helps
understand
the
eﬀectiveness
of
brand
marketing
strategy
as
shown
in
table
6.3
the
in-batch
negative
model
has
signiﬁcant
increase
on
gctr30
compared
to
other
models
for
awareness
ads
however
such
boost
might
be
due
to
the
huge
decrease
in
impressions
in-batch
negative
model
is
trained
only
on
candidates
with
user
long
clicks
the
awareness
ads
essentially
have
lower
chance
of
being
clicked
than
other
types
since
its
main
goal
is
to
increase
the
visibility
of
brand
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
115
as
result
the
in-batch
negative
model
could
bias
towards
other
ads
types
leading
to
the
huge
impression
drop
of
awareness
ads
traﬃc
ads
are
designed
to
drive
traﬃc
to
speciﬁc
website
or
landing
page
they
are
typically
used
to
increase
brand
awareness
generate
leads
or
drive
sales
by
analyzing
metrics
such
as
ctr
and
gctr30
businesses
can
determine
whether
their
ads
are
resonating
with
their
target
audience
and
whether
they
are
successfully
achieving
their
advertising
goals
as
can
be
seen
the
in-batch
negative
model
is
the
only
ones
that
yield
an
increase
in
both
ads
impression
and
gctr30
traﬃc
ads
aim
to
attract
users
to
click
and
could
occupy
big
portion
of
records
with
positive
user
actions
therefore
the
in-batch
negative
model
training
dataset
which
only
includes
candidates
with
positive
user
actions
may
have
higher
proportion
of
candidates
that
are
well-suited
for
driving
traﬃc
as
result
the
model
could
better
identify
candidates
that
are
likely
to
drive
traﬃc
resulting
in
an
improvement
in
the
gctr30
metric
for
traﬃc
ads
web-conversion
ads
aim
to
drive
users
to
take
speciﬁc
action
on
website
such
as
making
purchase
these
ads
can
provide
insight
for
measuring
the
success
of
an
online
advertising
campaign
as
shown
in
the
table
6.3
the
muda
model
favors
the
webconversion
ads
objective
type
as
it
has
the
highest
improvement
in
ctr
and
gctr30
among
all
models
for
this
objective
type
the
in-batch
negative
model
also
performs
well
for
web-conversion
ads
with
improvements
in
both
ctr
and
gctr30
the
muda
may
favor
web-conversion
ads
because
pseudo
labels
generated
by
the
ads
ranking
model
may
favor
web-conversion
ads
as
they
are
designed
to
attract
users
to
stay
on
target
websites
longer
for
potential
conversion
behaviors
additionally
the
threshold
selection
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
116
strategy
used
in
the
muda
model
may
be
more
eﬀective
at
identifying
high-quality
candidates
for
web-conversion
ads
which
could
also
contribute
to
its
better
performance
for
this
type
of
ad
6.4
5.3
conversion
ads
models
in-batch
negative
muda
δicvr
2.55
1.89
δcpa
1.11
4.40
table
6.4
online
metrics
performance
of
in-batch
negative
classiﬁcation
and
muda
models
on
web-conversion
ads
in-batch
negative
classiﬁcation
model
leads
to
lower
conversion
probability
on
each
ads
impression
icvr
and
thus
has
higher
cpa
cost
to
advertisers
in
contrast
muda
model
recommended
ad
candidates
with
higher
conversion
rate
and
therefore
lower
cpa
cost
in
table
6.4
we
show
the
performance
of
in-batch
negative
and
muda
models
with
regard
to
conversion
related
metrics
as
these
two
show
good
performance
on
webconversion
ads
in
general
the
in-batch
negative
model
has
decreased
icvr
and
increased
cpa
this
is
not
favorable
for
the
advertiser
as
it
increases
their
costs
as
measured
by
cpa
on
the
other
hand
the
muda
model
shows
an
opposite
result
with
increased
icvr
and
decreased
cpa
reducing
the
ads
campaign
cost
to
advertisers
these
metrics
indicate
that
while
both
increase
the
long
clicks
muda
model
performs
much
better
by
generating
more
conversions
out
of
these
increased
long
clicks
one
reason
why
the
muda
model
performs
better
is
that
the
model
could
improve
the
performance
of
identifying
the
high-quality
candidates
that
are
more
likely
to
lead
to
conversions
and
thus
decrease
the
cost
per
action
for
advertisers
additionally
the
fact
that
the
muda
model
is
trained
on
only
unbiased
data
with
pseudo
labels
generated
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
117
from
the
ads
ranking
model
could
have
an
impact
the
pseudo
labels
may
capture
more
relevant
information
about
the
users
behaviors
and
preferences
leading
to
better
performance
in
terms
of
cpa
6.4
variants
of
muda
in
the
muda
method
we
believe
diﬀerent
threshold
selection
mechanisms
could
impact
the
quality
of
binary
pseudo
labels
as
result
we
further
investigate
the
impact
of
different
thresholding
mechanisms
on
the
performance
of
trained
retrieval
models
in
the
unbiased
dataset
we
ﬁrst
bucketize
the
candidates
according
to
their
numerical
pseudo
labels
gctr30
predicted
by
the
ranking
model
where
we
compute
the
percentile
of
the
labels
and
use
the
adjacent
percentile
to
create
buckets
then
for
each
bucket
we
adapt
the
following
two
strategies
to
calculate
empirical
gctr30
compute
the
gctr30
for
candidates
with
the
real
user
actions
divide
the
number
of
true
good
clicks
by
the
number
of
candidates
in
the
bucket
models
muda
v1
muda
v2
muda
v3
δimp
0.07
0.56
0.92
δctr
11.26
3.52
0.47
δgctr30
30.78
13.04
5.07
table
6.5
online
lifts
of
impression
imp
click-through
rate
ctr
good
long
click
gctr30
observed
with
various
muda
variants
on
all
types
of
ads
muda
v1
achieves
the
highest
gain
on
ads
engagement
both
ctr
and
gctr30
and
muda
v3
achieves
the
most
balanced
gain
across
diﬀerent
metrics
with
good
gctr30
and
impression
lift
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
118
we
select
the
threshold
by
determining
the
elbow
point
of
the
graph
for
example
when
there
is
sudden
drop
of
true
good
clicks
or
good
clicks
rate
between
two
adjacent
bins
we
use
one
of
the
bins
as
the
negative
threshold
this
means
in
the
label
transformation
we
treat
candidates
with
pseudo
labels
smaller
than
the
threshold
as
the
negative
samples
for
positive
labels
we
check
if
there
is
sudden
increase
of
true
good
clicks
or
good
clicks
rate
between
the
bins
to
study
diﬀerent
threshold
selection
strategy
we
propose
three
variants
of
the
muda
models
v1
we
train
the
muda
model
on
both
the
biased
and
unbiased
datasets
with
the
ﬁrst
threshold
selection
strategy
v2
we
train
the
muda
model
on
only
the
unbiased
datasets
with
the
ﬁrst
threshold
selection
strategy
v3
we
train
the
muda
model
on
only
unbiased
datasets
with
the
second
threshold
selection
strategy
models
muda
v1
muda
v2
muda
v3
δimp
2.13
0.10
0.32
awareness
δctr
δgctr30
2.77
7.97
1.72
5.97
2.71
1.97
δimp
5.22
1.53
0.43
traﬃc
δctr
δgctr30
0.47
17.34
2.69
1.18
4.28
3.07
web
conversion
δimp
δctr
δgctr30
12.98
21.52
29.63
5.16
11.14
17.83
3.15
5.19
8.88
table
6.6
online
lifts
of
impression
imp
click-through
rate
ctr
and
good
long
click
gctr30
observed
with
muda
variants
on
each
type
awareness
traﬃc
web-conversion
of
ads
where
muda
v3
shows
best
balanced
impression
gains
among
them
table
6.5
shows
the
overall
performance
of
three
variants
of
the
uda
models
as
measured
by
several
evaluation
metrics
impression
and
click-through
rate
ctr
at
ﬁrst
glance
the
v1
model
may
seem
to
work
best
hugely
increasing
user
engagement
while
chapter
an
empirical
study
of
selection
bias
in
pinterest
ads
retrieval
119
keeping
the
impression
neutral
however
if
broken
down
by
ad
types
this
mode
actually
leads
to
large
impression
shift
from
awareness
2.13
and
traﬃc
ads
5.12
toward
web
conversion
ads
12.98
as
shown
in
table
6.6
this
observation
may
indicate
that
training
uda
models
on
biased
data
would
make
the
model
favor
webconversion
ads
more
than
others
comparing
v2
and
v3
models
the
latter
one
shows
better
balanced
impression
gains
across
all
ad
objective
types
it
could
be
due
to
the
second
strategy
of
calculating
the
approximate
gctr30
for
the
unbiased
dataset
this
strategy
may
better
represent
the
true
performance
of
the
candidates
and
result
in
more
accurate
threshold
selection
leading
to
improved
performance
in
the
muda
model
models
muda
v1
muda
v2
muda
v3
δhdr
4.80
6.35
2.81
δrpr
13.88
4.43
1.43
table
6.7
online
lifts
of
ads
hide
rate
hdr
re-pin
rate
rpr
observed
with
muda
variants
on
all
types
of
ads
muda
v3
achieves
the
most
balanced
performance
with
fewer
ads
being
hidden
and
more
ads
being
repined
by
the
users
to
better
understand
the
performance
of
these
muda
variants
we
also
measure
their
online
performance
on
two
other
useful
engagement
metrics
hide
rate
hdr
and
repin
rate
rpr
note
that
re-pin
is
user
action
indicating
if
the
user
saves
an
ad
to
pinterest
board
in
table
6.7
we
show
the
change
of
the
two
metrics
compared
to
the
production
model
although
the
rpr
is
increased
both
muda
v1
and
v2
models
recommend
more
ads
that
will
be
hidden
by
users
suggesting
some
of
the
recommended
ads
from
these
models
do
not
provide
good
user
experience
in
contrast
muda
v3
model
generally
has
the
most
balanced
improvement
across
all
metrics
which
shows
120
positive
lift
in
the
user
engagement
and
reduction
in
the
unwanted
user
experience
hdr
6.5
conclusion
in
conclusion
this
work
has
analyzed
the
impact
of
selection
bias
in
pinterest
online
advertising
system
we
propose
and
evaluate
several
debiasing
methods
to
mitigate
the
negative
impacts
of
selection
bias
on
recommender
performance
the
results
of
our
experiments
show
that
our
proposed
methods
speciﬁcally
the
muda
model
can
eﬀectively
improve
the
performance
of
advertising
systems
by
handling
the
selection
bias
additionally
our
online
experiment
shows
that
this
model
also
improves
the
cost
eﬃciency
of
the
ad
campaigns
these
ﬁndings
demonstrate
the
importance
of
addressing
selection
bias
in
recommendation
systems
and
provide
valuable
insights
for
practitioners
in
this
ﬁeld
chapter
conclusion
the
conclusions
of
the
presented
works
collectively
highlight
signiﬁcant
strides
in
addressing
fairness
within
ranking
and
search
systems
alongside
mitigating
selection
bias
in
online
advertising
platforms
through
innovative
approaches
like
the
meta-learning
based
fair
ranking
mfr
and
the
meta
curriculum-based
fair
ranking
mcfr
frameworks
we
have
demonstrated
the
potential
to
signiﬁcantly
improve
fairness
metrics
and
minority
group
exposure
by
re-weighting
training
losses
and
employing
metalearning
techniques
with
curriculum
learning
these
methods
have
shown
promising
results
in
real-world
datasets
underscoring
their
eﬀectiveness
over
traditional
fair
ranking
models
furthermore
our
exploration
into
large
language
models
llms
has
uncovered
biases
that
challenge
fairness
prompting
the
development
of
ﬁne-tuning
strategies
such
as
lora
to
foster
more
equitable
outcomes
in
ranking
tasks
our
research
also
121
122
delves
into
the
issue
of
selection
bias
in
pinterest
multi-cascade
advertising
recommendation
system
presenting
debiasing
methodologies
like
the
modiﬁed
unsupervised
domain
adaptation
muda
model
which
not
only
enhances
recommendation
system
performance
but
also
boosts
ad
campaign
cost-eﬃciency
future
directions
for
this
body
of
work
include
reﬁning
meta-dataset
collection
methods
for
meta-learning
expanding
the
applicability
of
fairness
frameworks
to
accommodate
multiple
protected
attributes
and
exploring
diverse
ranking
tasks
and
datasets
moreover
eﬀorts
will
focus
on
balancing
accuracy
and
equity
in
llm
applications
through
improved
ranking
performance
and
fairness
strategies
additionally
the
insights
garnered
from
mitigating
selection
bias
in
online
advertising
systems
pave
the
way
for
further
innovation
in
addressing
biases
across
recommendation
systems
contributing
to
the
broader
discourse
on
fairness
and
transparency
in
machine
learning
and
ai
applications
bibliography
abubakar
abid
maheen
farooqi
and
james
zou
persistent
anti-muslim
bias
in
large
language
models
in
marion
fourcade
benjamin
kuipers
seth
lazar
and
deirdre
mulligan
editors
aies
21
aaai
acm
conference
on
ai
ethics
and
society
virtual
event
usa
may
19
21
2021
pages
298
306
acm
2021
abubakar
abid
maheen
farooqi
and
james
zou
large
language
models
associate
muslims
with
violence
nature
machine
intelligence
461
463
06
2021
doi
10.1038
s42256-021-00359-2
marcin
andrychowicz
misha
denil
sergio
gómez
colmenarejo
matthew
hoﬀman
david
pfau
tom
schaul
brendan
shillingford
and
nando
de
freitas
learning
to
learn
by
gradient
descent
by
gradient
descent
in
proceedings
of
the
30th
international
conference
on
neural
information
processing
systems
page
3988
3996
red
hook
ny
usa
2016
curran
associates
inc
antreas
antoniou
harrison
edwards
and
amos
storkey
how
to
train
your
maml
in
international
conference
on
learning
representations
2019
123
bibliography
124
abolfazl
asudeh
jagadish
julia
stoyanovich
and
gautam
das
designing
fair
ranking
schemes
in
proceedings
of
the
2019
international
conference
on
management
of
data
page
1259
1276
new
york
ny
usa
2019
association
for
computing
machinery
isbn
9781450356435
yuntao
bai
saurav
kadavath
sandipan
kundu
amanda
askell
jackson
kernion
andy
jones
anna
chen
anna
goldie
azalia
mirhoseini
cameron
mckinnon
carol
chen
catherine
olsson
christopher
olah
danny
hernandez
dawn
drain
deep
ganguli
dustin
li
eli
tran-johnson
ethan
perez
jamie
kerr
jared
mueller
jeﬀrey
ladish
joshua
landau
kamal
ndousse
kamile
lukosuite
liane
lovitt
michael
sellitto
nelson
elhage
nicholas
schiefer
noemi
mercado
nova
dassarma
robert
lasenby
robin
larson
sam
ringer
scott
johnston
shauna
kravec
sheer
el
showk
stanislav
fort
tamera
lanham
timothy
telleen-lawton
tom
conerly
tom
henighan
tristan
hume
samuel
bowman
zac
hatﬁelddodds
ben
mann
dario
amodei
nicholas
joseph
sam
mccandlish
tom
brown
and
jared
kaplan
constitutional
ai
harmlessness
from
ai
feedback
2022
matias
barenstein
propublica
compas
data
revisited
arxiv
e-prints
art
arxiv
1906.04711
jun
2019
yoshua
bengio
jérôme
louradour
ronan
collobert
and
jason
weston
curriculum
learning
in
proceedings
of
the
26th
annual
international
conference
on
machine
learning
page
41
48
new
york
ny
usa
2009
acm
bibliography
125
alex
beutel
jilin
chen
tulsee
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
and
cristos
goodrow
fairness
in
recommendation
ranking
through
pairwise
comparisons
in
proceedings
of
the
25th
acm
sigkdd
international
conference
on
knowledge
discovery
data
mining
page
2212
2220
new
york
ny
usa
2019
association
for
computing
machinery
isbn
9781450362016
10
asia
biega
krishna
gummadi
and
gerhard
weikum
equity
of
attention
amortizing
individual
fairness
in
rankings
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
page
405
414
new
york
ny
usa
2018
association
for
computing
machinery
isbn
9781450356572
11
christopher
bishop
pattern
recognition
and
machine
learning
springer
2006
12
tom
brown
benjamin
mann
nick
ryder
melanie
subbiah
jared
kaplan
prafulla
dhariwal
arvind
neelakantan
pranav
shyam
girish
sastry
amanda
askell
sandhini
agarwal
ariel
herbert-voss
gretchen
krueger
tom
henighan
rewon
child
aditya
ramesh
daniel
ziegler
jeﬀrey
wu
clemens
winter
chris
hesse
mark
chen
eric
sigler
mateusz
litwin
scott
gray
benjamin
chess
jack
clark
christopher
berner
sam
mccandlish
alec
radford
ilya
sutskever
and
dario
amodei
language
models
are
few-shot
learners
in
larochelle
ranzato
hadsell
balcan
and
lin
editors
advances
in
neural
information
processing
systems
volume
33
pages
1877
1901
curran
associates
inc
2020
bibliography
126
13
sébastien
bubeck
varun
chandrasekaran
ronen
eldan
johannes
gehrke
eric
horvitz
ece
kamar
peter
lee
yin
tat
lee
yuanzhi
li
scott
lundberg
harsha
nori
hamid
palangi
marco
túlio
ribeiro
and
yi
zhang
sparks
of
artiﬁcial
general
intelligence
early
experiments
with
gpt-4
corr
abs
2303.12712
2023
14
chumphol
bunkhumpornpat
krung
sinapiromsaran
and
chidchanok
lursinsap
dbsmote
density-based
synthetic
minority
over-sampling
technique
applied
intelligence
36
664
684
2012
15
chris
burges
tal
shaked
erin
renshaw
ari
lazier
matt
deeds
nicole
hamilton
and
greg
hullender
learning
to
rank
using
gradient
descent
in
proceedings
of
the
22nd
international
conference
on
machine
learning
page
89
96
new
york
ny
usa
2005
acm
16
christopher
jc
burges
from
ranknet
to
lambdarank
to
lambdamart
an
overview
learning
11
23
581
81
2010
17
zhe
cao
tao
qin
tie-yan
liu
ming-feng
tsai
and
hang
li
learning
to
rank
from
pairwise
approach
to
listwise
approach
in
proceedings
of
the
24th
international
conference
on
machine
learning
pages
129
136
2007
18
elisa
celis
damian
straszak
and
nisheeth
vishnoi
ranking
with
fairness
constraints
in
ioannis
chatzigiannakis
christos
kaklamanis
dániel
marx
and
donald
sannella
editors
45th
international
colloquium
on
automata
languages
and
programming
2018
july
13
2018
prague
czech
republic
volume
107
pages
28
28
15
schloss
dagstuhl
leibniz-zentrum
für
informatik
2018
bibliography
127
19
nitesh
chawla
kevin
bowyer
lawrence
hall
and
philip
kegelmeyer
smote
synthetic
minority
over-sampling
technique
artif
int
res
16
321
357
jun
2002
issn
1076
9757
20
jiawei
chen
hande
dong
yang
qiu
xiangnan
he
xin
xin
liang
chen
guli
lin
and
keping
yang
autodebias
learning
to
debias
for
recommendation
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
page
21
30
new
york
ny
usa
2021
association
for
computing
machinery
21
jiawei
chen
hande
dong
yang
qiu
xiangnan
he
xin
xin
liang
chen
guli
lin
and
keping
yang
autodebias
learning
to
debias
for
recommendation
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
21
page
21
30
new
york
ny
usa
2021
association
for
computing
machinery
22
myra
cheng
esin
durmus
and
dan
jurafsky
marked
personas
using
natural
language
prompts
to
measure
stereotypes
in
language
models
in
anna
rogers
jordan
boyd-graber
and
naoaki
okazaki
editors
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
1504
1532
association
for
computational
linguistics
2023
23
xiangxiang
chu
bo
zhang
and
ruijun
xu
fairnas
rethinking
evaluation
fairness
of
weight
sharing
neural
architecture
search
in
international
conference
on
bibliography
128
computer
vision
2021
24
daniel
cohen
bhaskar
mitra
katja
hofmann
and
bruce
croft
cross
domain
regularization
for
neural
ranking
models
using
adversarial
learning
in
the
41st
international
acm
sigir
conference
on
research
development
in
information
retrieval
sigir
18
page
1025
1028
new
york
ny
usa
2018
association
for
computing
machinery
25
paul
covington
jay
adams
and
emre
sargin
deep
neural
networks
for
youtube
recommendations
in
proceedings
of
the
10th
acm
conference
on
recommender
systems
recsys
16
page
191
198
new
york
ny
usa
2016
association
for
computing
machinery
26
nick
craswell
arjen
de
vries
and
ian
soboroﬀ
overview
of
the
trec
2005
enterprise
track
in
trec
volume
pages
2005
27
andré
cruz
catarina
belém
joão
bravo
pedro
saleiro
and
pedro
bizarro
fairgbm
gradient
boosting
with
fairness
constraints
in
the
eleventh
international
conference
on
learning
representations
2023
28
michael
ekstrand
anubrata
das
robin
burke
fernando
diaz
et
al
fairness
in
information
access
systems
foundations
and
trends
in
information
retrieval
16
177
2022
29
michael
ekstrand
graham
mcdonald
amifa
raj
and
isaac
johnson
overview
of
the
trec
2021
fair
ranking
track
in
the
thirtieth
text
retrieval
conference
trec
2021
proceedings
2022
bibliography
129
30
chelsea
finn
pieter
abbeel
and
sergey
levine
model-agnostic
meta-learning
for
fast
adaptation
of
deep
networks
in
proceedings
of
the
34th
international
conference
on
machine
learning
volume
70
page
1126
1135
jmlr
org
2017
31
chelsea
finn
pieter
abbeel
and
sergey
levine
model-agnostic
meta-learning
for
fast
adaptation
of
deep
networks
in
doina
precup
and
yee
whye
teh
editors
proceedings
of
the
34th
international
conference
on
machine
learning
volume
70
of
proceedings
of
machine
learning
research
pages
1126
1135
pmlr
06
11
aug
2017
32
luca
franceschi
paolo
frasconi
saverio
salzo
riccardo
grazzi
and
massimiliano
pontil
bilevel
programming
for
hyperparameter
optimization
and
meta-learning
in
international
conference
on
machine
learning
pages
1568
1577
pmlr
2018
33
batya
friedman
and
helen
nissenbaum
bias
in
computer
systems
acm
trans
inf
syst
14
330
347
jul
1996
34
samuel
gehman
suchin
gururangan
maarten
sap
yejin
choi
and
noah
smith
realtoxicityprompts
evaluating
neural
toxic
degeneration
in
language
models
in
findings
2020
35
daniel
gillick
sayali
kulkarni
larry
lansing
alessandro
presta
jason
baldridge
eugene
ie
and
diego
garcia-olano
learning
dense
representations
for
entity
retrieval
in
proceedings
of
the
23rd
conference
on
computational
natural
language
learning
conll
pages
528
537
hong
kong
china
november
2019
association
for
computational
linguistics
bibliography
130
36
ian
goodfellow
jean
pouget-abadie
mehdi
mirza
bing
xu
david
warde-farley
sherjil
ozair
aaron
courville
and
yoshua
bengio
generative
adversarial
nets
in
ghahramani
welling
cortes
lawrence
and
weinberger
editors
advances
in
neural
information
processing
systems
volume
27
curran
associates
inc
2014
37
jianping
gou
baosheng
yu
stephen
maybank
and
dacheng
tao
knowledge
distillation
survey
international
journal
of
computer
vision
129
1789
1819
2021
38
cyril
goutte
and
eric
gaussier
probabilistic
interpretation
of
precision
recall
and
f-score
with
implication
for
evaluation
in
david
losada
and
juan
fernández-luna
editors
advances
in
information
retrieval
pages
345
359
berlin
heidelberg
2005
springer
berlin
heidelberg
39
fabian
haak
and
philipp
schaer
auditing
search
query
suggestion
bias
through
recursive
algorithm
interrogation
in
14th
acm
web
science
conference
2022
page
219
227
new
york
ny
usa
2022
acm
40
edward
hu
yelong
shen
phillip
wallis
zeyuan
allen-zhu
yuanzhi
li
shean
wang
lu
wang
and
weizhu
chen
lora
low-rank
adaptation
of
large
language
models
in
international
conference
on
learning
representations
2022
41
ben
hutchinson
vinodkumar
prabhakaran
emily
denton
kellie
webster
yu
zhong
and
stephen
denuyl
social
biases
in
nlp
models
as
barriers
for
persons
with
disabilities
in
dan
jurafsky
joyce
chai
natalie
schluter
and
joel
bibliography
131
tetreault
editors
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
pages
5491
5501
online
july
2020
association
for
computational
linguistics
42
gert
jacobusse
and
cor
veenman
on
selection
bias
with
imbalanced
classes
in
toon
calders
michelangelo
ceci
and
donato
malerba
editors
discovery
science
pages
325
340
cham
2016
springer
international
publishing
43
muhammad
abdullah
jamal
and
guo-jun
qi
task
agnostic
meta-learning
for
fewshot
learning
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
11719
11727
2019
44
albert
jiang
alexandre
sablayrolles
arthur
mensch
chris
bamford
devendra
singh
chaplot
diego
de
las
casas
florian
bressand
gianna
lengyel
guillaume
lample
lucile
saulnier
lélio
renard
lavaud
marie-anne
lachaux
pierre
stock
teven
le
scao
thibaut
lavril
thomas
wang
timothée
lacroix
and
william
el
sayed
mistral
7b
2023
45
vladimir
karpukhin
barlas
oguz
sewon
min
patrick
lewis
ledell
wu
sergey
edunov
danqi
chen
and
wen-tau
yih
dense
passage
retrieval
for
open-domain
question
answering
in
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
online
november
2020
association
for
computational
linguistics
46
günter
klambauer
thomas
unterthiner
andreas
mayr
and
sepp
hochreiter
selfnormalizing
neural
networks
in
proceedings
of
the
31st
international
conference
bibliography
132
on
neural
information
processing
systems
nips
17
page
972
981
red
hook
ny
usa
2017
curran
associates
inc
47
jon
kleinberg
and
manish
raghavan
selection
problems
in
the
presence
of
implicit
bias
in
anna
karlin
editor
9th
innovations
in
theoretical
computer
science
conference
volume
94
of
leibniz
international
proceedings
in
informatics
pages
33
33
17
dagstuhl
germany
2018
schloss
dagstuhl
leibniz-zentrum
fuer
informatik
48
william
knight
computer
method
for
calculating
kendall
tau
with
ungrouped
data
journal
of
the
american
statistical
association
61
314
436
439
1966
49
preethi
lahoti
gerhard
weikum
and
krishna
gummadi
ifair
learning
individually
fair
data
representations
for
algorithmic
decision
making
2019
ieee
35th
international
conference
on
data
engineering
pages
1334
1345
2019
50
percy
liang
rishi
bommasani
tony
lee
dimitris
tsipras
dilara
soylu
michihiro
yasunaga
yian
zhang
deepak
narayanan
yuhuai
wu
ananya
kumar
benjamin
newman
binhang
yuan
bobby
yan
ce
zhang
christian
cosgrove
christopher
manning
christopher
ré
diana
acosta-navas
drew
hudson
eric
zelikman
esin
durmus
faisal
ladhak
frieda
rong
hongyu
ren
huaxiu
yao
jue
wang
keshav
santhanam
laurel
orr
lucia
zheng
mert
yuksekgonul
mirac
suzgun
nathan
kim
neel
guha
niladri
chatterji
omar
khattab
peter
henderson
qian
huang
ryan
chi
sang
michael
xie
shibani
santurkar
surya
ganguli
bibliography
133
tatsunori
hashimoto
thomas
icard
tianyi
zhang
vishrav
chaudhary
william
wang
xuechen
li
yifan
mai
yuhui
zhang
and
yuta
koreeda
holistic
evaluation
of
language
models
2023
51
shichen
liu
fei
xiao
wenwu
ou
and
luo
si
cascade
ranking
for
operational
e-commerce
search
in
proceedings
of
the
23rd
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
kdd
17
page
1557
1565
new
york
ny
usa
2017
association
for
computing
machinery
52
hanchao
ma
sheng
guan
christopher
toomey
and
yinghui
wu
diversiﬁed
subgraph
query
generation
with
group
fairness
in
proceedings
of
the
fifteenth
acm
international
conference
on
web
search
and
data
mining
page
686
694
new
york
ny
usa
2022
acm
53
sourab
mangrulkar
sylvain
gugger
lysandre
debut
younes
belkada
sayak
paul
and
benjamin
bossan
peft
state-of-the-art
parameter-eﬃcient
ﬁne-tuning
methods
https://github.com/huggingface/peft,
2022
54
nikita
nangia
clara
vania
rasika
bhalerao
and
samuel
bowman
crowspairs
challenge
dataset
for
measuring
social
biases
in
masked
language
models
in
bonnie
webber
trevor
cohn
yulan
he
and
yang
liu
editors
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
pages
1953
1967
online
november
2020
association
for
computational
linguistics
55
rodrigo
nogueira
and
kyunghyun
cho
passage
re-ranking
with
bert
2020
bibliography
134
56
rodrigo
nogueira
zhiying
jiang
ronak
pradeep
and
jimmy
lin
document
ranking
with
pretrained
sequence-to-sequence
model
in
trevor
cohn
yulan
he
and
yang
liu
editors
findings
of
the
association
for
computational
linguistics
emnlp
2020
pages
708
718
online
november
2020
association
for
computational
linguistics
57
openai
gpt-4
technical
report
2023
58
long
ouyang
jeﬀ
wu
xu
jiang
diogo
almeida
carroll
wainwright
pamela
mishkin
chong
zhang
sandhini
agarwal
katarina
slama
alex
ray
john
schulman
jacob
hilton
fraser
kelton
luke
miller
maddie
simens
amanda
askell
peter
welinder
paul
christiano
jan
leike
and
ryan
lowe
training
language
models
to
follow
instructions
with
human
feedback
2022
59
sinno
jialin
pan
and
qiang
yang
survey
on
transfer
learning
ieee
transactions
on
knowledge
and
data
engineering
22
10
1345
1359
2010
60
german
parisi
ronald
kemker
jose
part
christopher
kanan
and
stefan
wermter
continual
lifelong
learning
with
neural
networks
review
neural
networks
113
54
71
2019
issn
0893
6080
61
alicia
parrish
angelica
chen
nikita
nangia
vishakh
padmakumar
jason
phang
jana
thompson
phu
mon
htut
and
samuel
bowman
bbq
hand-built
bias
benchmark
for
question
answering
in
smaranda
muresan
preslav
nakov
and
bibliography
135
aline
villavicencio
editors
findings
of
the
association
for
computational
linguistics
acl
2022
dublin
ireland
may
22
27
2022
pages
2086
2105
association
for
computational
linguistics
2022
62
ethan
perez
saﬀron
huang
francis
song
trevor
cai
roman
ring
john
aslanides
amelia
glaese
nat
mcaleese
and
geoﬀrey
irving
red
teaming
language
models
with
language
models
in
yoav
goldberg
zornitsa
kozareva
and
yue
zhang
editors
proceedings
of
the
2022
conference
on
empirical
methods
in
natural
language
processing
pages
3419
3448
abu
dhabi
united
arab
emirates
december
2022
association
for
computational
linguistics
63
andrea
dal
pozzolo
olivier
caelen
reid
johnson
and
gianluca
bontempi
calibrating
probability
with
undersampling
for
unbalanced
classiﬁcation
in
2015
ieee
symposium
series
on
computational
intelligence
pages
159
166
new
york
ny
usa
2015
ieee
64
jiarui
qin
jiachen
zhu
bo
chen
zhirong
liu
weiwen
liu
ruiming
tang
rui
zhang
yong
yu
and
weinan
zhang
rankﬂow
joint
optimization
of
multi-stage
cascade
ranking
systems
as
ﬂows
sigir
22
page
814
824
new
york
ny
usa
2022
association
for
computing
machinery
65
zhen
qin
rolf
jagerman
kai
hui
honglei
zhuang
junru
wu
jiaming
shen
tianqi
liu
jialu
liu
donald
metzler
xuanhui
wang
and
michael
bendersky
large
language
models
are
eﬀective
text
rankers
with
pairwise
ranking
prompting
2023
bibliography
136
66
aida
ramezani
and
yang
xu
knowledge
of
cultural
moral
norms
in
large
language
models
in
anna
rogers
jordan
boyd-graber
and
naoaki
okazaki
editors
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
428
446
association
for
computational
linguistics
2023
67
kuniaki
saito
yoshitaka
ushiku
and
tatsuya
harada
asymmetric
tri-training
for
unsupervised
domain
adaptation
in
proceedings
of
the
34th
international
conference
on
machine
learning
volume
70
icml
17
page
2988
2997
jmlr
org
2017
68
sebastin
santy
jenny
liang
ronan
le
bras
katharina
reinecke
and
maarten
sap
nlpositionality
characterizing
design
biases
of
datasets
and
models
in
anna
rogers
jordan
boyd-graber
and
naoaki
okazaki
editors
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
acl
2023
toronto
canada
july
14
2023
pages
9080
9102
association
for
computational
linguistics
2023
69
jun
shu
qi
xie
lixuan
yi
qian
zhao
sanping
zhou
zongben
xu
and
deyu
meng
meta-weight-net
learning
an
explicit
mapping
for
sample
weighting
advances
in
neural
information
processing
systems
32
2019
70
ashudeep
singh
and
thorsten
joachims
fairness
of
exposure
in
rankings
in
proceedings
of
the
24th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
page
2219
2228
new
york
ny
usa
2018
acm
bibliography
137
71
dylan
slack
sorelle
friedler
and
emile
givental
fairness
warnings
and
fairmaml
learning
fairly
with
minimal
data
page
200
209
new
york
ny
usa
2020
association
for
computing
machinery
72
julia
stoyanovich
ke
yang
and
hv
jagadish
online
set
selection
with
fairness
and
diversity
constraints
in
proceedings
of
the
edbt
conference
2018
73
weiwei
sun
lingyong
yan
xinyu
ma
shuaiqiang
wang
pengjie
ren
zhumin
chen
dawei
yin
and
zhaochun
ren
is
chatgpt
good
at
search
investigating
large
language
models
as
re-ranking
agents
in
houda
bouamor
juan
pino
and
kalika
bali
editors
proceedings
of
the
2023
conference
on
empirical
methods
in
natural
language
processing
pages
14918
14937
singapore
december
2023
association
for
computational
linguistics
74
ilya
sutskever
james
martens
george
dahl
and
geoﬀrey
hinton
on
the
importance
of
initialization
and
momentum
in
deep
learning
in
international
conference
on
machine
learning
pages
1139
1147
pmlr
2013
75
zhiqiang
tao
yaliang
li
bolin
ding
ce
zhang
jingren
zhou
and
yun
fu
learning
to
mutate
with
hypergradient
guided
population
in
advances
in
neural
information
processing
systems
volume
33
pages
17641
17651
curran
associates
inc
2020
bibliography
138
76
hugo
touvron
louis
martin
kevin
stone
peter
albert
amjad
almahairi
yasmine
babaei
nikolay
bashlykov
soumya
batra
prajjwal
bhargava
shruti
bhosale
dan
bikel
lukas
blecher
cristian
canton
ferrer
moya
chen
guillem
cucurull
david
esiobu
jude
fernandes
jeremy
fu
wenyin
fu
brian
fuller
cynthia
gao
vedanuj
goswami
naman
goyal
anthony
hartshorn
saghar
hosseini
rui
hou
hakan
inan
marcin
kardas
viktor
kerkez
madian
khabsa
isabel
kloumann
artem
korenev
punit
singh
koura
marie-anne
lachaux
thibaut
lavril
jenya
lee
diana
liskovich
yinghai
lu
yuning
mao
xavier
martinet
todor
mihaylov
pushkar
mishra
igor
molybog
yixin
nie
andrew
poulton
jeremy
reizenstein
rashi
rungta
kalyan
saladi
alan
schelten
ruan
silva
eric
michael
smith
ranjan
subramanian
xiaoqing
ellen
tan
binh
tang
ross
taylor
adina
williams
jian
xiang
kuan
puxin
xu
zheng
yan
iliyan
zarov
yuchen
zhang
angela
fan
melanie
kambadur
sharan
narang
aurelien
rodriguez
robert
stojnic
sergey
edunov
and
thomas
scialom
llama
open
foundation
and
ﬁne-tuned
chat
models
2023
77
boxin
wang
weixin
chen
hengzhi
pei
chulin
xie
mintong
kang
chenhui
zhang
chejian
xu
zidi
xiong
ritik
dutta
rylan
schaeﬀer
sang
truong
simran
arora
mantas
mazeika
dan
hendrycks
zinan
lin
yu
cheng
sanmi
koyejo
dawn
song
and
bo
li
decodingtrust
comprehensive
assessment
of
trustworthiness
in
gpt
models
corr
abs
2306.11698
2023
78
xiaojie
wang
rui
zhang
yu
sun
and
jianzhong
qi
combating
selection
biases
in
recommender
systems
with
few
unbiased
ratings
in
proceedings
of
the
14th
bibliography
139
acm
international
conference
on
web
search
and
data
mining
wsdm
21
page
427
435
new
york
ny
usa
2021
association
for
computing
machinery
79
xuanhui
wang
michael
bendersky
donald
metzler
and
marc
najork
learning
to
rank
with
selection
bias
in
personal
search
in
proceedings
of
the
39th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
16
page
115
124
new
york
ny
usa
2016
association
for
computing
machinery
80
yuan
wang
zhiqiang
tao
and
yi
fang
meta-learning
approach
to
fair
ranking
in
the
45th
international
acm
sigir
conference
on
research
development
in
information
retrieval
page
2539
2544
new
york
ny
usa
2022
acm
81
zhenlei
wang
jingsen
zhang
hongteng
xu
xu
chen
yongfeng
zhang
wayne
xin
zhao
and
ji-rong
wen
counterfactual
data-augmented
sequential
recommendation
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
21
page
347
356
new
york
ny
usa
2021
association
for
computing
machinery
82
linda
wightman
lsac
national
longitudinal
bar
passage
study
lsac
research
report
series
1998
83
garrett
wilson
and
diane
cook
survey
of
unsupervised
deep
domain
adaptation
acm
trans
intell
syst
technol
11
jul
2020
issn
2157
6904
bibliography
140
84
ledell
wu
fabio
petroni
martin
josifoski
sebastian
riedel
and
luke
zettlemoyer
scalable
zero-shot
entity
linking
with
dense
entity
retrieval
in
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
pages
6397
6407
online
november
2020
association
for
computational
linguistics
85
chenyan
xiong
zhuyun
dai
jamie
callan
zhiyuan
liu
and
russell
power
endto-end
neural
ad-hoc
ranking
with
kernel
pooling
in
proceedings
of
the
40th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
17
page
55
64
new
york
ny
usa
2017
association
for
computing
machinery
86
ke
yang
and
julia
stoyanovich
measuring
fairness
in
ranked
outputs
in
proceedings
of
the
29th
international
conference
on
scientiﬁc
and
statistical
database
management
new
york
ny
usa
2017
association
for
computing
machinery
isbn
9781450352826
87
ke
yang
vasilis
gkatzelis
and
julia
stoyanovich
balanced
ranking
with
diversity
constraints
in
proceedings
of
the
twenty-eighth
international
joint
conference
on
artiﬁcial
intelligence
pages
6035
6042
international
joint
conferences
on
artiﬁcial
intelligence
organization
2019
88
huaxiu
yao
xian
wu
zhiqiang
tao
yaliang
li
bolin
ding
ruirui
li
and
zhenhui
li
automated
relational
meta-learning
in
8th
international
conference
on
learning
representations
2020
bibliography
141
89
meike
zehlike
and
carlos
castillo
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
page
2849
2855
association
for
computing
machinery
new
york
ny
usa
2020
isbn
9781450370233
90
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
and
ricardo
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
in
proceedings
of
the
2017
acm
on
conference
on
information
and
knowledge
management
page
1569
1578
new
york
ny
usa
2017
association
for
computing
machinery
isbn
9781450349185
91
meike
zehlike
philipp
hacker
and
emil
wiedemann
matching
code
and
law
achieving
algorithmic
fairness
with
optimal
transport
data
min
knowl
discov
34
163
200
jan
2020
issn
1384
5810
92
meike
zehlike
ke
yang
and
julia
stoyanovich
fairness
in
ranking
survey
arxiv
preprint
arxiv
2103.14000
2021
93
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
and
cynthia
dwork
learning
fair
representations
in
sanjoy
dasgupta
and
david
mcallester
editors
proceedings
of
the
30th
international
conference
on
machine
learning
volume
28
of
proceedings
of
machine
learning
research
pages
325
333
atlanta
georgia
usa
17
19
jun
2013
pmlr
94
brian
hu
zhang
blake
lemoine
and
margaret
mitchell
mitigating
unwanted
biases
with
adversarial
learning
in
proceedings
of
the
2018
aaai
acm
conference
bibliography
142
on
ai
ethics
and
society
aies
18
page
335
340
new
york
ny
usa
2018
association
for
computing
machinery
95
jizhi
zhang
keqin
bao
yang
zhang
wenjie
wang
fuli
feng
and
xiangnan
he
is
chatgpt
fair
for
recommendation
evaluating
fairness
in
large
language
model
recommendation
in
proceedings
of
the
17th
acm
conference
on
recommender
systems
recsys
23
page
993
999
new
york
ny
usa
2023
association
for
computing
machinery
isbn
9798400702419
96
chen
zhao
and
feng
chen
unfairness
discovery
and
prevention
for
few-shot
regression
in
2020
ieee
international
conference
on
knowledge
graph
pages
137
144
2020
97
chen
zhao
feng
chen
zhuoyi
wang
and
latifur
khan
primal-dual
subgradient
approach
for
fair
meta
learning
in
2020
ieee
international
conference
on
data
mining
pages
821
830
ieee
2020
98
chen
zhao
feng
chen
and
bhavani
thuraisingham
fairness-aware
online
metalearning
in
proceedings
of
the
27th
acm
sigkdd
conference
on
knowledge
discovery
data
mining
page
2294
2304
new
york
ny
usa
2021
association
for
computing
machinery
isbn
9781450383325