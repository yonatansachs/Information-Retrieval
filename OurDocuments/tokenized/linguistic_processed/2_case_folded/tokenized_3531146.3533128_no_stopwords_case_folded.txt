subverting
fair
image
search
generative
adversarial
perturbations
avijit
ghosh
matthew
jagielski
christo
wilson
ghosh.a@northeastern.edu
northeastern
university
boston
ma
usa
jagielski@google.com
google
research
mountain
view
ca
usa
cbw@ccs.neu.edu
northeastern
university
boston
ma
usa
abstract
work
explore
intersection
fairness
robustness
context
ranking
ranking
model
calibrated
achieve
definition
fairness
possible
external
adversary
make
ranking
model
behave
unfairly
without
access
model
training
data
investigate
question
present
case
study
develop
attack
stateof-the-art
fairness-aware
image
search
engine
using
images
maliciously
modified
using
generative
adversarial
perturbation
gap
model
75
perturbations
attempt
cause
fair
re-ranking
algorithm
unfairly
boost
rank
images
containing
people
adversary-selected
subpopulation
present
results
extensive
experiments
demonstrating
attacks
can
successfully
confer
significant
unfair
advantage
people
majority
class
relative
fairly-ranked
baseline
search
results
demonstrate
attacks
robust
across
number
variables
close
zero
impact
relevance
search
results
succeed
strict
threat
model
findings
highlight
danger
deploying
fair
machine
learning
algorithms
in-the-wild
data
necessary
achieve
fairness
may
adversarially
manipulated
models
robust
attacks
machine
learning
ml
community
awoken
concerns
fairness
ml
models
elimination
unjustified
bias
specific
groups
people
models
now
extensive
literature
documenting
unfairness
deployed
ml
systems
21
well
techniques
training
fair
classification
42
47
50
65
ranking
28
86
98
models
companies
adopting
deploying
fair
ml
systems
many
real-world
contexts
10
96
ml
systems
strive
achieve
demographic
fairness
dependent
high-quality
demographic
data
control
unjustified
biases
13
94
recent
work
highlighted
critical
dependency
showing
unintentional
errors
demographic
data
can
dramatically
undermine
objectives
fair
ranking
algorithms
39
another
serious
concern
ml
community
model
robustness
especially
face
clever
dedicated
adversaries
field
adversarial
ml
demonstrated
seemingly
accurate
models
brittle
presented
maliciously
crafted
inputs
24
89
attacks
impact
models
across
variety
contexts
12
31
46
89
existence
adversarial
ml
challenges
use
models
real-world
deployments
work
explore
intersection
two
concerns
fairness
robustness
context
ranking
ranking
model
carefully
calibrated
achieve
definition
fairness
possible
external
adversary
make
ranking
model
behave
unfairly
without
access
model
training
data
words
can
attackers
intentionally
weaponize
demographic
markers
data
subvert
fairness
guarantees
investigate
question
present
case
study
develop
attack
fairness-aware
image
search
engine
using
images
maliciously
modified
adversarial
perturbations
chose
case
study
image
retrieval
based
text
queries
popular
real-world
use
case
neural
models
google
image
search
istock
getty
images
etc
prior
work
shown
models
can
potentially
fooled
using
adversarial
perturbations
101
although
context
fairness
strengthen
case
study
adopt
strict
threat
model
adversary
poison
training
data
48
ranking
model
knowledge
ranking
model
fairness
algorithm
used
victim
search
engine
instead
adversary
can
add
images
victim
database
image
retrieval
model
trained
experiments
develop
image
search
engine
uses
state-of-the-art
multimodal
transformer
mmt
37
retrieval
model
fair
re-ranking
algorithm
fmmr
51
aims
achieve
demographic
group
fairness
ranked
list
image
ccs
concepts
information
systems
retrieval
models
ranking
security
privacy
keywords
information
retrieval
fair
ranking
adversarial
machine
learning
demographic
inference
acm
reference
format
avijit
ghosh
matthew
jagielski
christo
wilson
2022
subverting
fair
image
search
generative
adversarial
perturbations
2022
acm
conference
fairness
accountability
transparency
facct
22
june
21
24
2022
seoul
republic
korea
acm
new
york
ny
usa
14
pages
https://doi.org/10.1145/3531146.3533128
permission
make
digital
hard
copies
part
work
personal
classroom
use
granted
without
fee
provided
copies
made
distributed
profit
commercial
advantage
copies
bear
notice
full
citation
first
page
copyrights
third-party
components
work
must
honored
uses
contact
owner
author
facct
22
june
21
24
2022
seoul
republic
korea
2022
copyright
held
owner
author
acm
isbn
978
4503
9352
22
06
https://doi.org/10.1145/3531146.3533128
637
introduction
facct
22
june
21
24
2022
seoul
republic
korea
tennis
player
avijit
ghosh
matthew
jagielski
christo
wilson
crawl
query
image
corpus
retrieval
model
gap
tennis
player
fairnessaware
ranker
deepface
prediction
light-skinned
male
gap
perturbation
magniÔ¨Åed
5x
deepface
prediction
dark-skinned
male
figure
diagram
showing
attack
approach
shows
example
search
results
image
search
engine
query
tennis
player
search
engine
attempts
provide
demographically-fair
results
point
images
corpus
adversarially
perturbed
search
engine
crawls
indexes
new
images
web
collects
images
adversarially
perturbed
using
gap
model
show
real
example
one
image
applying
generated
perturbation
causes
deepface
model
90
misclassify
person
skin
tone
response
future
query
tennis
player
retrieval
model
will
identify
relevant
images
perturbed
fairness-aware
ranker
target
attack
highlighted
red
mistakenly
elevates
rank
image
containing
light-skinned
male
also
highlighted
red
misclassifies
dark-skinned
due
perturbations
query
results
without
ever
explicitly
using
demographic
labels
normal
circumstances
images
unperturbed
search
engine
returns
demographically
balanced
sets
images
response
free
text
queries
train
generative
adversarial
perturbation
gap
model
75
learns
pretrained
demographic
classifiers
strategically
insert
human-imperceptible
perturbations
images
perturbations
attempt
cause
fmmr
unfairly
boost
rank
images
containing
people
adversary-selected
subpopulation
light-skinned
men
figure
shows
example
image
search
results
produced
search
engine
response
query
tennis
player
without
attack
present
results
extensive
experiments
demonstrating
attacks
can
successfully
confer
significant
unfair
advantage
people
majority
class
light-skinned
men
case
terms
overall
representation
position
search
results
relative
fairly-ranked
baseline
search
results
demonstrate
attack
robust
across
number
variables
including
length
search
result
lists
fraction
images
adversary
able
perturb
fairness
algorithm
used
search
engine
image
embedding
algorithm
used
search
engine
demographic
inference
algorithm
used
train
gap
models
training
objective
gap
models
additionally
attacks
stealthy
close
zero
impact
relevance
search
results
summary
show
gaps
can
used
subvert
fairness
guarantees
context
fair
image
retrieval
attack
successful
highly
restricted
threat
model
suggests
powerful
adversaries
will
also
able
implement
successful
attacks
hypothesize
similar
attacks
may
possible
classes
ml-based
systems
rely
highly
parameterized
models
make
fairness
decisions
inputs
based
data
controlled
adversaries
638
goal
work
hinder
deter
adoption
fair
ml
techniques
argue
fair
ml
techniques
must
adopted
practice
rather
goal
demonstrate
fairness
guarantees
can
potentially
weaponized
research
community
will
energized
develop
mitigations
making
models
robust
adopting
high-quality
sources
demographic
data
resistant
manipulation
facilitate
mitigation
development
without
arming
attackers
plan
release
code
data
researchers
request
background
2.1
fairness
ranking
algorithmic
decision
making
permeating
modern
life
including
high-stakes
decisions
like
credit
lending
11
bail
granting
hiring
96
etc
systems
great
scaling
processes
human
bottlenecks
also
unintended
property
embedding
entrenching
unfair
social
biases
sexism
homophobia
response
growing
body
academic
work
ways
detect
algorithmic
bias
40
develop
classes
fair
algorithms
instance
classification
42
47
50
65
causal
inference
62
70
word
embeddings
18
20
regression
15
retrieval
ranking
28
86
98
also
growing
body
legal
work
legislative
action
around
globe
30
33
53
73
tackle
algorithmic
bias
study
focus
fair
information
retrieval
ir
algorithms
class
algorithms
received
comparatively
less
attention
classification
algorithms
literature
initial
studies
examined
fair
ir
proposed
solve
binary
context
make
ranked
list
fair
two
groups
28
98
subsequent
work
uses
constrained
learning
solve
ranking
problems
using
classic
optimization
methods
86
also
methods
use
pairwise
comparisons
16
describe
methods
achieve
fairness
learning-to-rank
contexts
68
99
subverting
fair
image
search
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
korea
industrial
settings
researchers
linkedin
proposed
algorithm
uses
re-ranking
post-processing
achieve
representational
parity
38
however
recent
work
ghosh
et
al
39
shows
uncertainty
due
incorrect
inference
protected
demographic
attributes
can
undermine
fairness
guarantees
ir
contexts
fairness
methods
require
explicit
demographic
labels
runtime
writing
emerging
area
focus
classification
55
ranking
51
79
one
example
studied
large-scale
shopify
fair
maximal
marginal
relevance
fmmr
algorithm
51
describe
detail
3.2
study
examine
robustness
shopify
linkedin
fair
ranking
algorithms
2.2
adversarial
machine
learning
adversarial
ml
growing
field
research
aims
develop
methods
tools
can
subvert
objectives
ml
algorithms
example
prior
research
highlighted
deep
learning
models
often
robust
presented
inputs
intentionally
maliciously
crafted
24
43
67
74
85
95
97
several
proposed
defenses
state-of-the-art
adversarial
ml
attacks
defeated
91
adversarial
examples
maliciously
crafted
inputs
shown
transfer
across
models
performing
similar
tasks
60
92
promising
defense
method
adversarial
training
computationally
expensive
imperfect
results
decreased
standard
accuracy
still
somewhat
low
adversarial
accuracy
35
54
84
adversarial
ml
presents
significant
hurdle
deploying
neural
models
sensitive
real-world
systems
work
considers
adversarial
ml
attacks
ir
systems
previous
work
demonstrated
successful
attacks
image-to-image
search
systems
61
101
allowing
adversary
control
results
targeted
image
queries
using
localized
patches
19
universal
adversarial
perturbations1
57
work
demonstrated
attacks
text-to-text
retrieval
systems
77
personalized
ranking
systems
46
work
zhou
et
al
101
hypothesized
targeted
attacks
selected
items
ranked
list
might
possible
using
universal
adversarial
perturbations
none
works
consider
compromising
text-to-image
models
group
fairness
objectives
study
prior
work
demonstrated
adversarial
ml
attacks
fairness
objectives
ml
systems
training
time
attacks
adversary
supplies
poisoned
training
data
results
models
either
compromise
accuracy
targeted
subgroups
48
64
exacerbate
pre-existing
unfairness
subgroups
29
87
specific
classification
exists
theoretical
work
shows
learn
fair
models
sensitive
attributes
noisy
25
corrupted
poisoning
attack
27
consider
ranking
adversarial
ml
attacks
test
time
training
model
using
non-malicious
data
consider
work
relatively
unexplored
fairness
settings
nanda
et
al
71
show
adversarial
ml
attacks
can
harm
certain
subpopulations
others
classification
tasks
however
important
observation
harms
suggested
work
may
difficult
realize
practice
involve
disparity
examples
adversarially
corrupted
contrast
work
shows
test-time
attacks
can
harm
fairness
benign
data
launched
ranking
setting
methodology
now
present
plan
study
first
introduce
application
context
threat
model
attacker
will
attempt
compromise
application
second
discuss
ir
models
algorithms
underlying
fairness-aware
image
search
engine
third
discuss
strategy
attacking
search
engine
using
gaps
3.1
context
threat
model
study
consider
security
fairness-aware
image
search
engine
search
engine
indexes
images
around
web
either
automatically
via
crawler
user-provided
submissions
provides
free-text
interface
query
image
database
examples
image
search
engines
include
google
image
search
istock
getty
images
giphy
case
image
search
engine
attempts
produce
results
relevant
given
query
fair
according
fairness
objective
one
example
fairness
objective
demographic
representativeness
search
results
contain
images
people
consider
malicious
image
curator
imgur
4chan
similar
large
database
perturbed
images
eventually
scraped
uploaded
victim
image
search
engine
index
adversarial
image
curator
goal
perturb
images
database
subvert
fairness
guarantees
downstream
retrieval
system
assume
adversary
knowledge
internals
ranking
system
retrieval
model
used
images
index
fairness
algorithm
used
threat
model
constitutes
strict
realistic
limitation
adversary
notice
threat
model
also
apply
image
search
engine
compromised
giving
adversary
access
underlying
models
entire
dataset
images
consider
adversaries
experiments
also
note
adversary
seeks
target
small
set
queries
need
control
fraction
images
matching
query
rather
fraction
entire
image
database
useful
adversary
case
queries
equally
sensitive
3.2
building
image
search
engine
now
turn
attention
building
realistic
image
search
engine
will
serve
victim
attacks
3.2
image
retrieval
text
queries
first
choice
make
study
select
image
retrieval
model
several
frameworks
image
retrieval
literature
starting
adversarial
image
curator
also
threat
model
assumed
clean
label
poi1
universal
adversarial
perturbation
uap
adversarial
perturbation
soning
attacks
83
93
adversarial
image
curator
may
perturb
copies
images
taken
web
original
images
author
setup
also
used
85
defensive
method
unauthorized
models
generalizes
classes
target
classifier
one
patch
untargeted
attack
many
classes
possible
639
facct
22
june
21
24
2022
seoul
republic
korea
avijit
ghosh
matthew
jagielski
christo
wilson
tag-based
matching
56
state-of-the-art
vision-language
transformers
58
63
purpose
paper
used
multimodal
transformer
mmt
37
based
text-image
retrieval
model
model
consists
two
components
fast
although
somewhat
lower
quality
retrieval
step
identifies
large
set
relevant
images
followed
re-ranking
step
selects
best
images
retrieved
set
concretely
user
provides
string
queries
database
images
retrieval
step
query
string
encoded
embedding
function
fq
produce
embedding
vq
images
pre-computed
embeddings
embedding
function
cosine
distance
vq
embeddings
computed
collect
large
set
dq
size
plausible
image
matches
images
ranked
according
joint
model
takes
indicating
query
image
input
returning
scores
si
well
image
dq
matches
query
scores
used
produce
final
ranking
note
mmt
model
designed
fair
normative
sense
achieve
fairness
results
model
must
re-ranked
describe
next
section
thus
mmt
model
target
attacks
since
responsible
implementing
fairness
objectives
3.2
fairness-aware
re-ranking
second
choice
make
study
selecting
algorithm
takes
output
image
retrieval
model
input
produces
fair
re-ranking
items
fairness-aware
re-ranking
ranking
function
fr
post-processed
achieve
fairness
according
subgroup
denotes
score
labels
dataset
si
th
item
heuristic
score
according
list
sorted
denotes
item
ranked
re-ranking
algorithm
adopt
fair
maximal
marginal
relevance
fmmr
51
developed
used
shopify
representative
ranking
images
fmmr
builds
maximal
marginal
relevance
23
technique
ir
seeks
maximize
information
ranked
list
choosing
next
retrieved
item
list
dissimilar
current
items
present
list
possible
mmr
introduces
hyperparameter
allows
operator
choose
trade-off
similarity
relevance
fmmr
modifies
similarity
heuristic
mmr
encode
similarity
terms
demographics
idea
next
relevant
item
chosen
placed
re-ranked
list
will
demographically
different
existing
images
possible
similarity
calculated
using
image
embeddings
examine
three
models
faster
r-cnn
78
inceptionv3
88
resnet18
45
fix
trade-off
parameter
0.14
value
used
karako
manggala
51
fmmr
paper
notable
fmmr
require
demographic
labels
people
images
perform
fair
re-ranking
since
uses
heuristic
relies
embeddings
indeed
fmmr
comes
class
fair
ranking
algorithms
use
inherent
latent
representations
objects
re-ranking
strategy
51
79
said
since
fmmr
attempts
maximize
distance
centroids
embeddings
different
demographic
groups
can
thought
performing
indirect
demographic
inference
individuals
images
640
additionally
also
evaluated
attacks
second
fairness-aware
re-ranking
algorithm
detconstsort
38
developed
deployed
linkedin
talent
search
system
unlike
fmmr
detconstsort
explicitly
requires
demographic
labels
items
trying
fairly
re-rank
however
prior
work
39
shows
detconstsort
significant
limitations
demographic
inference
used
rather
ground-truth
demographic
labels
making
unfair
even
without
perturbed
images
result
evaluating
attack
detconstsort
meaningful
defer
discussion
detconstsort
3.3
attack
construction
described
search
engine
ready
turn
attention
attack
first
introduce
demographic
inference
models
deepface
90
fairface
52
use
train
attack
next
describe
generate
adversarial
perturbations
demographic
inference
model
modifying
images
way
imperceptible
human
eyes
yet
significant
enough
fool
fair
re-ranking
algorithm
search
engine
3.3
demographic
inference
algorithms
large-scale
datasets
images
scraped
web
demographic
meta-data
people
images
readily
available
prohibitively
expensive
collect
manual
annotation
17
pipelines
using
demographic
inference
commonly
used
practice
demographic
labels
available
example
bayesian
improved
surname
geocoding
bisg
tool
used
measure
fairness
violations
lending
decisions
22
relies
inferred
demographic
information
makes
attacks
demographic
inference
models
natural
candidate
adversely
affecting
ranking
fairness
consider
two
image
demographic
inference
models
train
attacks
deepface
90
face
recognition
model
gender
race
inference
developed
facebook
use
public
wrapper
82
includes
models
fine
tuned
roughly
22
000
samples
race
gender
classification
fairface
52
model
designed
race
gender
inference
trained
diverse
set
108
000
images
since
models
infer
race
ethnicity
used
mapping
infer
skin
tone
since
find
commercially
available
algorithms
infer
skin
tones
human
images
also
use
models
infer
demographics
input
detconstsort
algorithm
matching
pipeline
39
discuss
3.3
subpopulation
generative
adversarial
perturbations
recall
adversarial
image
curator
goal
produce
database
malicious
images
indexed
image
search
engine
undermine
purported
fairness
guarantees
concretely
means
fooling
fair
re-ranker
believes
given
set
search
results
fair
across
two
subgroups
fact
results
unfair
subgroups
over-represented
additionally
malicious
images
must
retain
relevance
mapping
used
white
east
asian
middle
eastern
light
black
south
asian
hispanic
dark
acknowledge
crude
mapping
enabled
us
train
successful
attack
subverting
fair
image
search
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
korea
given
query
perceived
manipulated
human
users
search
engine
prior
work
see
2.2
demonstrated
neural
image
classification
models
can
fooled
adding
adversarial
perturbations
images
high-level
adversary
goal
train
model
can
add
noise
images
specific
latent
characteristics
images
altered
case
altered
characteristics
impact
image
embeddings
calculated
image
embedding
model
inceptionv3
fmmr
relies
upon
fair
re-ranking
running
adversarial
perturbation
algorithm
images
adversary
database
prohibitive
algorithms
involve
computationally
expensive
optimization
algorithms
practical
scale
entire
database
avoid
limitation
training
generative
adversarial
perturbation
gap
model
75
gap
model
gap
takes
clean
image
input
returns
perturbed
image
misclassified
target
model
targ
replaces
per-image
optimization
problem
much
less
expensive
forward
pass
gap
training
gap
one
time
expense
adversary
amortized
large
number
image
perturbations
done
later
universal
adversarial
perturbations
uaps
66
another
approach
amortizing
runtime
require
images
dimensions
unrealistic
assumption
real-world
image
databases
motivated
choice
gap
model
attack
now
consider
problem
impacting
fairness
attacking
fair
re-ranking
algorithm
used
victim
search
engine
choose
design
gap
target
demographic
inference
model
di
will
produce
perturbations
deep
image
model
make
image
person
one
demographic
group
appear
different
demographic
group
attack
heavily
impact
demographic-aware
re-ranking
algorithm
detconstsort
38
see
used
accurate
demographic
inference
algorithm
produce
annotations
although
fmmr
use
annotations
show
attack
still
successful
compromising
fmmr
fairness
guarantees
attack
can
seen
application
transferability
property
adversarial
examples
additionally
training
gap
demographic
inference
model
causes
attack
independent
ranking
algorithm
image
corpus
used
victim
search
engine
strong
adversarial
assumptions
designing
gap
compromise
fairness
first
note
attack
simply
forces
di
make
arbitrarily
many
errors
may
impact
fairness
example
suppose
image
database
contained
two
subpopulations
advantaged
class
disadvantaged
class
suppose
attack
causes
di
misclassify
members
members
best
possible
result
attack
demographic
inference
algorithm
results
changes
fair
ranking
algorithm
will
simply
consider
disadvantaged
class
thus
produce
ranking
reason
adversary
must
incorporate
subpopulations
attack
propose
class-targeted
generative
adversarial
perturbation
cgap
definition
cgap
consider
loss
function
target
model
targ
distribution
inputs
outputs
adversary
provides
source
class
ys
target
class
yt
cgap
model
cgap
model
takes
input
image
returns
image
minimizing
following
loss
functions
‚Ñìcgap
cgap
yt
targ
ys
‚Ñìcgap
cgap
targ
ys
cgap
force
demographic
inference
model
misclassify
samples
class
ys
class
yt
maintaining
performance
samples
class
ys
also
consider
two
extensions
definition
first
permit
adversary
target
multiple
classes
extreme
adversary
may
want
samples
classified
class
approach
proposed
75
demographic
inference
algorithm
samples
demographic
label
will
cause
fair
re-ranking
system
similar
performance
unfair
ranking
system
points
will
appear
fall
subpopulation
second
extension
untargeted
attack
cgap
simply
increases
loss
points
class
ys
inducing
arbitrary
misclassifications
simultaneously
making
relaxations
recovers
original
untargeted
gap
approach
experiment
relaxations
independently
well
multiple
instantiations
cgap
defined
experiments
section
introduce
dataset
used
evaluation
describe
setup
experiments
define
metrics
use
evaluate
attacks
4.1
dataset
annotation
preprocessing
use
microsoft
common
objects
context
ms-coco
59
retrieval
dataset
since
contains
variety
images
variable
dimensions
depths
closely
mimics
realworld
image
search
dataset
might
contain
specifically
measure
demographic
bias
filter
dataset
keeping
images
contain
people
also
need
images
demographic
annotations
fair
ranking
use
annotated
subset
coco
2014
dataset
constructed
zhao
et
al
100
similar
prior
work
26
39
zhao
et
al
crowdsource
skin
color
fitzpatrick
skin
type
scale
authors
simplified
light
dark
binary
perceived
gender
expression
15
762
images
purposes
experiments
considered
692
images
contain
one
person
filtering
final
dataset
consisted
216
light
men
536
light
women
714
dark
men
226
dark
women
4.2
experimental
setup
starting
point
experiments
need
collect
ranked
lists
baseline
unfair
retrieval
system
described
3.2
run
three
different
search
queries
retrieval
system
tennis
player
person
eating
pizza
person
table
chose
queries
reference
human
uap
can
also
seen
gap
gap
fixed
therefore
expect
gap
will
perform
strictly
better
uap
recall
per
threat
model
3.1
attacker
know
fair
re-ranking
algorithm
used
victim
thus
train
directly
641
facct
22
june
21
24
2022
seoul
republic
korea
avijit
ghosh
matthew
jagielski
christo
wilson
search
queries
attack
training
embedding
training
objective
attack
probability
top
tennis
player
person
eating
pizza
person
table
deepface
fairface
f-rcnn
inceptionv3
resnet
light
men
light
men
dark
men
light
men
light
men
dark
men
0.2
0.5
0.7
1.0
10
15
20
45
50
table
variables
hyperparameters
used
evaluating
attack
ethnicity
gender
neutral
well-supported
coco
dataset
picked
popular
object
tags
see
set
upper
bound
baseline
retrieval
system
200
images
three
queries
return
131
75
124
images
respectively
along
relevance
scores
show
distribution
relevance
scores
skin
color
gender
distributions
images
within
top
40
search
results
query
figure
also
shown
zhao
et
al
100
light
men
comprise
overwhelming
majority
three
lists
also
high
relevance
scores
across
board
meaning
retrieval
system
places
light
men
near
top
search
results
call
lists
baseline
lists
also
need
produce
fair
versions
baseline
lists
pass
baseline
lists
three
queries
fmmr
three
embedding
algorithms
without
adversarial
perturbations
refer
nine
lists
obtained
via
fair
re-ranker
three
queries
times
three
image
embedding
models
oracle
lists
train
adversarial
attacks
first
remove
330
images
oracle
lists
original
dataset
leaving
362
images
362
images
split
randomly
training
testing
sets
ratio
train
cgap
models
possible
combinations
training
objectives
demographic
inference
algorithms
di
described
detail
ran
experiments
pytorch
cuda
backend
two
nvidia
rtx-a6000
gpus
trained
cgap
models
10
epochs
norm7
bound
set
10
describe
different
training
inference
combinations
table
shows
summary
different
settings
involved
training
testing
cgap
attacks
4.2
embedding
algorithm
discuss
3.2
fmmr
requires
image
embeddings
authors
original
paper
used
pretrained
inceptionv3
model
also
adopt
additionally
test
performance
fmmr
using
embeddings
generated
pretrained
faster
r-cnn
resnet
models
models
trained
standard
image
classification
tasks
inherent
concept
demographic
groups
4.2
attack
training
algorithm
detailed
3.3
train
cgap
models
induce
adversary-selected
misclassifications
two
target
demographic
inference
models
denoted
di
deepface
90
fairface
82
models
trained
demographic
inference
overlap
training
objective
image
embedding
models
fmmr
similarity
architecture
demographic
inference
fmmr
embedding
models
fairface
uses
resnet
architecture
4.2
training
objectives
discussed
3.3
select
certain
subpopulations
systematically
misclassified
two
di
described
four
cgaps
train
induce
misclassifications
following
source-target
pairs
light
men
every
subgroup
perturbed
predicted
light
men
light
men
light
men
arbitrarily
misclassified
dark
men
light
men
dark
men
misclassified
light
men
light
men
dark
men
light
men
misclassified
dark
men
4.2
attack
probability
pr
strong
assumption
adversary
can
perturb
entire
image
database
victim
search
engine
possible
search
engine
malicious
utterly
compromised
instead
measure
effect
attack
attacker
may
perturb
pr
20
50
70
100
image
database
relevant
query
small
number
queries
targeted
images
required
run
attack
4.2
top
ranking
sensitive
position
bias
39
80
measure
different
lengths
top
list
ranging
top
10
top
50
gauge
attack
impact
fair
ranking
algorithms
final
list
sizes
vary
4.3
evaluation
metrics
evaluate
impact
attacks
use
three
metrics
aim
measure
representation
bias
attention
exposure
bias
loss
ranking
utility
due
re-ranking
additionally
introduce
summarizing
meta-metric
enables
us
clearly
present
impact
attacks
respect
metric
4.3
skew
metric
use
measure
bias
representation
called
skew
38
39
ranked
list
skew
attribute
value
ai
position
defined
pœÑ
ai
skewai
pq
ai
pœÑ
ai
represents
fraction
members
attribute
ai
among
top
items
pq
ai
represents
fraction
members
subgroup
ai
overall
population
ideal
fair
representation
skew
value
subgroups
equal
indicating
representation
among
top
items
exactly
matches
proportion
overall
population
dark-skinned
women
appear
search
results
query
female
tennis
player
seems
reflect
stereotypical
bias
36
within
learned-word
representations
mmt
model
absolute
distance
pixel
space
one
pixel
changed
pixel
can
change
value
10
color
channel
642
4.3
attention
even
subgroups
fairly
represented
top
ranked
items
list
relative
position
ranked
items
adds
another
dimension
bias
unequal
exposure
previous
5.0
2.60
4.5
2.55
2.50
facct
22
june
21
24
2022
seoul
republic
korea
3.4
3.3
utility
score
2.65
utility
score
utility
score
subverting
fair
image
search
generative
adversarial
perturbations
4.0
3.5
3.0
2.45
query
tennis
player
light
men
3.1
3.0
2.9
2.5
2.40
3.2
2.8
query
person
eating
pizza
query
person
table
light
women
dark
women
dark
men
figure
utility
relevance
score
group
size
distribution
within
top
40
baseline
search
results
three
queries
black
dots
represent
average
utility
score
group
circle
size
represents
group
size
dark-skinned
women
appear
top
40
baseline
results
tennis
player
query
studies
69
72
shown
people
attention
rapidly
decreases
scan
list
attention
given
higher
ranking
items
ultimately
dropping
zero
attention
study
model
attention
decay
using
geometric
distribution
done
prior
work
sapiezynski
et
al
80
compute
attention
rank
attentionp
100
useful
since
group
sizes
vary
second
comparing
group
gets
minimum
boost
ensures
metric
presents
widest
fairness
disparity
regardless
total
number
groups
purposes
paper
set
light
men
socially
historically
advantaged
group
large
light
men
indicates
attack
causes
ranking
unfairly
boosted
relative
least
privileged
group
make
sure
fairness
impacts
observe
due
effectiveness
attack
re-ranking
algorithms
values
change
ndcg
measured
oracle
fairly
re-ranked
lists
compare
oracle
list
results
attack
probability
pr
will
fraction
total
attention
given
top
search
result
choice
application
specific
paper
fixed
0.36
based
study
44
reported
top
result
google
search
receives
36.4
total
clicks
calculate
average
attention
per
subgroup
average
attentionai
att
aœÑk
ai
ai
ideally
perfectly
fair
ranked
list
subgroups
receive
equal
average
attention
4.3
normalized
discounted
cumulative
gain
ndcg
widely
used
measure
ir
evaluate
quality
search
rankings
49
defined
ndcg
siœÑ
1x
lo–¥2
5.1
siœÑ
utility
score
mmt
retrieval
model
th
element
ranked
list
lo–¥
ndcg
scores
range
latter
capturing
ideal
search
results
4.3
summarizing
metric
purpose
quantifying
much
unfair
advantage
attacks
confer
members
majority
class
relative
classes
define
new
meta-metric
called
attack
effectiveness
given
metric
skew
attention
subgroup
defined
change
subgroup
minimum
change
subgroups
chose
formulation
two
reasons
first
comparing
percentage
changes
makes
metric
scale
invariant
643
results
section
evaluate
impact
attacks
fairness
guarantees
fmmr
set
results
examine
attack
effectiveness
varies
one
particular
variable
top
image
embedding
model
etc
attack
probability
pr
fraction
images
adversarial
control
varies
focusing
particular
variable
present
results
averaged
across
variables
three
queries
top
pr
begin
evaluating
impact
attacks
vary
length
top
list
fraction
images
query
list
adversarial
control
pr
plotted
figure
varying
pr
expected
effect
adversary
control
image
database
attacks
become
effective
skew
attention
increase
adversary
able
control
100
images
query
list
attacks
especially
strong
increasing
attention
unfairness
50
values
even
20
control
adversary
can
increase
attention
unfairness
30
recall
pr
measures
fraction
query
list
compromised
35
images
can
compromised
pr
0.5
person
eating
pizza
query
varying
also
impacts
ranking
fairness
increases
attention
unfairness
increases
modestly
skew
unfairness
decreases
skew
unfairness
decreases
indicates
composition
items
search
results
becomes
fairer
length
list
avijit
ghosh
matthew
jagielski
christo
wilson
30
20
10
0.1
50
change
ndcg
attention
light
men
skew
light
men
facct
22
june
21
24
2022
seoul
republic
korea
40
30
0.2
0.3
0.4
0.5
0.6
10
20
30
40
50
10
20
30
top
40
50
10
20
30
top
skew
40
50
top
attention
ndcg
attack
probability
0.2
0.5
0.7
1.0
figure
attack
effectiveness
function
attack
probability
pr
list
length
higher
effective
attack
search
results
favorable
light-skinned
men
unfairness
increases
pr
increases
yet
almost
impact
ranking
quality
ndcg
increases
skew
less
impacted
attention
impacted
somewhat
14
12
10
0.2
0.5
0.7
1.0
0.2
70
60
0.0
change
ndcg
attention
light
men
skew
light
men
16
50
40
30
20
0.2
0.4
0.6
10
attack
probability
0.8
0.2
0.5
0.7
1.0
0.2
attack
probability
skew
attention
f-rcnn
0.5
0.7
1.0
attack
probability
ndcg
inceptionv3
resnet
figure
attack
effectiveness
stable
model
used
fmmr
embedding
changed
resnet
embeddings
slightly
robust
attack
f-rcnn
slightly
less
robust
interestingly
resnet
robustness
spite
similar
model
architecture
fairface
grows
however
attack
able
cause
fmmr
reorder
list
top-most
items
remain
unfair
regardless
attention
unfairness
exhibits
less
dependency
lastly
observe
attacks
stealthy
regardless
pr
ndcg
never
changes
0.7
meaning
attack
effectively
zero
impact
search
result
relevance
5.2
cases
light
men
end
significant
unfair
advantage
explain
seeming
contradiction
example
figure
essence
using
gap
misclassify
people
minority
group
majority
group
reduces
minority
group
overall
share
population
since
group
fairness
case
based
overall
population
distribution
causes
fmmr
rerank
fewer
minority
group
members
top
search
results
based
results
figure
appears
way
advantage
minority
group
attacks
choice
training
objective
evaluate
attack
impact
fairness
four
cgap
models
one
misclassifies
dark
men
light
men
one
misclassifying
light
men
dark
men
relaxed
cgap
models
misclassify
people
light
men
light
men
groups
show
attacks
effectiveness
figure
attacks
performs
similarly
well
harming
fairness
terms
skew
attention
remaining
stealthy
terms
ndcg
one
surprising
observation
misclassifying
dark
men
light
men
performs
similarly
exact
opposite
attack
644
5.3
choice
attack
training
algorithm
measure
attacks
effectiveness
gap
models
trained
deepface
fairface
demographic
inference
models
observe
attack
effectiveness
largely
independent
choice
inference
model
attacks
remain
stealthy
defer
plot
results
supplementary
material
figure
subverting
fair
image
search
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
korea
0.1
14
12
10
0.2
0.5
0.7
1.0
50
0.0
change
ndcg
attention
light
men
skew
light
men
16
40
30
20
10
0.2
0.3
0.4
0.5
0.6
0.2
attack
probability
0.5
0.7
1.0
0.2
attack
probability
skew
light
men
0.1
attention
light
men
0.5
0.7
1.0
attack
probability
ndcg
dark
men
light
men
light
men
dark
men
figure
attack
effectiveness
relatively
stable
gap
training
objective
changed
top
skew
1.50
1.50
1.25
0.5
light
dark
light
dark
dark
light
figure
example
showing
incorrect
group
allocation
direction
always
harms
minority
group
members
fair
ranking
shows
baseline
unfair
list
people
sorted
relevance
query
dark
people
top
shows
fair
ranking
produced
fmmr
proportion
light
dark
people
top
overall
population
light
people
images
perturbed
using
gap
half
grouped
dark
people
fmmr
moves
relevant
dark
people
top
make
list
fair
case
relevant
dark
people
really
light
skinned
half
dark
people
perturbed
using
gap
grouped
light
people
fmmr
appears
reduce
overall
population
dark
people
needs
move
one
dark
person
top
make
list
proportionally
fair
note
light
people
grouped
dark
dark
people
grouped
light
ranking
remain
unfair
baseline
shown
5.4
choice
query
lastly
examine
effectiveness
attacks
three
different
queries
plot
results
figure
observe
attacks
successful
effectiveness
varies
query
differences
attack
effectiveness
explained
underlying
distributions
population
utility
scores
see
figure
tennis
results
exhibit
unfairness
post-attack
unfair
begin
difference
utility
scores
light
dark
skinned
people
greatest
tennis
results
compared
queries
contrast
pizza
results
exhibit
robustness
attack
terms
attention
645
results
among
queries
minority
people
higher
utility
scores
majority
people
baseline
results
figure
discussion
study
develop
novel
adversarial
ml
attack
fair
ranking
algorithms
use
fairness-aware
text-to-image
retrieval
case
study
demonstrate
attack
effectiveness
unfortunately
find
attack
successful
subverting
fairness
algorithm
search
engine
across
extensive
set
attack
variations
almost
zero
impact
search
result
relevance
facct
22
june
21
24
2022
seoul
republic
korea
avijit
ghosh
matthew
jagielski
christo
wilson
15
10
0.1
80
0.0
change
ndcg
attention
light
men
skew
light
men
20
60
40
20
0.1
0.2
0.3
0.4
0.5
0.6
0.2
0.5
0.7
1.0
attack
probability
skew
query
tennis
player
0.2
0.5
0.7
1.0
attack
probability
0.2
0.5
0.7
1.0
attack
probability
attention
ndcg
query
person
eating
pizza
query
person
table
figure
attacks
effective
three
queries
effectiveness
varies
relation
underlying
population
utility
score
distributions
see
figure
although
present
single
case
study
argue
attack
likely
generalize
adopt
strong
threat
model
demonstrate
attacks
succeed
even
attacker
poison
training
data
access
victim
whole
image
corpus
know
models
used
victim
thus
attack
highly
likely
succeed
cases
threat
model
relaxed
fairness
algorithm
used
victim
known
alarmingly
work
shows
adversary
can
attack
fairness
algorithm
like
fmmr
even
explicitly
rely
demographic
inference
thus
highly
likely
attack
will
also
succeed
ranking
algorithm
rely
demographic
inference
model
even
model
highly
accurate
explore
possibility
best
ability
hope
research
will
raise
awareness
spur
research
vulnerabilities
fair
algorithms
results
highlight
absence
safeguards
fairness
interventions
can
potentially
weaponized
malicious
parties
tool
oppression
absence
fair
ml
development
methods
algorithms
robust
adversarial
attacks
may
possible
policymakers
safely
mandate
use
fair
ml
algorithms
practice
believe
future
work
needed
develop
robust
fair
ml
interventions
adopt
broad
view
possible
mitigations
spanning
value
sensitive
design
34
methods
help
developers
preemptively
identify
attack
surface
plan
defenses
32
models
hardened
adversarial
perturbation
techniques
43
auditing
checklists
76
tools
help
developers
notice
triage
attacks
work
highlights
achieving
demographic
fairness
requires
high-quality
demographic
data
allowing
adversary
influence
demographic
meta-data
underlying
flaw
enables
attack
succeed
demographic
data
may
sourced
data
subjects
full
knowledge
consent
human
labelers
10
caveat
labels
will
need
de-biased
100
6.1
limitations
study
number
limitations
first
analysis
limited
two
discrete
racial
two
discrete
gender
categories
although
646
cgap
attack
tailored
select
group
unclear
well
attack
perform
situations
discrete
protected
groups
groups
continuous
attributes
people
multiple
partial
group
memberships
population
distributions
varied
significantly
dataset
second
dataset
sufficiently
large
demonstrate
attack
smaller
databases
real-world
image
search
engines
retrieve
third
proof-of-concept
tuned
attack
fairface
deepface
unclear
well
cgap
attack
generalize
models
real-world
deployed
systems
fourth
observe
figure
attack
successful
generating
unfairness
favor
already-advantaged
groups
limitation
way
diminishes
potential
real-world
harm
attack
inflict
marginalized
populations
finally
shown
figure
attack
effectiveness
varies
query
real
world
scenarios
attacker
mitigate
extent
devoting
resources
towards
perturbing
images
relevant
high-value
queries
unclear
much
attackers
effort
need
vary
practice
given
make
attempt
attack
deployed
search
engines
6.2
ethics
work
present
concrete
attack
fair
ranking
system
like
adversarial
attack
research
methods
can
potentially
misused
bad
actors
however
also
necessitates
research
since
documenting
vulnerabilities
first
step
towards
mitigating
best
knowledge
exception
shopify
linkedin
services
known
employ
fair
ranking
systems
practice
meaning
currently
exists
window
opportunity
preemptively
identify
attacks
raise
awareness
deploy
mitigations
prior
work
adversarial
ml
attacks
fairness
made
source
code
publicly
available
71
however
attack
tools
dual-use
opted
take
conservative
approach
will
share
source
code
researchers
research
universities
identified
taxonomies
like
carnegie
classification
companies
develop
potentially
vulnerable
products
given
attack
can
used
legitimate
blackbox
algorithm
auditing
purposes
opt
restrict
may
access
subverting
fair
image
search
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
korea
source
code
rather
uses
may
put
towards
opinion
process
will
facilitate
follow-up
research
mitigation
development
algorithm
auditing
without
supplying
bad
actors
ready-made
attack
tools
like
many
works
computer
vision
field
rely
images
crowdsourced
inferred
demographic
labels
processes
criticized
lack
consent
41
way
operationalize
identity
81
harm
may
cause
mis-identification
14
problems
reinforce
need
highquality
consensual
demographic
data
means
improve
ethical
norms
defend
adversarial
ml
attacks
acknowledgments
funding
support
work
supported
2019
sloan
fellowship
nsf
grants
iis-1553088
iis-1910064
opinions
findings
conclusions
recommendations
expressed
herein
authors
necessarily
reflect
views
funders
legal
department
google
participated
review
approval
manuscript
asked
authors
add
additional
examples
image
search
engines
besides
google
image
search
complied
aside
authors
google
role
design
conduct
study
access
collection
data
analysis
interpretation
data
preparation
manuscript
authors
thank
jane
adams
michael
davinroy
jeffrey
gleason
anonymous
reviewers
comments
references
116th
congress
2019
2020
2231
algorithmic
accountability
act
2019
https://www.congress.gov/bill/116th-congress/house-bill/2231.
dzifa
adjaye-gbewonyo
robert
bednarczyk
robert
davis
saad
omer
2014
using
bayesian
improved
surname
geocoding
method
bisg
create
working
classification
race
ethnicity
diverse
managed
care
population
validation
study
health
services
research
49
2014
268
283
alekh
agarwal
miroslav
dud√≠k
zhiwei
steven
wu
2019
fair
regression
quantitative
definitions
reduction-based
algorithms
arxiv
preprint
arxiv
1905.12843
2019
facebook
ai
2021
re
using
fairness
flow
help
build
ai
works
better
everyone
facebook
ai
https://ai.facebook.com/blog/how-wereusing-fairness-flow-to-help-build-ai-that-works-better-for-everyone/.
naveed
akhtar
jian
liu
ajmal
mian
2018
defense
universal
adversarial
perturbations
proceedings
ieee
conference
computer
vision
pattern
recognition
3389
3398
mckane
andrus
elena
spitzer
jeffrey
brown
alice
xiang
2021
can
measure
can
understand
challenges
demographic
data
procurement
pursuit
fairness
proceedings
2021
acm
conference
fairness
accountability
transparency
virtual
event
canada
facct
21
association
computing
machinery
new
york
ny
usa
249
260
https
doi
org
10.1145
3442188.3445888
julia
angwin
jeff
larson
surya
mattu
lauren
kirchner
2019
machine
bias
software
used
across
country
predict
future
criminals
biased
blacks
2016
url
https://www.
propublica
org
article
machinebias-risk-assessments-in-criminal-sentencing
2019
anish
athalye
nicholas
carlini
david
wagner
2018
obfuscated
gradients
give
false
sense
security
circumventing
defenses
adversarial
examples
international
conference
machine
learning
pmlr
274
283
solon
barocas
andrew
selbst
2016
big
data
disparate
impact
calif
rev
104
2016
671
10
sid
basu
ruthie
berman
adam
bloomston
john
campbell
anne
diaz
nanako
era
benjamin
evans
sukhada
palkar
skyler
wharton
2020
measuring
discrepancies
airbnb
guest
acceptance
rates
using
anonymized
demographic
data
airbnb
https://news.airbnb.com/wp-content/uploads/sites/4/2020/06/
project-lighthouse-airbnb-2020-06-12
pdf
11
thorsten
beck
patrick
behr
andreas
madestam
2018
sex
credit
gender
bias
lending
journal
banking
finance
87
2018
647
12
melika
behjati
seyed-mohsen
moosavi-dezfooli
mahdieh
soleymani
baghshah
pascal
frossard
2019
universal
adversarial
attacks
text
classifiers
icassp
2019
2019
ieee
international
conference
acoustics
speech
signal
processing
icassp
ieee
7345
7349
13
rachel
ke
bellamy
kuntal
dey
michael
hind
samuel
hoffman
stephanie
houde
kalapriya
kannan
pranay
lohia
jacquelyn
martino
sameep
mehta
aleksandra
mojsiloviƒá
et
al
2019
ai
fairness
360
extensible
toolkit
detecting
mitigating
algorithmic
bias
ibm
journal
research
development
63
2019
14
cynthia
bennett
cole
gleason
morgan
klaus
scheuerman
jeffrey
bigham
anhong
guo
alexandra
2021
complicated
negotiating
accessibility
mis
representation
image
descriptions
race
gender
disability
proceedings
2021
chi
conference
human
factors
computing
systems
yokohama
japan
chi
21
association
computing
machinery
new
york
ny
usa
article
375
19
pages
https://doi.org/10.1145/3411764.3445498
15
richard
berk
hoda
heidari
shahin
jabbari
matthew
joseph
michael
kearns
jamie
morgenstern
seth
neel
aaron
roth
2017
convex
framework
fair
regression
arxiv
preprint
arxiv
1706.02409
2017
16
alex
beutel
jilin
chen
tulsee
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
cristos
goodrow
2019
fairness
recommendation
ranking
pairwise
comparisons
kdd
https
arxiv
org
pdf
1903.00780
pdf
17
miranda
bogen
aaron
rieke
shazeda
ahmed
2020
awareness
practice
tensions
access
sensitive
attribute
data
antidiscrimination
proceedings
2020
conference
fairness
accountability
transparency
492
500
18
tolga
bolukbasi
kai-wei
chang
james
zou
venkatesh
saligrama
adam
kalai
2016
man
computer
programmer
woman
homemaker
debiasing
word
embeddings
advances
neural
information
processing
systems
4349
4357
19
tom
brown
dandelion
man√©
aurko
roy
mart√≠n
abadi
justin
gilmer
2017
adversarial
patch
arxiv
preprint
arxiv
1712.09665
2017
20
marc-etienne
brunet
colleen
alkalay-houlihan
ashton
anderson
richard
zemel
2019
understanding
origins
bias
word
embeddings
international
conference
machine
learning
803
811
21
joy
buolamwini
timnit
gebru
2018
gender
shades
intersectional
accuracy
disparities
commercial
gender
classification
conference
fairness
accountability
transparency
pmlr
77
91
22
consumer
financial
protection
bureau
2014
using
publicly
available
information
proxy
unidentified
race
ethnicity
report
available
https
files
consumerfinance
gov
201409_cfpb_report_proxy-methodology
pdf
2014
23
jaime
carbonell
jade
goldstein
1998
use
mmr
diversity-based
reranking
reordering
documents
producing
summaries
proceedings
21st
annual
international
acm
sigir
conference
research
development
information
retrieval
335
336
24
nicholas
carlini
david
wagner
2017
towards
evaluating
robustness
neural
networks
2017
ieee
symposium
security
privacy
sp
ieee
39
57
25
elisa
celis
lingxiao
huang
vijay
keswani
nisheeth
vishnoi
2021
fair
classification
noisy
protected
attributes
framework
provable
guarantees
international
conference
machine
learning
pmlr
1349
1361
26
elisa
celis
vijay
keswani
2020
implicit
diversity
image
summarization
proceedings
acm
human-computer
interaction
cscw2
2020
28
27
elisa
celis
anay
mehrotra
nisheeth
vishnoi
2021
fair
classification
adversarial
perturbations
arxiv
preprint
arxiv
2106.05964
2021
28
elisa
celis
damian
straszak
nisheeth
vishnoi
2018
ranking
fairness
constraints
45th
international
colloquium
automata
languages
programming
icalp
2018
schloss
dagstuhl-leibniz-zentrum
fuer
informatik
29
hongyan
chang
ta
duy
nguyen
sasi
kumar
murakonda
ehsan
kazemi
reza
shokri
2020
adversarial
bias
robustness
fair
machine
learning
arxiv
preprint
arxiv
2006.08669
2020
30
european
commission
proposal
regulation
laying
harmonised
rules
artificial
intelligence
artificial
intelligence
act
https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-layingdown-harmonised-rules-artificial-intelligence-artificial-intelligence.
31
hanjun
dai
hui
li
tian
tian
xin
huang
lin
wang
jun
zhu
le
song
2018
adversarial
attack
graph
structured
data
international
conference
machine
learning
pmlr
1115
1124
32
tamara
denning
batya
friedman
tadayoshi
kohno
2013
security
cards
security
threat
brainstorming
toolkit
university
washington
https://securitycards.cs.washington.edu/.
33
uk
office
artificial
intelligence
ethics
transparency
accountability
framework
automated
decision-making
https://www.gov.uk/government/publications/ethics-transparency-andaccountability-framework-for-automated-decision-making.
34
batya
friedman
david
hendry
2019
value
sensitive
design
shaping
technology
moral
imagination
mit
press
facct
22
june
21
24
2022
seoul
republic
korea
avijit
ghosh
matthew
jagielski
christo
wilson
35
yaroslav
ganin
evgeniya
ustinova
hana
ajakan
pascal
germain
hugo
larochelle
fran√ßois
laviolette
mario
marchand
victor
lempitsky
2016
domain-adversarial
training
neural
networks
journal
machine
learning
research
17
2016
2096
2030
36
nikhil
garg
londa
schiebinger
dan
jurafsky
james
zou
2018
word
embeddings
quantify
100
years
gender
ethnic
stereotypes
proceedings
national
academy
sciences
115
16
2018
e3635
e3644
37
gregor
geigle
jonas
pfeiffer
nils
reimers
ivan
vuliƒá
iryna
gurevych
2021
retrieve
fast
rerank
smart
cooperative
joint
approaches
improved
cross-modal
retrieval
arxiv
preprint
abs
2103.11920
2021
arxiv
2103.11920
http://arxiv.org/abs/2103.11920
38
sahin
cem
geyik
stuart
ambler
krishnaram
kenthapadi
2019
fairnessaware
ranking
search
recommendation
systems
application
linkedin
talent
search
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
2221
2231
39
avijit
ghosh
ritam
dutt
christo
wilson
2021
fair
ranking
meets
uncertain
inference
association
computing
machinery
new
york
ny
usa
1033
1043
https://doi.org/10.1145/3404835.3462850
40
avijit
ghosh
lea
genuit
mary
reagan
2021
characterizing
intersectional
group
fairness
worst-case
comparisons
proceedings
2nd
workshop
diversity
artificial
intelligence
aidbei
proceedings
machine
learning
research
vol
142
deepti
lamba
william
hsu
eds
pmlr
22
34
https://proceedings.mlr.press/v142/ghosh21a.html
41
william
gies
james
overby
nick
saraceno
jordan
frome
emily
york
ahmad
salman
2020
restricting
data
sharing
collection
facial
recognition
data
consent
user
systems
analysis
2020
systems
information
engineering
design
symposium
sieds
https
doi
org
10.1109
sieds49339
2020.9106661
42
naman
goel
mohammad
yaghini
boi
faltings
2018
non-discriminatory
machine
learning
convex
fairness
criteria
proceedings
aaai
conference
artificial
intelligence
43
ian
goodfellow
jonathon
shlens
christian
szegedy
2014
explaining
harnessing
adversarial
examples
arxiv
preprint
arxiv
1412.6572
2014
44
danny
goodwin
2011
top
google
result
gets
36.4
clicks
study
search
engine
watch
https://www.searchenginewatch.com/2011/04/21/top-googleresult-gets-36-4-of-clicks-study/.
45
kaiming
xiangyu
zhang
shaoqing
ren
jian
sun
2016
deep
residual
learning
image
recognition
proceedings
ieee
conference
computer
vision
pattern
recognition
770
778
46
xiangnan
zhankui
xiaoyu
du
tat-seng
chua
2018
adversarial
personalized
ranking
recommendation
41st
international
acm
sigir
conference
research
development
information
retrieval
355
364
47
lingxiao
huang
nisheeth
vishnoi
2019
stable
fair
classification
arxiv
preprint
arxiv
1902.07823
2019
48
matthew
jagielski
giorgio
severi
niklas
pousette
harger
alina
oprea
2020
subpopulation
data
poisoning
attacks
arxiv
preprint
arxiv
2006.14026
2020
49
kalervo
j√§rvelin
jaana
kek√§l√§inen
2002
cumulated
gain-based
evaluation
ir
techniques
acm
transactions
information
systems
tois
20
2002
422
446
50
toshihiro
kamishima
shotaro
akaho
hideki
asoh
jun
sakuma
2012
fairness-aware
classifier
prejudice
remover
regularizer
joint
european
conference
machine
learning
knowledge
discovery
databases
springer
35
50
51
chen
karako
putra
manggala
2018
using
image
fairness
representations
diversity-based
re-ranking
recommendations
adjunct
publication
26th
conference
user
modeling
adaptation
personalization
23
28
52
kimmo
karkkainen
jungseock
joo
2021
fairface
face
attribute
dataset
balanced
race
gender
age
bias
measurement
mitigation
proceedings
ieee
cvf
winter
conference
applications
computer
vision
1548
1558
53
alistair
knott
moving
towards
responsible
government
use
ai
new
zealand
https://digitaltechitp.nz/2021/03/22/moving-towardsresponsible-government-use-of-ai-in-new-zealand/.
54
alexey
kurakin
ian
goodfellow
samy
bengio
2016
adversarial
machine
learning
scale
arxiv
preprint
arxiv
1611.01236
2016
55
preethi
lahoti
alex
beutel
jilin
chen
kang
lee
flavien
prost
nithum
thain
xuezhi
wang
ed
chi
2020
fairness
without
demographics
adversarially
reweighted
learning
2020
56
kristina
lerman
anon
plangprasopchok
chio
wong
2007
personalizing
image
search
results
flickr
intelligent
information
personalization
2007
57
jie
li
rongrong
ji
hong
liu
xiaopeng
hong
yue
gao
qi
tian
2019
universal
perturbation
attack
image
retrieval
proceedings
ieee
cvf
international
conference
computer
vision
4899
4908
58
xiujun
li
xi
yin
chunyuan
li
pengchuan
zhang
xiaowei
hu
lei
zhang
lijuan
wang
houdong
hu
li
dong
furu
wei
et
al
2020
oscar
objectsemantics
aligned
pre-training
vision-language
tasks
european
conference
computer
vision
springer
121
137
648
59
tsung-yi
lin
michael
maire
serge
belongie
james
hays
pietro
perona
deva
ramanan
piotr
doll√°r
lawrence
zitnick
2014
microsoft
coco
common
objects
context
european
conference
computer
vision
springer
740
755
60
yanpei
liu
xinyun
chen
chang
liu
dawn
song
2016
delving
transferable
adversarial
examples
black-box
attacks
arxiv
preprint
arxiv
1611.02770
2016
61
zhuoran
liu
zhengyu
zhao
martha
larson
2019
afraid
adversarial
queries
impact
image
modifications
content-based
image
retrieval
proceedings
2019
international
conference
multimedia
retrieval
ottawa
canada
icmr
19
association
computing
machinery
new
york
ny
usa
306
314
https://doi.org/10.1145/3323873.3325052
62
joshua
loftus
chris
russell
matt
kusner
ricardo
silva
2018
causal
reasoning
algorithmic
fairness
arxiv
preprint
arxiv
1805.05859
2018
63
jiasen
lu
dhruv
batra
devi
parikh
stefan
lee
2019
vilbert
pretraining
task-agnostic
visiolinguistic
representations
vision-and-language
tasks
arxiv
preprint
arxiv
1908.02265
2019
64
ninareh
mehrabi
muhammad
naveed
fred
morstatter
aram
galstyan
2020
exacerbating
algorithmic
bias
fairness
attacks
arxiv
preprint
arxiv
2012.08723
2020
65
aditya
krishna
menon
robert
williamson
2018
cost
fairness
binary
classification
conference
fairness
accountability
transparency
107
118
66
seyed-mohsen
moosavi-dezfooli
alhussein
fawzi
omar
fawzi
pascal
frossard
2017
universal
adversarial
perturbations
arxiv
1610.08401
cs
cv
67
seyed-mohsen
moosavi-dezfooli
alhussein
fawzi
pascal
frossard
2016
deepfool
simple
accurate
method
fool
deep
neural
networks
proceedings
ieee
conference
computer
vision
pattern
recognition
2574
2582
68
marco
morik
ashudeep
singh
jessica
hong
thorsten
joachims
2020
controlling
fairness
bias
dynamic
learning-to-rank
arxiv
preprint
arxiv
2005.14713
2020
69
ankan
mullick
sayan
ghosh
ritam
dutt
avijit
ghosh
abhijnan
chakraborty
2019
public
sphere
2.0
targeted
commenting
online
news
media
european
conference
information
retrieval
springer
180
187
70
razieh
nabi
ilya
shpitser
2018
fair
inference
outcomes
proceedings
aaai
conference
artificial
intelligence
aaai
conference
artificial
intelligence
vol
2018
nih
public
access
1931
71
vedant
nanda
samuel
dooley
sahil
singla
soheil
feizi
john
dickerson
2021
fairness
robustness
investigating
robustness
disparity
deep
learning
proceedings
2021
acm
conference
fairness
accountability
transparency
466
477
72
jakob
nielsen
2003
usability
101
introduction
usability
jakob
nielsen
alertbox
73
government
canada
responsible
use
artificial
intelligence
ai
https://www.canada.ca/en/government/system/digital-government/
digital-government-innovations
responsible-use-ai
html
74
nicolas
papernot
fartash
faghri
nicholas
carlini
ian
goodfellow
reuben
feinman
alexey
kurakin
cihang
xie
yash
sharma
tom
brown
aurko
roy
et
al
2016
technical
report
cleverhans
v2
1.0
adversarial
examples
library
arxiv
preprint
arxiv
1610.00768
2016
75
omid
poursaeed
isay
katsman
bicheng
gao
serge
belongie
2018
generative
adversarial
perturbations
proceedings
ieee
conference
computer
vision
pattern
recognition
4422
4431
76
inioluwa
deborah
raji
andrew
smart
rebecca
white
margaret
mitchell
timnit
gebru
ben
hutchinson
jamila
smith-loud
daniel
theron
parker
barnes
2020
closing
ai
accountability
gap
defining
end-to-end
framework
internal
algorithmic
auditing
proc
fat
77
nisarg
raval
manisha
verma
2020
one
word
time
adversarial
attacks
retrieval
models
arxiv
preprint
arxiv
2008.02197
2020
78
shaoqing
ren
kaiming
ross
girshick
jian
sun
2015
faster
r-cnn
towards
real-time
object
detection
region
proposal
networks
advances
neural
information
processing
systems
28
2015
91
99
79
alexey
romanov
maria
de-arteaga
hanna
wallach
jennifer
chayes
christian
borgs
alexandra
chouldechova
sahin
geyik
krishnaram
kenthapadi
anna
rumshisky
adam
tauman
kalai
2019
name
reducing
bias
bios
without
access
protected
attributes
arxiv
preprint
arxiv
1904.05233
2019
80
piotr
sapiezynski
wesley
zeng
ronald
robertson
alan
mislove
christo
wilson
2019
quantifying
impact
user
attentionon
fair
group
representation
ranked
lists
companion
proceedings
2019
world
wide
web
conference
553
562
81
morgan
klaus
scheuerman
kandrea
wade
caitlin
lustig
jed
brubaker
2020
ve
taught
algorithms
see
identity
constructing
race
gender
image
databases
facial
analysis
cscw1
article
058
may
2020
35
pages
https://doi.org/10.1145/3392866
82
sefik
ilkin
serengil
alper
ozpinar
2020
lightface
hybrid
deep
face
recognition
framework
2020
innovations
intelligent
systems
applications
conference
asyu
ieee
subverting
fair
image
search
generative
adversarial
perturbations
facct
22
june
21
24
2022
seoul
republic
korea
83
ali
shafahi
ronny
huang
mahyar
najibi
octavian
suciu
christoph
studer
tudor
dumitras
tom
goldstein
2018
poison
frogs
targeted
clean-label
poisoning
attacks
neural
networks
arxiv
preprint
arxiv
1804.00792
2018
84
ali
shafahi
mahyar
najibi
amin
ghiasi
zheng
xu
john
dickerson
christoph
studer
larry
davis
gavin
taylor
tom
goldstein
2019
adversarial
training
free
arxiv
preprint
arxiv
1904.12843
2019
85
shawn
shan
emily
wenger
jiayun
zhang
huiying
li
haitao
zheng
ben
zhao
2020
fawkes
protecting
privacy
unauthorized
deep
learning
models
29th
usenix
security
symposium
usenix
security
20
1589
1604
86
ashudeep
singh
thorsten
joachims
2018
fairness
exposure
rankings
proceedings
24th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
2219
2228
87
david
solans
battista
biggio
carlos
castillo
2020
poisoning
attacks
algorithmic
fairness
arxiv
preprint
arxiv
2004.07401
2020
88
christian
szegedy
wei
liu
yangqing
jia
pierre
sermanet
scott
reed
dragomir
anguelov
dumitru
erhan
vincent
vanhoucke
andrew
rabinovich
2015
going
deeper
convolutions
proceedings
ieee
conference
computer
vision
pattern
recognition
89
christian
szegedy
wojciech
zaremba
ilya
sutskever
joan
bruna
dumitru
erhan
ian
goodfellow
rob
fergus
2013
intriguing
properties
neural
networks
arxiv
preprint
arxiv
1312.6199
2013
90
yaniv
taigman
ming
yang
marc
aurelio
ranzato
lior
wolf
2014
deepface
closing
gap
human-level
performance
face
verification
proceedings
ieee
conference
computer
vision
pattern
recognition
1701
1708
91
florian
tramer
nicholas
carlini
wieland
brendel
aleksander
madry
2020
adaptive
attacks
adversarial
example
defenses
arxiv
preprint
arxiv
2002.08347
2020
92
florian
tram√®r
nicolas
papernot
ian
goodfellow
dan
boneh
patrick
mcdaniel
2017
space
transferable
adversarial
examples
arxiv
2017
https://arxiv.org/abs/1704.03453
93
alexander
turner
dimitris
tsipras
aleksander
madry
2018
clean-label
backdoor
attacks
2018
94
sriram
vasudevan
krishnaram
kenthapadi
2020
lift
scalable
framework
measuring
fairness
ml
applications
proceedings
29th
acm
international
conference
information
knowledge
management
2773
2780
95
yevgeniy
vorobeychik
murat
kantarcioglu
2018
adversarial
machine
learning
synthesis
lectures
artificial
intelligence
machine
learning
12
2018
169
96
christo
wilson
avijit
ghosh
shan
jiang
alan
mislove
lewis
baker
janelle
szary
kelly
trindel
frida
polli
2021
building
auditing
fair
algorithms
case
study
candidate
screening
proceedings
2021
acm
conference
fairness
accountability
transparency
666
677
97
zuxuan
wu
ser-nam
lim
larry
davis
tom
goldstein
2020
making
invisibility
cloak
real
world
adversarial
attacks
object
detectors
european
conference
computer
vision
springer
17
98
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
proceedings
2017
acm
conference
information
knowledge
management
1569
1578
99
meike
zehlike
carlos
castillo
2020
reducing
disparate
exposure
ranking
learning
rank
approach
proceedings
web
conference
2020
2849
2855
100
dora
zhao
angelina
wang
olga
russakovsky
2021
understanding
evaluating
racial
biases
image
captioning
international
conference
computer
vision
iccv
101
mo
zhou
zhenxing
niu
le
wang
qilin
zhang
gang
hua
2020
adversarial
ranking
attack
defense
arxiv
preprint
arxiv
2002.11293
2020
supplementary
material
comparison
detconstsort
fmmr
section
compare
performance
two
fair
re-rankers
presence
gap
attack
already
described
details
first
algorithm
fmmr
3.2
1.1
detconstsort
second
algorithm
detconstsort
38
developed
currently
deployed
linkedin
talent
search
system
unlike
fmmr
detconstsort
requires
access
demographic
labels
items
trying
fairly
re-rank
detconstsort
rearranges
given
list
items
particular
rank
attribute
attribute
present
least
pa
times
ranked
list
pa
proportion
items
list
attribute
detconstsort
also
re-sorts
items
within
relevance
criteria
items
better
utility
scores
placed
higher
ranked
list
much
possible
maintaining
desired
attribute
ratio
thus
aims
solve
deterministic
interval
constrained
sorting
problem
ground-truth
demographic
labels
unavailable
detconstsort
may
instead
utilize
labels
sourced
demographic
inference
model
recent
work
however
shown
detconstsort
sensitive
errors
demographic
labels
one
example
errors
inaccurate
inferences
39
1.2
evaluation
results
present
results
gap
attacks
search
engine
uses
detconstsort
fmmr
fair
re-ranker
respectively
figure
results
averaged
across
three
queries
multiple
values
etc
detconstsort
skew
attention
metrics
impacted
attack
can
clearly
seen
comparing
values
pr
perturbed
images
values
pr
detconstsort
skew
attention
starts
high
unfair
pr
change
pr
increases
correct
interpretation
results
detconstsort
resilient
attack
rather
correct
interpretation
detconstsort
starts
unfair
due
use
inaccurate
inferred
demographic
data
39
attack
unable
make
unfairness
worse
thus
find
prerequisite
evaluating
success
attacks
detconstsort
accurate
demographic
inference
model
developing
models
still
active
area
research
out-of-scope
work
accurate
demographic
inference
model
designed
future
however
must
designed
adversarial
robustness
mind
prevent
attacks
choice
queries
facilitate
experiments
chose
select
search
query
terms
provide
sizeable
list
images
looked
list
terms
coco
image
captions
excluding
english
stop
words
words
related
ethnicity
gender
following
table
shows
top
terms
information
composed
three
queries
given
sitting
tennis
table
person
pizza
etc
among
popular
terms
649
facct
22
june
21
24
2022
seoul
republic
korea
avijit
ghosh
matthew
jagielski
christo
wilson
60
12
10
0.2
0.5
0.7
0.1
40
30
20
10
1.0
0.0
50
change
ndcg
attention
light
men
skew
light
men
14
0.2
0.3
0.4
0.5
0.2
attack
probability
0.5
0.7
1.0
0.2
attack
probability
skew
attention
deepface
0.5
0.7
1.0
attack
probability
ndcg
fairface
figure
gap
models
trained
different
demographic
inference
algorithms
offer
similar
attack
effectiveness
60
30
25
0.0
0.2
0.5
0.7
attack
probability
skew
1.0
80
0.2
change
ndcg
attention
light
men
skew
light
men
150
120
90
60
40
20
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.5
0.7
1.0
attack
probability
0.2
0.5
0.7
attack
probability
attention
detconstsort
0.0
ndcg
fmmr
figure
detconstsort
poor
performance
even
without
attack
making
results
uninteresting
term
count
sitting
standing
people
holding
large
person
street
table
small
tennis
riding
train
young
red
baseball
pizza
55084
44121
42133
29055
25305
25123
21609
20775
20661
19718
18809
18287
17767
17522
15362
11163
table
common
gender
race
unrelated
caption
terms
evaluation
dataset
650
1.0