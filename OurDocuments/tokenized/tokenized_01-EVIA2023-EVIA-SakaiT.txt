proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
doi
https://doi.org/10.20736/0002001350
fairness-based
evaluation
of
conversational
search
pilot
study
tetsuya
sakai
waseda
university
japan
tetsuyasakai@acm.org
abstract
related
work
2.1
gfr
group
fairness
and
relevance
ntcir-17
introduced
the
fairweb-1
task
which
evaluated
web
page
rankings
in
terms
of
both
relevance
and
group
fairness
the
present
study
shows
how
their
evaluation
framework
can
be
extended
for
the
evaluation
of
multi-turn
textual
conversational
search
systems
by
using
the
full
test
topic
set
of
fairweb-1
to
harvest
actual
usersystem
conversations
from
the
new
bing
and
google
bard
we
demonstrate
how
series
of
system
turns
can
be
evaluated
using
our
evaluation
framework
which
we
call
gfrc
group
fairness
and
relevance
of
conversations
in
addition
based
on
observations
from
our
pilot
experiment
we
briefly
discuss
few
open
questions
in
human-in-the-loop
evaluation
of
conversational
search
in
general
given
different
attribute
sets
sets
of
groups
to
consider
for
group
fairness
the
gfr
score
for
evaluating
ranked
list
of
documents
is
defined
as
follows
25
gfr
decay
utility
𝑤𝑚
distrsim
here
decay
denotes
the
err-based
decay
function
defined
over
the
ranked
list
utility
denotes
the
utility
of
the
search
engine
result
page
for
the
group
of
users
who
abandon
the
serp
at
rank
and
distrsim𝑚
is
the
similarity
between
the
distribution
over
groups
in
the
𝑚-th
attribute
set
achieved
by
the
top
documents
in
the
serp
and
the
target
distribution
for
the
same
attribute
set
the
weights
𝑤𝑖
balance
the
utility
and
distrsim
at
the
ntcir-17
fairweb-1
task
movie
topics
mtopics
had
attribute
sets
ratings
number
of
ratings
in
imdb
and
origin
geographical
region
based
on
country
of
origin
in
imdb
researcher
topics
r-topics
also
had
attribute
sets
hindex
google
scholar
h-index
and
gender
whether
he
or
she
was
used
in
the
researcher
biography
or
not
youtube
topics
y-topics
had
attribute
set
subscs
number
of
subscribers
of
the
content
uploader
participating
groups
were
encouraged
to
develop
systems
that
provide
more
exposure
to
entities
movies
researchers
or
youtube
videos
that
have
not
received
much
of
it
and
achieve
high
serp
quality
in
terms
of
relevance
sakai
et
al
25
explain
that
gfr
is
measure
of
expected
user
experience
over
user
population
for
given
information
need
and
discuss
its
advantages
over
measure
used
at
the
trec
2022
fair
ranking
track
which
has
been
discontinued
one
important
advantage
of
gfr
is
that
it
compares
distributions
over
ordinal
groups
such
as
the
rating
hindex
and
subscs
groups
of
the
fairweb-1
task
using
appropriate
divergences
more
specifically
while
gfr
uses
jensen-shannon
divergence
jsd
for
nominal
groups
such
as
origin
and
gender
groups
of
the
fairweb-1
task
it
uses
normalised
match
distance
nmd
and
root
normalised
order-aware
divergence
rnod
for
ordinal
groups
20
introduction
for
the
past
few
decades
offline
web
search
evaluation
usually
meant
evaluating
ranked
list
of
urls
with
measures
such
as
ndcg
normalised
discounted
cumulative
gain
12
and
err
expected
reciprocal
rank
however
the
advent
of
conversational
search
engines
based
on
large
language
models
llms
is
rapidly
changing
the
research
landscape
in
search
in
various
search
scenarios
the
user
may
prefer
to
receive
direct
answers
from
conversational
search
engine
rather
than
list
of
urls
from
traditional
one
evaluating
series
of
textual
responses
from
conversational
search
is
of
utmost
importance
as
while
their
fluency
may
mislead
the
user
into
thinking
that
their
responses
are
trustworthy
and
fair
they
in
fact
hallucinate
often
and
may
be
biased
or
even
be
harmful
see
for
example
askell
et
al
liang
et
al
14
liu
et
al
16
sakai
21
within
the
traditional
ranked
list
evaluation
paradigm
the
ntcir17
fairweb-1
task
26
evaluated
participating
runs
based
not
only
on
relevance
for
the
benefit
of
the
searcher
but
also
on
group
fairness
for
the
benefit
of
the
items
being
ranked
or
their
stakeholders
to
this
end
they
employed
the
gfr
group
fairness
and
relevance
framework
25
the
present
study
shows
how
this
framework
can
be
extended
for
the
evaluation
of
multi-turn
textual
conversational
search
systems
by
using
the
full
test
topic
set
of
fairweb-1
to
harvest
actual
user-system
conversations
from
the
new
bing2
and
google
bard
we
demonstrate
how
series
of
system
turns
can
be
evaluated
using
our
evaluation
framework
which
we
call
gfrc
group
fairness
and
relevance
of
conversations
in
addition
based
on
observations
from
our
pilot
experiment
we
briefly
discuss
few
open
questions
in
human-in-the-loop
evaluation
of
conversational
search
in
general
2.2
s-measure
and
m-measure
s-measure
23
which
was
adopted
at
the
ntcir
1click
tasks
24
is
measure
for
evaluating
textual
summary
returned
in
response
to
query
one
novel
feature
of
s-measure
is
that
it
discounts
the
values
of
iunits
information
units
or
nuggets
which
represent
atomic
pieces
of
relevant
information
within
the
summary
based
on
the
nugget
position
just
like
ndcg
discounts
the
value
of
each
http://sakailab.com/fairweb1/
https://www.bing.com/
this
generalises
the
normalised
user
utility
of
sakai
and
robertson
22
https://bard.google.com/
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
document
based
on
its
rank
let
be
the
set
of
iunits
found
within
the
summary
and
for
each
iunit
let
pos
denote
the
position
of
within
the
summary
moreover
let
be
the
weight
of
which
reflects
its
relevance
grade
then
s-measure
can
be
expressed
as
follows
𝑆-measure
max
pos
should
be
as
simple
and
explainable
to
the
researchers
as
possible
and
our
proposed
method
is
based
on
this
view
proposed
evaluation
method
3.1
gfrc
generic
formulation
our
proposal
is
to
combine
the
ideas
of
gfr
and
s-measure
in
order
to
evaluate
series
of
textual
system
turns
based
on
both
relevance
and
group
fairness
suppose
that
we
want
to
evaluate
round
user-system
textual
conversation
𝑈𝑇
𝑆𝑇
let
𝑛𝑖
denote
the
𝑗-th
nugget
in
the
𝑖-th
system
turn
𝑆𝑖
more
specifically
we
define
𝑛𝑖
to
be
any
substring
of
𝑆𝑖
that
represents
relevant
piece
of
information
in
the
context
of
the
previous
turn
sequence
𝑈𝑖
as
well
as
the
previous
nuggets
in
𝑆𝑖
𝑛𝑖1
𝑛𝑖
for
convenience
hereafter
we
denote
𝑛𝑖
as
an
element
of
set
𝑆𝑖
furthermore
let
pw
denote
the
position-based
weight
of
nugget
where
the
position
is
defined
in
the
context
of
conversation
we
shall
introduce
an
instantiation
of
pw
in
section
3.2
let
denote
the
relevance
score
or
gain
value
of
nugget
we
define
the
relevance-based
score
of
conversation
as
here
is
normalisation
factor
and
is
parameter
that
reflects
user
patience
at
the
1click
tasks
where
japanese
summaries
were
evaluated
the
organisers
let
000
based
on
the
observation
that
the
average
reading
speed
of
japanese
text
is
around
500
characters
per
minute
and
the
assumption
that
the
user
needs
to
gather
information
within
two
minutes
thus
the
value
of
an
iunit
decreases
linearly
as
pos
becomes
larger
and
becomes
nought
when
pos
1000
ntcir-11
introduced
the
mobileclick
task
13
as
successor
of
the
1click
task
the
new
task
attempted
to
evaluate
two-layered
summaries
where
the
first-layer
summaries
contained
several
clickable
links
that
represented
possible
intents
for
given
query
the
intent
probabilities
were
regarded
as
the
transition
probabilities
from
the
first-layer
summary
to
one
of
the
second-layer
summaries
the
resultant
extension
of
s-measure
was
named
m-measure
while
the
two-layer
summary
framework
resembles
two-round
conversation
that
branches
out
depending
on
the
previous
turn
these
previous
ntcir
tasks
considered
the
relevance
of
information
only
unlike
the
present
study
2.3
pw
𝑛𝑖
𝑛𝑖
𝑛𝑖
𝑆𝑖
where
is
normalisation
factor
next
we
describe
how
the
same
conversation
can
be
evaluated
in
terms
of
group
fairness
given
attribute
sets
and
target
distribution
probability
mass
function
to
be
more
specific
𝐷𝑚
for
each
for
every
system
turn
that
contains
relevant
nugget
we
first
compute
an
achieved
distribution
𝐷𝑚
using
one
of
the
following
two
possible
methods
the
independent
distribution
method
computes
𝐷𝑚
based
solely
on
the
group
memberships
of
relevant
nuggets
contained
in
the
cumulative
distribution
method
computes
𝐷𝑚
based
not
only
on
relevant
nuggets
in
but
also
on
relevant
nuggets
observed
in
the
previous
system
turns
on
the
other
hand
for
any
system
turn
that
does
not
contain
relevant
nugget
we
can
either
ignore
it
or
treat
it
as
if
its
achieved
distribution
is
uniform
because
if
the
turn
does
not
mention
any
relevant
entity
it
is
not
introducing
any
bias
towards
any
entity
group
and
therefore
fair
let
pw
denote
the
position-based
weight
of
turn
we
shall
instantiate
it
later
the
group
fairness
gf
score
of
conversation
can
be
computed
as
other
related
work
the
trec
2022
conversational
assistance
track
17
evaluated
multi-turn
conversational
search
using
topic
tree
structure
while
the
conversations
are
evaluated
in
terms
of
relevance
conciseness
and
naturalness
they
did
not
consider
group
fairness
the
track
has
been
discontinued
inspired
by
the
aforementioned
s-measure
our
proposed
framework
uses
word
counts
as
the
basis
for
discounting
value
of
relevant
piece
of
information
nugget
in
the
instantiation
of
our
framework
that
is
discussed
in
the
present
study
all
words
from
conversation
including
those
from
user
turns
contribute
to
the
word
count
this
means
that
if
the
user
enters
long
query
that
is
considered
large
cost
and
the
values
of
systems
nuggets
decrease
accordingly
this
mechanism
is
related
to
the
discussion
of
typing
cost
or
cost
of
querying
in
the
economic
interaction
models
of
azzopardi
and
zuccon
earlier
studies
that
consider
the
cost
of
querying
in
terms
of
time
spent
include
baskaya
et
al
and
azzopardi
et
al
regarding
the
evaluation
of
llm-based
systems
one
obvious
approach
would
be
to
build
an
llm-based
evaluator
however
if
black
box
llm
is
evaluated
with
another
black
box
llm
it
is
our
opinion
that
we
may
not
be
able
to
learn
much
the
two
may
be
based
on
similar
training
data
with
similar
algorithms
and
therefore
the
evaluator
may
overrate
the
system
being
evaluated
moreover
it
may
be
difficult
for
an
end-to-end
llm-based
evaluator
to
explain
to
us
why
the
system
is
good
bad
and
where
exactly
the
problem
lies
see
also
faggioli
et
al
and
recent
dagstuhl
seminar
report
47
for
related
discussions
we
argue
that
evaluation
methods
gf
pw
𝑤𝑚
distrsim𝑚
𝐷𝑚
𝑆𝑖
𝐷𝑚
where
is
normalisation
factor
and
𝑤𝑚
is
weight
assigned
to
í𝑀
𝑤𝑚
following
gfr
we
use
jsd
the
𝑚-th
attribute
set
for
computing
the
distrsim
function
if
the
attribute
set
contains
nominal
groups
and
we
use
either
nmd
or
rnod
if
the
attribute
the
latter
approach
resembles
the
gfr
framework
25
which
computes
an
achieved
distribution
for
each
user
group
that
is
assumed
to
abandon
the
ranked
list
at
particular
relevant
document
at
rank
the
group
memberships
of
all
relevant
entities
in
the
top
documents
contribute
to
the
achieved
distribution
which
means
that
the
group
memberships
of
relevant
entities
near
the
top
ranks
contribute
relatively
heavily
to
the
overall
gfr
score
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
set
contains
ordinal
groups
25
note
that
the
distrsim
value
is
obtained
as
one
minus
divergence
such
as
jsd
as
quick
summary
measure
for
ranking
systems
the
following
combined
measure
may
be
of
some
use
gfr
𝛼𝑅
𝐺𝐹
u1
u2
based
on
fairweb-1
test
topic
s2
can
you
name
few
more
adhere
to
the
output
format
that
specified
please
list
up
format
where
the
is
parameter
that
balances
relevance
and
group
fairness
in
practice
however
we
recommend
to
report
and
gf
scores
separately
and
to
visualise
the
relationship
between
the
two
3.2
s1
dialogue
breakdown
pw
when
wc
gfrc
an
instantiation
pw
when
wc
in
this
section
we
discuss
specific
and
practical
instantiation
of
gfrc
first
we
provide
specific
definition
of
pw
the
position-based
nugget
weight
unlike
the
ntcir
1click
tasks
which
primarily
dealt
with
japanese
summaries
we
discuss
evaluating
english
textual
conversations
and
therefore
consider
word
count-based
positions
rather
than
character
count-based
ones
let
us
assume
that
the
user
is
willing
to
spend
up
to
minutes
to
satisfy
particular
information
need
furthermore
we
assume
that
the
average
reading
speed
of
the
user
is
250
words
per
minute
it
then
follows
that
the
user
is
willing
to
read
up
to
250
words
for
any
word
from
conversation
let
wc
denote
its
word
count
for
example
for
the
first
word
in
user
turn
𝑤𝑐
furthermore
for
any
relevant
nugget
within
system
turn
we
define
its
word
count
wc
as
the
word
count
of
the
last
word
that
corresponds
to
then
we
instantiate
the
position-based
nugget
weight
in
eq
as
figure
two-round
conversation
protocol
used
in
the
pilot
experiment
consider
simpler
option
let
be
the
set
of
system
turns
excluding
those
that
do
not
contain
any
relevant
entity
and
simply
let
pw
𝑆𝑖
iff
𝑆𝑖
with
that
is
we
simply
average
the
distrsim
over
relevant
system
turns
similarly
when
considering
different
attribute
sets
in
eq
we
simply
average
the
distrsim
across
them
in
summary
our
instantiations
of
eq
and
gf
eq
are
wc
so
that
the
nugget
weight
linearly
decreases
as
the
conversation
proceeds
and
any
nugget
beyond
the
word
count
limit
of
250
will
be
ignored
the
accompanying
normalisation
factor
see
eq
can
then
be
given
by
pw
max
wc
𝑛𝑖
max
𝑛𝑖
𝑛𝑖
𝑆𝑖
gf
distrsim𝑚
𝐷𝑚
𝑆𝑖
𝐷𝑚
10
gf
where
gf𝑚
𝑆𝑖
this
is
hard
normalisation
factor
which
represents
practically
unattainable
situation
where
every
word
in
conversation
represents
relevant
nugget
nevertheless
this
can
be
applied
if
we
do
not
want
the
score
to
exceed
1.7
s-measure
which
was
designed
for
evaluating
textual
summaries
used
softer
normalisation
factor
based
on
minimal
summary
pseudo
minimal
output
23
which
is
analogous
to
the
ideal
ranked
list
of
ndcg
however
this
approach
assumes
recall
base
of
nuggets
and
is
probably
not
suitable
for
conversational
search
where
it
is
often
not
possible
to
enumerate
all
relevant
nuggets
for
evaluation
next
we
instantiate
the
position-based
weight
for
turn
to
implement
eq
while
it
is
possible
to
make
the
weight
actually
position-aware
by
letting
pw
function
of
pw
the
maximum
nugget
weight
within
in
the
present
study
we
or
gf
if
none
of
the
system
turns
are
relevant
pilot
experiment
4.1
task
and
topics
to
illustrate
how
gfrc
scores
can
be
computed
from
user-system
conversations
we
conducted
pilot
experiment
using
the
entire
45
topics
from
the
ntcir-17
fairweb-1
task
which
we
downloaded
from
the
aforementioned
fairweb-1
website
the
test
topic
set
contains
15
m-topics
15
r-topics
and
15
y-topics
figure
shows
the
two-round
conversation
protocol
we
used
to
harvest
textual
interactions
with
given
conversational
search
system
the
bottom
part
of
the
figure
shows
conceptual
diagram
of
the
linearly
decaying
position-based
nugget
weights
the
author
of
this
paper
manually
interacted
with
conversational
search
systems
on
web
browser
and
recorded
all
textual
exchanges
user
turn
asks
the
system
to
list
entities
according
to
fairweb-1
topic
for
example
for
topic
r001
was
the
new
bing
said
as
of
september
2023
according
to
meta-analysis
of
190
studies
on
reading
speed
the
average
silent
reading
speed
for
an
adult
person
is
238
words
per
minute
wpm
for
non-fiction
while
the
average
reading
speed
for
fiction
is
260
wpm
google
bard
returned
the
same
wpm
numbers
same
day
we
have
checked
multiple
websites
to
see
that
these
answers
are
not
hallucinations
an
alternative
would
be
score
standardisation
19
27
28
please
list
up
researchers
who
have
published
at
least
one
paper
any
track
at
the
chiir
conference
format
researcher
name
url
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
shown
in
yellow
in
response
to
adhere
to
the
output
format
that
specified
the
system
managed
to
improve
the
response
format
shown
in
orange
in
response
to
adhere
to
the
output
format
that
specified
the
system
still
failed
shown
in
grey
caused
dialogue
breakdown
the
aforementioned
case
with
g1
for
m001
this
is
form
of
zero-shot
prompting
as
the
figure
indicates
system
turn
may
cause
dialogue
breakdown
11
the
user
is
unable
to
continue
the
conversation
due
to
highly
inadequate
responses
such
as
text-based
ai
and
that
is
outside
of
my
capabilities
otherwise
the
same
author
entered
which
was
either
can
you
name
few
more
or
one
of
the
following
depending
on
the
topic
type
as
systems
often
fail
to
follow
the
output
format
specified
in
at
the
bottom
of
each
figure
some
simple
statistics
are
shown
they
are
not
the
focus
of
this
paper
in
each
column
useful
is
the
proportion
of
responses
in
the
columns
for
turn
we
have
ave
useful
which
simply
averages
the
useful
over
the
two
turns
we
also
have
recovery
defined
as
of
all
topics
for
which
the
system
first
turn
was
unsuccessful
what
is
the
proportion
that
the
system
managed
to
return
useful
second
turn
for
example
in
figure
g2
has
14
unsuccessful
but
recovered
with
for
of
the
topics
and
therefore
recovery
14
64.3
from
figures
the
following
observations
can
be
made
adhere
to
the
output
format
specified
format
movie
title
imdb
url
adhere
to
the
output
format
specified
format
researcher
name
url
adhere
to
the
output
format
specified
format
video
title
youtube
url
the
entire
conversations
that
we
harvested
in
this
way
can
be
found
in
our
supplementary
material
package
4.2
systems
first
and
foremost
the
behaviour
of
each
system
is
vastly
different
across
the
two
trials
even
though
they
were
tested
on
the
same
day
moreover
once
they
start
to
perform
inadequately
they
tend
to
continue
to
do
so
for
example
compare
b1
and
b2
in
figure
m-topics
and
g1
and
g2
in
figure
r-topics
in
both
cases
the
second
trial
was
disaster
even
though
the
first
was
not
as
poor
this
extremely
unstable
nature
of
llm-based
conversational
search
systems
poses
substantial
challenge
in
terms
of
evaluation
the
results
of
small-scale
experiment
such
as
the
one
reported
in
the
present
study
let
alone
few
cherry-picked
anecdotes
we
sometimes
see
in
recent
llm
papers
are
far
too
unreliable
and
unlikely
to
be
generalisable
llms
are
known
to
hallucinate
and
not
surprisingly
the
new
bing
and
google
bard
are
no
exceptions
for
example
as
indicated
in
figure
footnotes
and
both
of
these
systems
hallucinated
about
the
airs
asia
information
retrieval
societies
conference
in
fact
the
final
airs
conference
not
directly
related
to
sigir
took
place
in
2019
10
and
instead
the
sigir-ap
asia-pacific
conference
was
launched
in
2023.11
moreover
the
urls
returned
by
google
bard
were
often
incorrect
for
example
as
we
shall
see
later
most
of
the
imdb
urls
that
it
returned
for
topic
m002
time
travel
movies
were
those
for
wrong
movies
google
bard
behaved
in
the
light
green
pattern
very
often
for
r-topics
and
y-topics
while
this
never
happened
with
bing
in
our
experiment
google
often
did
not
make
sense
even
though
contained
some
seemingly
relevant
urls
and
therefore
the
system
seems
capable
of
performing
the
task
to
some
extent
when
asked
for
few
more
was
often
unable
to
help
you
with
that
as
only
language
model
and
don
have
the
necessary
information
or
abilities
or
something
similar
interestingly
during
the
conversation
for
r012
about
sigir
with
bing
trial
in
response
to
adhere
to
the
output
format
specified
format
researcher
on
september
2023
the
author
of
this
paper
followed
the
above
protocol
and
interacted
with
both
the
new
bing
and
google
bard
for
each
topic
type
the
topics
were
tested
in
the
original
order
and
both
systems
were
tested
in
parallel
for
example
topic
m001
was
tested
with
the
new
bing
and
then
with
google
bard
then
the
author
moved
on
to
topic
m002
and
so
on
moreover
as
llmbased
systems
behave
nondeterministically
after
completing
the
conversations
for
all
45
topics
we
repeated
the
same
experiment
thus
we
had
two
trials
with
the
new
bing
denoted
by
b1
and
b2
and
two
trials
with
the
google
bard
denoted
by
g1
and
g2
note
that
we
tested
the
topics
in
the
same
order
in
the
two
trials
as
we
wanted
to
avoid
confounding
the
effect
of
topic
ordering
with
that
of
nondeterministic
responses
as
we
shall
report
in
section
we
have
evidence
that
previous
topic
can
actually
affect
the
system
response
for
the
current
topic
system
response
overview
figures
visualise
the
outcomes
of
the
two-round
conversations
with
the
new
bing
and
google
bard
using
the
m-topics
r-topics
and
y-topics
from
fairweb-1
in
each
cell
means
that
the
system
turn
looks
useful
to
some
extent
that
is
it
provides
some
urls
as
requested
although
they
may
not
necessarily
be
relevant
as
we
shall
discuss
later
means
that
the
system
turn
is
not
useful
it
either
does
not
contain
useful
urls
for
example
for
m001
b1
lists
movies
without
providing
imdb
urls
or
it
refuses
the
user
request
for
example
for
m001
g1
says
unable
to
help
you
with
that
as
only
language
model
and
don
have
the
necessary
information
or
abilities
indicates
dialogue
breakdown
for
example
for
m001
g1
is
due
to
shown
above
as
the
three
figures
show
all
of
the
following
patterns
were
observed
in
the
experiment
shown
in
green
both
turns
returned
seemingly
relevant
urls
shown
in
light
green
in
response
to
can
you
name
few
more
the
system
failed
to
respond
adequately
https://www.promptingguide.ai/
10
https://link.springer.com/book/10.1007/978-3-030-42835-8
http://waseda.box.com/evia2023pack
11
http://www.sigir-ap.org/sigir-ap-2023/
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
s1
s2
s1
s2
s1
s2
s1
s2
figure
overview
of
system
responses
for
the
topics
b1
g1
etc
mean
bing
trial
and
google
trial1
and
so
on
means
that
the
system
turn
looks
useful
means
that
it
is
not
useful
means
that
there
was
no
second
system
turn
due
to
dialogue
breakdown
s1
s2
s1
s2
s1
s2
s1
s2
figure
overview
of
system
responses
for
the
topics
b1
g1
etc
mean
bing
trial
and
google
trial1
and
so
on
means
that
the
system
turn
looks
useful
means
that
it
is
not
useful
means
that
there
was
no
second
system
turn
due
to
dialogue
breakdown
contains
an
obvious
hallucination
which
caused
dialogue
breakdown
found
the
proceedings
of
the
airs
2022
conference
confusion
with
the
previous
topic
here
are
some
researchers
who
have
published
at
least
one
paper
any
track
at
the
kdd
conference
contains
an
obvious
hallucination
which
caused
dialogue
breakdown
the
airs
conference
has
been
renamed
to
the
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
since
2019
the
2nd
list
has
an
overlap
with
the
1st
list
name
url
the
system
responded
with
here
are
some
researchers
who
have
published
at
least
one
paper
any
track
at
the
kdd
conference
figure
footnote
this
is
because
the
previous
topic
discussed
was
r011
about
kdd
hence
previous
conversations
do
seem
to
affect
the
current
response
in
some
cases
despite
the
fact
that
the
systems
kept
ignoring
the
same
output
format
instructions
in
many
conversations
12
because
the
system
responses
are
dependent
on
previous
context
this
adds
further
challenge
to
the
evaluation
of
conversational
search
the
sampled
system
turns
are
clearly
not
independent
of
one
another
12
as
of
september
2023
the
new
bing
forced
the
user
to
start
new
topic
after
30
system
turns
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
s1
s2
s1
s2
s1
s2
s1
s2
figure
overview
of
system
responses
for
the
topics
b1
g1
etc
mean
bing
trial
and
google
trial1
and
so
on
means
that
the
system
turn
looks
useful
means
that
it
is
not
useful
means
that
there
was
no
second
system
turn
due
to
dialogue
breakdown
the
2nd
list
has
an
overlap
with
the
1st
list
system
suddenly
started
apologising
in
japanese
contains
duplicate
urls
with
different
titles
the
same
url
is
repeated
times
with
different
titles
same
as
and
the
url
is
the
same
as
the
one
in
turn
the
same
url
is
repeated
times
with
different
titles
experimenter
failed
to
record
the
response
but
probably
it
was
just
language
model
etc
an
countries
13
hence
we
can
demonstrate
how
group
fairness
can
be
quantified
in
such
evaluation
settings
for
m002
both
the
new
bing
and
google
bard
managed
to
return
results
that
are
not
useless
the
author
of
this
paper
was
the
gold
annotator
of
this
fairweb-1
topic
hence
the
author
gets
to
decide
which
movies
are
relevant
or
not
to
his
own
information
need
even
when
both
and
looked
useful
for
r-topics
and
y-topics
especially
the
latter
the
urls
returned
sometimes
had
an
overlap
across
the
two
turns
and
or
had
duplicate
urls
within
single
list
we
argue
that
such
duplicates
should
not
be
rewarded
when
evaluating
the
system
turns
it
seems
sensible
to
treat
all
duplicate
entities
as
nonrelevant
as
was
proposed
at
ntcir-4
in
the
context
of
factoid
question
answering
evaluation
where
only
one
answer
string
from
each
equivalence
class
of
relevant
answers
was
considered
relevant
in
ranked
answer
list
18
the
discussion
above
demonstrates
that
current
llm-based
conversational
search
engines
have
lot
of
room
for
improvement
and
also
that
evaluating
them
in
reliable
manner
is
highly
challenging
we
leave
the
grand
challenge
for
future
work
nevertheless
as
one
small
piece
that
will
hopefully
contribute
to
the
above
challenge
we
henceforth
discuss
case
study
in
which
we
apply
our
proposed
evaluation
framework
to
topic
from
the
above
experiment
6.1
gold
annotation
the
present
author
examined
two
text
files
that
contained
all
the
textual
interactions
between
himself
and
the
system
either
the
new
bing
or
google
bard
trial
on
september
2023
days
after
we
harvested
the
conversations
the
author
examined
the
texts
and
annotated
them
by
inserting
lines
after
each
entity
returned
by
the
system
figure
shows
the
results
the
annotations
are
shown
in
red
for
example
in
the
new
bing
turn
the
imdb
url
for
back
to
the
future
is
followed
by
the
following
pieces
of
information
case
study
this
entity
is
l2-relevant
highly
relevant
to
the
information
need
on
the
imdb
page
the
movie
currently
has
3m
ratings
and
therefore
falls
into
group
movies
with
1m
user
ratings
or
more
of
the
ratings
attribute
set
this
section
demonstrates
how
our
measures
can
be
computed
from
real
conversational
search
data
as
an
example
we
chose
the
trial
results
of
b1
and
g1
for
topic
m002
time
travel
movies
highlighted
in
red
in
figure
for
the
following
reasons
m-topics
are
the
only
ones
with
two
attribute
sets
where
one
of
them
allows
soft
group
membership
one
movie
may
belong
to
multiple
origin
groups
geographic
regions
if
its
country
of
origin
field
on
the
imdb
page
mentions
multiple
13
the
mapping
from
country
names
to
eight
geographic
regions
is
also
documented
on
http://sakailab.com/fairweb1/.
10
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
m002
u1
please
list
up
sci-fi
movies
that
feature
time
travel
listed
in
imdb
format
movie
title
imdb
url
s1
m002
u1
please
list
up
sci-fi
movies
that
feature
time
travel
listed
in
imdb
format
movie
title
imdb
url
s1
here
are
some
sci-fi
movies
that
feature
time
travel
listed
in
imdb
back
to
the
future
1985
https://www.imdb.com/title/tt0088763/
l2
wc
35
ratings
3m
origin
united
states
the
terminator
1984
https://www.imdb.com/title/tt0088247/
l2
wc
39
ratings
898k
origin
united
kingdom
united
states
groundhog
day
1993
https://www.imdb.com/title/tt0107048/
l2
wc
43
ratings
661k
origin
united
states
interstellar
2014
https://www.imdb.com/title/tt0816692/
l1
wc
46
ratings
2m
origin
united
states
united
kingdom
canada
the
time
machine
1960
https://www.imdb.com/title/tt0054387/
l2
wc
51
ratings
43k
origin
united
states
hope
this
helps
let
me
know
if
there
anything
else
can
help
you
with
u2
can
you
name
few
more
s2
sure
here
are
some
more
sci-fi
movies
that
feature
time
travel
listed
in
imdb
12
monkeys
1995
https://www.imdb.com/title/tt0114746/
l2
wc
91
ratings
634k
origin
united
states
the
butterfly
effect
2004
https://www.imdb.com/title/tt0289879/
l2
ratings
508k
wc
96
origin
united
states
canada
looper
2012
https://www.imdb.com/title/tt1276104/
l2
wc
99
ratings
592k
origin
united
states
china
edge
of
tomorrow
2014
https://www.imdb.com/title/tt1631867/
l2
wc
104
ratings
711k
origin
united
states
canada
predestination
2014
https://www.imdb.com/title/tt2397535/
l2
ratings
294k
wc
107
origin
australia
united
states
hope
this
helps
let
me
know
if
there
anything
else
can
help
you
with
452
word
response
that
does
not
follow
the
above
format
u2
adhere
to
the
output
format
specified
format
movie
title
imdb
url
s2
sure
here
are
the
sci-fi
movies
that
feature
time
travel
listed
in
imdb
in
the
specified
format
movie
title
imdb
url
back
to
the
future
1985
https://www.imdb.com/title/tt0088763/
l2
ratings
3m
wc
506
origin
united
states
back
to
the
future
1985
movie
posteropens
in
new
window
www.imdb.com
back
to
the
future
1985
movie
poster
pulp
fiction
1994
https://www.imdb.com/title/tt0110912/
l0
pulp
fiction
1994
movie
posteropens
in
new
window
www.originalfilmart.com
pulp
fiction
1994
movie
poster
looper
2012
https://www.imdb.com/title/tt1853739/
l0
wrong
url
you
re
next
looper
2012
movie
posteropens
in
new
window
www.imdb.com
looper
2012
movie
poster
interstellar
2014
https://www.imdb.com/title/tt0816692/
l1
wc
560
ratings
2m
origin
united
states
united
kingdom
canada
interstellar
2014
movie
posteropens
in
new
window
www.imdb.com
interstellar
2014
movie
poster
tenet
2020
https://www.imdb.com/title/tt10375988/
l0
wrong
url
the
eze-nwanyi
kate
ajike
love
show
tenet
2020
movie
posteropens
in
new
window
www.imdb.com
tenet
2020
movie
poster
doctor
strange
2016
https://www.imdb.com/title/tt3498496/
l0
wrong
url
michigan
pheasant
doctor
strange
2016
movie
posteropens
in
new
window
www.imdb.com
doctor
strange
2016
movie
poster
the
terminator
1984
https://www.imdb.com/title/tt0089531/
l0
wrong
url
madame
deborah
terminator
1984
movie
posteropens
in
new
window
www.imdb.com
terminator
1984
movie
poster
the
butterfly
effect
2004
https://www.imdb.com/title/tt0322997/
l0
wrong
url
cruiskeen
lawn
butterfly
effect
2004
movie
posteropens
in
new
window
www.imdb.com
butterfly
effect
2004
movie
poster
primer
2004
https://www.imdb.com/title/tt0389895/
l0
wrong
url
crystal
force
dark
angel
primer
2004
movie
posteropens
in
new
window
www.imdb.com
primer
2004
movie
poster
12
monkeys
1995
https://www.imdb.com/title/tt0112389/
l0
wrong
url
the
thief
and
the
cobbler
12
monkeys
1995
movie
posteropens
in
new
window
www.imdb.com
12
monkeys
1995
movie
poster
hope
this
is
more
helpful
figure
bing
trial
left
vs
google
trial
right
for
topic
m002
time
travel
movies
with
relevance
and
group
membership
annotations
with
word
counts
on
the
imdb
page
united
states
is
named
as
the
country
of
origin
of
this
movie
hence
this
movie
belongs
solely
to
group
america
of
the
origin
attribute
set
in
figure
word
count
information
is
also
provided
in
balloons
for
example
the
word
count
for
the
back
to
the
future
url
is
35
that
is
this
is
the
35th
word
in
this
conversation
recall
that
when
we
compute
the
word
count
words
from
the
user
turns
are
included
in
the
present
study
that
is
if
the
user
enters
many
words
that
is
considered
as
cost
and
the
system
effectiveness
is
discounted
accordingly
6.2
be
observed
that
while
the
new
bing
managed
to
return
10
relevant
entities
with
interstellar
considered
by
the
gold
annotator
to
be
an
l1-relevant
entity
unlike
the
other
movies
google
bard
managed
to
return
only
many
of
the
imdb
urls
it
returned
were
incorrect
note
that
the
rightmost
column
pw
is
analogous
to
the
discounted
gain
of
ndcg
for
example
while
both
bing
and
google
return
back
to
the
future
as
the
first
l2-relevant
entity
in
their
responses
the
pw
for
google
is
lower
1.9456
for
bing
but
1.1920
for
google
this
is
largely
because
as
figure
right
shows
google
first
turn
did
not
contain
any
relevant
nuggets
and
wasted
as
many
as
452
words
as
result
the
word
count
for
the
imdb
url
of
the
back
to
the
future
nugget
is
506
according
to
eq
with
250
the
score
for
bing
is
0.0143
while
that
for
google
is
only
0.0014
only
about
9.8
of
the
score
for
bing
measure
computation
table
shows
how
the
scores
are
computed
for
the
conversations
shown
in
figure
based
on
eq
here
l2-relevant
and
l1-relevant
entities
are
mapped
to
gain
values
of
and
0.5
respectively
it
can
11
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
table
computing
the
scores
for
the
conversations
shown
in
figure
movie
nugget
pw
new
bing
back
to
the
future
0.9728
the
terminator
0.9696
groundhog
day
0.9664
interstellar
0.9640
0.5
the
time
machine
0.9600
12
monkeys
0.9280
the
butterfly
effect
0.9240
looper
0.9216
edge
of
tomorrow
0.9176
0.9152
predestination
google
bard
back
to
the
future
0.5960
interstellar
0.5528
0.5
america
mapped
from
united
states
and
canada
while
acknowledging
some
presence
of
europe
mapped
from
united
kingdom
as
the
groups
regions
are
nominal
this
time
the
distrsm
values
are
obtained
using
jsd
it
can
be
observed
that
even
in
terms
of
origin-based
gf
scores
google
slightly
underperforms
bing
its
score
is
about
95.8
of
that
of
bing
0.4303
vs
0.4493
this
reflects
the
fact
that
google
only
covers
groups
and
america
and
europe
while
bing
covers
groups
and
asia
and
oceania
in
addition
as
can
be
verified
from
figure
in
summary
in
this
particular
example
bing
outperforms
google
in
terms
of
all
three
measures
the
relevance
score
the
ratingsbased
gf
score
and
the
origin-based
gf
score
this
example
suggests
that
our
framework
which
extends
the
gfr
framework
for
evaluating
ranked
lists
may
be
useful
for
improving
conversational
search
engine
responses
in
terms
of
relevance
and
group
fairness
pw
0.9728
0.9696
0.9664
0.4820
0.9600
0.9280
0.9240
0.9216
0.9176
0.9152
0.0143
0.5960
0.2764
0.0014
conclusions
we
have
demonstrated
how
the
gfr
framework
used
in
the
ntcir17
fairweb-1
task
can
be
extended
for
evaluating
series
of
textual
system
turns
in
conversational
search
by
using
the
full
test
topic
set
of
fairweb-1
to
harvest
actual
user-system
conversations
from
the
new
bing
and
google
bard
we
demonstrated
how
series
of
system
turns
can
be
evaluated
using
our
gfrc
group
fairness
and
relevance
of
conversations
framework
with
the
rapid
progress
of
llm-based
conversational
search
systems
and
the
various
social
problems
that
accompany
it
hallucinations
and
biases
we
argue
that
evaluating
sequence
of
system
turns
is
becoming
at
least
as
important
as
evaluating
ranked
list
of
documents
to
put
it
mildly
hence
the
ntcir
fairweb
organisers
are
currently
considering
new
conversational
subtask
for
ntcir-18
along
with
the
existing
web
page
ranking
task
so
that
group
fairness
can
be
studied
by
looking
across
these
two
paradigms
from
the
participants
point
of
view
can
the
same
algorithm
be
used
for
group
fair
ranking
and
group
fair
conversations
from
the
organisers
point
of
view
how
are
gfr
for
rankings
and
gfrc
for
conversations
related
to
each
other
and
how
can
conversation
trees
turns
that
branch
out
17
be
obtained
so
that
turn
transition
probabilities
can
be
incorporated
into
gfrc
our
pilot
experiment
also
exemplified
some
known
challenges
in
evaluating
conversational
search
the
following
are
few
open
questions
for
human-in-the-loop
conversational
search
evaluation
that
we
have
observed
note
also
how
the
relevant
entities
from
multiple
turns
are
treated
seamlessly
in
the
score
computation
for
example
figure
left
shows
that
relevant
entities
were
returned
in
system
turn
and
another
were
returned
in
system
turn
but
from
table
it
is
clear
that
the
score
treats
all
of
these
entities
just
as
those
that
lie
on
top
of
single
slope
see
figure
bottom
table
shows
how
the
gf
scores
are
computed
for
the
conversations
shown
in
figure
based
on
eq
10
first
let
us
discuss
the
left
part
of
the
table
that
computes
gf
scores
for
the
ratings
attribute
set
with
ordinal
groups
where
group
represents
movies
with
over
1m
ratings
from
figure
left
the
group
membership
vectors
for
the
relevant
entities
returned
by
bing
in
are
1.0
by
averaging
them
we
obtain
the
achieved
distribution
for
0.6
0.4
as
shown
in
table
left
by
comparing
this
achieved
distribution
with
uniform
gold
distribution
over
the
groups
in
terms
of
rnod
14
the
distrsim
value
is
0.6773
for
similarly
the
achieved
distribution
of
for
bing
is
and
the
distrsim
in
terms
of
rnod
is
0.4796
finally
by
applying
eq
10
we
obtain
ratingsbased
gf
score
of
0.5785
for
bing
on
the
other
hand
as
only
contains
relevant
entities
in
the
google
results
from
eq
10
only
the
distrsim
value
for
contributes
to
the
ratings-based
gf
score
for
google
which
amounts
to
0.4049
only
about
70.0
of
the
score
for
bing
it
can
be
observed
that
google
underperforms
bing
because
it
is
more
heavily
biased
towards
famous
movies
those
with
many
ratings
finally
let
us
discuss
the
right
part
of
table
that
computes
gf
scores
for
the
origin
attribute
set
with
nominal
groups
representing
geographic
regions
again
the
achieved
distributions
for
the
system
turns
are
computed
by
averaging
the
group
membership
vectors
shown
in
figure
for
example
the
distribution
for
bing
0.8333
0.1667
means
that
it
is
heavily
biased
towards
given
the
highly
unstable
and
context-dependent
nature
of
the
system
turns
based
on
llms
how
can
we
conduct
reliable
evaluation
with
appropriate
sampling
methods
and
sample
sizes
how
can
we
sample
and
evaluate
worst-case
situations
rather
than
typical
ones
represented
by
mean
scores
etc
so
that
any
possible
harm
on
users
can
be
detected
in
timely
manner
what
is
the
appropriate
methodology
to
combine
humanin-the-loop
evaluations
which
we
cannot
do
without
as
satisfying
and
protecting
the
users
is
our
goal
and
user
simulations
10
15
29
which
enable
large-scale
evaluations
and
hence
may
enable
better
coverage
of
the
above-mentioned
worst-case
scenarios
14
nmd
may
be
used
instead
as
the
groups
are
ordinal
12
proceedings
of
the
tenth
international
workshop
on
evaluating
information
access
evia
2023
satellite
workshop
of
the
ntcir-17
conference
december
12
15
2023
tokyo
japan
table
computing
the
gf
scores
for
the
conversations
shown
in
figure
ratings
𝑆𝑖
𝑆1
𝑆2
0.6
0.4
gfratings
𝑆1
𝑆2
gfratings
distrsim
rnod
0.6773
0.4796
0.5785
0.4049
0.4049
origin
𝑆𝑖
new
bing
0.8333
0.1667
0.8
0.1
google
bard
0.8333
references
0.1667
distrsim
jsd
0.1
gforigin
0.4303
0.4682
0.4493
gforigin
0.4303
0.4303
yao
jue
wang
keshav
santhanam
laurel
orr
lucia
zheng
mert
yuksekgonul
mirac
suzgun
nathan
kim
neel
guha
niladri
chatterji
omar
khattab
peter
henderson
qian
huang
ryan
chi
sang
michael
xie
shibani
santurkar
surya
ganguli
tatsunori
hashimoto
thomas
icard
tianyi
zhang
vishrav
chaudhary
william
wang
xuechen
li
yifan
mai
yuhui
zhang
and
yuta
koreeda
2023
holistic
evaluation
of
language
models
arxiv
2211.09110
cs
cl
15
aldo
lipani
ben
carterette
and
emine
yilmaz
2021
how
am
doing
evaluating
conversational
search
systems
offline
acm
tois
39
article
51
2021
16
yang
liu
yuanshun
yao
jean-francois
ton
xiaoying
zhang
ruocheng
guo
hao
cheng
yegor
klochkov
muhammad
faaiz
taufiq
and
hang
li
2023
trustworthy
llms
survey
and
guideline
for
evaluating
large
language
models
alignment
arxiv
2308.05374
cs
ai
17
paul
owoicho
jeffrey
dalton
mohammad
aliannejadi
leif
azzopardi
johanne
trippas4
and
svitlana
vakulenko
2023
trec
cast
2022
going
beyond
user
ask
and
system
retrieve
with
initiative
and
response
generation
in
nist
special
publication
500
338
the
thirty-first
text
retrieval
conference
proceedings
trec
2022
virtual
event
nist
18
tetsuya
sakai
2004
new
performance
metrics
based
on
multigrade
relevance
their
application
to
question
answering
in
working
notes
of
ntcir4
https://research.nii.ac.jp/ntcir/ntcir-ws4/ntcir4-wn/open/opensub_
tetsuya_sakai
pdf
19
tetsuya
sakai
2016
simple
and
effective
approach
to
score
standardisation
in
proceedings
of
acm
ictir
2016
newark
delaware
usa
acm
95
104
20
tetsuya
sakai
2020
evaluating
evaluation
measures
for
ordinal
classification
and
ordinal
quantification
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
online
association
for
computational
linguistics
2759
2769
https://aclanthology.org/2021.acllong.214.pdf
21
tetsuya
sakai
2023
swan
generic
framework
for
auditing
textual
conversational
systems
arxiv
2305.08290
cs
ir
22
tetsuya
sakai
and
noriko
kando
2008
modelling
user
population
for
designing
information
retrieval
metrics
in
proceedings
of
evia
2008
30
41
https://research.nii.ac.jp/ntcir/workshop/onlineproceedings7/pdf/evia2008/
07
evia2008-sakait
pdf
23
tetsuya
sakai
makoto
kato
and
young-in
song
2011
click
the
search
button
and
be
happy
evaluating
direct
and
immediate
information
access
in
proceedings
of
cikm
2011
glasgow
scotland
uk
acm
621
630
24
tetsuya
sakai
makoto
kato
and
young-in
song
2011
overview
of
ntcir-9
1click
in
proceedings
of
ntcir-9
tokyo
japan
180
201
25
tetsuya
sakai
jin
young
kim
and
inho
kang
2023
versatile
framework
for
evaluating
ranked
lists
in
terms
of
group
fairness
and
relevance
acm
tois
2023
https://dl.acm.org/doi/pdf/10.1145/3589763
26
sijie
tao
nuo
chen
tetsuya
sakai
zhumin
chu
hiromi
arai
ian
soboroff
nicola
ferro
and
maria
maistro
2023
overview
of
the
ntcir-17
fairweb-1
task
in
proceedings
of
ntcir-17
to
appear
https://doi.org/10.20736/0002001318
27
julián
urbano
harlley
lima
and
alan
hanjalic
2019
new
perspective
on
score
standardization
in
proceedings
of
acm
sigir
2019
paris
france
acm
new
york
ny
usa
1061
1064
28
william
webber
alistair
moffat
and
justin
zobel
2008
score
standardization
for
inter-collection
comparison
of
retrieval
systems
in
proceedings
of
acm
sigir
2008
singapore
singapore
acm
51
58
https://doi.org/10.1145/1390334.
1390346
29
shuo
zhang
mu-chun
wang
and
krisztian
balog
2022
analyzing
and
simulating
user
utterance
reformulation
in
conversational
recommender
systems
in
proceedings
of
the
45th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
madrid
spain
association
for
computing
machinery
133
143
amanda
askell
yuntao
bai
anna
chen
dawn
drain
deep
ganguli
tom
henighan
andy
jones
nicholas
joseph
ben
mann
nova
dassarma
nelson
elhage
zac
hatfield-dodds
danny
hernandez
jackson
kernion
kamal
ndousse
catherine
olsson
dario
amodei
tom
brown
jack
clark
sam
mccandlish
chris
olah
and
jared
kaplan
2021
general
language
assistant
as
laboratory
for
alignment
arxiv
2112.00861
cs
cl
leif
azzopardi
diane
kelly
and
kathy
brennan
2013
how
query
cost
affects
search
behavior
in
proceedings
of
the
36th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
dublin
ireland
sigir
13
acm
23
32
leif
azzopardi
and
guido
zuccon
2018
economic
models
of
interaction
in
computational
interaction
antti
oulasvirta
per
ola
kristensson
xiaojun
bi
and
andrew
howes
eds
oxford
university
press
feza
baskaya
heikki
keskustalo
and
kalervo
järvelin
2012
time
drives
interaction
simulating
sessions
in
diverse
searching
environments
in
proceedings
of
the
35th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
portland
oregon
usa
acm
105
114
christine
bauer
ben
carterette
nicola
ferro
and
norbert
fuhr
2023
report
from
dagstuhl
seminar
23031
frontiers
of
information
access
experimentation
for
research
and
education
arxiv
2305.01509
cs
ir
olivier
chapelle
donald
metzler
ya
zhang
and
pierre
grinspan
2009
expected
reciprocal
rank
for
graded
relevance
in
proceedings
of
acm
cikm
2009
621
630
michael
ekstrand
anubrata
das
robin
burke
and
fernando
diaz
2022
fairness
in
information
access
systems
arxiv
2105.05779
cs
ir
michael
ekstrand
graham
mcdonald
and
amifa
raj
2022
overview
of
the
trec
2021
fair
ranking
track
in
the
thirtieth
text
retrieval
conference
trec
2021
proceedings
nist
guglielmo
faggioli
laura
dietz
charles
clarke
gianluca
demartini
matthias
hagen
claudia
hauff
noriko
kando
evangelos
kanoulas
martin
potthast
benno
stein
and
henning
wachsmuth
2023
perspectives
on
large
language
models
for
relevance
judgment
in
proceedings
of
the
2023
acm
sigir
international
conference
on
theory
of
information
retrieval
taipei
taiwan
acm
39
50
10
david
griol
javier
carbó
and
josé
molina
2013
an
automatic
dialog
simulation
technique
to
develop
and
evaluate
interactive
conversational
agents
applied
artificial
intelligence
27
2013
759
780
11
ryuichiro
higashinaka
kotato
funakoshi
yuka
kobayashi
and
michimasa
inaba
2016
the
dialogue
breakdown
detection
challenge
task
description
datasets
and
evaluation
metrics
in
proceedings
of
the
tenth
international
conference
on
language
resources
and
evaluation
lrec
16
portorož
slovenia
european
language
resources
association
elra
3146
3150
12
kalervo
järvelin
and
jaana
kekäläinen
2002
cumulated
gain-based
evaluation
of
ir
techniques
acm
trans
inf
syst
20
2002
422
446
https://doi.org/10.
1145
582415.582418
13
makoto
kato
matthew
ekstrand-abueg
virgil
pavlu
tetsuya
sakai
takehiro
yamamoto
and
mayu
iwata
2014
overview
of
the
ntcir-11
mobileclick
task
in
proceedings
of
ntcir-11
195
207
https://research.nii.ac.jp/ntcir/workshop/onlineproceedings11/pdf/ntcir/
overview
01
ntcir11-ov-mobileclick-katom
pdf
14
percy
liang
rishi
bommasani
tony
lee
dimitris
tsipras
dilara
soylu
michihiro
yasunaga
yian
zhang
deepak
narayanan
yuhuai
wu
ananya
kumar
benjamin
newman
binhang
yuan
bobby
yan
ce
zhang
christian
cosgrove
christopher
manning
christopher
ré
diana
acosta-navas
drew
hudson
eric
zelikman
esin
durmus
faisal
ladhak
frieda
rong
hongyu
ren
huaxiu
13