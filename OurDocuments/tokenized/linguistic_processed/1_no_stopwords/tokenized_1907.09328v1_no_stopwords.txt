arxiv
1907
09328v1
cs
ir
22
jul
2019
conceptual
framework
evaluating
fairness
search
anubrata
das
matthew
lease
anubrata@utexas.edu
university
texas
austin
austin
texas
usa
ml@utexas.edu
university
texas
austin
austin
texas
usa
abstract
search
efficacy
evaluated
traditionally
basis
result
relevance
fairness
search
attracted
recent
attention
work
define
notion
distributional
fairness
provide
conceptual
framework
evaluating
search
results
based
part
formulate
set
axioms
ideal
evaluation
framework
satisfy
distributional
fairness
show
existing
trec
test
collections
can
repurposed
study
fairness
measure
potential
data
bias
inform
test
collection
design
fair
search
set
analyses
show
metric
divergence
relevance
fairness
describe
simple
flexible
interpolation
strategy
integrating
relevance
fairness
single
metric
optimization
evaluation
ir
systems
face
existing
societal
bias
gender
racial
14
inequality
application
areas
resume-search
19
political
news-search
10
demonstrate
two
types
fairness
individual
fairness
group
fairness
group
fairness
ensures
protected
groups
treated
equally
individual
fairness
ensures
individuals
treated
equally1
researchers
proposed
different
methods
tackle
bias
ir
systems
18
20
approaches
include
new
ranking
algorithms
taking
fairness
constraints
account
postprocessing
method
re-ranking
existing
systems
considering
individual
group
fairness
19
evaluation
ranking
systems
terms
fairness
18
group
fairness
perspective
ibm-3602
industry
standard
evaluating
fairness
machine
learning
algorithms
datasets
however
include
measurements
ranking
systems
recent
work
sapiezynski
et
al
15
yang
stoyanovich
18
begun
explore
evaluating
search
fairness
keywords
information
retrieval
fairness
evaluation
introduction
algorithmic
fairness
now
receiving
significant
attention
search
systems
increasingly
mediating
human
information
access
recognized
search
systems
must
fair
well
accurate
however
idea
fairness
intuitive
many
competing
definitions
operationalize
practice
also
increasing
recognition
today
dataset
bias
imbalance
can
lead
biased
training
evaluation
example
one
might
desire
balanced
search
results
imbalanced
dataset
distribution
can
make
goal
difficult
achieve
practice
extent
relevant
information
scarce
perspectives
categories
imbalance
relevant
information
may
lead
imbalance
retrieved
results
relevant
information
given
category
scarce
completely
absent
may
difficult
impossible
ir
systems
find
relevant
results
category
retrieve
diversity
thus
plays
role
test
collection
design
decisions
must
made
categories
will
important
ensure
diversity
terms
documents
included
collection
relevance
annotation
work
define
notion
distributional
fairness
provide
conceptual
framework
evaluating
search
results
based
section
section
formulates
set
axioms
ideal
evaluation
framework
satisfy
distributional
fairness
section
show
existing
trec
test
collections
can
repurposed
study
fairness
section
5.1
measures
potential
data
bias
inform
test
collection
design
fair
search
set
analyses
presented
section
show
metric
divergence
relevance
fairness
simple
flexible
interpolation
strategy
integrating
relevance
fairness
single
metric
optimization
evaluation
first
provide
brief
background
related
work
evaluation
desiderata
motivated
lioma
et
al
11
propose
desiderata
evaluating
ranking
systems
fair
relevant
also
include
notion
authoritativeness
especially
keeping
mind
modern
challenges
polarization
misinformation
d1
fairness
ranking
system
return
set
documents
fairly
represent
different
types
contents
fairness
can
different
definitions
two
described
d1
equality
ranking
system
return
documents
different
types
equal
proportion
regardless
distribution
content
d1
equity
ranking
system
return
documents
different
types
frequency
documents
reflects
distribution
contents
real
world
d2
exposure
bias
ranking
output
presentation
bias
relevant
documents
across
different
types
certain
types
document
always
appear
types
16
d2
property
fair
exposure
hold
across
number
arbitrary
queries
d3
relevance
system
always
return
document
relevant
less
relevant
document
d4
generalizability
k-th
intersection
ranking
documents
relevant
fairly
represented
d5
authoritativeness
specific
topics
ranking
system
can
deliberate
authoritative
bias
imposed
type
information
avoid
misinformation
fairness-measures
http://www.fairness-measures.org/
https://aif360.mybluemix.net/
july
2019
approach
section
describe
conceptual
framework
evaluating
fairness
search
results
key
tenet
approach
documents
can
organized
categories
ir
systems
ensure
degree
balanced
coverage
categories
search
results
key
concepts
include
document
categories
assume
single
set
static
document
categories
topical
news
vs
sports
documents
irrespective
topic
evaluation
know
number
categories
labels
document
results
distribution
actual
distribution
documents
categories
search
results
notated
results
distribution
estimated
categories
target
distribution
desired
distribution
documents
categories
search
results
target
distribution
may
arbitrarily
specified
uniform
reflect
distributional
prior
empirically-derived
dataset
distribution
assume
work
target
distribution
constant
across
search
topics
denote
target
distribution
qt
denotes
target
distribution
type
estimation
distributions
may
estimated
observed
data
simply
relative
frequency
maximum
likelihood
form
regulation
smoothing
work
apply
simple
add-1
laplacian
smoothing3
estimating
empirical
results
dataset
distributions
distributional
fairness
define
fairness
distributional
similarity
closely
results
distribution
matches
target
distribution
must
specify
distributional
similarity
integrative
measures
beyond
measuring
relevance
fairness
distinct
aspects
system
performance
can
useful
integrate
single
measure
requires
specifying
metrics
can
combined
target
distribution
distribution
targeted
things
equal
lacking
prior
information
uniform
distribution
targets
balanced
equal
coverage
respecting
principle
maximum
entropy4
however
given
prior
information
one
may
specify
non-uniform
target
distribution
example
may
expect
search
results
respect
prior
distribution
given
dataset
might
observe
given
population
distribution
perhaps
65
documents
written
english
want
search
results
representative
larger
population
fairness
noted
define
fairness
distributional
similarity
results
target
qt
distributions
specifically
compute
kl-divergence5
discussed
next
subsection
useful
relevance
fairness
scale
combining
single
measure
thus
apply
min-max
normalization6
produces
score
fair
distributional
divergence
least
fair
ease
interpretation
consistency
relevance
metrics
reverse
scale
fair
given
target
distribution
thus
compute
fairness
ft
kl
qt
https://en.wikipedia.org/wiki/additive_smoothing
https://en.wikipedia.org/wiki/principle_of_maximum_entropy
https://en.wikipedia.org/wiki/kullback-leibler_divergence
https://en.wikipedia.org/wiki/feature_scaling#rescaling_(min-max_normalization)
das
lease
combining
relevance
fairness
regardless
relevance
fairness
measured
may
useful
integrate
single
measure
example
f-measure
interpolates
precision
recall
via
harmonic
mean
work
apply
arithmetic
geometric
means
simple
interpolation
methods
normalized
fairness
relevance
scores
13
map
vs
gmap
arithmetic
mean
tolerant
imbalance
inputs
whereas
geometric
mean
heavily
penalizes
imbalance
general
one
can
specify
smoothing
parameter
weight
mixture
f-measure
lets
one
weight
precision
vs
recall
though
simple
unweighted
f-1
typically
used
leave
parameterized
interpolation
future
work
note
flexibility
exists
balancing
noted
apply
min-max
normalization
define
fairness
range
relevance
measures
typically
also
defined
interval
apply
consistent
normalization
well
fully
span
mixed
axiomatic
analysis
approach
measuring
fairness
grounded
idea
diversity
example
intent-aware
evaluation
metrics
originally
developed
topical
diversity
adapted
evaluating
protected
attributes
diversity
general
connection
topical
diversity
fairness
also
noticed
elsewhere
ekstrand
et
al
framework
incorporates
two
different
aspects
proposed
evaluation
desiderata
d1
d3
notion
distributional
fairness
can
satisfy
different
definitions
fairness
well
long
target
distribution
can
estimated
based
particular
definition
fairness
approach
can
used
score
search
system
idea
generalizability
d4
metrics
also
incorporated
approach
method
can
provide
insight
fairness
aspect
well
relevance
aspect
search
results
k-th
intersection
combined
metric
gmean
also
enforces
system
needs
perform
well
terms
relevance
fairness
implicit
sense
authoritativeness
d5
incorporated
approach
since
one
ways
measure
fairness
compare
dataset
distribution
test-collection
authoritative
stands
topics
encourages
systems
retrieve
results
perspectives
vs
others
however
since
consider
rank
order
calculating
fairness
system
performing
well
fairness
metrics
can
still
exposure
bias
defined
d2
experiments
evaluation
seeks
understand
two
over-arching
questions
system
performance
varies
relevance
vs
fairness
combined
metrics
well
metrics
meet
evaluation
desiderata
proposed
section
datasets
blogs07
trec-8
adhoc
first
describe
adapt
two
existing
trec
test
collections
study
fairness
trec-8
adhoc
17
considers
topics
401
450
binary
relevance
judgments
four
newswire
sources
financial
times
los
angeles
times
foreign
broadcast
information
service
federal
register
129
participant
rankings
obtained
trec
track
also
consider
fairness
re-interpret
system
performance
new
assumption
fair
ranking
provide
diverse
coverage
across
different
four
news
sources
conceptual
framework
evaluating
fairness
search
trec-8
adhoc
distribution
newswire
sources
foreign
broadcast
info
service
fed
register
financial
times
la
times
july
2019
trec
blogs07
distribution
categories
opinionated
content
opinion
negative
mixed
positive
figure
distribution
relevant
documents
across
topics
category
two
test
collections
r-precision
vs
fairness
scores
trec-8
system
runs
r-precision
vs
fairness
scores
blogs07
system
runs
figure
correlation
system
scores
metrics
relevance
vs
fairness
uniform
vs
dataset
target
distributions
blogs07
12
opinion
retrieval
task
participant
ranking
systems
retrieve
relevant
blog
posts
given
opinion
50
topics
collection
contains
binary
relevance
judgments
four
opinion
labels
opinion
negative
mixed
positive
104
participant
rankings
obtained
trec7
track
consider
fairness
re-interpret
performance
systems
new
assumption
fair
ranking
ensure
diverse
coverage
across
four
opinion
categories
5.1
identifying
potential
test
collection
bias
first
analysis
explores
whether
underlying
test
collections
balanced
imbalanced
across
categories
discussed
earlier
less
balance
relevant
information
across
categories
underlying
test
collection
will
likely
lead
less
balanced
coverage
categories
distributional
fairness
search
results
note
since
repurposing
existing
datasets
study
notion
distributional
fairness
disputing
anything
particular
test
collections
consideration
rather
describing
method
one
design
assess
test
collection
actual
categories
interest
ensuring
fairness
figures
1a
1b
present
distribution
relevant
documents
across
topics
category
collection
noted
trec8
categories
four
newswire
sources
blogs07
four
categories
opinion
see
relevant
information
categories
scarce
fr
trec-8
abundant
http://trec.nist.gov/results/
opinion
blogs07
potential
imbalance
relevant
information
lead
imbalance
retrieved
results
5.2
score
correlation
relevance
vs
fairness
next
aim
understand
degree
relevance
fairness
uniform
dataset
target
distributions
correlated
hypothesize
low
correlation
motivate
measuring
metrics
potentially
optimizing
retrieval
results
combination
thereof
measure
r-precision
relevance
metric
due
robustness
relevant
documents
given
category
per
previous
analysis
figure
shows
r-precision
vs
fairness
scores
participating
systems
trec-8
blogs07
tracks
system
scores
sorted
decreasing
r-prec
shown
green
bars
scores
measured
left
y-axis
system
also
see
corresponding
fairness
scores
two
target
distributions
uniform
blue
dataset
population
cyan
measured
right
y-axis
figures
confirm
systems
indeed
perform
differently
fairness
relevance
metrics
see
r-precision
scores
systems
uniform
target
fairness
scores
inversely
correlated
however
compare
r-precision
scores
dataset
population
target
distribution
scores
correlated
random
sample
relevant
documents
tend
representative
population
distribution
test
collection
imbalanced
relevant
documents
category
collection
population
uniform
will
diverge
also
infer
ranking
systems
usually
optimized
reflect
july
2019
distribution
documents
test
collection
figures
suggest
build
fair
ranking
systems
also
focus
developing
fair
test-collections
well
5.3
top
systems
relevance
vs
fairness
another
way
look
relationship
relevance
fairness
look
systems
perform
best
metric
tables
report
top-3
systems
metric
well
arithmetic
geometric
means
integrate
measures
see
highest
performing
systems
r-precision
low
fairness
scores
vice
versa
naturally
inverse
relationship
relevance
fairness
also
influences
arithmetic
geometric
means
tracks
see
top
systems
relevance
largely
also
top
systems
arithmetic
mean
geometric
mean
penalizes
low
fairness
heavily
tends
select
systems
balanced
across
relevance
fairness
recalling
high-level
evaluation
desiderata
section
rprecision
tell
us
much
beyond
d3
relevance
similarly
fairness
inform
us
evaluation
criteria
except
d1
fairness
however
interpolation
fairnessrelevance
scores
helps
incorporate
d1
d3
d4
systems
r-prec
fair
mean
gmean
sn1
1.0000
0.1158
0.5579
0.3403
sn2
0.8800
0.1578
0.5189
0.3727
sn3
0.8552
0.1638
0.5095
0.3743
sn129
0.0000
1.0000
0.5000
0.0000
sn127
0.0536
0.7306
0.3921
0.1979
sn125
0.0868
0.6916
0.3892
0.2450
sn1
see
sn1
sn2
see
sn2
sn4
0.8472
0.1766
0.5119
0.3868
sn56
0.6000
0.2514
0.4257
0.3884
sn4
see
sn4
sn15
0.6854
0.2112
0.4482
0.3805
table
top-3
scoring
systems
trec8
metrics
relevance
r-precision
fairness
section
r-f
arithmetic
geometric
means
indicates
min-max
normalized
scores
discussed
earlier
name
systems
rank
order
r-prec
sn1
achieves
best
r-prec
trec8
newswire
followed
sn2
etc
top
score
metric
underlined
systems
r-prec
fair
mean
gmean
sb1
1.0000
0.0861
0.5431
0.2935
sb2
0.9926
0.2997
0.6461
0.5454
sb3
0.9906
0.3006
0.6456
0.5457
sb104
0.0000
1.0000
0.5000
0.0000
sb103
0.0035
0.9560
0.4797
0.0579
sb102
0.0035
0.9560
0.4797
0.0579
sb4
0.9905
0.3031
0.6468
0.5479
sb5
0.9891
0.3040
0.6465
0.5483
sb2
see
sb2
sb5
0.9891
0.3039
0.6465
0.5483
sb4
see
sb4
sb3
see
sb3
table
blogs07
results
akin
trec8
results
table
das
lease
ranking
metric
trec8
blogs07
fu
fairtarget
uniform
0.01623
0.08028
fairtarget
population
0.03997
0.05489
mean
fu
r-prec
0.08503
0.03958
дmean
fu
r-prec
0.08503
0.12957
table
kendall
rank
correlation
participant
systems
ranked
relevance
metric
r-precision
vs
ranking
fairness
relevance-fairness
interpolation
5.4
rank
correlation
relevance
vs
fairness
another
question
evaluating
systems
based
relevance
vs
fairness
leads
different
relative
orderings
participant
systems
explore
assume
baseline
ordering
participant
systems
based
r-precision
consider
system
rankings
based
fairness
measure
differ
measured
kendall
table
show
rank-correlation
across
metrics
quite
low
top
rows
consider
target
distributions
uniform
population
ground
truth
dataset
bottom
rows
consider
ranking
induced
mean
gmean
uniform
target
r-precision
adds
evidence
earlier
results
showing
evaluating
systems
relevance
vs
fairness
leads
quite
different
results
assessment
ir
systems
moreover
highlights
need
consider
relevance
fairness
based
metrics
designing
optimizing
algorithms
conclusion
defined
notion
distributional
fairness
provide
conceptual
framework
evaluating
search
results
based
part
work
formulated
set
axioms
ideal
evaluation
framework
satisfy
distributional
fairness
showed
existing
trec
test
collections
can
repurposed
study
fairness
measured
potential
data
bias
inform
test
collection
design
fair
search
set
analyses
showed
metric
divergence
relevance
fairness
described
simple
flexible
interpolation
strategy
integrating
relevance
fairness
single
metric
optimization
evaluation
limitations
repurposed
existing
trec
test
collections
study
fairness
better
avoid
surrogate
data
defined
fairness
set-basis
distributional
approach
can
easily
extended
estimate
results
distribution
based
rank
information
assigning
greater
weight
categories
observed
higher
ranks
addressing
exposure
bias
d2
desiderata
currently
missed
min-max
normalization
simplifies
metric
combination
causes
scores
change
based
systems
compared
also
revisited
also
scope
feedback
loops
reinforcing
biases
search
systems
future
direction
expand
evaluation
desiderata
measure
feedback
loops
references
rakesh
agrawal
sreenivas
gollapudi
alan
halverson
samuel
ieong
2009
diversifying
search
results
proceedings
second
acm
international
conference
web
search
data
mining
acm
14
qingyao
ai
keping
bi
cheng
luo
jiafeng
guo
wb
croft
2018
unbiased
learning
rank
unbiased
propensity
estimation
arxiv
1804.05938
2018
asia
biega
krishna
gummadi
gerhard
weikum
2018
equity
attention
amortizing
individual
fairness
rankings
arxiv
1805.01788
2018
elisa
celis
damian
straszak
nisheeth
vishnoi
2018
ranking
fairness
constraints
icalp
conceptual
framework
evaluating
fairness
search
le
chen
ruijun
ma
anikó
hannák
christo
wilson
2018
investigating
impact
gender
rank
resume
search
engines
proceedings
2018
chi
conference
human
factors
computing
systems
acm
651
michael
ekstrand
robin
burke
fernando
diaz
2019
fairness
discrimination
retrieval
recommendation
proceedings
42nd
international
acm
sigir
conference
research
development
information
retrieval
sigir
19
acm
new
york
ny
usa
1403
1404
https
doi
org
10.1145
3331184.3331380
danielle
ensign
sorelle
friedler
scott
neville
carlos
scheidegger
suresh
venkatasubramanian
2017
runaway
feedback
loops
predictive
policing
arxiv
preprint
arxiv
1706.09847
2017
robert
epstein
ronald
robertson
2015
search
engine
manipulation
effect
seme
possible
impact
outcomes
elections
proceedings
national
academy
sciences
112
33
2015
e4512
e4521
matthew
lease
2018
fact
checking
information
retrieval
2018
10
vera
liao
wai-tat
fu
2013
beyond
filter
bubble
interactive
effects
perceived
threat
topic
involvement
selective
exposure
information
proceedings
chi
acm
2359
2368
11
christina
lioma
jakob
grue
simonsen
birger
larsen
2017
evaluation
measures
relevance
credibility
ranked
lists
proceedings
acm
sigir
international
conference
theory
information
retrieval
acm
91
98
12
craig
macdonald
iadh
ounis
ian
soboroff
2007
overview
trec
2007
blog
track
trec
july
2019
13
rishabh
mehrotra
james
mcinerney
hugues
bouchard
mounia
lalmas
fernando
diaz
2018
towards
fair
marketplace
counterfactual
evaluation
trade-off
relevance
fairness
satisfaction
recommendation
systems
proceedings
27th
acm
international
conference
information
knowledge
management
acm
2243
2251
14
safiya
umoja
noble
2018
algorithms
oppression
search
engines
reinforce
racism
nyu
press
15
piotr
sapiezynski
wesley
zeng
ronald
robertson
alan
mislove
christo
wilson
2019
quantifying
impact
user
attentionon
fair
group
representation
ranked
lists
companion
proceedings
2019
world
wide
web
conference
acm
553
562
16
ashudeep
singh
thorsten
joachims
2018
fairness
exposure
rankings
proceedings
24th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
acm
2219
2228
17
ellen
voorhees
donna
harman
1999
overview
eighth
text
retrieval
conference
trec-8
trec
18
ke
yang
julia
stoyanovich
2017
measuring
fairness
ranked
outputs
ssdbm
19
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
cikm
20
meike
zehlike
carlos
castillo
2018
reducing
disparate
exposure
ranking
learning
rank
approach
corr
abs
1805.08716
2018