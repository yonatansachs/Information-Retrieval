Fairness Vs. Personalization: Towards Equity in Epistemic Utility
JENNIFER CHIEN, UC San Diego, USA
DAVID DANKS, UC San Diego, USA
The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search
engine results, and more. These systems oﬀer a more eﬃcient way to navigate the vast array of items available. However, alongside
this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking

arXiv:2309.11503v1 [cs.IR] 5 Sep 2023

unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional implementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a
mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path
towards achieving fairness in personalized systems.
Additional Key Words and Phrases: fairness, personalization, recommender systems, equity, epistemic utility, epistemic harms, policy
ACM Reference Format:
Jennifer Chien and David Danks. 2023. Fairness Vs. Personalization: Towards Equity in Epistemic Utility. 1, 1 (September 2023),
11 pages.

1 INTRODUCTION
Personalized algorithmic systems are being increasingly deployed in a broad range of sectors. These algorithms hold
the potential to provide more appropriate outputs on an individual basis by personalizing based on preferences, values,
needs, or environmental conditions. Most prominently, recommender systems are now widely used to provide the most
useful recommendations to each individual in the given domain[1].
At the same time, we have seen a signiﬁcant increase in the recognition that algorithms can exhibit biases and
produce unfair or unjust outcomes. There are many diﬀerent potential sources of bias in algorithms and models, and
hence many diﬀerent responses may be appropriate or required. Algorithm development and deployment eﬀorts now
typically recognize the possibility of algorithmic bias, and the need to (often) do something to mitigate it[2].
We contend that these two desiderata for algorithms – personalization and fairness – stand in signiﬁcant tension.
At a high level, personalization is fundamentally about treating each individual in distinct ways; the goal is to not give
the same output for each individual, but rather to tailor the outputs to their speciﬁc situation. In contrast, fairness is
fundamentally about treating individuals similarly; the algorithm ought to be “the same”, in some sense, for everyone.
Of course, this high-level gloss on the tension is far too quick: for example, fairness allows for diﬀerential treatment or
outcomes, as long as it is based on morally or legally defensible grounds. Nonetheless, these high-level observations
point towards a tension that, we contend, continues to hold when we look deeper. More speciﬁcally, we analyze fairness
in the context of personalized systems using the notion of epistemic utility – essentially, the beneﬁt that an agent
receives by an epistemic improvement (e.g., reduction in uncertainty) – and provide both practical and policy guidance
about how to achieve fair, personalized systems.
Authors’ addresses: Jennifer Chien, jjchien@ucsd.edu, UC San Diego, USA; David Danks, ddanks@ucsd.edu, UC San Diego, USA.

2023. Manuscript submitted to ACM
Manuscript submitted to ACM

1

2

Jennifer Chien and David Danks

2 PRELIMINARIES
Recommender System. We consider a recommender system as that which uses an algorithmic approach to provide
an ordering or prioritization for items based on relevance to a user. This form of personalization is often based on their
preferences, past behavior, or similarities with other users with the overarching goal of improving user experience and
engagement.
Group Fairness. Algorithmic fairness is typically framed as metric parity1 across two or more relevantly comparable entities such that the diﬀerence between them is deemed socially, legally, and/or culturally irrelevant. For instance,
group fairness may be deﬁned as demographic parity, meaning equality in the likelihood of a given outcome conditioned on the group (e.g. gender, age, disability)[4]. Formally:
𝑃 (𝑦ˆ | 𝑔 = 𝑔𝑖 ) = 𝑃 (𝑦ˆ | 𝑔 = 𝑔 𝑗 )∀𝑔𝑖 , 𝑔 𝑗 ∈ {1, ..., 𝐺 }

(1)

where 𝐺 is the total number of groups and 𝑦ˆ is the likelihood of a given outcome.
Epistemic Utility. We deﬁne epistemic utility as the value or usefulness of information in terms of improving knowledge, understanding, or beliefs. It encompasses the epistemic beneﬁts gained from acquiring accurate, reliable, and
relevant information. These beneﬁts could, of course, enhance decision-making, prediction, problem-solving, or overall intellectual development, but epistemic utility refers only to the beneﬁts in terms of the agent’s epistemic state
(even if those improvements do not immediately lead to improved outcomes). It thereby captures the intrinsic value of
acquiring knowledge and understanding. It underscores the role of information as a valuable resource for intellectual
growth, social mobility and production, and captures the motivational value of information for people’s behavior.
3 RELATED WORK
This work is most closely related to eﬀorts to understand fairness in recommender systems, many of which deﬁne
fairness as parity in some performance measure across some sensitive attribute [5, 6, 7, 8, 9, 10]. Methods for individual
notions of fairness similarly require a domain-speciﬁc similarity function and focus on trade-oﬀs between individual
and group fairness[11, 12]. In contrast, we start from the position that fairness requires equity across all individuals,
rather than being deﬁned solely through comparisons. This approach enables us to account for cases of unfairness
where everyone is worse oﬀ (including situations with non-comparative injustices), not only those in which one group
is.
More speciﬁcally, prior work has aimed to ensure proportional representation and exposure, often in e-commerce
settings. Measures such as disparate exposure and visibility still require access to sensitive or group attributes [8, 9, 10].
Calibration, in which the expected proportions of predicted classes match those observed in available data, implements
group fairness without explicit knowledge of sensitive attribute status[13, 14, 15, 16, 17]. However, its application in
domains such as news can contribute to hegemonic regimes of representation, making it limited to applications where
less diversity is valued by users[17]. Additionally, fairness deﬁnitions that target disparities in utility compare average
sensitive attribute group performance rather than providing assurances for individuals or the entire population as a
whole [11, 18, 19, 20].
Concerns about unfairness in personalized algorithms is connected to research on epistemic injustice[21]. The core
concern in this latter work is that epistemic limitations may impair an agent’s ability to recognize or respond to
1 within some tolerance as is legally permissible, e.g., the four-ﬁfths rule[3]

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

3

injustice. For example, if an individual does not have the concept of ‘racial discrimination’, then they might fail to
detect its occurrence despite suﬀering harms from it. That is, the injustice is not only about what happens to the
individual, but also about their (epistemic) inability to recognize, understand, and respond to it. Personalized systems
can readily lead to diﬀerent individuals having diﬀerent information – in fact, that is one goal of such systems – and so
we expect that individuals would develop diﬀerent concepts as a result. Hence, personalized systems can signiﬁcantly
raise the risks of epistemic injustice.
Research on over-personalization and information access is similarly closely related. While the inﬂuence of personalization in ﬁlter bubbles and echo chambers has been denied for purely technical systems [22, 23, 24], when considering
human interactions, the inﬂuence becomes somewhat complex. This is, in part, due to signiﬁcant variability in how
users engage with a site, accept or reject information, and perceive the impartiality of the content they receive [25, 26,
27, 28, 29]. Our research adopts a sociotechnical approach to deﬁne and scope the problem, taking into account the
concept of epistemic utility and making reasonable assessments of human eﬀort.
4 CORE TENSION: PERSONALIZATION AND STANDARD APPROACHES TO FAIRNESS
Fairness is often deﬁned as metric parity across similar groups or individuals, where determining similarity across units
remains an open question. In search settings, one might start by grouping together individuals with similar values and
interests, as, on fairness grounds, they may be expected to have a similar disambiguation or content recommendation
experience. However, determining values can be diﬃcult to operationalize because explicit and implicit preferences
may be conﬂicting or diﬃcult to collect [30]. In addition, the level of granularity by which to measure such values
remains unknown. In this section, we construct arguments for why user similarity of values is diﬃcult, impractical,
or perhaps even impossible to determine, thereby making similarity-based fairness deﬁnitions diﬃcult to deploy or
conceptually impossible.
Sensitive attributes are problematic and coarse proxies for values. The simplest approach to constructing “similar”
groups for fairness considerations could use sensitive attributes (i.e. race, ethnicity, gender, age, religious aﬃliation,
disability, etc). However, this approach assumes that individuals with the same sensitive attributes not only face similar
struggles, lived experiences, discrimination, and prejudice, but also have relevantly similar values. In practice, this
approach would likely result in the treatment of minority groups as monoliths – perhaps even as a homogeneous
‘Other’. This is problematic not only in its instrumental failures of insuﬃcient speciﬁcity and complexity, but also in
its intrinsic treatment of a single sensitive attribute as representative of all values. Additionally, conditional groupings
only ensure parity guarantees for the average member of a group, not for each individual nor necessarily an individual
that exists, therefore enabling unfairness for anyone that doesn’t satisfy such assumptions.
Taking an intersectional approach, i.e. considering subgroups of sensitive attributes may better approximate similarity of values. Consider, for example, race and gender, meaning groups such as “Black Women” and “White Men”.
This still considers an entire group of people homogeneously, thereby rendering the plight of, for example, all Black
Women into a singular experience. In the extreme, considering all possible sub-groups across all sensitive attributes
faces practical challenges, as each subgroup becomes impractically small (for data availability purposes), thereby reifying sensitive attributes in a constant struggle for relative popularity and public consciousness [31].
User search history is noisy, underdetermines values, and can vary temporally. An alternative approach would be to
use the individual’s behavior to determine the relevant group for fairness considerations. For instance, in the context
Manuscript submitted to ACM

4

Jennifer Chien and David Danks

of a search engine, we could condition on exact search history. Unfortunately, this approach faces three open challenges which we highlight in the context of search, though the problems naturally generalize to other systems that
use observed behavior to personalize on the basis of unobserved (inferred) features.
First, not all search behaviors are equally informative. For example, a search query may be made due to diﬀerent
situational demands (e.g., homework, third party question) or may arise from very diﬀerent motivations (e.g., devil’s
advocate vs. searching for validative or supporting arguments). Hence, a speciﬁc history of search queries may or may
not be informative about an individual’s values; the history alone is not suﬃcient.
Second, even if search behaviors are “genuine,” they are noisy indicators for the totality of an individual’s interests
or values. For example, the individual agent might not search for a topic they already have suﬃcient information
on, but it may still be a core value. More generally, consider two users with identical search histories at any given
point in time. Conditioning on search history would imply that they should receive similar experiences. However, it is
highly unlikely that these individuals have the exact same sets of values and interests: that is, multiple sets of values
and interests could be consistent with the observed behavior (i.e., the space of values are underdetermined by the
evidence). Therefore, since queries subject to the limitations of observational data and missing contextual information,
conditioning on search history alone has the potential to greatly under-specify an individual’s values.
Finally, search history provides no guarantees of stability over time. Consider, again, two individuals with the same
user history. Adding a diﬀerent element to each user’s search history could lead them to be placed in fundamentally
diﬀerent groups. More generally, at any point in time, there are inﬁnitely many ways that a search history could
progress, thereby making it impossible to guarantee fairness over time for these two individuals, let alone all. Fairness
groupings not sensitive to the time and search history, therefore, may not ever converge to fair groupings in the limit.
This echoes a similar problem in fairness research, where fairness constraints at each round of deployment do not
ensure long-term aggregate parity in the same metric [32]. While this has not rendered particular fairness metrics
obsolete, this instability points to ambiguity in the extent to which fairness must provide guarantees temporally.
5 A DIFFERENT APPROACH
5.1

Personalization

Delving further into the notion of a system tailored to each person’s needs, let us deﬁne personalization. Given a search
engine, companies may strive to deliver a service that maximizes epistemic utility and convenience. In order to avoid
wasting time, resources, or eroding consumer patience, search engine providers may optimize to disambiguate queries
as eﬃciently as possible. Increases in eﬃciency can be made by leveraging all accessible information about a user.
This may include search history, publicly available data, social media proﬁles, and inferred or disclosed demographic
information to accurately disambiguate a user’s query. This motivates the following deﬁnition:
Deﬁnition 5.1. Inﬁnite Personalization. Consider a platform that recommends relevant content to a user 𝑢 𝑗 tailored to their previous history ℎ 𝑗,𝑡 at time 𝑡. Since the space of all possible information is very large, for each piece of
content 𝑐𝑖 ∈ 𝐶, the system calculates probabilities 𝑃 (𝑐𝑖 | ℎ 𝑗,𝑡 ) estimating the likelihood of relevance. We deﬁne inﬁnite
personalization as the phenomena in which a system is allowed to produce extremely ﬁne-grained personalization.
Formally, there exists some 𝑐𝑖 such that 𝑃 (𝑐𝑖 | ℎ 𝑗,𝑡 ) < 𝜃 and 𝑃 (𝑐𝑖 | ℎ 𝑗,𝑡+𝜖 ) < 𝜃, meaning that there exists some content
such that it is arbitrarily improbable < 𝜃 to recommend a particular piece of content 𝑐𝑖 to a user, even if a user were
to add 𝜖 queries to their search history.
Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

5

For instance, user 𝑢 𝑗 with a search history exclusively composed of cats inputs “bengal toys” into a search engine
and receives recommendations about Bengal cat toys. We consider the system to be exhibiting inﬁnite personalization
if the user can take 𝜖 queries about dogs (or anything else) and still not receive a recommendation for Bengal dog toys
when searching “bengal toys”.
Having deﬁned personalization and it’s eﬀects in the extreme case, we now deﬁne fairness as equity and provide
motivation for the outcome we target.

5.2

Equity

In ensuring parity across similar groups, fairness guarantees are only made across those deemed suﬃciently similar. An
equity-based approach, however, centers around every individual achieving their desired outcome. Such approaches
promote social cohesion, collective liberation, and address long-standing inequities, such as poverty, public health,
social mobility, and overall well-being. Moreover, they tackle concerns regarding temporal stability and robustness
by establishing a consistent criterion for each deployment phase. Equity-based approaches lend themselves to audits,
legislation, and transparency, as they deﬁne a singular outcome requirement for all users. An open challenge, however,
is what metrics to deﬁne equity with respect to, as each metric targets a diﬀerent kind of unfairness.
Equity across quality of information targets unfairness when one person gets more utility from their query compared to a diﬀerent query of another person. For example, this type of equity requires that we equalize the number
of recommendations and quality of information provided for any given query. However, this is infeasible as we cannot control the quantity of information known for a given topic. Thus, given that there are inﬁnitely many distinct
queries any user can make, ensuring that each query has the same number of relevant articles would require perfect
knowledge.
Equity in speed of access to information targets unfairness when one person receives the relevant information faster
than another. This equalizes the utility for every user by either: slowing down or acting intentionally uncooperatively
for queries that are more easily disambiguated, or speeding up the service for “slower” queries. Both are infeasible. The
former fails to maximize overall utility and sacriﬁces function in the name of fairness. This in turn renders adoption
by stakeholders subject to capitalistic pressures even more unlikely. The latter is also infeasible, as we cannot directly
control disambiguation rate and therefore speed up some queries over others. More generally, information is an inﬁnitely shareable resource, so providing information to one person does not deprive someone else the beneﬁt of that
same information. Therefore speed of access to information is a poor target for an equity-based intervention.
In contrast, equity in epistemic utility targets unfairness in personalized access to the information resources. This is
motivated by the intuition that although we cannot directly control the availability of information or disambiguation
rate, everyone should have some baseline access to all information. Thus, we propose an upper bound on the number
of queries to access to any piece of information. This means that if the information does exist, all users should be able to
access it by exerting a “reasonable” amount of eﬀort. This does not require that all queries have the same accessibility
or disambiguation, nor that all users be provided the exact same service. It does provide a soft guarantee that everyone
is able to achieve some base-level epistemic utility without requiring the grouping or comparison of individuals on the
basis of their values, needs, or histories. Epistemic utility, particularly through the internet, is recognized as a universal
human right by Article 19 of the Universal Declaration of Human Rights [33], stating that, “Everyone has the right to
freedom...to seek, receive and impart information and ideas through any media and regardless of frontiers.”
Manuscript submitted to ACM

6

Jennifer Chien and David Danks
Deﬁnition 5.2. 𝜖-Equity Fairness. Given a user 𝑢 𝑗 with history ℎ 𝑗,𝑡 , we deﬁne 𝜖-Equity fairness as the upper bound

(𝜖) of the number of additions to the search history such that for any given content 𝑐𝑖 ∈ 𝐶, adding 𝜖 queries to the
history will result in 𝑃 (𝑐𝑖 | ℎ 𝑗,𝑡+𝜖 ) ≥ 𝜃, enabling the user to obtain the relevant content. Here, we consider the case in
which the relevant content does not exist as out of scope and focus purely on access.
Embracing equity as a fundamental principle of fairness presents a range of complex challenges. These challenges
encompass deﬁning a baseline level of equity for all users, determining the pertinent factors that contribute to establishing such a baseline, and deliberating whether these considerations should be domain-speciﬁc or universally applicable.
Rather than proposing a singular standard, we propose adopting a heuristic operationalization that prioritizes those
who are most disadvantaged in terms of knowledge and understanding. Our intention is not to argue for a strictly prioritarian perspective, but that the incremental advancements can achieve the ultimate goal of equity in the long-term.
Thus, we advocate for equity as a guiding concept to achieve equality by inductively reaching fairness in the limit.
Having established deﬁnitions inﬁnite personalization and 𝜖-equity fairness, next we ﬂesh out the tension between
the two.
6 INHERENT CONFLICT AND TRADE-OFFS
Inﬁnite personalization, we posit, comes at a cost to 𝜖-equity fairness. More explicitly, in inﬁnite personalization, the
relevance probability of some information 𝑐𝑖 to a user can be less than 𝜃. This is due to the eﬀects of personalization:
indexing and re-ranking for eﬃcient navigation across the vast space of information. However, this violates 𝜖-equity
fairness, as some information will be eﬀectively rendered unreachable within a ﬁxed window of queries. We provide
some examples of trade-oﬀs below:
• Search engine personalization prioritizes results relative to a user’s input query and prior history. For any given
topic orthogonal to a user’s past history, personalization may rank desired items far beyond the average, expected, or typical number of results any given user explores, rendering it practically infeasible to reach.
• Social media serves many functions: dissemination of news and information, entertainment, and social connection. Personalization in the extreme may contribute to propagation of mis- and disinformation, echo chambers,
ﬁlter bubbles, radicalization, and social disconnection.
• Github Copilot (developed by OpenAI) focuses on code completion, providing suggestions for code lines or
entire functions directly integrated into interactive development environments. Here, personalization has the
potential to provide codebase-speciﬁc suggestions and completion, but may also infringe on a coder’s ability
construct novel functionality.
• Google Bard is a language model for synthesizing insights for settings with a wide range of opinions, perspectives, or no right answer. As an augmentation to search-engine information retrieval, this may facilitate more
eﬃcient ordering and disambiguation of search results, but may also infringe on access to seemingly irrelevant
information.
• ChatGPT is a general-purpose large language model (LLM) designed to engage in human-like conversations and
answer a wide range of questions. Personalization in an extreme case, may lead to conversational loops – or
conversations that remain within the scope of a single topic, rather than having the ﬂexibility to move between
topics despite a user’s prompt or request. 2
2 Here we list several examples that use LLMs due to their relevance to personalization, information synthesis and curation, and appeals to human-

language usability. We acknowledge that there are many open relevant problems related to personalization, such as contribution or exacerbation of
representational harms. For the scope of this work, we focus on how such systems can impact access to information.
Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

7

The key takeaway of this list is not that personalization necessitates unfairness or inequity, nor that fairness is not
achievable in personalized systems. Rather, to emphasize that personalization without consideration of fairness can
readily lead to systems that deny some individuals a base-level of expected epistemic utility.
7 POLICY GOALS AND EXAMPLES OF IMPLEMENTATIONS
While this conceptual analysis provides guidance about how best to think about fairness in the context of personalized
algorithms, it does not provide guidance about how to achieve fairer, less biased systems. As we turn to more practical
recommendations, we start by considering key stakeholders and their associated roles (Table 1). This creates a lexicon
and clariﬁes the expected function of each stakeholder.
Each of these stakeholders has a wide range of actions available that can lead to fairer personalized systems (see
Table 2). These actions include a wide range of governance mechanisms, ranging from hard (e.g., regulation) to soft (e.g.,
social norms). We emphasize that there is no single response to “ﬁx” unfairness in personalized systems: inevitably,
many interdependent actions will likely need to be taken in order to make progress on this problem. Moreover, these
are by no means intended to be exhaustive, but rather provide a grounded starting point that emphasizes the necessity
of cross-stakeholder collaborations.

Manuscript submitted to ACM

8

Jennifer Chien and David Danks

Stakeholder

Role

Government

Develop guidelines, principles, procedures, regulations, and standards for
safe, sustainable deployment. May hold the power to create structural supporting systems directly within the government or ﬁnancially support external systems of enforcement of such regulations. May also enact other means
of operationalizing policy, such as hosting interdisciplinary research summits
or sending representatives to build collaborations with other stakeholders to
better inform outputs. This stakeholder may also set roles and norms for other
stakeholder responsibilities and consequences.

Civil Society

Conduct evaluations and investigations of algorithms and software developed by others. This stakeholder may serve as an external, third-party entity
for conducting unbiased evaluations of performance and compliance with
policy. May be responsible for developing novel methods of measurement.

Industry

Implement technical methods, tools, and technological innovations, usually
manifesting as a consumer-facing service or product. May be subject to practical implementation constraints (i.e. ﬁnancial, competition, or legislative).
While a particular product may be theoretically feasible, it may not be practically viable or sustainable for a company to produce. Responsibilities may
also include foreseeing and adequately mitigating harms of deployed products on those directly and indirectly aﬀected.

Academia (broad)

Develop and implement technical, theoretical, and conceptual knowledge
without direct consideration of proﬁts. Results can include work that builds
upon understanding, implications, or evaluation methods (i.e. downstream
impacts, sociotechnical approaches). This stakeholder has the potential to act
as an unbiased actor in developing best-practices for sustainable, long-term
practices, potentially at the direct cost of proﬁts (i.e. considering environmental, social, and cultural norms and inﬂuences).

General Public

Provide input, feedback, criticism, and opinions on direction and alignment
with values for research and innovation via implicit and explicit signals on
the order of communities, groups, and/or individuals. Public support (or lack
thereof) can be collected across a multitude of avenues and granularities, including ﬁnancial support, interest, engagement, collective action, protests, organizations, and projects.

Table 1. Stakeholder Roles. We define the expected roles of stakeholders for clarity of responsibilities enumerated in Table 2. We
note that these responsibilities are not restrictive. There are many ways in which one stakeholder may overlap or even take over the
responsibilities of another. For instance, if a company develops a policy for user privacy while it’s competitors do not, self-imposed
constraints on data collection may impact their product performance. However, as this design value receives public support, this can
become a standard competitive service without the intervention of government or civil society.

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

Goal

Example Implementation/Operationalization

Mitigation

Industry, Academia: Develop algorithms that facilitate connections between disjoint parts of the data manifold. This might be analogous to connecting diﬀerent
parts of the internet together through links, but done by creating arbitrary injections that fabricate similarities, covariances, or links between disjoint parts of the
training data.
Government: Construct structural incentives (ﬁnancial, prestige/recognition,
standards) for achieving baseline levels of interconnectivity to ensure equitable
access.

Audit

Industry, Academia, Civil Society: Develop measures that quantify epistemic
utility for users using a range of behaviors and data (e.g. tool use, experienced frustration, time to complete a task, etc.). Produce formal, domain-speciﬁc representations of distributions of utility, such as non-uniform distributions being indicators
of threats to fairness. Measure utility and study barriers to utility for non-users and
those who have left the platform.
Civil Society: Report the utility distributions for non-users to compare against
those that are already using the service.
Academia: Deﬁning and measuring downstream impact may be essential to thorough quantiﬁcation of disparities and unfairness. Downstream impacts such as social inﬂuences on platform usage can be inﬂuenced by an individual’s experience.
Government: Regulation for timing and structure of fairness audits for personalized systems, including guidance on composition of internal and external stakeholders.

Transparency

Industry, Civil Society: Disclosure of aforementioned distributions of utility
across various groups of users. Reports should include comments about whether
these distributions are problematic, what are the proposed solutions, and on what
timeline.
Government: Clear consequences and/or reparations for users aﬀected throughout improvement and for continued negligence.
General Public: Active engagement and feedback on reports to inform appropriate
consequences and informed public (dis)favor.

Individual Control

Industry, Academia: Design control mechanisms over the degree of exploration
to a query when serving recommendations. This may include prompting users for
additional input before producing a prediction. Alternatively, allowing users to reset their history or set manual preferences over personalization ﬁlters.
Government: Regulation and enforcement of satisfaction and compliance.
General Public: Participation and feedback on norm setting of what features
should be available, how accessible services should be, determine embedded values such as degree of “reasonableness” that are feasible and realistic for all users.

9

Education & Awareness Industry, Academia, Government, General Public: Workshops and training
sessions can facilitate comprehension of the dangers, limitations, and better calibrate users to appropriate expectations of functionality. These workshops can include education on rights and how to advocate for them. For instance, if companies
fail to comply, where can people ﬁle their grievances so they can be aggregated and
collectively analyzed?
Industry, Academia, Civil Society: Development of interactive tools that enable
visualization and comparison of an individual’s utility compared to the general
population or those relevantly comparable.
Manuscript submitted
to ACM
Civil society, Government: Regulation and continued measurement
to ensure
such tools ﬂag anomalous data and ensure improvement within some time-frame.
Create and publicly release of reports of corrections, adjustments, and investigations made to remedy grievances.
Table 2. Example Policy Interventions and Goals by Stakeholder. Although we separate actions by stakeholders, we emphasize that
these goals require collaborations and contributions across multiple stakeholders to achieve such solutions.

10

Jennifer Chien and David Danks

REFERENCES
[1]

Zeynep Tufekci. How recommendation algorithms run the world. Apr. 2019. url: https://www.wired.com/story/how-recommendation-

[2]

Gurkan Solmaz, Jesmin Jahan Tithi, and Juan Miguel de Joya. Why algorithmic fairness? Oct. 2020. url: https://selects.acm.org/selectio

[3]

Elizabeth Anne Watkins, Michael McKenna, and Jiahao Chen. “The four-ﬁfths rule is not disparate impact: a
woeful tale of epistemic trespassing in algorithmic fairness”. In: arXiv preprint arXiv:2202.09519 (2022).

[4]

Cynthia Dwork et al. “Fairness through awareness”. In: Proceedings of the 3rd innovations in theoretical computer
science conference. 2012, pp. 214–226.

[5]

Rishabh Mehrotra et al. “Towards a fair marketplace: Counterfactual evaluation of the trade-oﬀ between relevance, fairness & satisfaction in recommendation systems”. In: Proceedings of the 27th acm international conference on information and knowledge management. 2018, pp. 2243–2251.

[6]

Nasim Sonboli et al. “Opportunistic multi-aspect fairness through personalized re-ranking”. In: Proceedings of
the 28th ACM Conference on User Modeling, Adaptation and Personalization. 2020, pp. 239–247.

[7]

Weiwen Liu et al. “Personalized fairness-aware re-ranking for microlending”. In: Proceedings of the 13th ACM
Conference on Recommender Systems. 2019, pp. 467–471.

[8]

Ludovico Boratto, Gianni Fenu, and Mirko Marras. “Interplay between upsampling and regularization for provider

[9]

Elizabeth Gómez et al. “The winner takes it all: geographic imbalance and provider (un) fairness in educational

fairness in recommender systems”. In: User Modeling and User-Adapted Interaction 31.3 (2021), pp. 421–455.
recommender systems”. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021, pp. 1808–1812.
[10]

Abhisek Dash et al. “When the umpire is also a player: Bias in private label product recommendations on ecommerce marketplaces”. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021, pp. 873–884.

[11]

Ziwei Zhu, Xia Hu, and James Caverlee. “Fairness-aware tensor-based recommendation”. In: Proceedings of the
27th ACM international conference on information and knowledge management. 2018, pp. 1153–1162.

[12]

Bora Edizel et al. “FaiRecSys: mitigating algorithmic bias in recommender systems”. In: International Journal of
Data Science and Analytics 9 (2020), pp. 197–213.

[13]

Himan Abdollahpouri et al. “Calibrated recommendations as a minimum-cost ﬂow problem”. In: Proceedings of
the Sixteenth ACM International Conference on Web Search and Data Mining. 2023, pp. 571–579.

[14]

Himan Abdollahpouri et al. “User-centered evaluation of popularity bias in recommender systems”. In: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization. 2021, pp. 119–129.

[15]

Bruna Wundervald. “Cluster-based quotas for fairness improvements in music recommendation systems”. In:

[16]

Diego Corrêa da Silva, Marcelo Garcia Manzato, and Frederico Araújo Durão. “Exploiting personalized calibra-

[17]

Yashar Deldjoo et al. “Fairness in recommender systems: research landscape and future directions”. In: User

International Journal of Multimedia Information Retrieval 10.1 (2021), pp. 25–32.
tion and metrics for fairness recommendation”. In: Expert Systems with Applications 181 (2021), p. 115112.
Modeling and User-Adapted Interaction (2023), pp. 1–50.
[18]

Yashar Deldjoo et al. “Recommender systems fairness evaluation via generalized cross entropy”. In: arXiv preprint
arXiv:1908.06708 (2019).

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility
[19]

11

Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. “Adversarial machine learning in recommender
systems (aml-recsys)”. In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020,
pp. 869–872.

[20]

Yunqi Li et al. “User-oriented fairness in recommendation”. In: Proceedings of the Web Conference 2021. 2021,
pp. 624–632.

[21]

Miranda Fricker. Epistemic injustice: Power and the ethics of knowing. Oxford University Press, 2007.

[22]

Cédric Courtois, Laura Slechten, and Lennert Coenen. “Challenging Google Search ﬁlter bubbles in social and
political information: Disconforming evidence from a digital methods case study”. In: Telematics and Informatics
35.7 (2018), pp. 2006–2015.

[23]

William H Dutton et al. “Searching through ﬁlter bubbles, echo chambers”. In: Society and the internet: How

[24]

Efrat Nechushtai and Seth C Lewis. “What kind of news gatekeepers do we want machines to be? Filter bub-

networks of information and communication are changing our lives (2019), p. 228.
bles, fragmentation, and the normative dimensions of algorithmic recommendations”. In: Computers in human
behavior 90 (2019), pp. 298–307.
[25]

Tawanna R Dillahunt, Christopher A Brooks, and Samarth Gulati. “Detecting and visualizing ﬁlter bubbles in
Google and Bing”. In: Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in
Computing Systems. 2015, pp. 1851–1856.

[26]

Axel G Ekström, Diederick C Niehorster, and Erik J Olsson. “Self-imposed ﬁlter bubbles: Selective attention and
exposure in online search”. In: Computers in Human Behavior Reports 7 (2022), p. 100226.

[27]

Mykola Makhortykh and Mariëlle Wijermars. “Can ﬁlter bubbles protect information freedom? Discussions of
algorithmic news recommenders in Eastern Europe”. In: Digital Journalism (2021), pp. 1–25.

[28]

Jaime Teevan. “How people recall, recognize, and reuse search results”. In: ACM Transactions on Information
Systems (TOIS) 26.4 (2008), pp. 1–27.

[29]

Kelly Shelton. Council post: The value of search results rankings. Nov. 2017. url: https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/th

[30]

Tessa ES Charlesworth and Mahzarin R Banaji. “Patterns of implicit and explicit attitudes: IV. change and stability from 2007 to 2020”. In: Psychological Science 33.9 (2022), pp. 1347–1371.

[31]

Youjin Kong. “Are “Intersectionally Fair” AI Algorithms Really Fair to Women of Color? A Philosophical Analysis”. In: 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 485–494.

[32]

Lily Hu and Yiling Chen. “A short-term intervention for long-term fairness in the labor market”. In: Proceedings
of the 2018 World Wide Web Conference. 2018, pp. 1389–1398.

[33]

url: https://www.unesco.org/en/right-information.

Manuscript submitted to ACM

This figure "acm-jdslogo.png" is available in "png" format from:
http://arxiv.org/ps/2309.11503v1

