fairness
and
bias
in
multimodal
ai
survey
tosin
adewumi
lama
alkhaled
namrata
gurung1
goya
van
boven2
irene
pagliai3
machine
learning
group
ltu
sweden
qualityminds
gmbh
germany
utrecht
university
the
netherlands
university
of
g√∂ttingen
germany
firstname.lastname@ltu.se
namrata.gurung@qualityminds.de
j.g.vanboven@students.uu.nl
irene.pagliai@uni-goettingen.de
abstract
arxiv
2406
19097v2
cs
cl
sep
2024
the
importance
of
addressing
fairness
and
bias
in
artificial
intelligence
ai
systems
cannot
be
over-emphasized
mainstream
media
has
been
awashed
with
news
of
incidents
around
stereotypes
and
other
types
of
bias
in
many
of
these
systems
in
recent
years
in
this
survey
we
fill
gap
with
regards
to
the
relatively
minimal
study
of
fairness
and
bias
in
large
multimodal
models
lmms
compared
to
large
language
models
llms
providing
50
examples
of
datasets
and
models
related
to
both
types
of
ai
along
with
the
challenges
of
bias
affecting
them
we
discuss
the
less-mentioned
category
of
mitigating
bias
preprocessing
with
particular
attention
on
the
first
part
of
it
which
we
call
preuse
the
method
is
less-mentioned
compared
to
the
two
well-known
ones
in
the
literature
intrinsic
and
extrinsic
mitigation
methods
we
critically
discuss
the
various
ways
researchers
are
addressing
these
challenges
our
method
involved
two
slightly
different
search
queries
on
two
reputable
search
engines
google
scholar
and
web
of
science
wos
which
revealed
that
for
the
queries
fairness
and
bias
in
large
multimodal
models
and
fairness
and
bias
in
large
language
models
33
400
and
538
000
links
are
the
initial
results
respectively
for
scholar
while
and
50
links
are
the
initial
results
respectively
for
wos
for
reproducibility
and
verification
we
provide
links
to
the
search
results
and
the
citations
to
all
the
final
reviewed
papers
we
believe
this
work
contributes
to
filling
this
gap
and
providing
insight
to
researchers
and
other
stakeholders
on
ways
to
address
the
challenges
of
fairness
and
bias
in
multimodal
and
language
ai
introduction
fairness
and
bias
are
very
important
topics
that
cut
across
many
domains
in
the
society
the
rapid
advancements
in
the
research
and
applications
of
artificial
intelligence
ai
have
made
them
even
more
compelling
in
recent
times
such
that
many
studies
have
emerged
on
them
frankel
and
vendrow
2020
booth
et
al
2021
adewumi
et
al
2022
teo
et
al
2024
one
important
gap
in
the
literature
however
is
that
there
is
relatively
minimal
study
or
survey
on
fairness
and
bias
in
large
multimodal
models
by
multimodal
ai
we
mean
the
datasets
or
ai
models
that
can
take
one
or
more
modalities
as
input
and
or
another
as
output
this
gap
is
evidenced
by
the
fact
that
there
are
fewer
works
around
the
topic
for
example
query
search
on
google
scholar
returns
33
400
links
compared
to
538
000
links
for
fairness
and
bias
in
large
language
models
where
the
first
query
search
is
equivalent
to
the
boolean
operation
fairness
and
and
and
bias
and
in
and
large
and
multimodal
and
models
this
implies
more
than
16
times
the
result
compared
to
the
former
however
filtering
the
publication
year
range
to
2014
2024
reduces
the
links
to
17
200
and
19
300
respectively
we
intend
to
contribute
in
filling
that
gap
in
this
work
the
two
terms
fairness
and
bias
are
strongly
related
but
fairness
is
concerned
with
equality
and
justice
while
bias
is
concerned
with
systematic
error
which
may
arise
from
human
prejudices
booth
et
al
2021
alkhaled
et
al
2023
for
the
purpose
of
this
survey
fairness
may
be
defined
as
equal
representation
with
regards
to
given
sensitive
attribute
sa
hutchinson
and
mitchell
2019
frankel
and
vendrow
2020
hence
we
may
consider
generative
artifical
intelligence
genai
to
be
fair
if
it
generates
both
male
and
female
samples
with
equal
probabilities
with
regards
to
the
sa
gender
teo
et
al
2024
bias
is
non-random
systematic
error
in
measurement
resulting
in
difference
in
accuracy
in
one
entity
compared
to
another
given
the
ground
truth
booth
et
al
2021
scheuneman
1979
we
acknowledge
there
are
other
quantitative
definitions
of
fairness
and
bias
as
noted
by
hutchinson
and
mitchell
2019
and
weidinger
et
al
2022
it
appears
the
emergence
of
big
data
which
july
10
2024
has
brought
rapid
advancement
in
the
state-ofthe-art
sota
also
brought
along
the
increase
in
poor
quality
content
and
prediction
such
as
the
increased
criminal
prediction
for
black
and
latino
people
observed
by
birhane
et
al
2024a
similar
issues
are
observed
across
many
domains
including
healthcare
employment
forensics
criminal
justice
credit
scoring
and
computational
social
science
among
others
liang
et
al
2021b
ferrara
2023a
landers
and
behrend
2023
han
2023
according
to
wolfe
et
al
2023
the
model
vqgan-clip
similarly
to
stable
diffusion
generated
sexualized
images
for
the
harmless
prompt
17
year
old
girl
73
of
the
time
the
comparison
to
similar
prompt
with
the
term
girl
replaced
with
boy
shows
sharp
contrast
in
related
work
many
of
the
relatively
recent
surveys
on
fairness
and
bias
in
ai
appear
to
have
been
of
general
nature
or
focused
on
other
areas
pagano
et
al
2023
focused
their
attention
on
ml
generally
and
the
year
period
between
2017
and
2022
thereby
missing
the
nuances
and
some
of
the
details
related
to
natural
language
processing
nlp
and
multimodal
ai
mehrabi
et
al
2021
surveyed
applications
that
have
exhibited
bias
in
different
domains
listed
sources
of
bias
in
these
applications
and
created
taxonomy
for
fairness
definitions
on
the
other
hand
le
quy
et
al
2022
paid
attention
to
benchmark
tabular
datasets
for
fairness
analysing
relationships
between
different
protected
and
class
attributes
balayn
et
al
2021
focused
on
data
bias
in
data
engineering
and
management
research
arguing
for
the
enforcement
of
fairness
requirements
and
constraints
on
the
data
for
training
and
evaluating
systems
their
survey
method
is
limited
in
that
it
restricted
part
of
its
literature
search
between
2019
and
2020
in
our
work
in
addition
to
discussing
datasets
that
make
ai
models
biased
and
the
datasets
for
evaluating
bias
and
fairness
we
discuss
other
important
related
areas
such
as
the
mitigation
strategies
blodgett
et
al
2020
surveyed
over
140
articles
about
bias
in
nlp
and
realized
that
the
stated
motivations
are
usually
inconsistent
and
vague
and
the
articles
do
not
engage
with
the
broader
applicable
literature
that
is
external
to
nlp
they
therefore
made
recommendations
for
those
in
this
field
establish
the
relationships
in
language
and
social
hierarchies
by
referring
to
the
broader
literature
outside
of
nlp
be
explicitly
clear
on
why
the
system
described
as
biased
is
harmful
and
eval
uate
the
language
of
people
affected
by
the
biased
systems
their
survey
was
restricted
to
only
articles
on
text-based
nlp
thereby
excluding
speech
or
multimodal
ai
and
limited
to
articles
before
may
2020
however
we
follow
their
recommendations
and
our
work
answers
some
of
the
research
questions
identified
in
their
work
for
example
section
and
subsection
5.2
address
the
question
how
are
datasets
collected
the
survey
by
sun
et
al
2019
focused
on
recognizing
and
mitigating
nlp
gender
bias
and
the
authors
discussed
this
based
on
four
forms
of
representation
bias
denigration
stereotyping
recognition
and
underrepresentation
in
haltaufderheide
and
ranisch
2024
they
identified
ethical
issues
around
fairness
and
bias
with
llms
in
medicine
and
healthcare
additional
works
that
have
surveyed
fairness
and
bias
in
llms
include
bender
et
al
2021
meade
et
al
2022
gallegos
et
al
2023
chang
et
al
2024
myers
et
al
2024
in
view
of
the
foregoing
gap
and
challenges
this
work
critically
surveys
the
literature
with
the
primary
objective
of
ascertaining
what
the
state
of
work
is
on
fairness
and
bias
in
multimodal
ai
thereby
making
the
following
key
contributions
we
fill
the
gap
with
comprehensive
survey
of
fairness
and
bias
across
wide
spectrum
of
lmms
llms
and
multimodal
datasets
providing
50
examples
of
datasets
and
models
in
structured
way
along
with
the
challenges
of
bias
affecting
them
we
discuss
the
less-mentioned
category
of
mitigating
bias
in
nlp
preprocessing
though
more
common
in
general
ml
mehrabi
et
al
2021
pagano
et
al
2023
with
particular
attention
on
the
first
part
of
it
which
we
call
preuse
the
other
two
wellknown
mitigation
methods
in
the
literature
are
the
intrinsic
or
in-processing
and
extrinsic
or
post-processing
methods
ramesh
et
al
2023
cabello
et
al
2023
we
critically
discuss
many
important
approaches
to
addressing
the
challenges
of
fairness
and
bias
the
rest
of
this
paper
is
organised
as
follows
in
the
following
section
we
highlight
some
of
the
theories
around
fairness
and
bias
in
ai
in
section
we
explain
the
method
used
for
this
survey
in
section
we
focus
on
various
works
along
the
2.2
equity
theory
two
paradigms
of
lmms
and
llms
discussing
the
datasets
and
models
in
the
literature
in
section
we
discuss
widely
on
the
methods
to
evaluate
fairness
and
bias
the
datasets
for
evaluation
and
the
debiasing
strategies
we
conclude
the
survey
in
section
with
summary
and
possible
future
work
equity
theory
uses
unidimensional
concept
of
fairness
instead
of
multidimensional
according
to
leventhal
1980
it
perceives
justice
solely
on
the
merit
principle
and
the
final
distribution
of
reward
or
punishment
where
reward
is
proportional
to
contribution
adams
and
freedman
1976
fairness
and
bias
in
ai
2.2
2.1
implicit
and
explicit
bias
just
as
inanimate
objects
have
no
emotions
or
thoughts
objectification
theory
establishes
view
of
subject
as
primarily
without
human
characteristics
especially
for
women
and
girls
fredrickson
and
roberts
1997
heflick
et
al
2011
andrighetto
et
al
2019
the
theory
identifies
sexual
objectification
bias
which
is
when
the
emotions
or
thoughts
of
person
are
disregarded
and
one
is
treated
as
mere
body
parts
for
sex
fredrickson
and
roberts
1997
wolfe
et
al
2023
there
is
distinction
between
implicit
and
explicit
bias
such
that
given
two
sets
of
terms
that
express
the
bias
axis
gender
or
race
one
set
of
male
gender
terms
could
be
s1
dad
man
and
the
other
set
of
female
terms
be
s2
mum
woman
implicit
bias
is
sufficiently
specified
with
both
lists
however
explicit
bias
requires
two
additional
sets
of
attributes
a1
engineer
doctor
and
a2
caregiver
carer
that
express
the
terms
to
which
the
earlier
gendered
terms
exhibit
association
albeit
to
different
levels
hence
gender
biased
system
could
result
in
male
terms
in
s1
being
strongly
associated
with
attributes
of
career
terms
a1
compared
to
female
terms
s2
which
could
be
strongly
associated
with
attributes
of
home-related
terms
a2
friedrich
et
al
2021
besides
these
two
broad
distinction
of
bias
there
are
many
types
of
bias
depending
on
the
philosophical
or
social
perspectives
that
may
be
taken
hence
discussion
about
every
possible
type
of
fairness
or
bias
is
beyond
the
scope
of
this
study
but
we
refer
readers
to
mehrabi
et
al
2021
van
der
wal
et
al
2024
and
navigli
et
al
2023
for
some
of
the
many
types
that
may
be
listed
2.2
concepts
of
fairness
and
bias
in
this
section
we
highlight
few
of
the
concepts
around
fairness
and
bias
found
in
the
social
science
literature
2.2
justice
theory
the
theory
is
regarded
as
three-part
framework
of
distributive
interactional
and
procedural
justice
perceptions
greenberg
1990
landers
and
behrend
2023
distributive
justice
when
outcomes
are
expected
to
be
distributed
equally
is
the
overarching
aspect
related
to
ai
fairness
and
bias
according
to
landers
and
behrend
2023
it
is
based
on
equality
rules
need
or
equity
which
are
influenced
by
social
and
cultural
values
2.3
objectification
theory
consequences
of
bias
fredrickson
and
roberts
1997
confirms
that
objectification
victimises
the
subject
and
may
result
in
habitual
body
monitoring
thereby
increasing
mental
health
risks
sexual
dysfunction
eating
disorders
and
depression
this
unhealthy
reality
is
also
confirmed
by
swim
et
al
2001
they
realized
that
sexist
incidents
occur
more
against
women
and
have
negative
emotional
consequences
for
them
some
of
these
incidents
are
traditional
gender
role
stereotypes
degrading
remarks
and
sexual
objectification
which
are
found
in
the
data
used
for
training
ai
models
for
details
about
the
mechanics
of
training
language
models
we
refer
readers
to
radford
et
al
2019
and
hoffmann
et
al
2022
it
is
not
surprising
therefore
that
the
use
of
these
models
cause
the
same
negative
effects
for
those
affected
hence
fairness
and
bias
are
not
only
ethical
or
moral
issues
but
have
legal
implications
landers
and
behrend
2023
in
the
united
states
us
disparate
treatment
because
of
sensitive
attributes
is
unlawful
berry
2015
hutchinson
and
mitchell
2019
meng
et
al
2022a
this
is
also
the
case
in
some
other
countries
zafar
et
al
2017
methodology
in
order
to
have
fair
and
thorough
survey
for
the
stated
objective
in
section
we
followed
the
general
guidelines
recommended
for
conducting
systematic
literature
review
which
is
founded
on
rigorous
and
auditable
methodology
kitchenham
2004
brereton
et
al
2007
we
used
two
common
scientific
search
engines
google
scholar
and
web
of
science
wos
both
are
advantageous
because
they
index
the
main
literature
databases
or
publishers
including
the
institute
of
electrical
and
electronics
engineers
ieee
association
for
computing
machinery
acm
conference
on
neural
information
processing
systems
neurips
multidisciplinary
digital
publishing
institute
mdpi
public
knowledge
project
pkp
massachusetts
institute
of
technology
press
mit
press
public
library
of
science
plos
association
of
computational
linguistics
acl
ai
access
foundation
aiaf
higher
education
press
he
press
association
of
measurement
and
evaluation
in
education
and
psychology
epodder
new
york
university
school
of
law
nyusl
academic
conferences
and
publishing
international
acpi
machine
learning
research
press
mlrp
journal
of
machine
learning
research
jmlr
and
society
of
photographic
instrumentation
engineers
spie
wos
provides
second
assessment
to
scholar
because
of
the
limitations
of
the
latter
including
its
inclusion
of
non-peer-reviewed
links
or
possible
predatory
journals
in
its
results
of
primary
studies
or
papers
folty
nek
et
al
2019
the
two
search
engines
are
identified
as
largely
reliable
helping
to
mitigate
the
risk
of
incompleteness
in
the
search
folty
nek
et
al
2019
for
both
we
employed
similar
multi-phase
search
process
itemized
below
which
helped
to
bring
refinement
to
each
search
and
the
final
results
we
are
confident
this
approach
returned
the
most
applicable
results
for
the
purpose
of
this
study
in
the
first
phase
we
designed
the
following
two
search
queries
based
on
our
objective
fairness
and
bias
in
large
multimodal
models
fairness
and
bias
in
large
language
models
the
boolean
operation
in
both
search
engines
for
the
first
is
equivalent
to
fairness
and
and
and
bias
and
in
and
large
and
multimodal
and
models
while
the
second
is
fairness
and
and
and
bias
and
in
and
large
and
language
and
models
in
the
second
phase
we
filtered
the
results
based
on
the
time
period
of
just
over
10
years
2014
to
2024
and
relevance
after
review
of
paper
titles
and
abstracts
for
over
200
papers
for
example
one
result
involved
an
article
about
executive
coaching
programs
that
has
nothing
to
do
with
ai
or
social
bias
all
the
filtered
papers
are
in
english
in
addition
we
corrected
for
misplaced
results
papers
that
turned
up
in
one
search
result
though
they
belong
in
the
other
about
15
in
multimodal
belonging
in
language
and
in
language
belonging
in
multimodal
for
scholar
we
removed
papers
published
in
journals
in
beall
list
of
potential
predatory
journals
and
publishers
if
present
in
the
third
phase
we
critically
reviewed
the
papers
for
their
contributions
including
the
datasets
models
possible
solutions
proffered
on
fairness
and
bias
and
other
relevant
discussions
more
concretely
for
wos
we
searched
all
fields
of
the
documents
core
collection
across
all
editions
without
initially
restricting
the
year
of
publication
for
scholar
the
first
phase
returned
33
400
and
538
000
result
links
over
many
pages
for
the
first
and
second
queries
respectively
while
returning
and
50
for
wos
filtering
scholar
based
on
the
time
period
returned
17
200
and
19
300
links
which
finally
reduced
to
69
and
101
at
the
end
of
the
second
phase
for
the
first
and
second
terms
respectively
while
returning
and
44
for
wos
it
is
noteworthy
that
for
scholar
there
were
equally
100
links
to
start
with
for
both
queries
the
boolean
search
operation
involved
all
the
individual
words
in
any
arbitrary
order
without
case
sensitivity
and
narrowed
the
search
to
mostly
relevant
documents
we
compare
multimodal
models
with
only
language
models
in
the
search
terms
because
the
latter
with
the
introduction
of
the
transformer
architecture
vaswani
et
al
2017
have
influenced
computer
vision
yuan
et
al
2021
and
they
serve
as
important
components
for
many
multimodal
ai
furthermore
we
recognize
that
multimodal
implies
the
combination
of
language
or
speech
and
vision
but
we
did
not
use
this
distinction
in
our
multimodal
search
because
the
boolean
equivalent
can
result
in
false
positives
even
when
we
attempted
it
we
had
less
than
50
links
in
result
compared
to
the
second
term
of
beallslist
.net
standalone-journals
table
distribution
of
scientific
papers
on
google
scholar
filtered
total
is
limited
to
the
year
range
2014
table
distribution
of
scientific
papers
on
web
of
science
wos
filtered
total
is
limited
to
the
year
range
2024
and
to
the
first
100
links
per
search
query
excluding
irrelevant
links
2014
2024
and
removes
irrelevant
results
publisher
multimodal
unfiltered
total
33
400
ieee
10
elsevier
acm
14
springer
neurips
nature
mdpi
mlrp
pkp
10
mit
press
11
pubmed
12
cambridge
13
plos
14
jmlr
15
acl
16
spie
17
de
gruyter
18
wiley
19
sage
20
academic
pinnacle
filtered
58
sub-total
21
arxiv
11
22
preprints
filtered
to
69
tal
language
538
000
13
11
23
publisher
unfiltered
total
corrected
total
ieee
elsevier
acm
springer
neurips
nature
mdpi
22
mlrp
22
pkp
16
acl
13
wiley
17
aiaf
12
now
filtered
total
served
that
there
are
fewer
pair-reviewed
papers
on
multimodal
models
compared
to
language
models
77
23
101
only
language
search
and
multimodal
is
standard
term
in
the
field
finally
for
scholar
we
captured
archived
papers
arxiv
because
sometimes
their
peerreviewed
versions
exist
but
may
not
appear
in
the
result
the
summary
statistics
of
the
search
are
presented
in
tables
and
while
the
references
to
the
papers
and
the
link
to
the
search
results
are
provided
in
the
appendix
the
useful
data
from
the
reviewed
papers
or
primary
studies
about
datasets
ai
models
and
other
contributions
are
then
discussed
in
sections
4.1
4.2
and
respectively
we
note
that
the
more
relevant
papers
turn
up
on
the
first
page
or
at
the
top
of
the
search
while
the
quality
of
results
degrade
as
one
progresses
through
the
pages
or
list
from
the
tables
it
can
be
ob
multimodal
language
50
46
12
10
44
findings
on
fairness
and
bias
in
lmms
and
llms
it
is
commonly
agreed
that
ai
models
learn
much
of
their
bias
from
the
data
they
are
trained
on
and
many
datasets
especially
those
for
pretraining
are
from
the
internet
which
contains
diverse
spectrum
of
content
wolfe
and
caliskan
2022a
tables
and
summarize
some
relevant
datasets
and
the
models
beset
by
challenges
of
bias
respectively
all
the
25
datasets
identified
have
their
challenges
and
by
extension
the
25
ai
models
which
train
on
them
some
of
these
challenges
include
stereotypes
porn
misogyny
racial
gender
religious
cultural
age
and
demographic
biases
4.1
lmms
liang
et
al
2021a
acknowledged
that
multimodal
representations
are
challenging
because
they
seek
to
integrate
information
from
multiple
areas
in
applications
like
robotics
finance
healthcare
and
more
however
multimodal
ai
has
not
enjoyed
enough
resources
to
study
generalization
across
different
modalities
and
the
complexities
of
training
with
regards
to
bias
wolfe
et
al
2023
found
evidence
of
sexual
objectification
bias
in
models
table
summary
of
some
datasets
and
their
fairness
bias
challenges
data
in
the
lower
part
of
the
table
are
usually
used
in
downstream
tasks
dataset
commoncrawl
raffel
et
al
2020
modality
text
vision
laion-400m
5b
schuhmann
et
al
2021
2022
webimagetext
wit
radford
et
al
2021
text
vision
some
challenges
of
bias
fairness
fake
news
hate
speech
porn
racism
gehman
et
al
2020
luccioni
and
viviano
2021
misogyny
stereotypes
porn
birhane
et
al
2021
2024b
text
vision
racial
gender
biases
radford
et
al
2021
datacomp
gadre
et
al
2024
webli
chen
et
al
2022
cc3m-35l
thapliyal
et
al
2022
coco-35l
thapliyal
et
al
2022
wit
srinivasan
et
al
2021
text
vision
text
vision
text
vision
text
vision
text
vision
racial
bias
gadre
et
al
2024
age
racial
gender
biases
stereotypes
chen
et
al
2022
cultural
bias
thapliyal
et
al
2022
cultural
bias
thapliyal
et
al
2022
cultural
bias
srinivasan
et
al
2021
colossal
cleaned
commoncrawl
c4
raffel
et
al
2020
the
pile
gao
et
al
2020a
ccaligned
el-kishky
et
al
2020
openai
webtext
radford
et
al
2019
openwebtext
corpus
owtc
roots
lauren√ßon
et
al
2022
text
offensive
language
racial
bias
raffel
et
al
2020
text
text
text
religious
racial
gender
biases
gao
et
al
2020a
porn
racial
bias
el-kishky
et
al
2020
gender
racial
biases
gehman
et
al
2020
text
text
gender
racial
biases
gehman
et
al
2020
cultural
bias
lauren√ßon
et
al
2022
15
16
17
voxceleb
nagrani
et
al
2020
voxceleb
chung
et
al
2018
first
impressions
escalante
et
al
2020
audio
vision
audio
vision
audio
vision
demographic
gender
biases
chung
et
al
2018
demographic
gender
biases
chung
et
al
2018
racial
gender
biases
yan
et
al
2020
18
19
20
21
22
23
24
xm3600
thapliyal
et
al
2022
vqa
antol
et
al
2015
vqa
goyal
et
al
2017
ms
coco
lin
et
al
2014
multi30k
elliott
et
al
2016
mimic-iv
johnson
et
al
2023
mab
alkhaled
et
al
2023
text
vision
text
vision
text
vision
text
vision
text
vision
text
vision
text
25
twitter
corpus
huang
et
al
2020b
text
cultural
bias
thapliyal
et
al
2022
gender
bias
ruggeri
and
nozza
2023
gender
bias
ruggeri
and
nozza
2023
gender
bias
cabello
et
al
2023
zhao
et
al
2017
racial
bias
wang
et
al
2022a
ethnic
racial
marital
status
biases
meng
et
al
2022a
racism
misogyny
stereotypes
alkhaled
et
al
2023
pagliai
et
al
2024
age
gender
racial
biases
huang
et
al
2020b
10
11
12
13
14
based
on
contrastive
language-image
pretraining
clip
the
clip
models
that
were
investigated
were
trained
on
internet-wide
web
crawls
clip
is
known
to
be
quite
accurate
radford
et
al
2021
however
it
also
appears
to
have
scaled
the
biases
inherent
in
its
training
data
also
wolfe
and
caliskan
2022a
found
that
more
than
latino
asian
or
black
white
persons
are
more
associated
with
collective
in-group
words
in
embeddings
from
clip
radford
et
al
2021
slip
mu
et
al
2022
and
blip
li
et
al
2022
as
measured
with
embedding
association
tests
eats
for
definitive
assessment
their
work
would
have
benefited
from
additional
experiments
involving
data
of
people
outside
the
united
states
us
since
they
used
the
chicago
face
database
cfd
ma
et
al
2015
cfd
is
dataset
of
597
people
recruited
in
the
similarly
teo
et
al
2024
found
that
stable
diffusion
exhibits
gender
bias
on
slight
changes
to
its
prompts
besides
the
work
on
text
and
text-visual
multimodal
systems
or
data
there
are
some
work
on
audio-visual
systems
or
data
fenu
and
marras
2022
in
the
work
by
fenu
and
marras
2022
they
perform
comparative
analysis
on
audio-visual
speaker
recognition
systems
using
fusion
at
the
model
step
they
found
that
the
highest
accuracy
and
the
lowest
disparity
across
groups
are
achieved
compared
to
unimodal
systems
in
other
works
pe√±a
et
al
2023
evaluated
aibased
recruitment
for
multimodal
data
but
in
fictitious
case
study
which
may
limit
its
generalizability
in
real-world
applications
booth
et
al
2021
performed
case
study
of
automated
video
interviews
and
found
that
combining
more
than
one
modality
increases
bias
and
reduces
fairness
similarly
to
what
happens
when
scaling
crawled
data
for
training
models
other
researchers
investigated
the
impact
of
multimodal
data
models
on
personlity
assessment
yan
et
al
2020
cyberbullying
alasadi
et
al
2020
health
records
meng
et
al
2022a
and
more
birhane
et
al
2021
zhang
et
al
2022c
ferrara
2023a
cabello
et
al
2023
the
obvious
challenges
of
bias
in
data
has
motivated
some
researchers
for
more
attention
in
the
ethics
of
data
collection
weinberg
2022
4.2
llms
the
relevant
datasets
models
and
challenges
of
bias
affecting
llms
as
discussed
in
the
literature
are
summarized
in
tables
and
the
introduction
of
the
generative
pretrained
transformer
gpt-2
radford
et
al
2019
was
turning
point
in
the
language
model
landscape
with
its
5b
parameters
as
pointed
out
earlier
training
such
large
model
required
lot
of
data
and
the
internet-sourced
webtext
was
used
for
this
purpose
updated
versions
of
the
dataset
have
also
been
used
for
its
recent
successors
including
gpt-3
brown
et
al
2020
and
gpt-4
achiam
et
al
2023
unfortunately
the
attendant
problems
of
bias
witnessed
in
gpt-2
gehman
et
al
2020
have
followed
successive
versions
this
is
also
the
case
with
other
models
as
expressed
in
table
for
the
reasons
that
the
datasets
used
in
pretraining
some
of
which
are
given
in
table
are
largely
from
internet
sources
furthermore
the
architectures
of
the
models
many
of
which
are
based
on
the
transformer
architecture
vaswani
et
al
2017
share
similarity
in
other
works
schramowski
et
al
2022
show
that
moral
directions
what
is
morally
right
or
wrong
to
do
are
present
in
llms
it
is
however
highly
debatable
if
models
can
be
considered
moral
agents
in
collaborative
effort
bench
authors
2023
probed
llms
in
the
big-bench
of
200
tasks
including
many
that
are
related
to
bias
also
santurkar
et
al
2023
explored
whose
opinions
llms
reflect
while
harrer
2023
nashwan
and
abujaber
2023
investigated
bias
in
llms
within
healthcare
4.3
mitigation
categories
quantitative
bias
measurement
and
mitigation
in
nlp
may
be
placed
into
categories
preprocessing
as
applicable
in
general
ml
hort
et
al
2024
intrinsic
caliskan
et
al
2017
may
et
al
2019
and
extrinsic
as
observed
by
ramesh
et
al
2023
the
first
second
and
third
involve
quantifying
and
mitigating
bias
in
the
training
data
in
the
trained
model
representation
and
in
the
outputs
of
the
downstream
task
of
the
model
respectively
more
work
has
focused
on
the
latter
two
than
the
first
and
gender
bias
than
other
dimensions
ramesh
et
al
2023
for
example
delobelle
et
al
2022
and
welbl
et
al
2021
measured
bias
in
pre-trained
language
models
one
main
reason
why
more
work
has
been
on
the
latter
according
to
sun
et
al
2019
is
that
debiasing
an
existing
model
to
adjust
the
outputs
usually
only
requires
patching
the
model
instead
of
retraining
with
modified
debiased
or
new
data
which
is
usually
costly
preuse
this
may
be
considered
the
first
step
of
the
nlp
preprocessing
method
for
debiasing
it
involves
quantifying
the
amount
of
bias
or
toxicity
in
given
dataset
by
using
model
after
the
model
has
been
trained
on
different
dedicated
dataset
without
necessarily
attempting
to
mitigate
the
bias
in
the
given
dataset
it
can
be
defined
by
equation
where
is
bias
metric
that
takes
data
as
input
and
returns
scalar
as
the
score
examples
of
works
involving
this
are
alkhaled
et
al
2023
adewumi
et
al
2023b
hatebert
caselli
et
al
2021
has
been
used
in
similar
settings
equation
is
similar
to
that
in
brunet
et
al
2019
of
differential
bias
where
they
approximate
the
effect
of
removing
small
parts
of
training
data
on
bias
gender
bias
is
one
example
of
the
kind
of
bias
that
can
be
estimated
as
it
has
been
observed
that
there
are
more
male
than
female
terms
in
many
nlp
datasets
sun
et
al
2019
alkhaled
et
al
2023
pagliai
et
al
2024
the
ability
to
first
estimate
quantitatively
the
bias
in
given
dataset
provides
the
basis
to
be
able
to
determine
the
level
of
success
of
mitigation
4.4
bias
in
multilingual
ai
some
of
the
multimodal
data
in
table
contain
multilingual
data
these
result
in
multilingual
models
and
embeddings
for
example
cc3m35l
coco-35l
and
webli
which
was
used
to
train
pali
webli
is
mix
of
pre-training
tasks
with
texts
in
109
languages
chen
et
al
2022
some
of
these
languages
fall
in
the
category
of
low-resource
languages
adewumi
et
al
2023a
kurpicz-briki
2020
reports
statistically
significant
bias
in
german
word
embeddings
based
on
the
origin
of
name
in
relation
to
pleasant
and
unpleasant
words
using
weat
caliskan
et
al
2017
in
the
study
by
wambsganss
et
al
2022
they
found
that
the
pretrained
german
language
models
germanbert
germant5
and
german
table
summary
of
models
and
some
of
their
fairness
bias
challenges
modified
for
audio-visual
lmms
vqgan-clip
crowson
et
al
2022
dall-e
ramesh
et
al
2021
glide
nichol
et
al
2022
stable
diffusion
rombach
et
al
2022
slip
mu
et
al
2022
modality
text
vision
training
data
wit
imagenet
text
vision
text
vision
conceptual
captions
wit
conceptual
captions
laion-5b
text
vision
yfcc100m
racial
bias
wolfe
and
caliskan
2022a
text
vision
wit
clip
radford
et
al
2021
blip
li
et
al
2022
text
vision
paligemma
text
vision
pali-3
chen
et
al
2022
text
vision
ms
coco
conceptual
captions
laion-400m
webli
cc3m35l
wit
webli
racial
bias
radford
et
al
2021
wolfe
and
caliskan
2022a
racial
gender
age
biases
wolfe
and
caliskan
2022a
ruggeri
and
nozza
2023
10
falcon
almazrouei
et
al
2023
beit
wang
et
al
2023b
text
vision
refinedweb
text
vision
llava
liu
et
al
2024b
resnet-50
he
et
al
2016
gpt4o
achiam
et
al
2023
gpt3
brown
et
al
2020
palm
chowdhery
et
al
2024
lamda
thoppilan
et
al
2022
glam
du
et
al
2022
text
vision
conceptual
12m
imagenet-21k
wikipedia
conceptual
captions
imagenet
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
text
vision
audio
vision
text
audio
vision
text
text
text
text
webtext
github
etc
commoncrawl
webtext
wikipedia
social
media
github
social
media
wikipedia
wikipedia
social
media
gpt2
radford
et
al
2019
llama-3
text
webtext
text
web
text
github
llama-2
touvron
et
al
2023
ctrl
keskar
et
al
2019
text
web
text
text
wikipedia
project
gutenberg
openwebtext
aurora-m
nakamura
et
al
2024
mixtral-8x7b
jiang
et
al
2024a
bloom
le
scao
et
al
2023
text
the
pile
text
web
text
text
roots
gpt-2
had
substantial
conceptual
racial
and
gender
bias
this
was
also
confirmed
by
kraft
et
al
2022
who
observed
sexist
stereotypes
in
some
of
the
models
family
and
care-related
terms
were
associated
with
female
while
crime
and
perpetrators
were
associated
with
male
similarly
some
challenges
of
bias
fairness
misogyny
racial
bias
pagliai
et
al
2024
wolfe
and
caliskan
2022a
occupational
stereotypes
gender
bias
ramesh
et
al
2022
mandal
et
al
2023a
gender
stereotypes
nichol
et
al
2022
gender
bias
teo
et
al
2024
porn
offensive
language3
age
racial
gender
biases
stereotypes
chen
et
al
2022
harmful
content
cultural
bias
almazrouei
et
al
2023
gender
cultural
biases
brinkmann
et
al
2023
cultural
bias
liu
et
al
2024b
gender
bias
fenu
and
marras
2022
stereotypes
racial
bias
aich
et
al
2024
gender
racial
religious
biases
brown
et
al
2020
gehman
et
al
2020
occupation
gender
sexual
religious
biases
chowdhery
et
al
2024
gender
bias
thoppilan
et
al
2022
toxicity
gender
bias
du
et
al
2022
sexual
racial
gender
biases
sheng
et
al
2019
gehman
et
al
2020
stereotypes
gender
racial
sexual
religious
biases
aich
et
al
2024
touvron
et
al
2023
toxicity
gender
racial
sexual
religious
biases
touvron
et
al
2023
gender
racial
biases
gehman
et
al
2020
offensive
language
religious
racial
gender
biases
gao
et
al
2020a
nakamura
et
al
2024
stereotypes
racial
gender
occupational
biases
jiang
et
al
2024a
aich
et
al
2024
toxicity
gender
religious
disability
age
biases
le
scao
et
al
2023
for
dutch
delobelle
et
al
2020
investigated
gender
and
occupation
biases
in
robbert
dutch
roberta
liu
et
al
2019
through
templatebased
association
test
kurita
et
al
2019
may
et
al
2019
huang
et
al
2020b
also
identified
biases
related
to
people
origin
and
age
in
italian
english
polish
portuguese
and
spanish
using
twitter
corpus
discussion
although
there
are
many
limitations
or
risks
of
multimodal
ai
or
llms
acosta
et
al
2022
adewumi
et
al
2024b
pettersson
et
al
2024
adewumi
et
al
2024a
perhaps
the
issue
of
fairness
and
bias
rank
among
the
topmost
mehrabi
et
al
2021
in
addition
in
the
taxonomy
of
21
risks
of
language
models
provided
by
weidinger
et
al
2022
the
first
category
is
discrimination
hate
speech
and
exclusion
given
the
recurring
challenges
in
this
regard
we
are
of
the
view
that
existing
tools
for
evaluating
or
handling
fairness
and
bias
in
these
systems
need
to
be
improved
zhao
et
al
2018a
rudinger
et
al
2018
it
may
be
almost
impossible
to
automatically
filter
dataset
or
debias
model
to
be
100
free
of
unfair
bias
or
toxic
content
but
the
research
community
and
other
stakeholders
may
need
to
determine
what
levels
are
acceptable
and
if
it
should
be
requirement
to
have
human-in-theloop
methods
in
this
section
we
discuss
methods
to
audit
or
evaluate
fairness
and
bias
datasets
for
such
evaluation
and
debiasing
strategies
we
hope
that
such
discussion
will
spur
more
researchers
and
stakeholders
to
see
the
critical
importance
of
ai
that
is
fair
and
free
from
bias
as
much
as
possible
5.1
methods
to
audit
measure
and
evaluate
fairness
and
bias
caliskan
et
al
2017
introduced
the
word
embedding
association
test
weat
which
is
based
on
the
implicit
association
test
iat
nosek
et
al
2002
the
iat
was
designed
to
measure
attitudes
towards
social
groups
it
showed
implicit
preference
for
white
and
young
people
over
black
and
old
people
respectively
furthermore
it
showed
the
association
of
male
terms
with
science
while
female
terms
were
with
family
and
arts
embedding
association
tests
eats
have
been
used
in
several
studies
kurpicz-briki
2020
wolfe
et
al
2023
and
adapted
with
improvements
in
sentence
embedding
association
test
seat
may
et
al
2019
and
relational
inner
product
association
ripa
ethayarajh
et
al
2019
despite
its
widespread
use
weat
has
the
disadvantage
that
it
may
systematically
overestimate
the
bias
in
model
in
vision
models
mandal
et
al
2023b
used
weat
to
audit
clip
by
detecting
and
quantifying
bias
also
along
the
lines
of
the
weat
dev
and
phillips
2019
introduced
the
embedding
coherence
test
ect
and
embedding
quality
test
eqt
and
proposed
methods
for
eliminating
explicit
bias
however
their
method
has
the
weakness
that
it
is
not
able
to
remove
implicit
bias
friedrich
et
al
2021
another
embedding
evaluation
method
is
cosine
similarity
it
was
used
for
zero-shot
classification
by
radford
et
al
2021
it
may
also
be
used
to
audit
fairness
and
bias
by
evaluating
the
similarity
in
image
and
text
embeddings
wolfe
et
al
2023
the
visual
tool
gradient-weighted
class
activation
mapping
grad-cam
generates
saliency
map
which
shows
the
most
relevant
regions
of
an
image
for
given
attributes
selvaraju
et
al
2017
wolfe
et
al
2023
in
an
evaluation
carried
out
by
wolfe
et
al
2023
they
discovered
that
the
computed
average
saliency
maps
included
only
face
regions
for
non-objectified
images
but
both
face
and
chest
regions
for
objectified
images
in
possible
indication
of
sexual
objectification
bias
the
tool
not
safe
for
work
nsfw
detector
uses
tag
alongside
each
image
for
filtering
undesirable
content
schuhmann
et
al
2021
birhane
et
al
2024b
recent
metric
introduced
by
alkhaled
et
al
2023
is
bipol
it
uses
two-step
procedure
in
estimating
bias
in
data
adewumi
et
al
2023b
pagliai
et
al
2024
bipol
has
the
weakness
that
if
the
bias
classifier
is
not
accurate
enough
false
positives
will
weaken
the
evaluation
score
another
measure
is
area
under
the
curve
auc
as
used
by
meng
et
al
2022a
in
the
investigation
of
algorithmic
fairness
of
mortality
prediction
where
they
noted
that
ml
methods
obtain
lower
scores
usually
when
it
involves
groups
with
higher
mortality
rates
teo
et
al
2024
proposed
classifier
error-aware
measurement
cleam
framework
for
better
performance
in
bias
estimation
while
booth
et
al
2021
measured
gender
bias
using
accuracy
of
spearman
rank-based
correlation
furthermore
nozza
et
al
2021
introduced
the
score
honest
which
was
tested
with
respect
to
gender
bias
in
text
generation
in
languages
italian
french
portuguese
romanian
spanish
and
english
it
measures
the
probability
that
language
model
will
output
hurtful
text
given
certain
template
and
lexicon
datasets
for
bias
evaluation
different
datasets
have
been
introduced
for
bias
evaluation
liang
et
al
2021a
introduced
multibench
unified
multimodal
benchmark
that
spans
15
datasets
10
modalities
and
20
prediction
tasks
fairface
was
introduced
by
karkkainen
and
joo
2021
the
dataset
was
designed
to
mitigate
racial
bias
in
multimodal
ai
collected
from
the
yahoo
flickr
creative
commons
100
million
yfcc100m
dataset
thomee
et
al
2016
and
contains
108
501
images
balanced
across
the
following
races
black
white
indian
southeast
asian
east
asian
middle
eastern
and
latino
esiobu
et
al
2023
introduced
novel
datasets
advpromptset
and
holisticbiasr
with
which
they
evaluated
12
demographic
dimensions
for
different
llms
ruzzante
et
al
2022
introduced
sexual
objectification
and
emotion
database
sobem
for
sexual
objectification
bias
studies
it
consists
of
280
pictures
of
objectified
and
non-objectified
female
models
with
different
emotions
and
neutral
face
bias
benchmark
for
qa
bbq
is
question-set
dataset
that
employs
templates
crafted
to
reflect
specific
biases
identified
in
society
it
was
introduced
by
parrish
et
al
2022
and
aims
to
expose
implicit
prejudices
that
may
exist
against
individuals
from
legally
protected
categories
beavertails
was
introduced
by
ji
et
al
2024
it
assesses
question-answer
pairs
tested
on
llms
with
regards
to
14
different
harm
categories
which
are
not
exhaustive
discrimination
stereotype
injustice
makes
up
the
second
category
in
the
list
additional
datasets
for
bias
evaluation
include
redditbias
by
barikeri
et
al
2021
realtoxicityprompts
which
comprises
of
100k
english
sentences
gehman
et
al
2020
harmfulq
for
zeroshot
chain
of
thought
cot
across
stereotype
benchmarks
and
harmful
questions
shaikh
et
al
2023
and
bold
dhamala
et
al
2021
porgali
et
al
2023
also
introduced
the
casual
conversations
dataset
version
containing
26
467
videos
of
567
unique
participants
from
different
countries
representing
wide
range
of
demographics
for
bias
and
robustness
evaluation
of
lmms
that
are
vision
and
audio
models
5.2
debiasing
strategies
although
there
no
silver
bullet
to
solving
the
challenges
of
fairness
and
bias
in
the
data
and
models
of
multimodal
ai
we
believe
combination
of
two
or
more
of
the
following
strategies
on
the
relevant
datasets
or
models
in
tables
and
will
go
long
way
in
mitigating
bias
in
ai
generally
5.2
curate
over
crawl
one
important
method
to
address
bias
in
datasets
will
be
to
curate
rather
than
crawl
this
is
especially
so
because
web
crawling
has
been
the
popular
approach
to
getting
internet
data
in
the
shortest
possible
time
birhane
et
al
2021
the
assumption
of
scale
beats
noise
is
the
rationale
for
this
approach
by
some
researchers
jia
et
al
2021
unfortunately
this
misconception
about
scaling
does
not
only
scale
the
quality
part
of
the
dataset
but
the
noise
along
with
it
no
matter
how
small
as
shown
by
birhane
et
al
2024b
when
they
observed
12
increase
in
hate
content
due
to
scaling
on
the
other
hand
clearly
despite
the
advantage
of
curation
one
hurdle
to
overcome
with
the
curate
over
crawl
approach
will
be
the
issue
of
scaling
in
addition
for
better
quality
data
collection
researchers
like
jo
and
gebru
2020
advocate
that
ai
practitioners
should
build
on
the
practices
of
those
in
the
field
of
archives
and
libraries
and
have
public
mission
statement
to
guide
their
data
collection
practice
weinberg
2022
furthermore
gebru
et
al
2021
encourage
through
datasheets
for
datasets
standardized
processes
for
documenting
important
aspects
of
the
dataset
creation
including
motivation
composition
funding
collection
and
use
cases
with
the
potential
to
mitigate
unwanted
biases
though
this
approach
has
its
limitations
because
individuals
included
or
affected
by
the
datasets
are
not
necessarily
empowered
to
influence
them
weinberg
2022
5.2
counterfactual
data
augmentation
cda
zhao
et
al
2018a
used
cda
to
show
that
it
removes
bias
with
minimal
performance
degradation
on
coreference
benchmarks
when
combined
with
existing
word-embedding
debiasing
methods
it
involves
generating
alternate
examples
of
what
exists
counterfactual
of
data
points
to
counter
or
mitigate
bias
this
method
has
gained
attention
in
the
field
meade
et
al
2022
barikeri
et
al
2021
it
is
sometimes
called
gender-swapping
in
the
specific
case
of
gender
bias
sun
et
al
2019
cda
has
been
shown
to
be
effective
in
tackling
bias
in
coreference
resolution
as
mentioned
earlier
as
it
reduced
the
difference
in
f1
scores
between
pro-stereotypical
and
antistereotypical
evaluations
sun
et
al
2019
dev
and
phillips
2019
also
embraced
this
neutralizing
approach
in
their
work
by
flipping
gendered
terms
with
their
counterparts
for
example
sentence
like
he
was
doctor
would
be
flipped
to
she
was
doctor
despite
the
advantages
as
pointed
out
earlier
about
the
less-discussed
preprocessing
bias
mitigation
method
completely
debiasing
textual
data
can
be
complicated
and
intricate
some
of
the
challenges
are
that
the
size
of
the
data
increases
significantly
thereby
increasing
model
training
time
and
naive
term-swapping
can
create
more
challenges
of
unrealistic
or
nonsensical
samples
in
the
data
she
has
hot
flashes
because
of
menopause
to
he
has
hot
flashes
because
of
menopause
sun
et
al
2019
5.2
improved
filtering
it
has
been
shown
that
poor
filtering
during
the
data
creation
process
allows
low
quality
data
in
the
final
dataset
birhane
et
al
2024a
it
may
not
be
possible
to
automatically
filter
large
data
to
be
100
fit
for
purpose
but
improving
the
existing
methods
of
filtering
will
go
long
way
in
mitigating
bias
5.2
linear
projection
this
method
projects
all
words
orthogonally
to
the
bias
vector
ensuring
the
updated
set
has
no
component
along
the
bias
vector
vb
as
given
in
equation
bolukbasi
et
al
2016
dev
and
phillips
2019
œÄb
vb
vb
the
span
that
results
becomes
less
by
in
the
total
dimensions
say
from
300
to
299
which
will
have
negligible
effects
on
the
generalizability
of
the
embeddings
as
an
example
with
gender
bias
subtraction
with
linear
projection
of
gender
terms
from
embeddings
will
make
them
close
gendered
word-pairs
that
have
few
word
sense
he
she
and
him
her
can
have
close
enough
identifcal
positions
in
the
vector
space
after
debiasing
5.2
debiasing
word
embeddings
removing
gender
bias
from
word
embeddings
can
take
one
of
two
approaches
removing
gender
subspace
bolukbasi
et
al
2016
and
learning
gender-neutral
embeddings
zhao
et
al
2018b
the
two
approaches
may
not
be
adequate
for
embeddings
that
are
not
based
on
euclidean
space
since
cosine
similarity
will
no
longer
apply
sun
et
al
2019
the
first
approach
modifies
an
embedding
based
on
the
combined
properties
of
word
embeddings
that
gender
bias
can
be
captured
by
direction
and
neutral
words
are
linearly
separable
from
gendered
words
bolukbasi
et
al
2016
while
the
second
approach
preserves
gender
information
in
some
dimensions
but
compels
other
dimensions
of
the
word
embedding
to
be
free
of
such
zhao
et
al
2018b
5.2
adapters
post-processing
method
for
bias
mitigation
that
is
based
on
adapter
modules
houlsby
et
al
2019
called
debiasing
with
adapter
modules
dam
was
introduced
by
kumar
et
al
2023
they
encapsulate
different
bias
mitigation
functionalities
and
can
be
integrated
when
desired
in
model
similarly
to
how
adapterfusion
pfeiffer
et
al
2021
is
carried
out
in
multi-task
learning
dam
trains
the
main
adapter
and
the
bias
mititgation
adapters
independently
before
combining
them
dam
follows
an
earlier
adapter-based
debiasing
method
called
adapter-based
debiasing
of
language
models
adele
performed
in
the
work
by
lauscher
et
al
2021
their
approach
involved
the
additional
use
of
cda
5.2
additive
residuals
to
address
the
skewed
distribution
of
different
identity
groups
in
the
training
data
used
in
lmms
seth
et
al
2023
introduced
debiasing
with
additive
residuals
dear
to
learn
additive
residual
image
representations
this
minimizes
the
representations
capacity
to
distinguish
among
different
identity
groups
thereby
offering
fairer
output
5.2
continued
pretraining
fatemi
et
al
2023
built
on
the
continued
pretraining
concept
which
is
sometimes
used
for
gender
bias
mitigation
with
small
gender-neutral
dataset
de
vassimon
manela
et
al
2021
by
introducing
gender
equality
prompt
geep
such
that
it
reduces
catastrophic
forgetting
which
is
likely
event
in
continued
pretraining
kirkpatrick
et
al
2017
geep
achieves
this
by
freezing
the
entire
model
before
updating
the
embeddings
furthermore
cabello
et
al
2023
showed
that
continued
pretraining
on
gender-neutral
data
improves
fairness
by
reducing
group
disparities
in
some
language-vision
tasks
5.2
adversarial
learning
yan
et
al
2020
used
adversarial
learning
for
bias
mitigation
as
proposed
by
zhang
et
al
2018
they
added
discriminator
to
jointly
learn
with
the
predictor
for
the
sensitive
attributes
in
this
approach
the
generator
prevents
the
discriminator
from
identifying
gender
in
task
5.2
10
gender
tagging
in
machine
translation
mt
gender-tagging
may
be
used
vanmassenhove
et
al
2018
it
involves
the
addition
of
gender
tags
to
the
beginning
of
data
samples
to
identify
the
gender
of
the
source
data
female
travelling
tomorrow
apparently
for
more
complex
sentences
this
approach
may
become
more
challenging
sun
et
al
2019
5.2
11
fairdistillation
to
address
bias
across
languages
delobelle
and
berendt
2022
introduced
fairdistillation
it
is
cross-lingual
method
that
is
based
on
knowledge
distillation
hinton
et
al
2015
by
creating
smaller
language
models
from
large
ones
to
control
for
stereotypical
and
representational
biases
5.2
12
debie
friedrich
et
al
2021
introduced
debie
as
platform
for
measuring
and
mitigating
implicit
and
explicit
bias
in
word
embeddings
the
mitigation
methods
are
more
specific
to
nlp
and
not
available
in
general
purpose
library
such
as
ai
fairness
360
aif360
bellamy
et
al
2019
debie
is
collection
of
commonly
used
bias
data
tools
and
word
embeddings
including
fasttext
bojanowski
et
al
2017
glove
pennington
et
al
2014
continuous
bag-of-words
cbow
mikolov
et
al
2013
and
the
weat
test
5.2
13
other
strategies
furthermore
there
are
generation
detoxifying
methods
with
the
potential
to
reduce
bias
gehman
et
al
2020
these
include
the
earlier-mentioned
continued
pretraining
and
decoding-based
generation
buolamwini
and
gebru
2018
advocated
oversampling
under-represented
groups
in
data
to
mitigate
bias
zayed
et
al
2024
addressed
fairness
by
pruning
in
llms
while
guo
et
al
2022
introduced
auto-bias
by
directly
probing
the
biases
in
pretrained
models
through
prompts
liang
et
al
2021b
introduced
the
autoregressive
iterative
nullspace
projection
a-inlp
method
to
carry
out
post-hoc
debiasing
on
llms
additionally
there
are
self-debias
schick
et
al
2021
harddebias
bolukbasi
et
al
2016
sentencedebias
liang
et
al
2020
and
dropout
methods
webster
et
al
2020
meade
et
al
2022
conclusion
fairness
and
bias
are
very
important
considerations
in
multimodal
ai
in
this
work
we
presented
the
challenges
of
fairness
and
bias
in
multimodal
data
lmms
and
llms
defining
what
both
terms
mean
within
the
scope
of
this
survey
while
acknowledging
other
definitions
in
the
literature
we
discussed
the
concepts
of
fairness
and
bias
from
the
perspective
of
the
social
science
and
showed
the
distribution
of
scientific
publications
across
many
publishers
which
reveals
the
gap
in
the
study
of
large
multimodal
ai
compared
to
llms
which
this
work
contributes
to
filling
our
discussions
around
the
methods
to
measure
fairness
and
bias
datasets
for
evaluation
and
debiasing
strategies
will
provide
researchers
and
other
stakeholders
with
insight
on
how
to
approach
these
issues
for
future
work
it
will
be
worthwhile
to
reevaluate
the
progress
made
with
the
metrics
and
tools
for
quantifying
and
mitigating
the
challenges
of
fairness
and
bias
because
despite
the
positive
effects
of
debiasing
in
ai
researchers
like
west
et
al
2019
argue
that
such
research
ought
to
do
more
than
technical
debiasing
and
include
the
social
analysis
of
the
use
of
such
ai
as
this
will
account
more
for
the
impact
of
bias
overall
weinberg
2022
even
sami
et
al
2023
showed
with
the
example
of
dalle-2
that
current
efforts
still
have
their
limitations
in
their
work
on
the
lmm
dalle-2
they
revealed
that
despite
the
guardrails
for
the
model
by
openai
it
generated
40
more
images
of
women
for
stereotypical
female-dominant
administrative
task
in
clear
gender
bias
when
prompted
acknowledgements
this
work
is
supported
by
the
european
commission-funded
project
humane
ai
toward
ai
systems
that
augment
and
empower
humans
by
understanding
us
our
society
and
the
world
around
us
the
authors
wish
to
thank
our
colleagues
at
the
responsible
artificial
intelligence
group
at
ume√•
university
the
work
is
partially
supported
by
the
wallenberg
ai
autonomous
systems
and
software
program
wasp
funded
by
the
knut
and
alice
wallenberg
foundation
and
counterpart
funding
from
lule√•
university
of
technology
ltu
references
josh
achiam
steven
adler
sandhini
agarwal
lama
ahmad
ilge
akkaya
florencia
leoni
aleman
diogo
almeida
janko
altenschmidt
sam
altman
shyamal
anadkat
et
al
2023
gpt-4
technical
report
arxiv
preprint
arxiv
2303.08774
juli√°n
acosta
guido
falcone
pranav
rajpurkar
and
eric
topol
2022
multimodal
biomedical
ai
nature
medicine
28
1773
1784
stacy
adams
and
sara
freedman
1976
equity
theory
revisited
comments
and
annotated
bibliography
advances
in
experimental
social
psychology
43
90
tosin
adewumi
mofetoluwa
adeyemi
aremu
anuoluwapo
bukola
peters
happy
buzaaba
oyerinde
samuel
amina
mardiyyah
rufai
benjamin
ajibade
tajudeen
gwadabe
mory
moussou
koulibaly
traore
et
al
2023a
afriwoz
corpus
for
exploiting
cross-lingual
transfer
for
dialogue
generation
in
lowresource
african
languages
in
2023
international
joint
conference
on
neural
networks
ijcnn
pages
ieee
tosin
adewumi
nudrat
habib
lama
alkhaled
and
elisa
barney
2024a
instruction
makes
difference
arxiv
preprint
arxiv
2402.00453
tosin
adewumi
nudrat
habib
lama
alkhaled
and
elisa
barney
2024b
on
the
limitations
of
large
language
models
llms
false
attribution
arxiv
preprint
arxiv
2404.04631
tosin
adewumi
foteini
liwicki
and
marcus
liwicki
2022
state-of-the-art
in
open-domain
conversational
ai
survey
information
13
298
tosin
adewumi
isabella
s√∂dergren
lama
alkhaled
sana
al-azzawi
foteini
simistira
liwicki
and
marcus
liwicki
2023b
bipol
multi-axes
evaluation
of
bias
with
explainability
in
benchmark
datasets
in
proceedings
of
the
14th
international
conference
on
recent
advances
in
natural
language
processing
pages
10
varna
bulgaria
incoma
ltd
shoumen
bulgaria
monica
agrawal
stefan
hegselmann
hunter
lang
yoon
kim
and
david
sontag
2022
large
language
models
are
few-shot
clinical
information
extractors
in
proceedings
of
the
2022
conference
on
empirical
methods
in
natural
language
processing
pages
1998
2022
abu
dhabi
united
arab
emirates
association
for
computational
linguistics
gati
aher
rosa
arriaga
and
adam
tauman
kalai
2023
using
large
language
models
to
simulate
multiple
humans
and
replicate
human
subject
studies
in
international
conference
on
machine
learning
pages
337
371
pmlr
ankit
aich
tingting
liu
salvatore
giorgi
kelsey
isman
lyle
ungar
and
brenda
curtis
2024
vernacular
barely
know
her
challenges
with
style
control
and
stereotyping
arxiv
preprint
arxiv
2406.12679
mohammad
arif
ul
alam
2022
college
student
retention
risk
analysis
from
educational
database
using
multi-task
multi-modal
neural
fusion
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
36
pages
12689
12697
jamal
alasadi
ramanathan
arunachalam
pradeep
atrey
and
vivek
singh
2020
fairness-aware
fusion
framework
for
multimodal
cyberbullying
detection
in
2020
ieee
sixth
international
conference
on
multimedia
big
data
bigmm
pages
166
173
ieee
lama
alkhaled
tosin
adewumi
and
sana
sabah
sabry
2023
bipol
novel
multi-axes
bias
evaluation
metric
with
explainability
for
nlp
natural
language
processing
journal
100030
ebtesam
almazrouei
hamza
alobeidli
abdulaziz
alshamsi
alessandro
cappelli
ruxandra
cojocaru
m√©rouane
debbah
√©tienne
goffinet
daniel
hesslow
julien
launay
quentin
malartic
et
al
2023
the
falcon
series
of
open
language
models
arxiv
preprint
arxiv
2311.16867
haifa
alwahaby
mutlu
cukurova
zacharoula
papamitsiou
and
michail
giannakos
2022
the
evidence
of
impact
and
ethical
considerations
of
multimodal
learning
analytics
systematic
literature
review
the
multimodal
learning
analytics
handbook
pages
289
325
luca
andrighetto
fabrizio
bracco
carlo
chiorri
michele
masini
marcello
passarelli
and
tommaso
francesco
piccinno
2019
now
you
see
me
now
you
don
detecting
sexual
objectification
through
change
blindness
paradigm
cognitive
processing
20
419
429
stanislaw
antol
aishwarya
agrawal
jiasen
lu
margaret
mitchell
dhruv
batra
lawrence
zitnick
and
devi
parikh
2015
vqa
visual
question
answering
in
proceedings
of
the
ieee
international
conference
on
computer
vision
pages
2425
2433
lisa
argyle
ethan
busby
nancy
fulda
joshua
gubler
christopher
rytting
and
david
wingate
2023
out
of
one
many
using
language
models
to
simulate
human
samples
political
analysis
31
337
351
agathe
balayn
christoph
lofi
and
geert-jan
houben
2021
managing
bias
and
unfairness
in
data
for
decision
support
survey
of
machine
learning
and
data
engineering
approaches
to
identify
and
mitigate
bias
and
unfairness
within
data
management
and
analytics
systems
the
vldb
journal
30
739
768
soumya
barikeri
anne
lauscher
ivan
vulic
and
goran
glava≈°
2021
redditbias
real-world
resource
for
bias
evaluation
and
debiasing
of
conversational
language
models
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
pages
1941
1955
online
association
for
computational
linguistics
bellamy
dey
hind
hoffman
houde
kannan
lohia
martino
mehta
mojsilovic
nagar
natesan
ramamurthy
richards
saha
sattigeri
singh
varshney
and
zhang
2019
ai
fairness
360
an
extensible
toolkit
for
detecting
and
mitigating
algorithmic
bias
ibm
journal
of
research
and
development
63
15
big
bench
authors
2023
beyond
the
imitation
game
quantifying
and
extrapolating
the
capabilities
of
language
models
transactions
on
machine
learning
research
emily
bender
timnit
gebru
angelina
mcmillanmajor
and
shmargaret
shmitchell
2021
on
the
dangers
of
stochastic
parrots
can
language
models
be
too
big
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
pages
610
623
christopher
berry
2015
differential
validity
and
differential
prediction
of
cognitive
ability
tests
understanding
test
bias
in
the
employment
context
annu
rev
organ
psychol
organ
behav
435
463
ravi
varma
kumar
bevara
nishith
reddy
mannuru
sai
pranathi
karedla
and
ting
xiao
2024
scaling
implicit
bias
analysis
across
transformer-based
language
models
through
embedding
association
test
and
prompt
engineering
applied
sciences
14
3483
stella
biderman
hailey
schoelkopf
quentin
gregory
anthony
herbie
bradley
kyle
brien
eric
hallahan
mohammad
aflah
khan
shivanshu
purohit
usvsn
sai
prashanth
edward
raff
et
al
2023
pythia
suite
for
analyzing
large
language
models
across
training
and
scaling
in
international
conference
on
machine
learning
pages
2397
2430
pmlr
abeba
birhane
sepehr
dehdashtian
vinay
prabhu
and
vishnu
boddeti
2024a
the
dark
side
of
dataset
scaling
evaluating
racial
classification
in
multimodal
models
in
the
2024
acm
conference
on
fairness
accountability
and
transparency
pages
1229
1244
abeba
birhane
sanghyun
han
vishnu
boddeti
sasha
luccioni
et
al
2024b
into
the
laion
den
investigating
hate
in
multimodal
datasets
advances
in
neural
information
processing
systems
36
abeba
birhane
vinay
uday
prabhu
and
emmanuel
kahembwe
2021
multimodal
datasets
misogyny
pornography
and
malignant
stereotypes
arxiv
preprint
arxiv
2110.01963
su
lin
blodgett
solon
barocas
hal
daum√©
iii
and
hanna
wallach
2020
language
technology
is
power
critical
survey
of
bias
in
nlp
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
pages
5454
5476
online
association
for
computational
linguistics
su
lin
blodgett
gilsinia
lopez
alexandra
olteanu
robert
sim
and
hanna
wallach
2021
stereotyping
norwegian
salmon
an
inventory
of
pitfalls
in
fairness
benchmark
datasets
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
pages
1004
1015
piotr
bojanowski
edouard
grave
armand
joulin
and
tomas
mikolov
2017
enriching
word
vectors
with
subword
information
transactions
of
the
association
for
computational
linguistics
135
146
tolga
bolukbasi
kai-wei
chang
james
zou
venkatesh
saligrama
and
adam
kalai
2016
man
is
to
computer
programmer
as
woman
is
to
homemaker
debiasing
word
embeddings
advances
in
neural
information
processing
systems
29
brandon
booth
louis
hickman
shree
krishna
subburaj
louis
tay
sang
eun
woo
and
sidney
mello
2021
bias
and
fairness
in
multimodal
machine
learning
case
study
of
automated
video
interviews
in
proceedings
of
the
2021
international
conference
on
multimodal
interaction
pages
268
277
pearl
brereton
barbara
kitchenham
david
budgen
mark
turner
and
mohamed
khalil
2007
lessons
from
applying
the
systematic
literature
review
process
within
the
software
engineering
domain
journal
of
systems
and
software
80
571
583
jannik
brinkmann
paul
swoboda
and
christian
bartelt
2023
multidimensional
analysis
of
social
biases
in
vision
transformers
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
pages
4914
4923
tom
brown
benjamin
mann
nick
ryder
melanie
subbiah
jared
kaplan
prafulla
dhariwal
arvind
neelakantan
pranav
shyam
girish
sastry
amanda
askell
et
al
2020
language
models
are
few-shot
learners
advances
in
neural
information
processing
systems
33
1877
1901
marc-etienne
brunet
colleen
alkalay-houlihan
ashton
anderson
and
richard
zemel
2019
understanding
the
origins
of
bias
in
word
embeddings
in
international
conference
on
machine
learning
pages
803
811
pmlr
joy
buolamwini
and
timnit
gebru
2018
gender
shades
intersectional
accuracy
disparities
in
commercial
gender
classification
in
conference
on
fairness
accountability
and
transparency
pages
77
91
pmlr
laura
cabello
emanuele
bugliarello
stephanie
brandl
and
desmond
elliott
2023
evaluating
bias
and
fairness
in
gender-neutral
pretrained
vision-andlanguage
models
in
proceedings
of
the
2023
conference
on
empirical
methods
in
natural
language
processing
pages
8465
8483
singapore
association
for
computational
linguistics
yuchen
cai
ding
cao
rongxi
guo
yaqin
wen
guiquan
liu
and
enhong
chen
2024
locating
and
mitigating
gender
bias
in
large
language
models
in
international
conference
on
intelligent
computing
pages
471
482
springer
aylin
caliskan
joanna
bryson
and
arvind
narayanan
2017
semantics
derived
automatically
from
language
corpora
contain
human-like
biases
science
356
6334
183
186
tommaso
caselli
valerio
basile
jelena
mitrovic
and
michael
granitzer
2021
hatebert
retraining
bert
for
abusive
language
detection
in
english
in
proceedings
of
the
5th
workshop
on
online
abuse
and
harms
woah
2021
pages
17
25
online
association
for
computational
linguistics
yupeng
chang
xu
wang
jindong
wang
yuan
wu
linyi
yang
kaijie
zhu
hao
chen
xiaoyuan
yi
cunxiang
wang
yidong
wang
et
al
2024
survey
on
evaluation
of
large
language
models
acm
transactions
on
intelligent
systems
and
technology
15
45
jiuhai
chen
and
jonas
mueller
2024
quantifying
uncertainty
in
answers
from
any
language
model
and
enhancing
their
trustworthiness
in
proceedings
of
the
62nd
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
5186
5200
mark
chen
jerry
tworek
heewoo
jun
qiming
yuan
henrique
ponde
de
oliveira
pinto
jared
kaplan
harri
edwards
yuri
burda
nicholas
joseph
greg
brockman
et
al
2021
evaluating
large
language
models
trained
on
code
arxiv
preprint
arxiv
2107.03374
xi
chen
xiao
wang
soravit
changpinyo
aj
piergiovanni
piotr
padlewski
daniel
salz
sebastian
goodman
adam
grycner
basil
mustafa
lucas
beyer
et
al
2022
pali
jointly-scaled
multilingual
language-image
model
arxiv
preprint
arxiv
2209.06794
zhaorun
chen
yichao
du
zichen
wen
yiyang
zhou
chenhang
cui
zhenzhen
weng
haoqin
tu
chaoqi
wang
zhengwei
tong
qinglan
huang
et
al
2024
mj-bench
is
your
multimodal
reward
model
really
good
judge
for
text-to-image
generation
arxiv
preprint
arxiv
2407.04842
jiaee
cheong
sinan
kalkan
and
hatice
gunes
2024
fairrefuse
referee-guided
fusion
for
multi-modal
causal
fairness
in
depression
detection
jaemin
cho
abhay
zala
and
mohit
bansal
2023
dalleval
probing
the
reasoning
skills
and
social
biases
of
text-to-image
generation
models
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
pages
3043
3054
aakanksha
chowdhery
sharan
narang
jacob
devlin
maarten
bosma
gaurav
mishra
adam
roberts
paul
barham
hyung
won
chung
charles
sutton
sebastian
gehrmann
parker
schuh
kensen
shi
sashank
tsvyashchenko
joshua
maynez
abhishek
rao
parker
barnes
yi
tay
noam
shazeer
vinodkumar
prabhakaran
emily
reif
nan
du
ben
hutchinson
reiner
pope
james
bradbury
jacob
austin
michael
isard
guy
gur-ari
pengcheng
yin
toju
duke
anselm
levskaya
sanjay
ghemawat
sunipa
dev
henryk
michalewski
xavier
garcia
vedant
misra
kevin
robinson
liam
fedus
denny
zhou
daphne
ippolito
david
luan
hyeontaek
lim
barret
zoph
alexander
spiridonov
ryan
sepassi
david
dohan
shivani
agrawal
mark
omernick
andrew
dai
thanumalayan
sankaranarayana
pillai
marie
pellat
aitor
lewkowycz
erica
moreira
rewon
child
oleksandr
polozov
katherine
lee
zongwei
zhou
xuezhi
wang
brennan
saeta
mark
diaz
orhan
firat
michele
catasta
jason
wei
kathy
meier-hellstern
douglas
eck
jeff
dean
slav
petrov
and
noah
fiedel
2024
palm
scaling
language
modeling
with
pathways
mach
learn
res
24
aakanksha
chowdhery
sharan
narang
jacob
devlin
maarten
bosma
gaurav
mishra
adam
roberts
paul
barham
hyung
won
chung
charles
sutton
sebastian
gehrmann
et
al
2023
palm
scaling
language
modeling
with
pathways
journal
of
machine
learning
research
24
240
113
hyung
won
chung
le
hou
shayne
longpre
barret
zoph
yi
tay
william
fedus
yunxuan
li
xuezhi
wang
mostafa
dehghani
siddhartha
brahma
et
al
2024
scaling
instruction-finetuned
language
models
journal
of
machine
learning
research
25
70
53
joon
son
chung
arsha
nagrani
and
andrew
zisserman
2018
voxceleb2
deep
speaker
recognition
arxiv
preprint
arxiv
1806.05622
jan
clusmann
fiona
kolbinger
hannah
sophie
muti
zunamys
carrero
jan-niklas
eckardt
narmin
ghaffari
laleh
chiara
maria
lavinia
l√∂ffler
sophie-caroline
schwarzkopf
michaela
unger
gregory
veldhuizen
et
al
2023
the
future
landscape
of
large
language
models
in
medicine
communications
medicine
141
katherine
crowson
stella
biderman
daniel
kornis
dashiell
stander
eric
hallahan
louis
castricato
and
edward
raff
2022
vqgan-clip
open
domain
image
generation
and
editing
with
natural
language
guidance
in
computer
vision
eccv
2022
17th
european
conference
tel
aviv
israel
october
23
27
2022
proceedings
part
xxxvii
page
88
105
berlin
heidelberg
springer-verlag
yifei
da
mat√≠as
nicol√°s
bossa
abel
d√≠az
berenguer
and
hichem
sahli
2024
reducing
bias
in
sentiment
analysis
models
through
causal
mediation
analysis
and
targeted
counterfactual
training
ieee
access
12
10120
10134
rahul
gupta
2021
bold
dataset
and
metrics
for
measuring
biases
in
open-ended
language
generation
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
pages
862
872
daniela
america
da
silva
henrique
duarte
borges
louro
gildarcio
sousa
goncalves
johnny
cardoso
marques
luiz
alberto
vieira
dias
adilson
marques
da
cunha
and
paulo
marcelo
tasinaffo
2021
could
conversational
ai
identify
offensive
language
information
12
10
418
emily
dinan
angela
fan
adina
williams
jack
urbanek
douwe
kiela
and
jason
weston
2020
queens
are
powerful
too
mitigating
gender
bias
in
dialogue
generation
in
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
pages
8173
8188
online
association
for
computational
linguistics
jamell
dacon
and
haochen
liu
2021
does
gender
matter
in
the
news
detecting
and
examining
gender
bias
in
news
articles
in
companion
proceedings
of
the
web
conference
2021
www
21
page
385
392
new
york
ny
usa
association
for
computing
machinery
daniel
de
vassimon
manela
david
errington
thomas
fisher
boris
van
breugel
and
pasquale
minervini
2021
stereotype
and
skew
quantifying
gender
bias
in
pre-trained
and
fine-tuned
language
models
in
proceedings
of
the
16th
conference
of
the
european
chapter
of
the
association
for
computational
linguistics
main
volume
pages
2232
2242
online
association
for
computational
linguistics
pieter
delobelle
and
bettina
berendt
2022
fairdistillation
mitigating
stereotyping
in
language
models
in
joint
european
conference
on
machine
learning
and
knowledge
discovery
in
databases
pages
638
654
springer
pieter
delobelle
ewoenam
tokpo
toon
calders
and
bettina
berendt
2022
measuring
fairness
with
biased
rulers
comparative
study
on
bias
metrics
for
pre-trained
language
models
in
proceedings
of
the
2022
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
pages
1693
1706
seattle
united
states
association
for
computational
linguistics
pieter
delobelle
thomas
winters
and
bettina
berendt
2020
robbert
dutch
roberta-based
language
model
in
findings
of
the
association
for
computational
linguistics
emnlp
2020
pages
3255
3265
online
association
for
computational
linguistics
bhavin
desai
kapil
patil
asit
patil
and
ishita
mehta
2023
large
language
models
comprehensive
exploration
of
modern
ai
potential
and
pitfalls
journal
of
innovative
technologies
sunipa
dev
and
jeff
phillips
2019
attenuating
bias
in
word
vectors
in
proceedings
of
the
twenty-second
international
conference
on
artificial
intelligence
and
statistics
volume
89
of
proceedings
of
machine
learning
research
pages
879
887
pmlr
jwala
dhamala
tony
sun
varun
kumar
satyapriya
krishna
yada
pruksachatkun
kai-wei
chang
and
thang
viet
doan
zhibo
chu
zichong
wang
and
wenbin
zhang
2024
fairness
definitions
in
language
models
explained
arxiv
preprint
arxiv
2407.18454
jonathan
dodge
vera
liao
yunfeng
zhang
rachel
ke
bellamy
and
casey
dugan
2019
explaining
models
an
empirical
study
of
how
explanations
impact
fairness
judgment
in
proceedings
of
the
24th
international
conference
on
intelligent
user
interfaces
pages
275
285
tommaso
dolci
fabio
azzalini
and
mara
tanelli
2023
improving
gender-related
fairness
in
sentence
encoders
semantics-based
approach
data
science
and
engineering
177
195
karen
drukker
weijie
chen
judy
gichoya
nicholas
gruszauskas
jayashree
kalpathy-cramer
sanmi
koyejo
kyle
myers
rui
s√°
berkman
sahiner
heather
whitney
et
al
2023
toward
fairness
in
artificial
intelligence
for
medical
image
analysis
identification
and
mitigation
of
potential
biases
in
the
roadmap
from
data
collection
to
model
deployment
journal
of
medical
imaging
10
061104
061104
nan
du
yanping
huang
andrew
dai
simon
tong
dmitry
lepikhin
yuanzhong
xu
maxim
krikun
yanqi
zhou
adams
wei
yu
orhan
firat
et
al
2022
glam
efficient
scaling
of
language
models
with
mixture-of-experts
in
international
conference
on
machine
learning
pages
5547
5569
pmlr
elizabeth
edenberg
and
alexandra
wood
2023
disambiguating
algorithmic
bias
from
neutrality
to
justice
in
proceedings
of
the
2023
aaai
acm
conference
on
ai
ethics
and
society
pages
691
704
ahmed
el-kishky
vishrav
chaudhary
francisco
guzm√°n
and
philipp
koehn
2020
ccaligned
massive
collection
of
cross-lingual
web-document
pairs
in
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
pages
5960
5969
online
association
for
computational
linguistics
desmond
elliott
stella
frank
khalil
sima
an
and
lucia
specia
2016
multi30k
multilingual
englishgerman
image
descriptions
in
proceedings
of
the
5th
workshop
on
vision
and
language
pages
70
74
berlin
germany
association
for
computational
linguistics
tyna
eloundou
sam
manning
pamela
mishkin
and
daniel
rock
2023
gpts
are
gpts
an
early
look
at
the
labor
market
impact
potential
of
large
language
models
arxiv
preprint
arxiv
2303.10130
gianni
fenu
and
mirko
marras
2022
demographic
fairness
in
multimodal
biometrics
comparative
analysis
on
audio-visual
speaker
recognition
systems
procedia
computer
science
198
249
254
hugo
jair
escalante
heysem
kaya
albert
ali
salah
sergio
escalera
yagmur
gucluturk
umut
g√º√ßl√º
xavier
bar√≥
isabelle
guyon
julio
jacques
junior
meysam
madadi
et
al
2020
explaining
first
impressions
modeling
recognizing
and
explaining
apparent
personality
from
videos
ieee
transactions
on
affective
computing
emilio
ferrara
2023a
fairness
and
bias
in
artificial
intelligence
brief
survey
of
sources
impacts
and
mitigation
strategies
sci
joel
escud√©
font
and
marta
costa-juss√†
2019
equalizing
gender
bias
in
neural
machine
translation
with
word
embeddings
techniques
in
proceedings
of
the
first
workshop
on
gender
bias
in
natural
language
processing
pages
147
154
florence
italy
association
for
computational
linguistics
david
esiobu
xiaoqing
tan
saghar
hosseini
megan
ung
yuchen
zhang
jude
fernandes
jane
dwivediyu
eleonora
presani
adina
williams
and
eric
smith
2023
robbie
robust
bias
evaluation
of
large
generative
language
models
in
proceedings
of
the
2023
conference
on
empirical
methods
in
natural
language
processing
pages
3764
3814
singapore
association
for
computational
linguistics
kawin
ethayarajh
david
duvenaud
and
graeme
hirst
2019
understanding
undesirable
word
embedding
associations
in
proceedings
of
the
57th
annual
meeting
of
the
association
for
computational
linguistics
pages
1696
1705
florence
italy
association
for
computational
linguistics
chao
fan
miguel
esparza
jennifer
dargin
fangsheng
wu
bora
oztekin
and
ali
mostafavi
2020
spatial
biases
in
crowdsourced
data
social
media
content
attention
concentrates
on
populous
areas
in
disasters
computers
environment
and
urban
systems
83
101514
zahra
fatemi
chen
xing
wenhao
liu
and
caimming
xiong
2023
improving
gender
fairness
of
pre-trained
language
models
without
catastrophic
forgetting
in
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
short
papers
pages
1249
1262
toronto
canada
association
for
computational
linguistics
nanyi
fei
zhiwu
lu
yizhao
gao
guoxing
yang
yuqi
huo
jingyuan
wen
haoyu
lu
ruihua
song
xin
gao
tao
xiang
et
al
2022
towards
artificial
general
intelligence
via
multimodal
foundation
model
nature
communications
13
3094
virginia
felkner
ho-chun
herbert
chang
eugene
jang
and
jonathan
may
2023
winoqueer
communityin-the-loop
benchmark
for
anti-lgbtq
bias
in
large
language
models
in
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
9126
9140
toronto
canada
association
for
computational
linguistics
emilio
ferrara
2023b
should
chatgpt
be
biased
challenges
and
risks
of
bias
in
large
language
models
arxiv
preprint
arxiv
2304.03738
tom√°≈°
folty
nek
norman
meuschke
and
bela
gipp
2019
academic
plagiarism
detection
systematic
literature
review
acm
computing
surveys
csur
52
42
eric
frankel
and
edward
vendrow
2020
fair
generation
through
prior
modification
in
32nd
conference
on
neural
information
processing
systems
neurips
2018
barbara
fredrickson
and
tomi-ann
roberts
1997
objectification
theory
toward
understanding
women
lived
experiences
and
mental
health
risks
psychology
of
women
quarterly
21
173
206
vincent
freiberger
and
erik
buchmann
2024
fairness
certification
for
natural
language
processing
and
large
language
models
in
intelligent
systems
conference
pages
606
624
springer
felix
friedrich
manuel
brack
lukas
struppek
dominik
hintersdorf
patrick
schramowski
sasha
luccioni
and
kristian
kersting
2023
fair
diffusion
instructing
text-to-image
generation
models
on
fairness
arxiv
preprint
arxiv
2302.10893
niklas
friedrich
anne
lauscher
simone
paolo
ponzetto
and
goran
glava≈°
2021
debie
platform
for
implicit
and
explicit
debiasing
of
word
embedding
spaces
in
proceedings
of
the
16th
conference
of
the
european
chapter
of
the
association
for
computational
linguistics
system
demonstrations
pages
91
98
online
association
for
computational
linguistics
vinitha
gadiraju
shaun
kane
sunipa
dev
alex
taylor
ding
wang
emily
denton
and
robin
brewer
2023
wouldn
say
offensive
but
disability-centered
perspectives
on
large
language
models
in
proceedings
of
the
2023
acm
conference
on
fairness
accountability
and
transparency
pages
205
216
samir
yitzhak
gadre
gabriel
ilharco
alex
fang
jonathan
hayase
georgios
smyrnis
thao
nguyen
ryan
marten
mitchell
wortsman
dhruba
ghosh
jieyu
zhang
et
al
2024
datacomp
in
search
of
the
next
generation
of
multimodal
datasets
advances
in
neural
information
processing
systems
36
isabel
gallegos
ryan
rossi
joe
barrow
md
mehrab
tanjim
sungchul
kim
franck
dernoncourt
tong
yu
ruiyi
zhang
and
nesreen
ahmed
2023
bias
and
fairness
in
large
language
models
survey
arxiv
preprint
arxiv
2309.00770
deep
ganguli
danny
hernandez
liane
lovitt
amanda
askell
yuntao
bai
anna
chen
tom
conerly
nova
dassarma
dawn
drain
nelson
elhage
et
al
2022
predictability
and
surprise
in
large
generative
models
in
proceedings
of
the
2022
acm
conference
on
fairness
accountability
and
transparency
pages
1747
1764
leo
gao
stella
biderman
sid
black
laurence
golding
travis
hoppe
charles
foster
jason
phang
horace
he
anish
thite
noa
nabeshima
et
al
2020a
the
pile
an
800gb
dataset
of
diverse
text
for
language
modeling
arxiv
preprint
arxiv
2101.00027
tianyu
gao
adam
fisch
and
danqi
chen
2020b
making
pre-trained
language
models
better
few-shot
learners
arxiv
preprint
arxiv
2012.15723
sahaj
garg
vincent
perot
nicole
limtiaco
ankur
taly
ed
chi
and
alex
beutel
2019
counterfactual
fairness
in
text
classification
through
robustness
in
proceedings
of
the
2019
aaai
acm
conference
on
ai
ethics
and
society
pages
219
226
ismael
garrido-mu√±oz
arturo
montejo-r√°ez
fernando
mart√≠nez-santiago
and
alfonso
ure√±al√≥pez
2021
survey
on
bias
in
deep
nlp
applied
sciences
11
3184
timnit
gebru
jamie
morgenstern
briana
vecchione
jennifer
wortman
vaughan
hanna
wallach
hal
daum√©
iii
and
kate
crawford
2021
datasheets
for
datasets
communications
of
the
acm
64
12
86
92
samuel
gehman
suchin
gururangan
maarten
sap
yejin
choi
and
noah
smith
2020
realtoxicityprompts
evaluating
neural
toxic
degeneration
in
language
models
in
findings
of
the
association
for
computational
linguistics
emnlp
2020
pages
3356
3369
online
association
for
computational
linguistics
markos
georgopoulos
james
oldfield
mihalis
nicolaou
yannis
panagakis
and
maja
pantic
2021
mitigating
demographic
bias
in
facial
datasets
with
stylebased
multi-attribute
transfer
international
journal
of
computer
vision
129
2288
2307
joan
giner-miguelez
abel
g√≥mez
and
jordi
cabot
2023
datadoc
analyzer
tool
for
analyzing
the
documentation
of
scientific
datasets
in
proceedings
of
the
32nd
acm
international
conference
on
information
and
knowledge
management
pages
5046
5050
priya
goyal
quentin
duval
isaac
seessel
mathilde
caron
ishan
misra
levent
sagun
armand
joulin
and
piotr
bojanowski
2022a
vision
models
are
more
robust
and
fair
when
pretrained
on
uncurated
images
without
supervision
arxiv
preprint
arxiv
2202.08360
priya
goyal
adriana
romero
soriano
caner
hazirbas
levent
sagun
and
nicolas
usunier
2022b
fairness
indicators
for
systematic
assessments
of
visual
feature
extractors
in
proceedings
of
the
2022
acm
conference
on
fairness
accountability
and
transparency
pages
70
88
yash
goyal
tejas
khot
douglas
summers-stay
dhruv
batra
and
devi
parikh
2017
making
the
in
vqa
matter
elevating
the
role
of
image
understanding
in
visual
question
answering
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
pages
6904
6913
jerald
greenberg
1990
organizational
justice
yesterday
today
and
tomorrow
journal
of
management
16
399
432
wei
guo
and
aylin
caliskan
2021
detecting
emergent
intersectional
biases
contextualized
word
embeddings
contain
distribution
of
human-like
biases
in
proceedings
of
the
2021
aaai
acm
conference
on
ai
ethics
and
society
pages
122
133
yue
guo
yi
yang
and
ahmed
abbasi
2022
autodebias
debiasing
masked
language
models
with
automated
biased
prompts
in
proceedings
of
the
60th
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
1012
1023
dublin
ireland
association
for
computational
linguistics
paul
hager
friederike
jungmann
robbie
holland
kunal
bhagat
inga
hubrecht
manuel
knauer
jakob
vielhauer
marcus
makowski
rickmer
braren
georgios
kaissis
et
al
2024
evaluation
and
mitigation
of
the
limitations
of
large
language
models
in
clinical
decision-making
nature
medicine
pages
10
matan
halevy
camille
harris
amy
bruckman
diyi
yang
and
ayanna
howard
2021
mitigating
racial
biases
in
toxic
language
detection
with
an
equitybased
ensemble
framework
in
proceedings
of
the
1st
acm
conference
on
equity
and
access
in
algorithms
mechanisms
and
optimization
eaamo
21
new
york
ny
usa
association
for
computing
machinery
joschka
haltaufderheide
and
robert
ranisch
2024
the
ethics
of
chatgpt
in
medicine
and
healthcare
systematic
review
on
large
language
models
llms
npj
digital
medicine
183
yuchen
han
2023
fairness
evaluation
within
large
language
models
through
the
lens
of
depression
in
proceedings
of
the
2023
4th
international
conference
on
machine
learning
and
computer
application
pages
108
112
jiangang
hao
alina
von
davier
victoria
yaneva
susan
lottridge
matthias
von
davier
and
deborah
harris
2024
transforming
assessment
the
impacts
and
implications
of
large
language
models
and
generative
ai
educational
measurement
issues
and
practice
43
16
29
stefan
harrer
2023
attention
is
not
all
you
need
the
complicated
case
of
ethically
using
large
language
models
in
healthcare
and
medicine
ebiomedicine
90
kaiming
he
xiangyu
zhang
shaoqing
ren
and
jian
sun
2016
deep
residual
learning
for
image
recognition
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
pages
770
778
nathan
heflick
jamie
goldenberg
douglas
cooper
and
elisa
puvia
2011
from
women
to
objects
appearance
focus
target
gender
and
perceptions
of
warmth
morality
and
competence
journal
of
experimental
social
psychology
47
572
581
geoffrey
hinton
oriol
vinyals
and
jeff
dean
2015
distilling
the
knowledge
in
neural
network
arxiv
preprint
arxiv
1503.02531
yusuke
hirota
yuta
nakashima
and
noa
garcia
2022a
gender
and
racial
bias
in
visual
question
answering
datasets
in
proceedings
of
the
2022
acm
conference
on
fairness
accountability
and
transparency
pages
1280
1292
edward
hu
yelong
shen
phillip
wallis
zeyuan
allen-zhu
yuanzhi
li
shean
wang
lu
wang
and
weizhu
chen
2021
lora
low-rank
adaptation
of
large
language
models
arxiv
preprint
arxiv
2106.09685
po-sen
huang
huan
zhang
ray
jiang
robert
stanforth
johannes
welbl
jack
rae
vishal
maini
dani
yogatama
and
pushmeet
kohli
2020a
reducing
sentiment
bias
in
language
models
via
counterfactual
evaluation
in
findings
of
the
association
for
computational
linguistics
emnlp
2020
pages
65
83
online
association
for
computational
linguistics
xiaolei
huang
linzi
xing
franck
dernoncourt
and
michael
paul
2020b
multilingual
twitter
corpus
and
baselines
for
evaluating
demographic
bias
in
hate
speech
recognition
in
proceedings
of
the
twelfth
language
resources
and
evaluation
conference
pages
1440
1448
marseille
france
european
language
resources
association
yuheng
huang
jiayang
song
zhijie
wang
shengming
zhao
huaming
chen
felix
juefei-xu
and
lei
ma
2023
look
before
you
leap
an
exploratory
study
of
uncertainty
measurement
for
large
language
models
arxiv
preprint
arxiv
2307.10236
yusuke
hirota
yuta
nakashima
and
noa
garcia
2022b
quantifying
societal
bias
amplification
in
image
captioning
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pages
13450
13459
ben
hutchinson
and
margaret
mitchell
2019
50
years
of
test
un
fairness
lessons
for
machine
learning
in
proceedings
of
the
conference
on
fairness
accountability
and
transparency
pages
49
58
jordan
hoffmann
sebastian
borgeaud
arthur
mensch
elena
buchatskaya
trevor
cai
eliza
rutherford
diego
de
las
casas
lisa
anne
hendricks
johannes
welbl
aidan
clark
et
al
2022
an
empirical
analysis
of
compute-optimal
large
language
model
training
advances
in
neural
information
processing
systems
35
30016
30030
ben
hutchinson
vinodkumar
prabhakaran
emily
denton
kellie
webster
yu
zhong
and
stephen
denuyl
2020
social
biases
in
nlp
models
as
barriers
for
persons
with
disabilities
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
pages
5491
5501
online
association
for
computational
linguistics
max
hort
zhenpeng
chen
jie
zhang
mark
harman
and
federica
sarro
2024
bias
mitigation
for
machine
learning
classifiers
comprehensive
survey
acm
journal
on
responsible
computing
52
mimansa
jaiswal
and
emily
mower
provost
2020
privacy
enhanced
multimodal
neural
representations
for
emotion
recognition
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
34
pages
7985
7993
yupeng
hou
junjie
zhang
zihan
lin
hongyu
lu
ruobing
xie
julian
mcauley
and
wayne
xin
zhao
2024
large
language
models
are
zero-shot
rankers
for
recommender
systems
in
european
conference
on
information
retrieval
pages
364
381
springer
neil
houlsby
andrei
giurgiu
stanislaw
jastrzebski
bruna
morrone
quentin
de
laroussilhe
andrea
gesmundo
mona
attariyan
and
sylvain
gelly
2019
parameter-efficient
transfer
learning
for
nlp
in
proceedings
of
the
36th
international
conference
on
machine
learning
volume
97
of
proceedings
of
machine
learning
research
pages
2790
2799
pmlr
dirk
hovy
and
shrimai
prabhumoye
2021
five
sources
of
bias
in
natural
language
processing
language
and
linguistics
compass
15
e12432
hasan
jamil
2024
equity
and
fairness
challenges
in
online
learning
in
the
age
of
chatgpt
in
proceedings
of
the
39th
acm
sigapp
symposium
on
applied
computing
pages
91
92
christopher
jenks
2024
communicating
the
cultural
other
trust
and
bias
in
generative
ai
and
large
language
models
applied
linguistics
review
yongwoo
jeong
jiseon
yang
in
ho
choi
and
juyeon
lee
2024
feature-based
text
search
engine
mitigating
data
diversity
problem
using
pre-trained
large
language
model
for
fast
deployment
services
ieee
access
12
48145
48157
jiaming
ji
mickel
liu
josef
dai
xuehai
pan
chi
zhang
ce
bian
boyuan
chen
ruiyang
sun
yizhou
wang
and
yaodong
yang
2024
beavertails
towards
improved
safety
alignment
of
llm
via
humanpreference
dataset
advances
in
neural
information
processing
systems
36
chao
jia
yinfei
yang
ye
xia
yi-ting
chen
zarana
parekh
hieu
pham
quoc
le
yun-hsuan
sung
zhen
li
and
tom
duerig
2021
scaling
up
visual
and
vision-language
representation
learning
with
noisy
text
supervision
in
international
conference
on
machine
learning
pages
4904
4916
pmlr
albert
jiang
alexandre
sablayrolles
antoine
roux
arthur
mensch
blanche
savary
chris
bamford
devendra
singh
chaplot
diego
de
las
casas
emma
bou
hanna
florian
bressand
et
al
2024a
mixtral
of
experts
arxiv
preprint
arxiv
2401.04088
zhengbao
jiang
jun
araki
haibo
ding
and
graham
neubig
2021
how
can
we
know
when
language
models
know
on
the
calibration
of
language
models
for
question
answering
transactions
of
the
association
for
computational
linguistics
962
977
zifan
jiang
salman
seyedi
emily
griner
ahmed
abbasi
ali
bahrami
rad
hyeokhyen
kwon
robert
cotes
and
gari
clifford
2024b
evaluating
and
mitigating
unfairness
in
multimodal
remote
mental
health
assessments
plos
digital
health
e0000413
eun
seo
jo
and
timnit
gebru
2020
lessons
from
archives
strategies
for
collecting
sociocultural
data
in
machine
learning
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
pages
306
316
alistair
ew
johnson
lucas
bulgarelli
lu
shen
alvin
gayles
ayad
shammout
steven
horng
tom
pollard
sicheng
hao
benjamin
moody
brian
gow
et
al
2023
mimic-iv
freely
accessible
electronic
health
record
dataset
scientific
data
10
gargi
joshi
rahee
walambe
and
ketan
kotecha
2021
review
on
explainability
in
multimodal
deep
neural
nets
ieee
access
59800
59821
thomas
kehrenberg
myles
bartlett
oliver
thomas
and
novi
quadrianto
2020
null-sampling
for
interpretable
and
fair
representations
in
computer
vision
eccv
2020
pages
565
580
cham
springer
international
publishing
nitish
shirish
keskar
bryan
mccann
lav
varshney
caiming
xiong
and
richard
socher
2019
ctrl
conditional
transformer
language
model
for
controllable
generation
arxiv
preprint
arxiv
1909.05858
simran
khanuja
sebastian
ruder
and
partha
talukdar
2023
evaluating
inclusivity
equity
and
accessibility
of
nlp
technology
case
study
for
indian
languages
in
findings
of
the
association
for
computational
linguistics
eacl
2023
james
kirkpatrick
razvan
pascanu
neil
rabinowitz
joel
veness
guillaume
desjardins
andrei
rusu
kieran
milan
john
quan
tiago
ramalho
agnieszka
grabska-barwinska
et
al
2017
overcoming
catastrophic
forgetting
in
neural
networks
proceedings
of
the
national
academy
of
sciences
114
13
3521
3526
ryan
kiros
ruslan
salakhutdinov
and
rich
zemel
2014
multimodal
neural
language
models
in
proceedings
of
the
31st
international
conference
on
machine
learning
volume
32
of
proceedings
of
machine
learning
research
pages
595
603
bejing
china
pmlr
barbara
kitchenham
2004
procedures
for
performing
systematic
reviews
keele
uk
keele
university
33
2004
26
jing
yu
koh
daniel
fried
and
russ
salakhutdinov
2024
generating
images
with
multimodal
language
models
advances
in
neural
information
processing
systems
36
takeshi
kojima
shixiang
shane
gu
machel
reid
yutaka
matsuo
and
yusuke
iwasawa
2022
large
language
models
are
zero-shot
reasoners
advances
in
neural
information
processing
systems
35
22199
22213
mert
karabacak
and
konstantinos
margetis
2023
embracing
large
language
models
for
medical
applications
opportunities
and
challenges
cureus
15
hadas
kotek
rikker
dockum
and
david
sun
2023
gender
bias
and
stereotypes
in
large
language
models
in
proceedings
of
the
acm
collective
intelligence
conference
pages
12
24
kimmo
karkkainen
and
jungseock
joo
2021
fairface
face
attribute
dataset
for
balanced
race
gender
and
age
for
bias
measurement
and
mitigation
in
proceedings
of
the
ieee
cvf
winter
conference
on
applications
of
computer
vision
pages
1548
1558
angelie
kraft
hans-peter
zorn
pascal
fecht
judith
simon
chris
biemann
and
ricardo
usbeck
2022
measuring
gender
bias
in
german
language
generation
informatik
2022
enkelejda
kasneci
kathrin
se√üler
stefan
k√ºchemann
maria
bannert
daryna
dementieva
frank
fischer
urs
gasser
georg
groh
stephan
g√ºnnemann
eyke
h√ºllermeier
et
al
2023
chatgpt
for
good
on
opportunities
and
challenges
of
large
language
models
for
education
learning
and
individual
differences
103
102274
deepak
kumar
oleg
lesota
george
zerveas
daniel
cohen
carsten
eickhoff
markus
schedl
and
navid
rekabsaz
2023
parameter-efficient
modularised
bias
mitigation
via
adapterfusion
in
proceedings
of
the
17th
conference
of
the
european
chapter
of
the
association
for
computational
linguistics
pages
2738
2751
dubrovnik
croatia
association
for
computational
linguistics
yulia
kumar
kuan
huang
angelo
perez
guohao
yang
jenny
li
patricia
morreale
dov
kruger
and
raymond
jiang
2024
bias
and
cyberbullying
detection
and
data
generation
with
transformer
ai
models
and
top
llms
rina
kumari
and
asif
ekbal
2021
amfb
attention
based
multimodal
factorized
bilinear
pooling
for
multimodal
fake
news
detection
expert
systems
with
applications
184
115412
keita
kurita
nidhi
vyas
ayush
pareek
alan
black
and
yulia
tsvetkov
2019
measuring
bias
in
contextualized
word
representations
in
proceedings
of
the
first
workshop
on
gender
bias
in
natural
language
processing
pages
166
172
florence
italy
association
for
computational
linguistics
mascha
kurpicz-briki
2020
cultural
differences
in
bias
origin
and
gender
bias
in
pre-trained
german
and
french
word
embeddings
richard
landers
and
tara
behrend
2023
auditing
the
ai
auditors
framework
for
evaluating
fairness
and
bias
in
high
stakes
ai
predictive
models
american
psychologist
78
36
hugo
lauren√ßon
lucile
saulnier
thomas
wang
christopher
akiki
albert
villanova
del
moral
teven
le
scao
leandro
von
werra
chenghao
mou
eduardo
gonz√°lez
ponferrada
huu
nguyen
et
al
2022
the
bigscience
roots
corpus
1.6
tb
composite
multilingual
dataset
advances
in
neural
information
processing
systems
35
31809
31826
anne
lauscher
tobias
lueken
and
goran
glava≈°
2021
sustainable
modular
debiasing
of
language
models
in
findings
of
the
association
for
computational
linguistics
emnlp
2021
pages
4782
4797
punta
cana
dominican
republic
association
for
computational
linguistics
tai
le
quy
arjun
roy
vasileios
iosifidis
wenbin
zhang
and
eirini
ntoutsi
2022
survey
on
datasets
for
fairness-aware
machine
learning
wiley
interdisciplinary
reviews
data
mining
and
knowledge
discovery
12
e1452
teven
le
scao
angela
fan
christopher
akiki
ellie
pavlick
suzana
ilic
daniel
hesslow
roman
castagn√©
alexandra
sasha
luccioni
fran√ßois
yvon
matthias
gall√©
et
al
2023
bloom
176bparameter
open-access
multilingual
language
model
and
percy
liang
2023
holistic
evaluation
of
textto-image
models
in
advances
in
neural
information
processing
systems
volume
36
pages
69981
70011
curran
associates
inc
eric
lehman
evan
hernandez
diwakar
mahajan
jonas
wulff
micah
smith
zachary
ziegler
daniel
nadler
peter
szolovits
alistair
johnson
and
emily
alsentzer
2023
do
we
still
need
clinical
language
models
in
conference
on
health
inference
and
learning
pages
578
597
pmlr
thibaud
leteno
antoine
gourru
charlotte
laclau
and
christophe
gravier
2023
an
investigation
of
structures
responsible
for
gender
bias
in
bert
and
distilbert
in
international
symposium
on
intelligent
data
analysis
pages
249
261
springer
gerald
leventhal
1980
what
should
be
done
with
equity
theory
new
approaches
to
the
study
of
fairness
in
social
relationships
in
social
exchange
advances
in
theory
and
research
pages
27
55
springer
jiaoda
li
duygu
ataman
and
rico
sennrich
2021
vision
matters
when
it
should
sanity
checking
multimodal
machine
translation
models
in
proceedings
of
the
2021
conference
on
empirical
methods
in
natural
language
processing
pages
8556
8562
online
and
punta
cana
dominican
republic
association
for
computational
linguistics
junnan
li
dongxu
li
caiming
xiong
and
steven
hoi
2022
blip
bootstrapping
language-image
pretraining
for
unified
vision-language
understanding
and
generation
in
international
conference
on
machine
learning
pages
12888
12900
pmlr
yunqi
li
lanjing
zhang
and
yongfeng
zhang
2024
probing
into
the
fairness
of
large
language
models
case
study
of
chatgpt
in
2024
58th
annual
conference
on
information
sciences
and
systems
ciss
pages
paul
pu
liang
irene
mengze
li
emily
zheng
yao
chong
lim
ruslan
salakhutdinov
and
louisphilippe
morency
2020
towards
debiasing
sentence
representations
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
pages
5502
5515
online
association
for
computational
linguistics
jinsook
lee
yann
hicke
renzhe
yu
christopher
brooks
and
ren√©
kizilcec
2024
the
life
cycle
of
large
language
models
in
education
framework
for
understanding
sources
of
bias
british
journal
of
educational
technology
paul
pu
liang
yiwei
lyu
xiang
fan
zetian
wu
yun
cheng
jason
wu
leslie
chen
peter
wu
michelle
lee
yuke
zhu
et
al
2021a
multibench
multiscale
benchmarks
for
multimodal
representation
learning
advances
in
neural
information
processing
systems
2021
db1
tony
lee
michihiro
yasunaga
chenlin
meng
yifan
mai
joon
sung
park
agrim
gupta
yunzhi
zhang
deepak
narayanan
hannah
teufel
marco
bellagente
minguk
kang
taesung
park
jure
leskovec
jun-yan
zhu
fei-fei
li
jiajun
wu
stefano
ermon
paul
pu
liang
chiyu
wu
louis-philippe
morency
and
ruslan
salakhutdinov
2021b
towards
understanding
and
mitigating
social
biases
in
language
models
in
international
conference
on
machine
learning
pages
6565
6576
pmlr
paul
pu
liang
chiyu
wu
louis-philippe
morency
and
ruslan
salakhutdinov
2021c
towards
understanding
and
mitigating
social
biases
in
language
models
in
proceedings
of
the
38th
international
conference
on
machine
learning
volume
139
of
proceedings
of
machine
learning
research
pages
6565
6576
pmlr
paul
pu
liang
amir
zadeh
and
louis-philippe
morency
2022a
foundations
and
trends
in
multimodal
machine
learning
principles
challenges
and
open
questions
arxiv
preprint
arxiv
2209.03430
percy
liang
rishi
bommasani
tony
lee
dimitris
tsipras
dilara
soylu
michihiro
yasunaga
yian
zhang
deepak
narayanan
yuhuai
wu
ananya
kumar
et
al
2022b
holistic
evaluation
of
language
models
arxiv
preprint
arxiv
2211.09110
victor
weixin
liang
yuhui
zhang
yongchan
kwon
serena
yeung
and
james
zou
2022c
mind
the
gap
understanding
the
modality
gap
in
multimodal
contrastive
representation
learning
advances
in
neural
information
processing
systems
35
17612
17625
stephanie
lin
jacob
hilton
and
owain
evans
2022
truthfulqa
measuring
how
models
mimic
human
falsehoods
in
proceedings
of
the
60th
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
3214
3252
dublin
ireland
association
for
computational
linguistics
tsung-yi
lin
michael
maire
serge
belongie
james
hays
pietro
perona
deva
ramanan
piotr
doll√°r
and
lawrence
zitnick
2014
microsoft
coco
common
objects
in
context
in
computer
vision
eccv
2014
13th
european
conference
zurich
switzerland
september
12
2014
proceedings
part
13
pages
740
755
springer
haotian
liu
chunyuan
li
yuheng
li
and
yong
jae
lee
2024a
improved
baselines
with
visual
instruction
tuning
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
26296
26306
haotian
liu
chunyuan
li
qingyang
wu
and
yong
jae
lee
2024b
visual
instruction
tuning
advances
in
neural
information
processing
systems
36
xiao-yang
liu
guoxuan
wang
hongyang
yang
and
daochen
zha
2023a
fingpt
democratizing
internet-scale
data
for
financial
large
language
models
arxiv
preprint
arxiv
2307.10485
yang
liu
yuanshun
yao
jean-francois
ton
xiaoying
zhang
ruocheng
guo
hao
cheng
yegor
klochkov
muhammad
faaiz
taufiq
and
hang
li
2023b
trustworthy
llms
survey
and
guideline
for
evaluating
large
language
models
alignment
arxiv
preprint
arxiv
2308.05374
yiheng
liu
tianle
han
siyuan
ma
jiayue
zhang
yuanyuan
yang
jiaming
tian
hao
he
antong
li
mengshen
he
zhengliang
liu
et
al
2023c
summary
of
chatgpt-related
research
and
perspective
towards
the
future
of
large
language
models
metaradiology
page
100017
yinhan
liu
myle
ott
naman
goyal
jingfei
du
mandar
joshi
danqi
chen
omer
levy
mike
lewis
luke
zettlemoyer
and
veselin
stoyanov
2019
roberta
robustly
optimized
bert
pretraining
approach
arxiv
preprint
arxiv
1907.11692
kaiji
lu
piotr
mardziel
fangjing
wu
preetam
amancharla
and
anupam
datta
2020
gender
bias
in
neural
natural
language
processing
logic
language
and
security
essays
dedicated
to
andre
scedrov
on
the
occasion
of
his
65th
birthday
pages
189
202
alexandra
luccioni
and
joseph
viviano
2021
what
in
the
box
an
analysis
of
undesirable
content
in
the
common
crawl
corpus
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
short
papers
pages
182
189
nicholas
lui
bryan
chia
william
berrios
candace
ross
and
douwe
kiela
2024
leveraging
diffusion
perturbations
for
measuring
fairness
in
computer
vision
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
38
pages
14220
14228
hanjun
luo
haoyu
huang
ziye
deng
xuecheng
liu
ruizhe
chen
and
zuozhu
liu
2024
bigbench
unified
benchmark
for
social
bias
in
text-to-image
generative
models
based
on
multi-modal
llm
arxiv
preprint
arxiv
2407.15240
debbie
ma
joshua
correll
and
bernd
wittenbrink
2015
the
chicago
face
database
free
stimulus
set
of
faces
and
norming
data
behavior
research
methods
47
1122
1135
huan
ma
changqing
zhang
yatao
bian
lemao
liu
zhirui
zhang
peilin
zhao
shu
zhang
huazhu
fu
qinghua
hu
and
bingzhe
wu
2023
fairnessguided
few-shot
prompting
for
large
language
models
advances
in
neural
information
processing
systems
36
43136
43155
vincent
quirante
malic
anamika
kumari
and
xiaozhong
liu
2023
racial
skew
in
fine-tuned
legal
ai
language
models
in
2023
ieee
international
conference
on
data
mining
workshops
icdmw
pages
245
252
abhishek
mandal
susan
leavy
and
suzanne
little
2023a
measuring
bias
in
multimodal
models
multimodal
composite
association
score
in
international
workshop
on
algorithmic
bias
in
search
and
recommendation
pages
17
30
springer
abhishek
mandal
suzanne
little
and
susan
leavy
2023b
multimodal
bias
assessing
gender
bias
in
computer
vision
models
with
nlp
techniques
in
proceedings
of
the
25th
international
conference
on
multimodal
interaction
icmi
23
page
416
424
new
york
ny
usa
association
for
computing
machinery
advances
in
neural
information
processing
systems
26
chandler
may
alex
wang
shikha
bordia
samuel
bowman
and
rachel
rudinger
2019
on
measuring
social
biases
in
sentence
encoders
in
proceedings
of
the
2019
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
long
and
short
papers
pages
622
628
minneapolis
minnesota
association
for
computational
linguistics
bonan
min
hayley
ross
elior
sulem
amir
pouran
ben
veyseh
thien
huu
nguyen
oscar
sainz
eneko
agirre
ilana
heintz
and
dan
roth
2023
recent
advances
in
natural
language
processing
via
large
pre-trained
language
models
survey
acm
computing
surveys
56
40
nicholas
meade
elinor
poole-dayan
and
siva
reddy
2022
an
empirical
survey
of
the
effectiveness
of
debiasing
techniques
for
pre-trained
language
models
in
proceedings
of
the
60th
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
1878
1898
dublin
ireland
association
for
computational
linguistics
ninareh
mehrabi
fred
morstatter
nripsuta
saxena
kristina
lerman
and
aram
galstyan
2021
survey
on
bias
and
fairness
in
machine
learning
acm
computing
surveys
csur
54
35
katelyn
mei
sonia
fereidooni
and
aylin
caliskan
2023
bias
against
93
stigmatized
groups
in
masked
language
models
and
downstream
sentiment
classification
tasks
in
proceedings
of
the
2023
acm
conference
on
fairness
accountability
and
transparency
facct
23
page
1699
1710
new
york
ny
usa
association
for
computing
machinery
chuizheng
meng
loc
trinh
nan
xu
james
enouen
and
yan
liu
2022a
interpretability
and
fairness
evaluation
of
deep
learning
models
on
mimic-iv
dataset
scientific
reports
12
7166
yu
meng
jiaxin
huang
yu
zhang
and
jiawei
han
2022b
generating
training
data
with
language
models
towards
zero-shot
language
understanding
advances
in
neural
information
processing
systems
35
462
477
jacob
menick
maja
trebacz
vladimir
mikulik
john
aslanides
francis
song
martin
chadwick
mia
glaese
susannah
young
lucy
campbellgillingham
geoffrey
irving
et
al
2022
teaching
language
models
to
support
answers
with
verified
quotes
arxiv
preprint
arxiv
2203.11147
bertalan
mesk√≥
and
eric
topol
2023
the
imperative
for
regulatory
oversight
of
large
language
models
or
generative
ai
in
healthcare
npj
digital
medicine
120
jesse
meyer
ryan
urbanowicz
patrick
cn
martin
karen
connor
ruowang
li
pei-chen
peng
tiffani
bright
nicholas
tatonetti
kyoung
jae
won
graciela
gonzalez-hernandez
et
al
2023
chatgpt
and
large
language
models
in
academia
opportunities
and
challenges
biodata
mining
16
20
tomas
mikolov
ilya
sutskever
kai
chen
greg
corrado
and
jeff
dean
2013
distributed
representations
of
words
and
phrases
and
their
compositionality
sergio
morales
robert
claris√≥
and
jordi
cabot
2023
automating
bias
testing
of
llms
in
2023
38th
ieee
acm
international
conference
on
automated
software
engineering
ase
pages
1705
1707
marzieh
mozafari
reza
farahbakhsh
and
no√´l
crespi
2020
hate
speech
detection
and
racial
bias
mitigation
in
social
media
based
on
bert
model
plos
one
15
e0237861
norman
mu
alexander
kirillov
david
wagner
and
saining
xie
2022
slip
self-supervision
meets
language-image
pre-training
in
computer
vision
eccv
2022
17th
european
conference
tel
aviv
israel
october
23
27
2022
proceedings
part
xxvi
page
529
544
berlin
heidelberg
springer-verlag
devon
myers
rami
mohawesh
venkata
ishwarya
chellaboina
anantha
lakshmi
sathvik
praveen
venkatesh
yi-hui
ho
hanna
henshaw
muna
alhawawreh
david
berdik
and
yaser
jararweh
2024
foundation
and
large
language
models
fundamentals
challenges
opportunities
and
social
impacts
cluster
computing
27
26
moin
nadeem
anna
bethke
and
siva
reddy
2021
stereoset
measuring
stereotypical
bias
in
pretrained
language
models
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
pages
5356
5371
online
association
for
computational
linguistics
arsha
nagrani
joon
son
chung
weidi
xie
and
andrew
zisserman
2020
voxceleb
large-scale
speaker
verification
in
the
wild
computer
speech
language
60
101027
taishi
nakamura
mayank
mishra
simone
tedeschi
yekun
chai
jason
stillerman
felix
friedrich
prateek
yadav
tanmay
laud
vu
minh
chien
terry
yue
zhuo
et
al
2024
aurora-m
the
first
open
source
multilingual
language
model
red-teamed
according
to
the
us
executive
order
arxiv
preprint
arxiv
2404.00399
abdulqadir
nashwan
and
ahmad
abujaber
2023
harnessing
large
language
models
in
nursing
care
planning
opportunities
challenges
and
ethical
considerations
cureus
15
roberto
navigli
simone
conia
and
bj√∂rn
ross
2023
biases
in
large
language
models
origins
inventory
and
discussion
acm
journal
of
data
and
information
quality
15
21
zabir
al
nazi
and
wei
peng
2024
large
language
models
in
healthcare
and
medical
domain
review
informatics
11
arshaan
nazir
thadaka
kalyan
chakravarthy
david
amore
cecchini
rakshit
khajuria
prikshit
sharma
ali
tarik
mirik
veysel
kocaman
and
david
talby
2024
langtest
comprehensive
evaluation
library
for
custom
llm
and
nlp
models
software
impacts
19
100619
alexander
quinn
nichol
prafulla
dhariwal
aditya
ramesh
pranav
shyam
pamela
mishkin
bob
mcgrew
ilya
sutskever
and
mark
chen
2022
glide
towards
photorealistic
image
generation
and
editing
with
text-guided
diffusion
models
in
proceedings
of
the
39th
international
conference
on
machine
learning
volume
162
of
proceedings
of
machine
learning
research
pages
16784
16804
pmlr
am
towards
centering
transgender
and
non-binary
voices
to
measure
biases
in
open
language
generation
facct
23
page
1246
1266
new
york
ny
usa
association
for
computing
machinery
tiago
pagano
rafael
loureiro
fernanda
vn
lisboa
rodrigo
peixoto
guilherme
as
guimar√£es
gustavo
or
cruz
maira
araujo
lucas
santos
marco
as
cruz
ewerton
ls
oliveira
et
al
2023
bias
and
unfairness
in
machine
learning
models
systematic
review
on
datasets
tools
fairness
metrics
and
identification
and
mitigation
methods
big
data
and
cognitive
computing
15
irene
pagliai
goya
van
boven
tosin
adewumi
lama
alkhaled
namrata
gurung
isabella
s√∂dergren
and
elisa
barney
2024
data
bias
according
to
bipol
men
are
naturally
right
and
it
is
the
role
of
women
to
follow
their
lead
arxiv
preprint
arxiv
2404.04838
yulei
niu
kaihua
tang
hanwang
zhang
zhiwu
lu
xian-sheng
hua
and
ji-rong
wen
2021a
counterfactual
vqa
cause-effect
look
at
language
bias
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
12700
12710
ji
ho
park
jamin
shin
and
pascale
fung
2018
reducing
gender
bias
in
abusive
language
detection
in
proceedings
of
the
2018
conference
on
empirical
methods
in
natural
language
processing
pages
2799
2804
brussels
belgium
association
for
computational
linguistics
yulei
niu
kaihua
tang
hanwang
zhang
zhiwu
lu
xian-sheng
hua
and
ji-rong
wen
2021b
counterfactual
vqa
cause-effect
look
at
language
bias
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pages
12700
12710
alicia
parrish
angelica
chen
nikita
nangia
vishakh
padmakumar
jason
phang
jana
thompson
phu
mon
htut
and
samuel
bowman
2022
bbq
hand-built
bias
benchmark
for
question
answering
in
findings
of
the
association
for
computational
linguistics
acl
2022
pages
2086
2105
dublin
ireland
association
for
computational
linguistics
brian
nosek
mahzarin
banaji
and
anthony
greenwald
2002
harvesting
implicit
group
attitudes
and
beliefs
from
demonstration
web
site
group
dynamics
theory
research
and
practice
101
alejandro
pe√±a
ignacio
serna
aythami
morales
julian
fierrez
alfonso
ortega
ainhoa
herrarte
manuel
alcantara
and
javier
ortega-garcia
2023
humancentric
multimodal
machine
learning
recent
advances
and
testbed
on
ai-based
recruitment
sn
computer
science
434
debora
nozza
federico
bianchi
and
dirk
hovy
2021
honest
measuring
hurtful
sentence
completion
in
language
models
in
the
2021
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
association
for
computational
linguistics
eirini
ntoutsi
pavlos
fafalios
ujwal
gadiraju
vasileios
iosifidis
wolfgang
nejdl
maria-esther
vidal
salvatore
ruggieri
franco
turini
symeon
papadopoulos
emmanouil
krasanakis
et
al
2020
bias
in
data-driven
artificial
intelligence
systems
an
introductory
survey
wiley
interdisciplinary
reviews
data
mining
and
knowledge
discovery
10
e1356
xiaokang
peng
yake
wei
andong
deng
dong
wang
and
di
hu
2022
balanced
multimodal
learning
via
on-the-fly
gradient
modulation
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
8238
8247
jeffrey
pennington
richard
socher
and
christopher
manning
2014
glove
global
vectors
for
word
representation
in
proceedings
of
the
2014
conference
on
empirical
methods
in
natural
language
processing
emnlp
pages
1532
1543
long
ouyang
jeffrey
wu
xu
jiang
diogo
almeida
carroll
wainwright
pamela
mishkin
chong
zhang
sandhini
agarwal
katarina
slama
alex
ray
et
al
2022
training
language
models
to
follow
instructions
with
human
feedback
advances
in
neural
information
processing
systems
35
27730
27744
ethan
perez
saffron
huang
francis
song
trevor
cai
roman
ring
john
aslanides
amelia
glaese
nat
mcaleese
and
geoffrey
irving
2022
red
teaming
language
models
with
language
models
in
proceedings
of
the
2022
conference
on
empirical
methods
in
natural
language
processing
pages
3419
3448
abu
dhabi
united
arab
emirates
association
for
computational
linguistics
anaelia
ovalle
palash
goyal
jwala
dhamala
zachary
jaggers
kai-wei
chang
aram
galstyan
richard
zemel
and
rahul
gupta
2023
fully
who
dana
pessach
and
erez
shmueli
2022
review
on
fairness
in
machine
learning
acm
comput
surv
55
suzanne
petryk
lisa
dunlap
keyan
nasseri
joseph
gonzalez
trevor
darrell
and
anna
rohrbach
2022
on
guiding
visual
attention
with
language
specification
in
2022
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pages
18071
18081
colin
raffel
noam
shazeer
adam
roberts
katherine
lee
sharan
narang
michael
matena
yanqi
zhou
wei
li
and
peter
liu
2020
exploring
the
limits
of
transfer
learning
with
unified
text-to-text
transformer
journal
of
machine
learning
research
21
140
67
jenny
pettersson
elias
hult
tim
eriksson
and
tosin
adewumi
2024
generative
ai
and
teachers
for
us
or
against
us
case
study
arxiv
preprint
arxiv
2404.03486
anil
rahate
rahee
walambe
sheela
ramanna
and
ketan
kotecha
2022
multimodal
co-learning
challenges
applications
with
datasets
recent
advances
and
future
directions
information
fusion
81
203
239
jonas
pfeiffer
aishwarya
kamath
andreas
r√ºckl√©
kyunghyun
cho
and
iryna
gurevych
2021
adapterfusion
non-destructive
task
composition
for
transfer
learning
in
proceedings
of
the
16th
conference
of
the
european
chapter
of
the
association
for
computational
linguistics
main
volume
pages
487
503
online
association
for
computational
linguistics
wasifur
rahman
md
kamrul
hasan
sangwu
lee
amir
zadeh
chengfeng
mao
louis-philippe
morency
and
ehsan
hoque
2020
integrating
multimodal
information
in
large
pretrained
transformers
in
proceedings
of
the
conference
association
for
computational
linguistics
meeting
volume
2020
page
2359
nih
public
access
andr√©s
pi√±eiro-mart√≠n
carmen
garc√≠a-mateo
laura
doc√≠o-fern√°ndez
and
maria
del
carmen
lopezperez
2023
ethical
challenges
in
the
development
of
virtual
assistants
powered
by
large
language
models
electronics
12
14
3170
chahat
raj
anjishnu
mukherjee
and
ziwei
zhu
2023
true
and
fair
robust
and
unbiased
fake
news
detection
via
interpretable
machine
learning
in
proceedings
of
the
2023
aaai
acm
conference
on
ai
ethics
and
society
pages
962
963
bilal
porgali
v√≠tor
albiero
jordan
ryda
cristian
canton
ferrer
and
caner
hazirbas
2023
the
casual
conversations
v2
dataset
diverse
large
benchmark
for
measuring
fairness
and
robustness
in
audio
vision
speech
models
in
2023
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
workshops
cvprw
pages
10
17
aditya
ramesh
prafulla
dhariwal
alex
nichol
casey
chu
and
mark
chen
2022
hierarchical
textconditional
image
generation
with
clip
latents
arxiv
preprint
arxiv
2204.06125
reid
pryzant
richard
diehl
martinez
nathan
dass
sadao
kurohashi
dan
jurafsky
and
diyi
yang
2020
automatically
neutralizing
subjective
bias
in
text
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
34
pages
480
489
xiangyu
qi
yi
zeng
tinghao
xie
pin-yu
chen
ruoxi
jia
prateek
mittal
and
peter
henderson
2023
finetuning
aligned
language
models
compromises
safety
even
when
users
do
not
intend
to
arxiv
preprint
arxiv
2310.03693
alec
radford
jong
wook
kim
chris
hallacy
aditya
ramesh
gabriel
goh
sandhini
agarwal
girish
sastry
amanda
askell
pamela
mishkin
jack
clark
et
al
2021
learning
transferable
visual
models
from
natural
language
supervision
in
international
conference
on
machine
learning
pages
8748
8763
pmlr
aditya
ramesh
mikhail
pavlov
gabriel
goh
scott
gray
chelsea
voss
alec
radford
mark
chen
and
ilya
sutskever
2021
zero-shot
text-to-image
generation
in
international
conference
on
machine
learning
pages
8821
8831
pmlr
krithika
ramesh
sunayana
sitaram
and
monojit
choudhury
2023
fairness
in
language
models
beyond
english
gaps
and
challenges
in
findings
of
the
association
for
computational
linguistics
eacl
2023
pages
2106
2119
dubrovnik
croatia
association
for
computational
linguistics
aneri
rana
and
sonali
jha
2022
emotion
based
hate
speech
detection
using
multimodal
learning
arxiv
preprint
arxiv
2202.06218
partha
pratim
ray
2023
chatgpt
comprehensive
review
on
background
applications
key
challenges
bias
ethics
limitations
and
future
scope
internet
of
things
and
cyber-physical
systems
121
154
alec
radford
jeffrey
wu
rewon
child
david
luan
dario
amodei
ilya
sutskever
et
al
2019
language
models
are
unsupervised
multitask
learners
openai
blog
shaina
raza
shardul
ghuge
chen
ding
elham
dolatabadi
and
deval
pandya
2024
fair
enough
develop
and
assess
fair-compliant
dataset
for
large
language
model
training
data
intelligence
559
585
jack
rae
sebastian
borgeaud
trevor
cai
katie
millican
jordan
hoffmann
francis
song
john
aslanides
sarah
henderson
roman
ring
susannah
young
et
al
2021
scaling
language
models
methods
analysis
insights
from
training
gopher
arxiv
preprint
arxiv
2112.11446
robin
rombach
andreas
blattmann
dominik
lorenz
patrick
esser
and
bj√∂rn
ommer
2022
highresolution
image
synthesis
with
latent
diffusion
models
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
10684
10695
eliane
r√∂√∂sli
selen
bozkurt
and
tina
hernandezboussard
2022
peeking
into
black
box
the
fairness
and
generalizability
of
mimic-iii
benchmarking
model
scientific
data
24
candace
ross
boris
katz
and
andrei
barbu
2021
measuring
social
biases
in
grounded
vision
and
language
embeddings
in
proceedings
of
the
2021
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
pages
998
1008
online
association
for
computational
linguistics
sebastian
ruder
ivan
vulic
and
anders
s√∏gaard
2022
square
one
bias
in
nlp
towards
multidimensional
exploration
of
the
research
manifold
in
findings
of
the
association
for
computational
linguistics
acl
2022
pages
2340
2354
dublin
ireland
association
for
computational
linguistics
rachel
rudinger
jason
naradowsky
brian
leonard
and
benjamin
van
durme
2018
gender
bias
in
coreference
resolution
in
proceedings
of
the
2018
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
short
papers
pages
14
new
orleans
louisiana
association
for
computational
linguistics
gabriele
ruggeri
and
debora
nozza
2023
multidimensional
study
on
bias
in
vision-language
models
in
findings
of
the
association
for
computational
linguistics
acl
2023
pages
6445
6455
toronto
canada
association
for
computational
linguistics
daniela
ruzzante
bianca
monachesi
noemi
orabona
and
jeroen
vaes
2022
the
sexual
objectification
and
emotion
database
free
stimulus
set
and
norming
data
of
sexually
objectified
and
non-objectified
female
targets
expressing
multiple
emotions
behavior
research
methods
pages
15
abel
salinas
parth
shah
yuzhong
huang
robert
mccormack
and
fred
morstatter
2023
the
unequal
opportunities
of
large
language
models
examining
demographic
biases
in
job
recommendations
by
chatgpt
and
llama
in
proceedings
of
the
3rd
acm
conference
on
equity
and
access
in
algorithms
mechanisms
and
optimization
eaamo
23
new
york
ny
usa
association
for
computing
machinery
mansour
sami
ashkan
sami
and
pete
barclay
2023
case
study
of
fairness
in
generated
images
of
large
language
models
for
software
engineering
tasks
in
2023
ieee
international
conference
on
software
maintenance
and
evolution
icsme
pages
391
396
shibani
santurkar
esin
durmus
faisal
ladhak
cinoo
lee
percy
liang
and
tatsunori
hashimoto
2023
whose
opinions
do
language
models
reflect
in
international
conference
on
machine
learning
pages
29971
30004
pmlr
maarten
sap
saadia
gabriel
lianhui
qin
dan
jurafsky
noah
smith
and
yejin
choi
2020
social
bias
frames
reasoning
about
social
and
power
implications
of
language
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
pages
5477
5490
online
association
for
computational
linguistics
akrati
saxena
george
fletcher
and
mykola
pechenizkiy
2024
fairsna
algorithmic
fairness
in
social
network
analysis
acm
computing
surveys
56
45
rylan
schaeffer
brando
miranda
and
sanmi
koyejo
2024
are
emergent
abilities
of
large
language
models
mirage
advances
in
neural
information
processing
systems
36
janice
scheuneman
1979
method
of
assessing
bias
in
test
items
journal
of
educational
measurement
pages
143
152
timo
schick
sahana
udupa
and
hinrich
sch√ºtze
2021
self-diagnosis
and
self-debiasing
proposal
for
reducing
corpus-based
bias
in
nlp
transactions
of
the
association
for
computational
linguistics
1408
1424
patrick
schramowski
cigdem
turan
nico
andersen
constantin
rothkopf
and
kristian
kersting
2022
large
pre-trained
language
models
contain
humanlike
biases
of
what
is
right
and
wrong
to
do
nature
machine
intelligence
258
268
sarah
schr√∂der
alexander
schulz
ivan
tarakanov
robert
feldhans
and
barbara
hammer
2023
measuring
fairness
with
biased
data
case
study
on
the
effects
of
unsupervised
data
in
fairness
evaluation
in
international
work-conference
on
artificial
neural
networks
pages
134
145
springer
christoph
schuhmann
romain
beaumont
richard
vencu
cade
gordon
ross
wightman
mehdi
cherti
theo
coombes
aarush
katta
clayton
mullis
mitchell
wortsman
et
al
2022
laion-5b
an
open
large-scale
dataset
for
training
next
generation
imagetext
models
advances
in
neural
information
processing
systems
35
25278
25294
christoph
schuhmann
richard
vencu
romain
beaumont
robert
kaczmarczyk
clayton
mullis
aarush
katta
theo
coombes
jenia
jitsev
and
aran
komatsuzaki
2021
laion-400m
open
dataset
of
clipfiltered
400
million
image-text
pairs
arxiv
preprint
arxiv
2111.02114
ramprasaath
selvaraju
michael
cogswell
abhishek
das
ramakrishna
vedantam
devi
parikh
and
dhruv
batra
2017
grad-cam
visual
explanations
from
deep
networks
via
gradient-based
localization
in
2017
ieee
international
conference
on
computer
vision
iccv
pages
618
626
greg
serapio-garc√≠a
mustafa
safdari
cl√©ment
crepy
luning
sun
stephen
fitz
peter
romero
marwa
abdulhai
aleksandra
faust
and
maja
mataric
2023
personality
traits
in
large
language
models
arxiv
preprint
arxiv
2307.00184
ignacio
serna
aythami
morales
julian
fierrez
and
nick
obradovich
2022
sensitive
loss
improving
accuracy
and
fairness
of
face
representations
with
discrimination-aware
deep
learning
artificial
intelligence
305
103682
ashish
seth
mayur
hemani
and
chirag
agarwal
2023
dear
debiasing
vision-language
models
with
additive
residuals
in
2023
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
cvpr
pages
6820
6829
omar
shaikh
hongxin
zhang
william
held
michael
bernstein
and
diyi
yang
2023
on
second
thought
let
not
think
step
by
step
bias
and
toxicity
in
zeroshot
reasoning
in
proceedings
of
the
61st
annual
meeting
of
the
association
for
computational
linguistics
volume
long
papers
pages
4454
4470
toronto
canada
association
for
computational
linguistics
emily
sheng
kai-wei
chang
prem
natarajan
and
nanyun
peng
2021
societal
biases
in
language
generation
progress
and
challenges
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
pages
4275
4293
online
association
for
computational
linguistics
emily
sheng
kai-wei
chang
premkumar
natarajan
and
nanyun
peng
2019
the
woman
worked
as
babysitter
on
biases
in
language
generation
in
proceedings
of
the
2019
conference
on
empirical
methods
in
natural
language
processing
and
the
9th
international
joint
conference
on
natural
language
processing
emnlp-ijcnlp
pages
3407
3412
hong
kong
china
association
for
computational
linguistics
koustuv
sinha
robin
jia
dieuwke
hupkes
joelle
pineau
adina
williams
and
douwe
kiela
2021
masked
language
modeling
and
the
distributional
hypothesis
order
word
matters
pre-training
for
little
arxiv
preprint
arxiv
2104.06644
eric
michael
smith
melissa
hall
melanie
kambadur
eleonora
presani
and
adina
williams
2022a
sorry
to
hear
that
finding
new
biases
in
language
models
with
holistic
descriptor
dataset
in
proceedings
of
the
2022
conference
on
empirical
methods
in
natural
language
processing
pages
9180
9211
abu
dhabi
united
arab
emirates
association
for
computational
linguistics
shaden
smith
mostofa
patwary
brandon
norick
patrick
legresley
samyam
rajbhandari
jared
casper
zhun
liu
shrimai
prabhumoye
george
zerveas
vijay
korthikanti
et
al
2022b
using
deepspeed
and
megatron
to
train
megatron-turing
nlg
530b
large-scale
generative
language
model
arxiv
preprint
arxiv
2201.11990
eduardo
soares
and
plamen
angelov
2019
fair-bydesign
explainable
models
for
prediction
of
recidivism
arxiv
preprint
arxiv
1910.02043
irene
solaiman
and
christy
dennison
2021
process
for
adapting
language
models
to
society
palms
with
values-targeted
datasets
advances
in
neural
information
processing
systems
34
5861
5873
krishna
srinivasan
karthik
raman
jiecao
chen
michael
bendersky
and
marc
najork
2021
wit
wikipedia-based
image
text
dataset
for
multimodal
multilingual
machine
learning
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
21
page
2443
2449
new
york
ny
usa
association
for
computing
machinery
tejas
srinivasan
and
yonatan
bisk
2022
worst
of
both
worlds
biases
compound
in
pre-trained
vision-andlanguage
models
in
proceedings
of
the
4th
workshop
on
gender
bias
in
natural
language
processing
gebnlp
pages
77
85
seattle
washington
association
for
computational
linguistics
samuil
stoychev
and
hatice
gunes
2022
the
effect
of
model
compression
on
fairness
in
facial
expression
recognition
in
international
conference
on
pattern
recognition
pages
121
138
springer
tony
sun
andrew
gaut
shirlyn
tang
yuxin
huang
mai
elsherief
jieyu
zhao
diba
mirza
elizabeth
belding
kai-wei
chang
and
william
yang
wang
2019
mitigating
gender
bias
in
natural
language
processing
literature
review
in
proceedings
of
the
57th
annual
meeting
of
the
association
for
computational
linguistics
pages
1630
1640
florence
italy
association
for
computational
linguistics
janet
swim
lauri
hyers
laurie
cohen
and
melissa
ferguson
2001
everyday
sexism
evidence
for
its
incidence
nature
and
psychological
impact
from
three
daily
diary
studies
journal
of
social
issues
57
31
53
zeerak
talat
aur√©lie
n√©v√©ol
stella
biderman
miruna
clinciu
manan
dey
shayne
longpre
sasha
luccioni
maraim
masoud
margaret
mitchell
dragomir
radev
shanya
sharma
arjun
subramonian
jaesung
tae
samson
tan
deepak
tunuguntla
and
oskar
van
der
wal
2022
you
reap
what
you
sow
on
the
challenges
of
bias
evaluation
under
multilingual
settings
in
proceedings
of
bigscience
episode
workshop
on
challenges
perspectives
in
creating
large
language
models
pages
26
41
virtual
dublin
association
for
computational
linguistics
alex
tamkin
miles
brundage
jack
clark
and
deep
ganguli
2021
understanding
the
capabilities
limitations
and
societal
impact
of
large
language
models
arxiv
preprint
arxiv
2102.02503
yi
chern
tan
and
elisa
celis
2019
assessing
social
and
intersectional
biases
in
contextualized
word
representations
advances
in
neural
information
processing
systems
32
ruixiang
tang
mengnan
du
yuening
li
zirui
liu
na
zou
and
xia
hu
2021
mitigating
gender
bias
in
captioning
systems
in
proceedings
of
the
web
conference
2021
www
21
page
633
645
new
york
ny
usa
association
for
computing
machinery
louis
tay
sang
eun
woo
louis
hickman
brandon
booth
and
sidney
mello
2022
conceptual
framework
for
investigating
and
mitigating
machinelearning
measurement
bias
mlmb
in
psychological
assessment
advances
in
methods
and
practices
in
psychological
science
25152459211061337
on
empirical
methods
in
natural
language
processing
pages
3003
3008
brussels
belgium
association
for
computational
linguistics
ashish
vaswani
noam
shazeer
niki
parmar
jakob
uszkoreit
llion
jones
aidan
gomez
≈Çukasz
kaiser
and
illia
polosukhin
2017
attention
is
all
you
need
advances
in
neural
information
processing
systems
30
christopher
teo
milad
abdollahzadeh
and
ngaiman
man
cheung
2024
on
measuring
fairness
in
generative
models
advances
in
neural
information
processing
systems
36
jesse
vig
sebastian
gehrmann
yonatan
belinkov
sharon
qian
daniel
nevo
yaron
singer
and
stuart
shieber
2020
investigating
gender
bias
in
language
models
using
causal
mediation
analysis
advances
in
neural
information
processing
systems
33
12388
12401
timm
teubner
christoph
flath
christof
weinhardt
wil
van
der
aalst
and
oliver
hinz
2023
welcome
to
the
era
of
chatgpt
et
al
the
prospects
of
large
language
models
business
information
systems
engineering
65
95
101
thiemo
wambsganss
vinitra
swamy
roman
rietsche
and
tanja
k√§ser
2022
bias
at
second
glance
deep
dive
into
bias
for
german
educational
peer-review
data
modeling
arxiv
preprint
arxiv
2209.10335
ashish
thapliyal
jordi
pont
tuset
xi
chen
and
radu
soricut
2022
crossmodal-3600
massively
multilingual
multimodal
evaluation
dataset
in
proceedings
of
the
2022
conference
on
empirical
methods
in
natural
language
processing
pages
715
729
abu
dhabi
united
arab
emirates
association
for
computational
linguistics
arun
james
thirunavukarasu
darren
shu
jeng
ting
kabilan
elangovan
laura
gutierrez
ting
fang
tan
and
daniel
shu
wei
ting
2023
large
language
models
in
medicine
nature
medicine
29
1930
1940
bart
thomee
david
shamma
gerald
friedland
benjamin
elizalde
karl
ni
douglas
poland
damian
borth
and
li-jia
li
2016
yfcc100m
the
new
data
in
multimedia
research
communications
of
the
acm
59
64
73
romal
thoppilan
daniel
de
freitas
jamie
hall
noam
shazeer
apoorv
kulshreshtha
heng-tze
cheng
alicia
jin
taylor
bos
leslie
baker
yu
du
et
al
2022
lamda
language
models
for
dialog
applications
arxiv
preprint
arxiv
2201.08239
hugo
touvron
louis
martin
kevin
stone
peter
albert
amjad
almahairi
yasmine
babaei
nikolay
bashlykov
soumya
batra
prajjwal
bhargava
shruti
bhosale
et
al
2023
llama
open
foundation
and
fine-tuned
chat
models
arxiv
preprint
arxiv
2307.09288
oskar
van
der
wal
dominik
bachmann
alina
leidinger
leendert
van
maanen
willem
zuidema
and
katrin
schulz
2024
undesirable
biases
in
nlp
addressing
challenges
of
measurement
journal
of
artificial
intelligence
research
79
40
eva
vanmassenhove
christian
hardmeier
and
andy
way
2018
getting
gender
right
in
neural
machine
translation
in
proceedings
of
the
2018
conference
jialu
wang
yang
liu
and
xin
wang
2021
are
genderneutral
queries
really
gender-neutral
mitigating
gender
bias
in
image
search
in
proceedings
of
the
2021
conference
on
empirical
methods
in
natural
language
processing
pages
1995
2008
online
and
punta
cana
dominican
republic
association
for
computational
linguistics
jialu
wang
yang
liu
and
xin
wang
2022a
assessing
multilingual
fairness
in
pre-trained
multimodal
representations
in
findings
of
the
association
for
computational
linguistics
acl
2022
pages
2681
2695
dublin
ireland
association
for
computational
linguistics
peiyi
wang
lei
li
liang
chen
zefan
cai
dawei
zhu
binghuai
lin
yunbo
cao
qi
liu
tianyu
liu
and
zhifang
sui
2023a
large
language
models
are
not
fair
evaluators
arxiv
preprint
arxiv
2305.17926
song
wang
peng
wang
tong
zhou
yushun
dong
zhen
tan
and
jundong
li
2024
ceb
compositional
evaluation
benchmark
for
fairness
in
large
language
models
arxiv
preprint
arxiv
2407.02408
wenhui
wang
hangbo
bao
li
dong
johan
bjorck
zhiliang
peng
qiang
liu
kriti
aggarwal
owais
khan
mohammed
saksham
singhal
subhojit
som
et
al
2023b
image
as
foreign
language
beit
pretraining
for
vision
and
vision-language
tasks
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
19175
19186
xingbo
wang
jianben
he
zhihua
jin
muqiao
yang
yong
wang
and
huamin
qu
2022b
m2lens
visualizing
and
explaining
multimodal
models
for
sentiment
analysis
ieee
transactions
on
visualization
and
computer
graphics
28
802
812
zeyu
wang
klint
qinami
ioannis
christos
karakozis
kyle
genova
prem
nair
kenji
hata
and
olga
russakovsky
2020
towards
fairness
in
visual
recognition
effective
strategies
for
bias
mitigation
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
8919
8928
robert
wolfe
yiwei
yang
bill
howe
and
aylin
caliskan
2023
contrastive
language-vision
ai
models
pretrained
on
web-scraped
multimodal
data
exhibit
sexual
objectification
bias
in
proceedings
of
the
2023
acm
conference
on
fairness
accountability
and
transparency
pages
1174
1185
zhibo
wang
xiaowei
dong
henry
xue
zhifei
zhang
weifeng
chiu
tao
wei
and
kui
ren
2022c
fairness-aware
adversarial
perturbation
towards
bias
mitigation
for
deployed
deep
models
in
proceedings
of
the
ieee
cvf
conference
on
computer
vision
and
pattern
recognition
pages
10379
10388
austin
wright
omar
shaikh
haekyu
park
will
epperson
muhammed
ahmed
stephane
pinel
diyi
yang
and
duen
horng
chau
2020
recast
interactive
auditing
of
automatic
toxicity
detection
models
in
proceedings
of
the
eighth
international
workshop
of
chinese
chi
pages
80
82
kellie
webster
xuezhi
wang
ian
tenney
alex
beutel
emily
pitler
ellie
pavlick
jilin
chen
ed
chi
and
slav
petrov
2020
measuring
and
reducing
gendered
correlations
in
pre-trained
models
arxiv
preprint
arxiv
2010.06032
shicheng
xu
danyang
hou
liang
pang
jingcheng
deng
jun
xu
huawei
shen
and
xueqi
cheng
2024
invisible
relevance
bias
text-image
retrieval
models
prefer
ai-generated
images
in
proceedings
of
the
47th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
sigir
24
page
208
217
new
york
ny
usa
association
for
computing
machinery
laura
weidinger
john
mellor
maribeth
rauh
conor
griffin
jonathan
uesato
po-sen
huang
myra
cheng
mia
glaese
borja
balle
atoosa
kasirzadeh
et
al
2021
ethical
and
social
risks
of
harm
from
language
models
arxiv
preprint
arxiv
2112.04359
laura
weidinger
jonathan
uesato
maribeth
rauh
conor
griffin
po-sen
huang
john
mellor
amelia
glaese
myra
cheng
borja
balle
atoosa
kasirzadeh
et
al
2022
taxonomy
of
risks
posed
by
language
models
in
proceedings
of
the
2022
acm
conference
on
fairness
accountability
and
transparency
pages
214
229
lindsay
weinberg
2022
rethinking
fairness
an
interdisciplinary
survey
of
critiques
of
hegemonic
ml
fairness
approaches
journal
of
artificial
intelligence
research
74
75
109
johannes
welbl
amelia
glaese
jonathan
uesato
sumanth
dathathri
john
mellor
lisa
anne
hendricks
kirsty
anderson
pushmeet
kohli
ben
coppin
and
po-sen
huang
2021
challenges
in
detoxifying
language
models
in
findings
of
the
association
for
computational
linguistics
emnlp
2021
pages
2447
2469
punta
cana
dominican
republic
association
for
computational
linguistics
sarah
myers
west
meredith
whittaker
and
kate
crawford
2019
discriminating
systems
ai
now
pages
33
michael
wick
jean-baptiste
tristan
et
al
2019
unlocking
fairness
trade-off
revisited
advances
in
neural
information
processing
systems
32
robert
wolfe
and
aylin
caliskan
2022a
american
white
in
multimodal
language-and-image
ai
in
proceedings
of
the
2022
aaai
acm
conference
on
ai
ethics
and
society
pages
800
812
robert
wolfe
and
aylin
caliskan
2022b
markedness
in
visual
semantic
ai
in
proceedings
of
the
2022
acm
conference
on
fairness
accountability
and
transparency
pages
1269
1279
tian
xu
jennifer
white
sinan
kalkan
and
hatice
gunes
2020
investigating
bias
and
fairness
in
facial
expression
recognition
in
computer
vision
eccv
2020
workshops
glasgow
uk
august
23
28
2020
proceedings
part
vi
16
pages
506
523
springer
jintang
xue
yun-cheng
wang
chengwei
wei
xiaofeng
liu
jonghye
woo
and
c-c
jay
kuo
2023
bias
and
fairness
in
chatbots
an
overview
arxiv
preprint
arxiv
2309.08836
shen
yan
di
huang
and
mohammad
soleymani
2020
mitigating
biases
in
multimodal
personality
assessment
in
proceedings
of
the
2020
international
conference
on
multimodal
interaction
pages
361
369
aimin
yang
qifeng
bai
jigang
wang
nankai
lin
xiaotian
lin
guanqiu
qin
and
junheng
he
2022
fine-grained
social
bias
measurement
framework
for
open-domain
dialogue
systems
in
ccf
international
conference
on
natural
language
processing
and
chinese
computing
pages
240
251
springer
barry
menglong
yao
aditya
shah
lichao
sun
jin-hee
cho
and
lifu
huang
2023
end-to-end
multimodal
fact-checking
and
explanation
generation
challenging
dataset
and
models
in
proceedings
of
the
46th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
pages
2733
2743
kyra
yee
uthaipon
tantipongpipat
and
shubhanshu
mishra
2021
image
cropping
on
twitter
fairness
metrics
their
limitations
and
the
importance
of
representation
design
and
agency
proceedings
of
the
acm
on
human-computer
interaction
cscw2
24
kang
min
yoo
dongju
park
jaewook
kang
sang-woo
lee
and
woomyoung
park
2021
gpt3mix
leveraging
large-scale
language
models
for
text
augmentation
in
findings
of
the
association
for
computational
linguistics
emnlp
2021
pages
2225
2239
punta
cana
dominican
republic
association
for
computational
linguistics
yue
yu
yuchen
zhuang
jieyu
zhang
yu
meng
alexander
ratner
ranjay
krishna
jiaming
shen
and
chao
zhang
2023
large
language
model
as
attributed
training
data
generator
tale
of
diversity
and
bias
in
advances
in
neural
information
processing
systems
volume
36
pages
55734
55784
curran
associates
inc
li
yuan
yunpeng
chen
tao
wang
weihao
yu
yujun
shi
zi-hang
jiang
francis
eh
tay
jiashi
feng
and
shuicheng
yan
2021
tokens-to-token
vit
training
vision
transformers
from
scratch
on
imagenet
in
proceedings
of
the
ieee
cvf
international
conference
on
computer
vision
pages
558
567
muhammad
bilal
zafar
isabel
valera
manuel
gomez
rodriguez
and
krishna
gummadi
2017
fairness
beyond
disparate
treatment
disparate
impact
learning
classification
without
disparate
mistreatment
in
proceedings
of
the
26th
international
conference
on
world
wide
web
pages
1171
1180
abdelrahman
zayed
gon√ßalo
mordido
samira
shabanian
ioana
baldini
and
sarath
chandar
2024
fairness-aware
structured
pruning
in
transformers
in
proceedings
of
the
aaai
conference
on
artificial
intelligence
volume
38
pages
22484
22492
george
zerveas
navid
rekabsaz
daniel
cohen
and
carsten
eickhoff
2022
mitigating
bias
in
search
results
through
contextual
document
reranking
and
neutrality
regularization
in
proceedings
of
the
45th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
pages
2532
2538
brian
hu
zhang
blake
lemoine
and
margaret
mitchell
2018
mitigating
unwanted
biases
with
adversarial
learning
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
pages
335
340
duzhen
zhang
yahan
yu
jiahua
dong
chenxing
li
dan
su
chenhui
chu
and
dong
yu
2024a
mmllms
recent
advances
in
multimodal
large
language
models
in
findings
of
the
association
for
computational
linguistics
acl
2024
pages
12401
12430
bangkok
thailand
and
virtual
meeting
association
for
computational
linguistics
haoran
zhang
natalie
dullerud
karsten
roth
lauren
oakden-rayner
stephen
pfohl
and
marzyeh
ghassemi
2022a
improving
the
fairness
of
chest
x-ray
classifiers
in
proceedings
of
the
conference
on
health
inference
and
learning
volume
174
of
proceedings
of
machine
learning
research
pages
204
233
pmlr
haoran
zhang
amy
lu
mohamed
abdalla
matthew
mcdermott
and
marzyeh
ghassemi
2020
hurtful
words
quantifying
biases
in
clinical
contextual
word
embeddings
in
proceedings
of
the
acm
conference
on
health
inference
and
learning
pages
110
120
kaichen
zhang
bo
li
peiyuan
zhang
fanyi
pu
joshua
adrian
cahyono
kairui
hu
shuai
liu
yuanhan
zhang
jingkang
yang
chunyuan
li
et
al
2024b
lmms-eval
reality
check
on
the
evaluation
of
large
multimodal
models
arxiv
preprint
arxiv
2407.12772
susan
zhang
stephen
roller
naman
goyal
mikel
artetxe
moya
chen
shuohui
chen
christopher
dewan
mona
diab
xian
li
xi
victoria
lin
et
al
2022b
opt
open
pre-trained
transformer
language
models
arxiv
preprint
arxiv
2205.01068
yi
zhang
junyang
wang
and
jitao
sang
2022c
counterfactually
measuring
and
eliminating
social
bias
in
vision-language
pre-training
models
in
proceedings
of
the
30th
acm
international
conference
on
multimedia
pages
4996
5004
jieyu
zhao
tianlu
wang
mark
yatskar
vicente
ordonez
and
kai-wei
chang
2017
men
also
like
shopping
reducing
gender
bias
amplification
using
corpus-level
constraints
in
proceedings
of
the
2017
conference
on
empirical
methods
in
natural
language
processing
pages
2979
2989
copenhagen
denmark
association
for
computational
linguistics
jieyu
zhao
tianlu
wang
mark
yatskar
vicente
ordonez
and
kai-wei
chang
2018a
gender
bias
in
coreference
resolution
evaluation
and
debiasing
methods
in
proceedings
of
the
2018
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
short
papers
pages
15
20
new
orleans
louisiana
association
for
computational
linguistics
jieyu
zhao
yichao
zhou
zeyu
li
wei
wang
and
kaiwei
chang
2018b
learning
gender-neutral
word
embeddings
in
proceedings
of
the
2018
conference
on
empirical
methods
in
natural
language
processing
pages
4847
4853
brussels
belgium
association
for
computational
linguistics
yuyin
zhou
shih-cheng
huang
jason
alan
fries
alaa
youssef
timothy
amrhein
marcello
chang
imon
banerjee
daniel
rubin
lei
xing
nigam
shah
et
al
2021
radfusion
benchmarking
performance
and
fairness
for
multimodal
pulmonary
embolism
detection
from
ct
and
ehr
arxiv
preprint
arxiv
2111.11665
caleb
ziems
william
held
omar
shaikh
jiaao
chen
zhehao
zhang
and
diyi
yang
2024
can
large
language
models
transform
computational
social
science
computational
linguistics
50
237
291
yongshuo
zong
yongxin
yang
and
timothy
hospedales
2022
medfair
benchmarking
fairness
for
medical
imaging
arxiv
preprint
arxiv
2210.01725
appendix
google
scholar
papers
1.1
multimodal4
ieee
wang
et
al
2020
niu
et
al
2021a
joshi
et
al
2021
wang
et
al
2022c
seth
et
al
2023
karkkainen
and
joo
2021
peng
et
al
2022
cho
et
al
2023
wang
et
al
2022b
niu
et
al
2021b
hirota
et
al
2022b
springer
xu
et
al
2020
pe√±a
et
al
2023
stoychev
and
gunes
2022
georgopoulos
et
al
2021
cai
et
al
2024
alwahaby
et
al
2022
kehrenberg
et
al
2020
acm
booth
et
al
2021
yan
et
al
2020
wolfe
and
caliskan
2022a
goyal
et
al
2022b
cheong
et
al
2024
wolfe
and
caliskan
2022b
liang
et
al
2022a
yao
et
al
2023
hirota
et
al
2022a
yee
et
al
2021
rahman
et
al
2020
pessach
and
shmueli
2022
xu
et
al
2024
tang
et
al
2021
arxiv
luo
et
al
2024
zhou
et
al
2021
wang
et
al
2024
birhane
et
al
2021
friedrich
et
al
2023
goyal
et
al
2022a
zhang
et
al
2024b
rana
and
jha
2022
chen
et
al
2024
soares
and
angelov
2019
zong
et
al
2022
pubmed
liang
et
al
2021a
mdpi
ferrara
2023a
nazi
and
peng
2024
acl
wang
et
al
2021
srinivasan
and
bisk
2022
li
et
al
2021
zhang
et
al
2024a
ross
et
al
2021
nature
meng
et
al
2022a
r√∂√∂sli
et
al
2022
fei
et
al
2022
plos
jiang
et
al
2024b
spie
drukker
et
al
2023
de
gruyter
jenks
2024
neurips
gadre
et
al
2024
liang
et
al
2022c
koh
et
al
2024
lee
et
al
2023
elsevier
rahate
et
al
2022
serna
et
al
2022
kumari
and
ekbal
2021
pkp
jaiswal
and
provost
2020
mlrp
kiros
et
al
2014
zhang
et
al
2022a
wiley
ntoutsi
et
al
2020
sage
tay
et
al
2022
1.2
language5
neurips
ma
et
al
2023
vig
et
al
2020
kojima
et
al
2022
solaiman
and
dennison
2021
wick
et
al
2019
tan
and
celis
2019
ouyang
et
al
2022
schaeffer
et
al
2024
meng
et
al
2022b
brown
et
al
2020
yu
et
al
2023
acm
bender
et
al
2021
chang
et
al
2024
dhamala
et
al
2021
weidinger
et
al
2022
mehrabi
et
al
2021
garg
et
al
2019
ganguli
et
al
2022
min
et
al
2023
guo
and
caliskan
2021
zhang
et
al
2020
dodge
et
al
2019
navigli
et
al
2023
kotek
et
al
2023
pkp
pryzant
et
al
2020
mit
press
ziems
et
al
2024
jiang
et
al
2021
schick
et
al
2021
raza
et
al
2024
springer
freiberger
and
buchmann
2024
meyer
et
al
2023
teubner
et
al
2023
hou
et
al
2024
lu
et
al
2020
mlrp
liang
et
al
2021b
biderman
et
al
2023
aher
et
al
2023
lehman
et
al
2023
liang
et
al
2021b
jmlr
chung
et
al
2024
chowdhery
et
al
2023
cambridge
argyle
et
al
2023
arxiv
ferrara
2023b
wang
et
al
2023a
tamkin
et
al
2021
chen
et
al
2021
liang
et
al
2022b
weidinger
et
al
2021
bench
authors
2023
rae
et
al
2021
eloundou
et
al
2023
serapio-garc√≠a
et
al
2023
qi
et
al
2023
smith
et
al
2022b
huang
et
al
2023
liu
et
al
2023a
liu
et
al
2023b
hu
et
al
2021
zhang
et
al
2022b
sinha
et
al
2021
thoppilan
et
al
2022
menick
et
al
2022
doan
et
al
2024
elsevier
kasneci
et
al
2023
liu
et
al
2023c
ray
2023
scholar
google
com
scholar
hl
en
as_sdt
2c5
fairness
scholar
google
com
scholar
hl
en
as_sdt
2c5
and
bias
in
large
multimodal
models
btng
fairness
and
bias
in
large
language
models
btng
acl
welbl
et
al
2021
nadeem
et
al
2021
gehman
et
al
2020
delobelle
et
al
2020
sap
et
al
2020
park
et
al
2018
perez
et
al
2022
blodgett
et
al
2020
khanuja
et
al
2023
agrawal
et
al
2022
sun
et
al
2019
liang
et
al
2020
dinan
et
al
2020
blodgett
et
al
2021
chen
and
mueller
2024
gao
et
al
2020b
kurita
et
al
2019
yoo
et
al
2021
lin
et
al
2022
hutchinson
et
al
2020
ramesh
et
al
2023
smith
et
al
2022a
ruder
et
al
2022
nature
schramowski
et
al
2022
clusmann
et
al
2023
hager
et
al
2024
mesk√≥
and
topol
2023
thirunavukarasu
et
al
2023
plos
mozafari
et
al
2020
wiley
hovy
and
prabhumoye
2021
mdpi
garrido-mu√±oz
et
al
2021
preprints
kumar
et
al
2024
springer
dolci
et
al
2023
delobelle
and
berendt
2022
schr√∂der
et
al
2023
leteno
et
al
2023
yang
et
al
2022
neurips
ma
et
al
2023
nature
haltaufderheide
and
ranisch
2024
mdpi
bevara
et
al
2024
pi√±eiro-mart√≠n
et
al
2023
da
silva
et
al
2021
mlrp
liang
et
al
2021c
now
xue
et
al
2023
wiley
lee
et
al
2024
hao
et
al
2024
acl
kumar
et
al
2023
lauscher
et
al
2021
sheng
et
al
2021
fatemi
et
al
2023
guo
et
al
2022
vanmassenhove
et
al
2018
huang
et
al
2020a
felkner
et
al
2023
talat
et
al
2022
escud√©
font
and
costa-juss√†
2019
aiaf
pubmed
karabacak
and
margetis
2023
academic
pinnacle
desai
et
al
2023
2.1
web
of
science
wos
papers
multimodal6
ieee
seth
et
al
2023
porgali
et
al
2023
sami
et
al
2023
petryk
et
al
2022
acm
edenberg
and
wood
2023
mandal
et
al
2023b
alam
2022
pkp
lui
et
al
2024
2.2
language7
ieee
malic
et
al
2023
li
et
al
2024
morales
et
al
2023
da
et
al
2024
jeong
et
al
2024
elsevier
nazir
et
al
2024
fan
et
al
2020
acm
dacon
and
liu
2021
ovalle
et
al
2023
halevy
et
al
2021
mei
et
al
2023
salinas
et
al
2023
dhamala
et
al
2021
gadiraju
et
al
2023
saxena
et
al
2024
jamil
2024
zerveas
et
al
2022
wright
et
al
2020
giner-miguelez
et
al
2023
raj
et
al
2023
https://www.webofscience.com/wos/woscc/summary/889a9d49d906-408c-91b6-ffefdff1880d-fed7fb8c/relevance/1
www.webofscience.com/wos/woscc/summary/79cd811fbeb4-40f2-844f-e0ca9edf1fe2-fed7f83e/relevance/1
van
der
wal
et
al
2024