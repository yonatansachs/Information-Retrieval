irwin
joan
jacobs
center
communication
information
technologies
efficient
search
engine
measurements
ziv
bar-yossef
maxim
gurevich
ccit
report
743
august
2009
electronics
computers
communications
department
electrical
engineering
technion
israel
institute
technology
haifa
32000
israel
ccit
report
743
august
2009
efficient
search
engine
measurements
ziv
bar-yossef
maxim
gurevich
august
30
2009
abstract
address
problem
externally
measuring
aggregate
functions
documents
indexed
search
engines
like
corpus
size
index
freshness
density
duplicates
corpus
recently
proposed
estimators
quantities
biased
due
inaccurate
approximation
called
document
degrees
addition
estimators
quite
costly
due
reliance
rejection
sampling
present
new
estimators
able
overcome
bias
introduced
approximate
degrees
estimators
based
careful
implementation
approximate
importance
sampling
procedure
comprehensive
theoretical
empirical
analysis
estimators
demonstrates
essentially
bias
even
situations
document
degrees
poorly
approximated
avoiding
costly
rejection
sampling
approach
new
importance
sampling
estimators
significantly
efficient
estimators
proposed
furthermore
building
idea
discuss
rao-blackwellization
generic
method
reducing
variance
search
engine
estimators
show
rao-blackwellizing
estimators
results
performance
improvements
compromising
accuracy
introduction
background
paper
focus
external
methods
measuring
aggregate
functions
search
engine
corpora
methods
interact
public
interfaces
search
engines
rely
privileged
access
internal
search
engine
data
specific
knowledge
search
engines
work
one
application
techniques
external
evaluation
global
quality
metrics
search
engines
evaluation
provides
means
objective
benchmarking
search
engines
benchmarks
can
used
search
engine
users
clients
gauge
quality
service
get
preliminary
version
paper
appeared
proceedings
16th
international
world-wide
web
conference
www2007
supported
israel
science
foundation
google
haifa
engineering
center
israel
dept
electrical
engineering
technion
haifa
32000
israel
email
zivby@ee.technion.ac.il
dept
electrical
engineering
technion
haifa
32000
israel
email
gmax@tx.technion.ac.il
supported
eshkol
fellowship
israeli
ministry
science
researchers
compare
search
engines
even
search
engines
may
benefit
external
benchmarks
can
help
reveal
strengths
weaknesses
relative
competitors
assuming
search
engines
index
large
fraction
useful
web
pages
one
can
use
techniques
study
also
properties
web
study
concentrates
measurement
global
metrics
search
engines
like
corpus
size
index
freshness
density
spam
duplicate
pages
corpus
metrics
relevance
neutral
therefore
human
judgment
required
computing
still
external
access
search
engine
data
highly
restricted
designing
automatic
methods
measuring
metrics
challenging
objective
design
measurement
algorithms
accurate
efficient
efficiency
particularly
important
two
reasons
first
efficient
algorithms
can
executed
even
parties
whose
resources
limited
like
researchers
second
search
engines
highly
dynamic
efficient
algorithms
necessary
capturing
instantaneous
snapshots
search
engines
one
might
wonder
study
measurement
global
index
metrics
rather
focusing
important
pages
actually
served
users
search
results
see
relevant
discussion
25
latter
indeed
worthy
goal
requires
access
search
engine
query
log
publicly
available
argue
global
index
metrics
useful
provide
insight
search
engine
crawling
indexing
architecture
typically
less
sensitive
constantly-changing
user
query
stream
furthermore
usage-based
metrics
good
evaluating
quality
search
engine
relative
popular
queries
global
metrics
better
demonstrating
well
search
engine
copes
long-tail
queries
predicted
priori
problem
statement
let
denote
corpus
documents
indexed
search
engine
measured
focus
measurement
quantities
can
expressed
either
sums
averages
given
target
function
sum
average
sum
avg
sum
fact
address
sums
averages
arbitrary
measures
just
uniform
one
see
details
section
simplicity
exposition
introduction
focus
uniform
measure
almost
search
engine
metrics
aware
can
expressed
sum
average
function
example
corpus
size
sum
constant
function
density
spam
pages
corpus
average
spam
indicator
function
spam
page
otherwise
number
unique
documents
corpus
sum
inverse
duplicate-count
function
dx
dx
number
duplicates
including
many
metrics
like
search
engine
overlap
sizes
subsets
corpus
index
freshness
can
expressed
sums
averages
well
allow
also
sums
averages
vector-valued
functions
rm
can
used
compute
histograms
indexed
pages
language
country
domain
topic
search
engine
estimator
sum
resp
avg
probabilistic
procedure
submits
queries
search
engine
fetches
pages
web
computes
target
function
documents
choice
eventually
outputs
estimate
sum
resp
avg
quality
estimator
measured
terms
bias
variance
efficiency
estimator
measured
terms
number
queries
submits
search
engine
number
web
pages
fetches
number
documents
computes
target
function
state
art
brute-force
computation
functions
search
engine
corpus
infeasible
due
huge
size
corpus
highly
restricted
access
every
user
limited
thousand
queries
per
day
top
matches
returned
typically
000
may
vary
query
query
depending
search
engine
architecture
current
load
etc
aware
technique
accessing
results
query
alternative
brute-force
computation
sampling
one
samples
random
uniform
pages
corpus
uses
estimate
desired
quantity
samples
unbiased
small
number
sufficient
obtain
accurate
estimations
main
challenge
design
algorithms
can
efficiently
generate
uniform
unbiased
samples
corpus
using
queries
public
interface
bharat
broder
first
propose
algorithm
samples
produced
algorithm
however
suffered
severe
bias
towards
long
content-rich
documents
previous
paper
able
correct
bias
proposing
technique
simulating
unbiased
sampling
biased
sampling
end
applied
several
stochastic
simulation
methods
like
rejection
sampling
30
metropolis-hastings
algorithm
26
16
stochastic
simulation
however
incurs
significant
overhead
order
generate
unbiased
uniform
sample
numerous
biased
samples
used
translates
elevated
query
fetch
costs
instance
efficient
sampler
needed
000
queries
generate
uniform
sample
attempt
address
lack
efficiency
also
experimented
importance
sampling
estimation
importance
sampling
24
19
enables
estimation
sums
averages
directly
biased
samples
without
first
generating
unbiased
samples
technique
can
significantly
reduce
stochastic
simulation
overhead
nevertheless
estimators
used
stochastic
simulation
twice
select
random
queries
select
random
documents
able
use
importance
sampling
eliminate
latter
two
furthermore
importance
sampling
estimator
still
wasteful
used
single
result
submitted
query
discarded
rest
broder
et
al
recently
made
remarkable
progress
proposing
new
estimator
search
engine
corpus
size
estimator
implicitly
employs
importance
sampling
moreover
estimator
somehow
makes
use
query
results
estimation
thus
less
wasteful
estimators
broder
et
al
claimed
method
can
generalized
estimate
metrics
provided
details
degree
mismatch
problem
prerequisite
applying
importance
sampling
ability
compute
biased
sample
importance
weight
importance
weights
used
balance
contributions
different
biased
samples
final
estimator
estimators
computing
importance
weight
sample
document
translates
calculation
document
degree
given
large
pool
queries
phrase
queries
length
digit
string
queries
degree
document
pool
number
queries
pool
whose
results
belongs
estimators
choose
sample
documents
results
random
queries
drawn
query
pool
samples
biased
towards
high
degree
documents
document
degrees
therefore
constitute
primary
factor
determining
importance
weights
sample
documents
importance
weights
hence
degrees
computed
every
sample
document
degree
computation
extremely
efficient
ideally
done
based
content
alone
without
submitting
queries
search
engine
estimators
extracting
terms
phrases
counting
many
belong
pool
resulting
number
document
predicted
degree
used
approximation
real
degree
practice
predicted
degree
may
quite
different
actual
degree
since
exactly
know
search
engine
parses
documents
extracts
terms
selects
terms
index
document
moreover
document
may
fail
belong
result
sets
queries
matches
ranked
low
beyond
top
results
search
engine
return
high
load
factors
give
rise
degree
mismatch
gap
predicted
degree
actual
degree
degree
mismatch
implies
importance
weights
used
estimators
accurate
can
significantly
affect
quality
produced
estimates
proved
density
overflowing
results
queries
among
queries
document
matches
low
variance
bias
incurred
degree
mismatch
small
however
work
analyze
additional
factors
causing
degree
mismatch
broder
et
al
analyzed
effect
degree
mismatch
quality
estimations
several
heuristic
methods
used
overcome
degree
mismatch
problem
order
reduce
effect
overflowing
queries
pool
queries
unlikely
overflow
chosen
used
pool
phrases
length
used
pool
digit
strings
remove
potentially
overflowing
queries
pool
eliminating
terms
occur
frequently
training
corpus
however
heuristic
can
many
false
positives
false
negatives
depending
choice
frequency
threshold
contributions
paper
show
overcome
degree
mismatch
problem
present
four
search
engine
estimators
remain
nearly
unbiased
efficient
even
presence
highly
mismatching
degrees
starting
point
section
new
importance
sampling
estimator
search
engine
metrics
generalizes
corpus
size
estimator
broder
et
al
previous
estimator
worked
two
steps
first
sampled
queries
query
pool
using
rejection
sampling
sampled
documents
results
queries
using
either
rejection
sampling
importance
sampling
new
estimator
hand
samples
queries
documents
together
single
step
using
importance
sampling
avoiding
costly
rejection
sampling
step
makes
estimator
significantly
efficient
estimators
analyze
section
4.6
effect
degree
mismatch
problem
accuracy
importance
sampling
estimator
analysis
applies
also
previous
estimators
prove
estimator
suffers
significant
bias
depends
far
approximate
degrees
real
degrees
since
dependence
multiplicative
even
slightly
skewed
degrees
may
result
significant
estimation
bias
indeed
empirical
study
section
10.2
reveals
broder
et
al
corpus
size
estimator
suffers
relative
bias
7400
due
effect
degree
mismatch
algorithms
used
compute
document
degrees
deterministic
base
calculations
content
document
whose
degree
computed
algorithms
efficient
yet
show
section
accurate
present
new
algorithm
estimating
document
degrees
algorithm
probabilistic
needs
send
search
engine
small
number
queries
order
produce
estimate
however
output
algorithm
provably
unbiased
estimate
real
document
degree
plugging
estimated
degrees
importance
sampling
estimator
obtain
sumest
provably
unbiased
estimator
sum
metrics
section
average
metrics
tricky
handle
metrics
target
measure
known
normalization
example
target
measure
uniform
distribution
normalization
constant
known
advance
result
bias
importance
sampling
estimator
depends
well
degrees
approximated
also
unknown
normalization
constant
standard
approach
dealing
unknown
normalization
constants
apply
ratio
importance
sampling
cf
23
divides
normal
importance
sampling
estimator
estimate
normalization
constant
present
new
variant
ratio
importance
sampling
call
approximate
ratio
importance
sampling
aris
short
neutralizes
effect
unknown
normalization
constant
approximate
degrees
use
aris
probabilistic
degree
estimator
obtain
avgest
search
engine
estimator
average
metrics
section
estimator
nearly
unbiased
bias
diminishes
number
samples
sumest
avgest
resort
probabilistic
degree
estimator
accurate
also
costly
deterministic
degree
estimators
used
previous
works
section
show
replace
probabilistic
degree
estimators
sumest
avgest
efficient
deterministic
degree
estimator
compromising
estimation
accuracy
significantly
resulting
estimators
call
effsumest
effavgest
considerably
efficient
sumest
avgest
last
contribution
builds
observation
estimator
broder
et
al
implicitly
applies
rao-blackwellization
well-known
statistical
tool
reducing
estimation
variance
technique
makes
estimator
efficient
since
rao-blackwellization
increases
number
degree
computations
efficiently
applied
sumest
avgest
degree
computations
require
submitting
search
engine
queries
thus
apply
rao-blackwellization
effsumest
effavgest
estimators
prove
guaranteed
make
efficient
long
results
queries
sufficiently
variable
emphasize
estimators
applicable
sum
average
metrics
contrast
estimators
efficiently
applicable
average
metrics
estimator
applicable
sum
metrics
experimental
results
evaluated
bias
efficiency
estimators
well
estimators
local
search
engine
built
corpus
2.4
million
documents
estimator
unbiased
relative
documents
covered
query
pool
used
like
estimators
use
query
pools
estimator
may
bias
due
uncovered
documents
see
details
section
3.5
end
used
estimators
estimate
two
different
metrics
corpus
size
density
sports
pages
empirical
study
confirms
analytical
findings
presence
significant
degree
mismatch
estimators
essentially
bias
estimator
broder
et
al
suffers
significant
bias
example
relative
bias
sumest
corpus
size
estimation
0.01
relative
bias
estimator
broder
et
al
60
study
also
showed
new
estimators
1500
times
efficient
rejection
sampling
estimator
finally
study
demonstrated
effectiveness
rao-blackwellization
reducing
query
cost
estimators
80
used
estimators
measure
absolute
sizes
two
major
search
engines
results
study
may
underestimate
true
search
engine
sizes
largely
due
limited
coverage
search
engine
corpora
pool
queries
used
even
showed
estimates
3.5
times
higher
estimates
produced
implementation
broder
et
al
estimator
related
work
apart
several
studies
estimated
global
metrics
search
engine
indices
like
relative
corpus
size
studies
based
analyzing
anecdotal
queries
queries
collected
user
query
logs
20
13
queries
selected
randomly
pool
la
bharat
broder
15
10
using
capture-recapture
techniques
cf
22
studies
infer
measurements
whole
web
due
bias
samples
though
estimates
lack
statistical
guarantees
different
approach
evaluating
search
quality
sampling
pages
whole
web
21
17
18
27
sampling
whole
web
however
difficult
problem
therefore
known
algorithms
suffer
severe
bias
anagnostopoulos
broder
carmel
showed
technique
measuring
parameters
results
single
search
engine
query
rather
entire
corpus
technique
however
assumes
privileged
access
internal
data
structures
search
engine
dasgupta
das
mannila
11
presented
random
walk
algorithm
sampling
records
database
hidden
behind
web
form
search
engine
essentially
example
database
however
work
aimed
structured
database
setting
directly
applicable
sampling
free
text
search
engine
subsequent
work
proposed
technique
sampling
real
user
queries
via
query
suggestion
services
search
engines
query
samples
can
used
measure
metrics
observed
users
observed
diversity
search
results
amount
spam
etc
also
showed
using
query
samples
instead
synthetic
query
pool
one
can
sample
web
pages
proportionally
impressionrank
actual
visibility
search
engine
users
order
avoid
disturbing
flow
paper
proofs
postponed
appendix
notation
ρn
zρ
sumπ
avgπ
πd
documentsg
queriesg
degg
meaning
set
documents
indexed
search
engine
query
space
search
engine
generic
finite
space
generic
target
measure
distribution
induced
measure
normalization
constant
measure
sum
relative
target
measure
average
relative
target
measure
target
measure
generic
queries-documents
graph
query
pool
set
documents
incident
query
set
queries
incident
document
vertex
degree
function
table
notation
used
section
framework
search
engine
measurements
section
introduce
notations
definitions
used
formally
describe
search
engine
estimators
3.1
search
engines
definition
3.1
search
engine
search
engine
tuple
hd
results
ki
document
corpus
indexed
documents
assumed
pre-processed
may
truncated
maximum
size
limit
query
space
supported
search
engine
query
sequence
one
terms
results
mapping
maps
every
query
ordered
sequence
documents
called
results
whenever
user
sends
query
query
search
engine
search
engine
returns
results
user
result
set
size
limit
typically
000
highly
ranked
query
results
returned
results
actual
number
results
may
lower
may
vary
query
query
depending
search
engine
architecture
current
load
etc
stress
results
actual
results
returned
search
engine
many
queries
matches
ones
actually
returned
typical
search
engines
google
bing
yahoo
return
000
highly
ranked
results
example
google
results
britney
spears
consists
747
results
total
number
matches
reported
google
query
77
000
000
3.2
search
engine
estimators
interested
measurement
quantities
can
written
sums
functions
finite
space
sumπ
finite
space
target
function
target
measure
average
function
essentially
sum
target
measure
probability
distribution
say
measure
induces
corresponding
probability
distribution
ρn
zρ
zρ
normalization
constant
say
two
different
measures
normalization
induce
probability
distribution
different
normalization
constants
average
function
reduced
sum
follows
avgπ
sumπn
example
suppose
set
documents
indexed
search
engine
corpus
size
search
engine
sum
function
relative
uniform
target
measure
density
spam
pages
corpus
average
function
iff
spam
page
relative
uniform
target
measure
alternatively
sum
function
relative
uniform
distribution
everything
paper
can
generalized
deal
vector-valued
functions
rm
yet
simplicity
exposition
focus
scalar
functions
definition
3.2
search
engine
estimator
let
πd
target
measure
document
corpus
indexed
search
engine
search
engine
estimator
sumπd
resp
avgπd
randomized
procedure
given
access
four
oracle
procedures
computetargetmeasure
returns
weight
πd
document
relative
target
measure
πd
getresults
returns
results
returned
search
engine
query
getdocument
returns
http
header
content
document
computefunction
returns
value
function
document
invocation
estimator
produces
estimate
sumπd
resp
avgπd
different
invocations
estimator
produce
identically
distributed
independent
outputs
estimation
quality
quality
estimator
measured
terms
bias
variance
bias
bias
sumπd
variance
var
called
unbiased
bias
variance
estimator
high
may
high
error
even
low
bias
circumvent
problem
estimators
typically
designed
two
steps
first
design
basic
estimator
low
bias
possibly
high
variance
reduce
variance
create
final
estimator
aggregating
multiple
independent
instances
basic
estimator
simplest
aggregation
method
averaging
n1
ni
mi
m1
mn
independent
instances
basic
estimator
bias
basic
variance
tends
tends
estimator
var
infinity
chebyshev
inequality
ǫ2
e2
independent
instances
sufficient
guarantee
estimate
produced
falls
within
confidence
interval
constant
confidence
estimation
costs
three
expensive
resources
used
search
engine
estimators
queries
submitted
search
engine
web
pages
fetched
calculations
function
queries
web
page
fetches
may
take
substantial
amount
time
require
usage
network
bandwidth
queries
costly
also
search
engines
pose
daily
quotas
number
queries
willing
accept
single
user
depending
function
calculation
may
require
substantial
processing
time
fetching
web
pages
submitting
queries
search
engine
therefore
use
three
measures
efficiency
search
engine
estimators
query
cost
fetch
cost
function
cost
expected
query
cost
estimator
denoted
qcost
expected
number
queries
submits
search
engine
note
order
compare
efficiency
different
estimators
expected
query
cost
variance
estimators
amortized
query
cost
defined
qcost
var
robust
measure
efficiency
chebyshev
inequality
amortized
query
cost
determines
number
queries
required
obtain
estimate
within
given
confidence
interval
expected
amortized
fetch
function
costs
defined
similarly
3.3
queries-documents
graph
queries-documents
graph
bipartite
graph
whose
left
side
query
pool
whose
right
side
document
corpus
estimators
consider
paper
sample
documents
working
properly
defined
queries-documents
graph
sample
queries
query
pool
sample
document
neighbors
queries
graph
section
define
three
types
queries-documents
graphs
used
paper
somewhat
efficient
aggregation
techniques
like
median
averages
cf
14
exist
simplicity
exposition
will
focus
mainly
averaging
paper
daily
quotas
apply
developer
apis
provided
search
engines
queries
sent
standard
web
interfaces
also
rate
limited
denote
documentsg
documents
incident
degree
query
isp
degg
documentsg
degree
set
queries
defined
degg
degg
similarly
queriesg
denote
queries
incident
degree
inpg
degg
queriesg
degree
set
documents
defined
degg
degg
3.4
incidence
computation
estimators
use
paper
require
local
accessibility
queries-documents
graph
mean
estimator
needs
efficient
implementations
following
procedures
compute
incidences
graph
getdocumentsg
given
query
returns
documents
incident
documentsg
getdegreeg
given
document
returns
number
queries
incident
degg
note
algorithms
need
compute
queriesg
rather
degg
show
later
allows
efficient
implementation
since
can
estimate
degg
lower
cost
computing
queriesg
describe
implementation
procedures
section
3.5
corpus
coverage
documents
may
isolated
edges
incident
queriesdocuments
graph
means
estimators
rely
base
estimates
non-isolated
documents
dg
degg
clearly
like
dg
cover
much
possible
denote
coverage
relative
target
measure
πd
πd
distribution
induced
πd
instance
πd
uniform
distribution
coverage
dg
reasons
like
coverage
close
possible
note
even
coverage
lower
dg
sufficiently
representative
estimators
use
produce
accurate
estimates
parameters
whole
corpus
simplify
presentation
assume
now
dg
reader
keep
mind
estimators
present
may
incur
additional
bias
case
coverage
low
3.6
assumptions
like
pool-based
estimators
estimator
assumes
knowledge
explicit
query
pool
example
experiments
used
pool
2.37
billion
numeric
english
terms
two-term
conjunctions
pool
can
constructed
preprocessing
step
10
crawling
representative
corpus
web
documents
extracting
terms
occur
therein
used
wikipedia
odp
12
directory
purpose
can
run
estimator
pool
yet
choice
pool
may
affect
bias
efficiency
estimator
see
section
6.2
explanation
properties
pool
affect
quality
estimators
estimators
indirect
highly
restricted
access
search
engines
indices
guarantee
correctness
algorithms
settings
need
make
following
assumptions
dynamic
corpora
algorithms
assume
search
engine
corpora
change
estimation
process
obviously
assumption
hold
practice
search
engine
indices
constantly
updated
experiments
noticed
slight
differences
results
returned
queries
different
steps
experiment
note
duration
experiments
determined
limited
resources
used
resources
shortened
duration
drastically
diminished
effect
corpus
changes
versioned
indices
search
engines
may
maintain
multiple
non-identical
versions
index
simultaneously
serve
users
different
versions
index
based
user
profile
based
load-balancing
criteria
algorithms
assume
queries
served
single
coherent
index
queries
indeed
served
version
index
results
produced
algorithms
reflect
properties
specific
index
version
used
anomalies
may
occur
samplers
work
multiple
index
versions
simultaneously
assuming
differences
among
versions
significant
believe
case
search
engines
search
engine
importance
sampling
estimation
notation
supp
pg
pse
ais
meaning
support
measure
importance
sampling
estimator
importance
weight
function
extension
target
measure
πd
queries
least
one
incident
document
trial
distribution
random
query-document
pair
distributed
according
pool
size
estimator
approximate
importance
sampling
estimator
table
notation
used
section
section
present
basic
importance
sampling
search
engine
estimator
will
used
basis
accurate
efficient
estimators
presented
subsequent
sections
11
4.1
naive
estimator
naive
monte
carlo
estimator
cf
23
avgπ
works
follows
generate
random
sample
distribution
induced
output
easy
check
estimator
unbiased
setting
however
simple
estimator
inefficient
following
reasons
sampling
distribution
may
hard
costly
uniform
distribution
random
variable
may
high
variance
moreover
naive
estimator
suitable
computing
sums
although
sum
can
computed
zπ
avgπ
zπ
computed
using
naive
estimator
requiring
computing
means
may
hard
costly
corpus
size
estimation
zπ
corpus
size
exactly
quantity
need
estimate
will
use
importance
sampling
obtain
efficient
estimator
also
able
estimate
sums
4.2
importance
sampling
estimator
basic
idea
importance
sampling
24
19
23
following
instead
generating
sample
target
distribution
estimator
generates
sample
different
trial
distribution
can
distribution
long
supp
supp
supp
support
supp
defined
similarly
particular
can
choose
distribution
easy
sample
importance
sampling
estimator
defined
follows
correction
term
called
importance
weight
theorem
4.1
unbiased
estimator
sumπ
proof
supp
supp
sumπ
supp
fact
compute
importance
weights
exactly
enough
unbiased
estimator
sumπ
supp
supp
12
variance
importance
sampling
estimator
depends
variance
product
var
var
observe
variance
minimized
anti-correlated
typically
trial
distribution
selected
correlated
leading
low
estimation
variance
case
however
little
freedom
choosing
trial
distribution
depends
available
data
provided
search
engine
public
interface
thus
use
predefined
trial
distribution
measure
resulting
estimation
variance
empirically
implementation
importance
sampling
estimator
requires
ability
sample
efficiently
trial
distribution
ability
compute
importance
weight
estimator
function
value
given
element
need
know
normalization
constant
zπ
able
sample
basic
importance
sampling
estimator
suitable
estimating
sums
later
section
extend
estimating
averages
4.3
sample
space
sample
space
importance
sampling
estimator
proposed
previous
paper
corpus
documents
trial
distribution
document
degree
distribution
documents
sampled
proportionally
degrees
queries-documents
graph
induced
order
sample
documents
distribution
sample
queries
pool
proportionally
degrees
referred
cardinalities
previous
paper
sample
random
documents
result
sets
queries
degrees
queries
known
advance
sampling
queries
required
application
rejection
sampling
step
incurred
significant
overhead
paper
propose
different
sample
space
rather
sampling
queries
documents
two
separate
steps
sample
together
let
queries-documents
graph
induced
pool
define
later
section
sample
space
use
sample
query-document
pair
extend
target
measure
πd
target
measure
function
function
extension
done
way
sumπ
equals
sumπd
thus
reduce
problem
estimating
sumπd
problem
estimating
sumπ
latter
can
apply
importance
sampling
directly
two-dimensional
sample
space
without
resort
rejection
sampling
intermediate
step
extend
function
function
follows
similarly
extend
πd
measure
follows
documentsg
πd
degg
indicator
function
condition
condition
true
otherwise
marginal
weight
document
relative
measure
equals
weight
relative
measure
πd
weight
splits
evenly
among
edges
incident
connection
πd
given
following
proposition
13
proposition
4.2
πd
marginal
measure
furthermore
normalization
constants
πd
proof
topprove
πd
marginal
measure
must
show
every
πd
documentsg
πd
degg
πd
degg
queriesg
πd
degg
degg
πd
now
identity
two
normalization
constants
easily
follows
xx
zπ
πd
zπd
follows
proposition
distribution
πd
distribution
proposition
4.3
sumπ
sumπd
proof
sumπ
4.4
πd
sumπd
trial
distribution
next
describe
trial
distribution
sampling
edges
let
pg
denote
collection
queries
least
one
incident
document
pg
documentsg
trial
distribution
selects
edge
follows
pick
query
pg
uniformly
random
pick
document
documentsg
uniformly
random
documentsg
pg
degg
sampling
can
done
easily
see
function
repeatedly
select
queries
uniformly
random
call
getdocumentsg
get
incident
documents
submitting
queries
search
engine
detailed
implementation
section
stop
reaching
query
least
one
incident
document
select
document
set
incident
documents
query
uniformly
random
14
function
samplepair
true
uniformly
chosen
query
documentsg
getdocumentsg
documentsg
uniformly
chosen
document
documentsg
return
4.5
importance
sampling
search
engine
estimator
importance
weights
corresponding
target
measure
trial
distribution
following
πd
pg
degg
degg
thus
importance
sampling
estimator
sumπd
πd
pg
degg
degg
sample
trial
distribution
term
yet
know
compute
pg
exactly
computing
infeasible
due
large
size
resort
probabilistic
estimation
pg
pool
size
estimator
pse
see
function
sample
query
uniformly
random
pg
probability
least
one
incident
document
therefore
can
estimate
follows
repeatedly
sample
queries
uniformly
random
compute
degree
fraction
submitted
queries
non-zero
degree
unbiased
estimator
estimation
error
can
reduced
increasing
number
iterations
performed
function
estimatepoolsize
uniformly
chosen
query
degg
getdocumentsg
degg
return
function
implementation
search
engine
estimator
4.6
importance
sampling
approximate
degrees
unfortunately
due
degree
mismatch
problem
see
detailed
description
section
unable
accurately
compute
document
degrees
consequently
importance
weight
function
15
function
pse
estimatepoolsize
samplepair
πd
computetargetmeasure
degg
getdocumentsg
degg
getdegreeg
πd
pse
degg
degg
return
computefunction
now
analyze
effect
approximate
importance
weights
importance
sampling
estimator
approximate
importance
sampling
estimator
employs
approximate
importance
weight
function
rather
exact
one
prove
estimation
generated
approximate
importance
sampling
close
true
value
long
importance
weight
function
approximate
importance
weight
function
similar
extend
result
previous
work
considered
special
case
approximate
trial
weights
best
knowledge
previous
study
addressed
scenario
independent
interest
let
approximate
weight
function
define
approximate
importance
sampling
estimator
ais
distributed
according
trial
distribution
suppose
supp
supp
following
lemma
analyzes
bias
approximate
importance
sampling
estimator
theorem
4.4
ais
sumπ
zπ
cov
zπ
normalization
constant
proof
see
appendix
follows
theorem
two
sources
bias
estimator
multiplicative
bias
depending
expectation
relative
additive
bias
depending
correlation
normalization
constant
zπ
note
multiplicative
factor
even
small
may
significant
effect
estimator
bias
thus
must
eliminated
additive
bias
typically
less
significant
many
practical
situations
uncorrelated
constant
function
case
corpus
size
estimation
estimators
present
sections
use
two
alternative
strategies
eliminating
multiplicative
bias
approximate
importance
sampling
estimator
former
employs
ais
probabilistic
approximate
importance
weights
unbiased
estimates
corresponding
real
importance
weights
consequently
multiplicative
bias
latter
estimates
multiplicative
bias
incurred
ais
divides
ais
estimate
effectively
neutralizing
multiplicative
bias
16
incidence
degree
computation
notation
vdensity
meaning
search
queries
graph
predicted
queries
graph
valid
queries
graph
validity
density
document
table
notation
used
section
section
analyze
causes
degree
mismatch
problem
demonstrate
computation
incidences
degrees
search
queries
graph
tricky
simple
workarounds
like
ones
used
previous
works
give
rise
degree
mismatch
propose
new
probabilistic
algorithm
estimating
document
degrees
provably
unbiased
5.1
search
queries
graph
search
queries
graph
es
query
connected
document
search
engine
returns
result
query
results
thus
efficient
implementation
procedure
getdocumentss
see
function
trivial
just
submit
search
engine
output
results
returned
cost
implementation
single
search
engine
query
function
getdocumentss
submit
search
engine
documents
results
returned
search
engine
return
documents
remark
overflowing
query
query
matching
documents
search
engine
actually
returns
query
every
query
typically
000
matches
overflowing
search
engines
return
results
query
queries
fewer
matches
can
overflowing
technical
reasons
search
engines
sometimes
return
matches
query
previous
papers
estimators
use
overflowing
queries
query
pool
found
restriction
unnecessary
therefore
search
queries
graph
consists
overflowing
non-overflowing
queries
overflowing
query
connected
documents
search
engine
actually
returns
matching
documents
return
unfortunately
know
efficiently
implement
second
procedure
getdegrees
can
implemented
submitting
query
search
engine
returning
result
implementation
impractical
due
large
number
queries
typical
query
pool
17
5.2
predicted
queries
graph
now
describe
different
queries-documents
graph
admitting
efficient
method
degree
computation
let
ep
predicted
queries
graph
neighbors
particular
document
defined
based
content
alone
let
predicted
queries
queriesp
set
queries
occur
content
example
pool
single
term
queries
queriesp
set
distinct
terms
occur
text
also
occur
pool
two-term
conjunctions
queriesp
set
pairs
distinct
terms
occur
text
also
occur
function
show
compute
queriesp
using
single
page
fetch
without
submitting
queries
search
engine
function
getqueriesp
download
queriesp
queries
content
matches
return
queriesp
degp
degree
predicted
queries
graph
queriesp
can
efficiently
computed
using
single
document
fetch
see
function
function
getdegreep
queriesp
getqueriesp
return
queriesp
unfortunately
predicted
queries
graph
implementation
getdocumentsp
becomes
difficult
order
find
documents
incident
query
need
go
documents
find
ones
occurs
amounts
full-fledged
indexing
whole
corpus
clearly
infeasible
setting
consider
paper
note
use
search
engine
implement
getdocumentsp
least
directly
returns
documentss
may
different
documentsp
see
details
5.3
combining
search
queries
predicted
queries
graphs
just
saw
efficiently
compute
query
incidences
search
queries
graph
document
incidences
predicted
queries
graph
can
somehow
combine
two
obtain
efficient
implementations
straightforward
solution
used
previous
works
simply
call
getdocumentss
whenever
need
compute
query
incidences
call
getdegreep
whenever
need
compute
document
degrees
assumption
degp
good
approximation
degs
show
assumption
frequently
false
may
lead
call
degree
mismatch
problem
mismatch
results
significant
estimation
error
demonstrate
section
10
assume
efficient
data
structure
checking
membership
18
distinguish
two
types
error
approximating
queriess
queriesp
false
negatives
queriesp
may
miss
queries
belong
queriess
false
positives
queriesp
may
contain
queries
belong
queriess
following
list
several
factors
cause
false
negatives
indexing
depth
assumed
first
constant
set
20
000
experiments
terms
document
indexed
search
engine
indexing
depth
greater
terms
phrases
occur
beyond
d-th
position
will
included
queriesp
although
search
engine
may
return
one
results
parsing
tokenization
different
search
engines
may
slightly
different
algorithms
parsing
tokenizing
documents
example
two
words
separated
comma
may
may
indexed
phrase
search
engine
parser
determines
sequence
characters
term
sequence
terms
phrase
parser
corresponding
term
phrase
will
included
queriesp
although
search
engine
may
return
one
results
indexing
terms
appearing
document
content
search
engines
index
documents
terms
occur
text
anchor
text
terms
synonyms
procedure
computing
queriesp
obviously
will
find
document
match
terms
unless
appear
document
content
false
positives
caused
following
factors
overflowing
queries
mentioned
search
engines
always
return
matches
queries
receive
low
rank
may
returned
overflowing
queries
matches
duplicates
near-duplicates
search
engine
filters
duplicate
near-duplicate
documents
query
matches
may
return
result
one
documents
similar
returned
result
host
collapsing
search
engine
collapses
documents
belonging
host
query
matches
may
return
result
another
document
host
returned
result
indexing
depth
search
engine
indexing
depth
smaller
search
engine
may
return
result
terms
occur
beyond
indexing
depth
parsing
tokenization
parser
determines
sequence
characters
term
sequence
terms
phrase
search
engine
parser
search
engine
may
return
result
corresponding
term
phrase
5.4
valid
queries
graph
can
bridge
gap
queriess
queriesp
define
valid
queriesdocuments
graph
ev
ev
es
ep
edge
exists
19
valid
queries
graph
exists
search
queries
predicted
queries
graphs
set
queries
incident
document
queriesv
queriess
queriesp
set
documents
incident
query
documentsv
documentss
documentsp
algorithms
use
valid
queries
graph
rather
search
queries
predicted
queries
graphs
order
work
valid
queries
graph
need
clarify
three
points
can
efficiently
implement
procedure
getdocumentsv
can
efficiently
implement
getdegreev
coverage
start
first
question
note
documentsv
can
rewritten
follows
documentsv
documentss
queriesp
implementing
getdocumentsv
straightforward
see
function
since
know
efficiently
compute
documentss
queriesp
procedure
first
gets
incident
documents
search
queries
graph
documentss
see
function
fetches
documents
checks
document
whether
getqueriesp
returns
set
documents
documentsv
cost
implementation
single
search
engine
query
documentss
document
fetches
function
getdocumentsv
documentsv
documentss
getdocumentss
documentss
download
queriesp
getqueriesp
queriesp
documentsv
documentsv
return
documentsv
address
second
question
degree
calculation
next
subsection
third
question
note
coverage
may
indeed
decrease
compared
search
queries
graph
edges
incident
document
present
document
will
become
isolated
valid
queries
graph
happen
example
documents
little
text
content
indexed
mainly
anchor
text
terms
left
future
work
study
reduction
graphs
coverage
due
smaller
number
edges
compared
5.5
computing
document
degrees
valid
queries
graph
left
show
compute
document
degrees
valid
queries
graph
next
present
three
alternative
techniques
20
naive
approximation
can
simply
use
degp
approximation
degv
see
function
advantage
technique
low
cost
queries
search
engine
submitted
disadvantage
shown
section
5.3
imprecision
know
degp
degv
always
overestimate
document
degrees
quality
approximation
depends
precision
within
predicted
queries
graph
approximates
search
queries
graph
formally
validity
density
document
defined
fraction
predicted
queries
also
search
queries
queriesp
queriess
degv
vdensity
queriesp
degp
vdensity
closer
better
naive
approximation
degv
degp
brute
force
calculation
degv
can
calculated
submitting
query
queriesp
search
engine
returning
number
queries
documentss
see
function
function
getdegreev
queriesp
getqueriesp
queriesp
documentss
getdocumentss
documentss
return
advantage
technique
accuracy
returns
exact
degree
disadvantage
cost
requires
submitting
queries
queriesp
thousands
queries
per
document
search
engine
prohibitively
expensive
task
sampling-based
approximation
combine
two
techniques
create
reasonably
accurate
yet
practical
method
computing
degrees
start
computing
queriesp
randomly
select
queriesp
small
subset
queries
submit
selected
queries
search
engine
finally
probabilistically
estimate
vdensity
fraction
queries
sample
documentss
unbiased
estimator
degv
degp
see
function
show
later
will
need
calculate
degv
rather
degv
unfortunately
inverse
estimator
yields
biased
estimator
degv
thus
resort
directly
estimating
degv
inverse
degree
estimator
ide
described
function
10
shows
procedure
estimating
degv
given
document
using
limited
number
queries
procedure
repeatedly
samples
queries
uniformly
random
set
predicted
queries
queriesp
submits
query
search
engine
checks
whether
documentss
procedure
stops
reaching
first
query
documentss
number
queries
sampled
far
geometrically
distributed
vdensity
success
21
function
estimatedegreev
queriesp
getqueriesp
uniformly
chosen
query
queriesp
documentss
getdocumentss
documentss
return
queriesp
parameter
expectation
estimator
exactly
vdensity
degp
degv
note
procedure
always
guaranteed
terminate
apply
documents
degv
function
10
estimateinversedegreev
queriesp
getqueriesp
true
uniformly
chosen
query
queriesp
documentss
getdocumentss
documentss
return
queriesp
overview
estimators
following
sections
combine
tools
developed
previous
sections
design
search
engine
estimators
sums
averages
sums
estimator
sumest
described
section
straightforward
application
importance
sampling
estimator
valid
queries
graph
averages
apply
section
approximate
importance
sampling
design
avgest
finally
show
section
apply
rao-blackwellization
obtain
efficient
versions
estimators
effsumest
effavgest
6.1
summary
tables
summarize
bias
efficiency
guarantees
search
engine
estimators
propose
empirical
analysis
estimation
variance
given
experimental
results
section
10
table
shows
sumest
bias
bias
avgest
diminishes
number
iterations
performed
estimator
effsumest
effavgest
may
higher
bias
depending
correlation
target
function
expected
validity
density
22
estimator
sumest
avgest
effsumest
objective
sum
average
sum
effavgest
average
zπd
bias
cov
vdensity
vdensity
cov
vdensity
vdensity
πd
πd
table
worst-case
guarantees
bias
search
engine
estimators
iterations
estimator
sumest
avgest
effsumest
effavgest
query
cost
vdensity
pv
pd
pv
fetch
cost
function
cost
pv
avgq
degs
pv
avgq
degs
avgq
pv
degv
table
expected
costs
iterations
search
engine
estimators
documents
note
target
functions
like
constant
function
used
corpus
size
estimation
correlation
irrespective
document
validity
density
costs
sumest
avgest
see
table
combinations
four
components
number
iterations
performed
order
reduce
estimation
bias
variance
ratio
total
number
queries
number
queries
pv
expected
inverse
document
validity
density
average
degree
queries
gap
query
cost
sumest
avgest
query
cost
effsumest
effavgest
vdensity
larger
two
terms
comprise
query
cost
sumest
avgest
fetch
costs
estimators
function
cost
effsumest
effavgest
higher
function
cost
sumest
avgest
factor
avgq
pv
degv
tables
demonstrate
superiority
sumest
avgest
terms
bias
advantage
effsumest
effavgest
terms
cost
bias
effsumest
effavgest
estimators
remains
low
long
target
function
correlated
document
validity
density
6.2
considerations
choosing
query
pool
predicted
queries
graph
recall
section
5.4
valid
queries
graph
used
estimators
intersection
search
queries
graph
predicted
queries
graph
order
estimators
accurate
efficient
need
close
possible
since
document
corpus
right
hand
side
graphs
edges
search
graph
dependent
search
engine
control
choice
freedom
choosing
query
pool
left
hand
side
graphs
edges
predicted
queries
graph
overview
considerations
taken
account
making
23
choices
corpus
coverage
mentioned
section
3.5
estimator
bias
depends
coverage
order
achieve
high
coverage
need
achieve
high
coverage
make
sure
contains
enough
edges
coverage
similar
coverage
coverage
depends
choice
query
pool
pool
must
consist
sufficiently
many
queries
almost
every
document
actually
returned
search
engine
query
main
difficulty
covering
low
pagerank
documents
usually
returned
popular
queries
address
documents
pool
must
consist
many
long-tail
queries
matching
results
fewer
result
set
size
limit
search
engine
likely
return
low
pagerank
documents
queries
found
phrase
queries
consisting
multiple
terms
good
source
long-tail
queries
another
example
inurl
queries
return
pages
whose
url
matches
query
keywords
empirical
study
used
inurl
queries
long
numerical
strings
conjunctions
two
terms
extracted
wikipedia
odp
corpus
order
make
sure
enough
edges
like
many
search
queries
incident
document
also
incident
predicted
queries
graph
access
index
search
engine
connected
queries
indexed
note
set
queries
superset
queries
incident
latter
queries
search
engine
returns
one
top
results
access
index
search
engine
can
try
mimic
indexing
process
search
engine
need
find
sources
queries
document
indexed
document
url
content
anchor
text
parse
sources
tokens
determine
phrase
boundaries
etc
empirical
study
used
document
url
source
predicted
queries
tried
mimic
tokenization
techniques
used
major
search
engines
validity
density
query
cost
sumest
avgest
depends
document
validity
density
higher
expected
validity
density
lower
query
cost
sumest
avgest
also
bias
effsumest
effavgest
depends
validity
density
validity
density
high
close
variance
low
consequently
also
correlation
target
function
low
thus
high
validity
density
beneficial
reducing
bias
effsumest
effavgest
promote
high
validity
density
recall
section
5.3
major
factor
causing
low
validity
density
overflowing
queries
queries
matching
documents
returned
one
way
dilute
effect
overflowing
queries
use
many
long-tail
queries
pool
query
small
number
matching
documents
collectively
cover
sufficiently
many
documents
corpus
pool
size
recall
pv
denotes
collection
queries
least
one
incident
document
valid
queries
graph
query
fetch
costs
increase
decreases
one
thus
avoid
pools
including
queries
likely
return
little
results
pools
consisting
long
phrase
queries
conjunctive
queries
comprising
many
unrelated
terms
average
query
degree
fetch
function
costs
depend
average
degree
queries
24
respectively
thus
like
average
query
degrees
graphs
low
possible
course
may
interfere
coverage
average
degree
low
queries
needed
achieve
sufficient
coverage
result
set
size
limit
higher
typically
leads
higher
variance
query
degrees
higher
variance
turn
increases
variance
importance
weights
consequently
variance
importance
sampling
estimators
thus
amortized
costs
increase
importance
sampling
estimator
sums
notation
sumest
wsum
pd
meaning
estimator
sums
importance
weight
function
estimator
sums
approximate
importance
weight
function
estimators
sums
averages
marginal
distribution
table
notation
used
section
recall
order
employ
importance
sampling
estimator
see
section
4.5
need
compute
importance
weight
function
wsum
πd
pv
degv
degv
sample
trial
distribution
use
estimation
procedures
described
section
function
estimating
pv
function
computing
degv
function
10
estimating
degv
applied
valid
queries
graph
compute
following
estimate
importance
weight
function
πd
pse
degv
ide
recall
pse
estimator
pv
ide
estimator
degv
following
proposition
based
fact
pse
ide
independent
estimators
pse
ide
pse
ide
proposition
7.1
unbiased
estimator
wsum
proof
πd
degv
pse
ide
πd
degv
pv
degv
wsum
25
therefore
shown
section
4.2
can
use
place
wsum
basic
estimator
sums
defined
follows
distributed
according
function
11
implementation
full
estimator
sums
sumest
defined
follows
sumest
1x
xi
qi
xi
function
11
estimatesum
pse
estimatepoolsize
sumπd
samplepair
πd
computetargetmeasure
degv
getdocumentsv
ide
estimateinversedegreev
πd
pse
degv
ide
sumπd
sumπd
computefunction
10
return
sumπd
caching
naively
implementing
estimator
result
waste
search
engine
queries
document
fetches
queries
documents
submitted
fetched
multiple
times
example
last
query
submitted
samplepair
line
submitted
getdocumentsv
line
completely
avoid
resource
waste
employ
query
document
caching
submit
query
search
engine
first
check
whether
already
query
results
cache
avoid
submitting
query
documents
handled
similarly
7.1
analysis
theorem
4.1
proposition
7.2
sumest
unbiased
estimator
sumπd
now
analyze
query
cost
fetch
cost
function
cost
estimatesum
following
proposition
used
state
query
cost
proposition
7.3
marginal
distribution
pd
pv
queriesv
26
degv
proof
prove
pd
marginal
distribution
trial
distribution
must
show
every
pd
documentsv
pv
degv
pv
queriesv
degv
theorem
7.4
qcost
estimatesum
pv
vdensity
distributed
according
pd
fetchcost
estimatesum
avgq
degs
pv
function
cost
estimatesum
recall
query
pool
see
section
3.3
predicted
queries
graph
see
section
5.2
valid
queries
graph
see
section
5.4
search
queries
graph
see
section
5.1
proof
search
engine
queries
submitted
following
functions
estimator
estimatepoolsize
samplepair
getdocumentsv
estimateinversedegreev
now
analyze
query
cost
procedures
assume
estimatepoolsize
called
preprocessing
step
estimate
used
subsequent
estimator
invocations
therefore
include
cost
cost
estimatesum
following
estimators
easy
see
requires
little
pv
queries
produce
accurate
estimate
samplepair
submits
queries
selected
uniformly
random
encounters
query
documentsv
query
cost
procedure
geometric
random
variable
success
parameter
whose
expectation
qcost
samplepair
pv
getdocumentsv
submits
exactly
one
search
engine
query
however
since
query
necessarily
already
submitted
samplepair
since
query
results
cached
query
cost
getdocumentsv
27
degp
shown
section
5.5
estimateinversedegreev
submits
deg
vdensity
queries
expectation
since
random
choices
estimateinversedegreev
independent
qcost
estimateinversedegreev
vdensity
now
analyze
fetch
cost
documents
fetched
following
functions
estimator
samplepair
getdocumentsv
estimateinversedegreev
samplepair
submits
queries
selected
uniformly
random
encounters
query
documentsv
saw
earlier
expected
number
queries
probes
returning
query
pv
queries
getdocumentsv
called
getdocumentsv
fetches
exactly
degs
documents
chosen
uniformly
random
expected
fetch
cost
getdocumentsv
fetchcost
getdocumentsv
degs
avgq
degs
wald
identity
see
28
section
2.2
expected
fetch
cost
samplepair
fetchcost
samplepair
avgq
degs
pv
getdocumentsv
called
estimatesum
getdocumentsv
already
called
iteration
samplepair
thus
since
documents
cached
fetch
cost
calling
getdocumentsv
estimatesum
estimateinversedegreev
fetches
single
document
however
since
document
necessarily
already
fetched
getdocumentsv
since
documents
cached
fetch
cost
estimateinversedegreev
obviously
computefunction
performs
single
function
calculation
thus
function
cost
estimator
importance
sampling
estimator
averages
thus
order
employ
recall
estimator
averages
estimates
avg
sumπd
importance
sampling
estimator
see
section
4.5
need
compute
importance
weight
function
pv
degv
wavg
degv
28
notation
avgest
wavg
wse
aris
meaning
estimator
averages
importance
weight
function
estimator
averages
weight
skew
estimator
approximate
ratio
importance
sampling
estimator
random
query-document
pair
distributed
according
table
notation
used
section
sample
trial
distribution
normalizahowever
since
computetargetmeasure
computes
πd
equals
πd
exactly
overcome
difficulty
first
estimating
sum
tion
calculate
πd
πd
fixing
bias
incurred
using
πd
instead
πd
similarly
sumest
use
valid
queries
graph
estimation
procedures
compute
importance
weight
function
πd
pse
degv
ide
recall
pse
estimator
pv
ide
estimator
degv
note
unbiased
estimator
wavg
since
pse
degv
ide
zπd
wavg
zπd
πd
zπd
unknown
normalization
constant
πd
theorem
4.4
using
approximate
importance
weight
function
instead
exact
one
wavg
incurs
multiplicative
bias
zπd
ais
sumπd
zπd
zπd
cov
zπd
sumπd
zπd
order
obtain
unbiased
estimator
sumπd
still
eliminate
zπd
factor
next
subsection
propose
method
decreasing
bias
8.1
approximate
ratio
importance
sampling
instance
ratio
called
weight
skew
multiplicative
bias
factor
expected
weight
skew
relative
target
distribution
order
eliminate
bias
need
somehow
estimate
expected
weight
skew
now
let
us
assume
unbiased
weight
skew
estimator
wse
example
case
wavg
zπd
wse
estimator
zπd
note
wse
depends
sample
used
importance
sampling
estimator
follows
theorem
4.4
cov
ais
sumπ
wse
29
thus
ratio
expectations
two
estimators
ais
wse
gives
us
desired
result
sumπ
modulo
additive
bias
factor
ignoring
moment
additive
bias
ais
seem
good
estimator
sumπ
ratio
wse
however
one
problem
ais
ais
wse
expectation
ratio
ratio
expectations
wse
solve
problem
resort
well-known
trick
statistics
replace
numerator
denominator
averages
multiple
independent
instances
numerator
estimator
denominator
estimator
difference
expected
ratio
ratio
expectations
diminishes
can
therefore
define
approximate
ratio
importance
sampling
estimator
sumπ
follows
pn
ais
xi
aris
x1
xn
pni
wse
xi
x1
xn
independent
samples
trial
distribution
following
two
lemmas
analyze
bias
variance
estimator
lemma
8.1
wse
aris
x1
xn
sumπ
zπ
cov
appendix
provide
proof
lemma
specify
high
order
term
expression
term
depends
sumπ
variance
covariance
conclude
lemma
use
sufficiently
many
samples
likely
get
estimate
sumπ
additive
bias
depends
correlation
next
lemma
shows
variance
estimator
decreases
lemma
8.2
var
aris
x1
xn
appendix
provide
proof
lemma
specify
high
order
term
expression
term
depends
sumπ
bias
aris
coefficients
variation
coefficient
covariation
8.2
estimator
averages
recall
section
4.3
estimators
sample
query-document
pairs
set
edges
valid
queries
graph
ev
extension
πd
onto
ev
employ
approximate
ratio
importance
sampling
estimator
described
need
come
weight
skew
estimator
whose
expectation
equals
wavg
distributed
according
observe
approximate
weight
unbiased
estimator
zπd
according
30
theorem
4.4
covariance
term
since
constant
function
since
wavg
zπd
set
wse
avgest
defined
follows
avgest
pn
xi
qi
xi
pn
qi
xi
q1
x1
qn
xn
independent
samples
trial
distribution
function
12
implementation
avgest
note
pse
factor
cancels
since
appears
numerator
denominator
like
estimatesum
cache
query
document
requests
made
estimator
function
12
estimateaverage
sumπd
wse
samplepair
πd
computetargetmeasure
degv
getdocumentsv
ide
estimateinversedegreev
πd
degv
ide
sumπd
sumπd
computefunction
10
wse
wse
11
return
sumπd
wse
8.3
analysis
lemma
8.1
bias
estimator
zπd
cov
wavg
wavg
wavg
zπd
see
equation
constant
since
cov
wavg
hence
get
proposition
8.3
bias
avgest
proposition
8.4
costs
estimateaverage
equal
estimatesum
see
theorem
7.4
proof
estimateaverage
differs
estimatesum
calculation
wse
incur
additional
costs
31
notation
effsumest
effavgest
ueff
meaning
efficient
estimator
sums
efficient
estimator
averages
approximate
importance
weight
function
efficient
estimators
sums
averages
table
notation
used
section
efficient
implementation
estimators
estimators
proposed
previous
sections
used
accurate
rather
expensive
procedures
calculating
importance
weights
one
main
bottlenecks
estimation
degv
required
submitting
several
queries
search
engine
per
sample
trial
distribution
estimators
consider
section
instead
using
ide
estimate
degv
use
degp
compute
degp
need
single
document
fetch
search
engine
queries
unfortunately
since
degp
degv
resulting
estimator
biased
analyze
bias
propose
bias
reduction
techniques
recall
importance
weight
function
sumest
wsum
πd
pv
degv
degv
importance
weight
function
avgest
wavg
deg
πd
degv
use
approximate
importance
weight
function
ueff
efficient
estimator
sums
efficient
estimator
averages
ueff
πd
pse
degv
degp
recall
pse
estimator
pv
start
describing
efficient
estimator
averages
derive
efficient
estimator
sums
since
ueff
approximation
wavg
resort
approximate
importance
sampling
see
section
4.6
now
come
weight
skew
estimator
theorem
4.4
ueff
ueff
cov
ueff
sumπd
zπd
wavg
wavg
distribution
sum
note
since
πd
πd
πd
covariance
term
since
constant
function
thus
ueff
ueff
wavg
32
ueff
unbiased
weight
skew
estimator
need
effavgest
defined
follows
effavgest
pn
xi
ueff
qi
xi
pn
ueff
qi
xi
q1
x1
qn
xn
independent
samples
trial
distribution
proposition
9.1
bias
effavgest
cov
vdensity
vdensity
πd
proof
weight
skew
estimator
ueff
degv
zπd
zπd
vdensity
wavg
degp
substituting
lemma
8.1
get
bias
estimator
cov
zπ
zπd
vdensity
zπd
vdensity
distribution
since
πd
πd
πd
cancels
lemma
follows
conclude
long
target
function
correlated
validity
density
bias
low
9.1
rao-blackwellization
inherent
inefficiency
importance
sampling
estimators
although
random
query
submit
search
engine
returns
many
results
use
single
result
per
query
results
discarded
corpus
size
estimator
broder
et
al
uses
query
results
just
one
observe
instance
wellknown
rao-blackwellization
technique
reducing
estimation
variance
thanks
cheap
importance
weight
computation
propose
section
can
apply
rao-blackwellization
importance
sampling
estimators
recall
effavgest
repeatedly
computes
basic
approximate
importance
sampling
estimator
ais
ueff
sample
trial
distribution
ueff
approximate
importance
weight
order
obtain
sample
choose
random
query
pv
pick
single
random
document
documentsv
discarding
results
suppose
now
instead
using
single
document
approximate
importance
sampling
estimator
use
query
results
aisrb
ueff
degv
documentsv
33
instance
aisrb
average
several
correlated
instances
ais
mathematically
aisrb
conditional
expectation
ais
given
aisrb
ais
main
point
computing
correlated
instances
bulk
can
done
single
search
engine
query
rao-blackwell
theorem
implies
aisrb
can
better
ais
estimator
sumπd
theorem
9.2
aisrb
bias
ais
aisrb
ais
variance
aisrb
can
lower
var
aisrb
var
ais
var
ais
proof
follows
immediately
rao-blackwell
theorem
cf
theorem
expected
reduction
variance
var
ueff
uniformly
chosen
query
pv
uniformly
chosen
document
documentsv
variable
results
queries
target
function
higher
chances
rao-blackwellization
will
help
worst-case
scenario
results
query
case
rao-blackwellization
reduce
basic
estimation
variance
empirical
study
however
show
practice
rao-blackwellization
can
make
dramatic
effect
see
section
10
variance
reduction
achieved
rao-blackwellization
can
lead
lower
costs
fewer
instances
estimator
needed
order
obtain
desired
accuracy
guarantee
hand
instance
estimator
requires
many
weight
function
calculations
many
number
results
sampled
query
costly
case
estimators
proposed
previous
sections
increase
cost
per
instance
may
outweigh
reduction
number
instances
eventually
leading
higher
amortized
costs
conclude
rao-blackwellization
used
judiciously
case
expect
query
cost
main
bottleneck
using
rao-blackwellization
justified
thus
redefine
effavgest
follows
pn
degv
qi
documentsv
qi
ueff
qi
effavgest
pn
deg
documentsv
qi
ueff
qi
q1
qn
uniform
independent
samples
pv
function
13
implementation
effavgest
note
pse
degv
qi
factors
ueff
cancel
like
estimatesum
estimateaverage
cache
query
document
requests
made
estimator
now
analyze
query
cost
fetch
cost
function
cost
estimator
34
function
13
estimateaverageefficiently
sumπd
wse
samplepair
documentsv
getdocumentsv
documentsv
πd
computetargetmeasure
degp
getdegreep
ueff
πd
degp
10
sumπd
sumπd
computefunction
ueff
11
wse
wse
ueff
12
return
sumπd
wse
theorem
9.3
pv
avgq
degs
fetchcost
estimateaverageefficiently
pv
funccost
estimateaverageefficiently
avgq
pv
degv
qcost
estimateaverageefficiently
proof
search
engine
queries
submitted
following
functions
estimator
estimatepoolsize
samplepair
getdocumentsv
query
costs
procedures
analyzed
proof
theorem
7.4
pv
respectively
documents
fetched
following
three
functions
estimator
samplepair
getdocumentsv
getdegreep
fetch
costs
first
two
procedures
analyzed
proof
theorem
7.4
pv
avgq
degs
respectively
getdegreep
fetches
single
document
however
since
document
necessarily
already
fetched
getdocumentsv
since
documents
cached
effective
fetch
cost
getdegreep
35
easy
see
computefunction
called
exactly
degv
times
per
external
iteration
estimateaverageefficiently
since
sampled
queries
distributed
uniformly
pv
expected
function
cost
estimateaverageefficiently
funccost
estimateaverageefficiently
degv
avgq
pv
degv
pv
pv
9.2
adaptation
estimation
sums
recall
efficient
importance
weight
function
defined
follows
ueff
πd
pv
degv
degp
similarly
efficient
estimator
averages
since
ueff
approximation
wsum
use
approximate
importance
sampling
order
come
unbiased
ueff
weight
skew
estimator
whose
expectation
equals
wsum
unfortunately
ueff
unbiased
weight
skew
estimator
efficient
estimator
averages
since
theorem
4.4
covariance
term
since
constant
function
ueff
ueff
ueff
sumπd
zπd
wsum
wsum
note
sumπd
zπd
unknown
suppose
moment
zπd
known
thus
can
used
estimator
effsumest
can
defined
follows
effsumest
zπd
pn
xi
ueff
qi
xi
pn
ueff
qi
xi
q1
x1
qn
xn
independent
samples
trial
distribution
observe
effsumest
equal
effavgest
factor
zπd
therefore
can
reduce
effsumest
effavgest
follows
effsumest
zπd
effavgest
zπd
can
estimated
preprocessing
step
using
sumest
cost
one-time
preprocessing
can
amortized
multiple
estimations
use
target
measure
proposition
9.4
bias
effsumest
zπd
cov
vdensity
vdensity
πd
36
proof
weight
skew
estimator
ueff
degv
vdensity
wsum
degp
lemma
follows
substituting
lemma
8.1
costs
estimator
equivalent
estimateaverageefficiently
see
theorem
9.3
costs
estimating
zπd
indeed
estimate
zπd
available
advance
estimatesumefficiently
efficient
estimatesum
however
many
practical
situations
estimate
zπd
available
byproduct
another
estimation
cost
estimateaverageefficiently
zero
10
experimental
results
conducted
two
sets
experiments
first
set
performed
comparative
evaluation
bias
amortized
cost
new
estimators
rejection
sampling
estimator
previous
paper
broder
et
al
estimator
end
ran
estimators
local
search
engine
built
2.4
million
english
documents
fetched
odp
12
ground
truth
search
engine
compare
measurements
produced
estimators
real
values
second
set
experiments
conducted
two
major
real
search
engines
used
sumest
estimate
corpus
size
search
engines
without
duplicate
elimination
accurately
estimated
sizes
large
subsets
search
engine
corpora
used
avgest
estimate
index
freshness
fraction
pages
containing
advertisements
distribution
web
server
types
10.1
evaluation
experiments
experimental
setup
used
local
search
engine
previous
paper
corpus
search
engine
consists
english-language
text
html
pdf
documents
odp
hierarchy
document
given
serial
id
indexed
single
terms
phrases
first
10
000
terms
document
considered
exact
phrases
allowed
cross
boundaries
paragraph
boundaries
used
static
ranking
serial
id
rank
query
results
order
construct
query
pool
evaluation
experiments
split
odp
data
set
two
parts
training
set
consisting
every
fifth
page
ordered
id
test
set
consisting
rest
pages
used
training
set
create
pool
phrases
length
measurements
done
test
set
compared
following
estimator
configurations
sumest
effsumest
avgest
effavgest
broder
et
al
estimator
rejection
sampling
estimator
previous
paper
37
used
estimators
measure
two
metrics
corpus
size
size
test
set
density
pages
test
set
sports
used
simple
keyword
based
classifier
determine
whether
page
sports
note
corpus
size
sum
metric
density
sports
pages
average
metric
use
rejection
sampling
estimator
estimating
corpus
size
can
handle
average
metrics
use
broder
et
al
estimator
estimating
density
sports
pages
can
handle
sum
metrics
order
common
baseline
allowed
estimator
sample
exactly
million
nonunderflowing
queries
pool
estimator
submitted
queries
local
search
engine
computed
estimate
estimator
used
additional
search
engine
requests
estimating
document
degrees
ran
experiment
four
times
different
values
result
set
size
limit
20
100
200
done
order
track
dependence
estimator
bias
validity
density
lower
higher
density
expected
dependence
estimator
cost
result
set
sizes
increase
produced
estimate
measured
relative
bias
amortized
cost
follows
let
estimation
result
let
true
value
parameter
estimated
relative
bias
amortized
query
cost
estimator
qcost
var
see
section
3.2
used
total
number
queries
made
estimator
execution
estimate
qcost
measured
empirical
variance
used
estimate
var
output
used
estimate
results
70
60
60
50
effavgest
40
rejection
sampling
broder
et
al
50
relative
bias
relative
bias
avgest
sumest
40
30
20
30
20
10
10
20
100
200
result
set
size
limit
20
100
200
result
set
size
limit
corpus
size
density
sports
pages
figure
relative
bias
estimators
figure
compares
relative
bias
sumest
estimator
broder
et
al
measuring
corpus
size
corpus
size
estimation
effsumest
equivalent
sumest
therefore
omitted
figure
figure
compares
relative
bias
two
estimators
rejection
38
sampling
estimator
measuring
density
sports
pages
results
corpus
size
clearly
show
estimator
bias
estimator
broder
et
al
suffers
significant
bias
grows
density
overflowing
queries
pool
example
relative
bias
broder
et
al
estimator
60
relative
bias
estimators
results
density
sports
pages
show
avgest
practically
unbiased
expected
effavgest
small
bias
emanates
weak
correlation
function
value
validity
density
rejection
sampling
method
large
observed
bias
primarily
produced
small
number
uniform
samples
thus
variance
still
high
1000
300
250
amortized
query
cost
amortized
query
cost
sumest
effsumest
without
rb
200
effsumest
rb
broder
et
al
150
100
50
900
avgest
800
effavgest
without
rb
700
600
effavgest
rb
500
400
300
200
100
20
100
200
result
set
size
limit
20
100
200
result
set
size
limit
sum
metric
average
metric
figure
amortized
query
cost
estimators
figures
compare
amortized
query
costs
estimators
demonstrate
effect
rao-blackwellization
query
cost
efficient
estimators
cost
estimator
broder
et
al
cost
effsumest
rao-blackwellization
amortized
query
costs
rejection
sampling
estimator
5600
14100
80200
84700
20
100
200
respectively
omitted
figure
significantly
higher
costs
estimators
distort
plot
cost
effsumest
include
cost
preprocessing
step
amortized
query
cost
increases
due
increasing
variance
query
degrees
see
section
6.2
results
clearly
indicate
rao-blackwellization
effective
reducing
estimation
variance
therefore
also
amortized
cost
estimators
example
estimation
density
sports
pages
200
rao-blackwellization
reduced
amortized
query
cost
effavgest
80
furthermore
amortized
cost
rejection
sampling
estimator
tremendously
higher
amortized
cost
new
estimators
even
non-rao-blackwellized
ones
example
100
rao-blackwellized
effavgest
500
times
efficient
rejection
sampling
39
10.2
experiments
real
search
engines
experimental
setup
experiments
real
search
engines
conducted
july
2009
unlike
previous
works
used
phrase
queries
used
inurl
queries
inurl
term
single
terms
two-term
conjunctions
inurl
queries
return
pages
whose
url
contains
query
terms
different
regular
queries
return
pages
based
matching
content
url
anchor
text
etc
main
advantage
pool
inurl
queries
pool
used
covers
types
documents
images
flash
etc
non-english
documents
note
pool
guaranteed
cover
100
pages
corpus
example
pages
whose
urls
consist
non-english
terms
less
likely
covered
pool
used
estimators
pool
2.37
billion
queries
consisting
1.11
billion
decimal
strings
digits
7.26
million
single
terms
extracted
snapshots
english
wikipedia
site
english
odp
directory
10.7
million
single
terms
extracted
list
urls
odp
directory
1.25
billion
two-term
conjunctions
50
000
frequent
single
terms
excluding
100
frequent
ones
increase
corpus
coverage
disabled
standard
result
filtering
performed
engines
duplicate
filtering
host
collapsing
adding
suitable
arguments
requests
sent
search
engines
corpus
size
used
accurate
sampler
sumest
estimate
corpus
sizes
two
major
search
engines
reference
also
ran
broder
et
al
estimator
query
pool
addition
corpus
measurement
result
filtering
disabled
performed
additional
measurements
duplicate
filtering
host
collapsing
enabled
default
setting
regular
web
search
results
standard
deviations
plotted
figure
note
estimates
may
underestimate
true
corpus
sizes
search
engines
since
effectively
measure
sizes
subsets
corpora
indexed
pages
match
least
one
query
pool
results
show
degree
mismatch
problem
affects
broder
et
al
estimator
also
live
search
engines
estimate
corpus
size
first
search
engine
3.8
times
lower
estimate
estimate
corpus
size
second
search
engine
3.5
times
lower
estimate
finally
experiments
reveal
duplicate
filtering
host
collapsing
make
half
corpora
invisible
queries
pool
corpus
freshness
used
avgest
estimate
percentage
dead
pages
ones
returning
4xx
http
return
code
full
corpora
duplicate
filtering
host
collapsing
disabled
results
standard
deviations
plotted
figure
observe
corpora
comparable
yet
non-negligible
fraction
inaccessible
pages
ads
used
avgest
estimate
fraction
indexed
pages
containing
least
one
ad
used
simple
ad
detection
heuristic
checked
whether
page
contains
string
form
http://..."
including
quotes
containing
one
following
substrings
googlesyndication
40
filtering
broder
et
al
filtering
default
filtering
30
25
percent
dead
pages
absolute
corpus
size
billions
35
20
15
10
se1
se1
se2
figure
corpus
sizes
two
major
search
engines
without
host
collapsing
elimination
duplicates
se2
figure
percentage
inaccessible
pages
45
80
40
70
35
percent
pages
percent
pages
ads
googlesyndication
googleadservices
doubleclick
adsense
omniture
atdmt
aolcdn
eiv
baidu
com
ma
baidu
com
spcjs
php
ck
php
adserver
yahoo
com
adsfac
.net
ad
yieldmanager
com
adbureau
.net
ads
revsci
.net
blogads
one
regular
expressions
ads
ad
results
standard
deviations
plotted
figure
results
indicate
30
indexed
pages
contains
least
one
ad
30
25
20
15
10
apache
microsoft-iis
others
60
50
40
30
20
10
se1
se2
se1
se2
figure
percentage
pages
containing
ads
figure
distribution
web
server
types
hosting
indexed
pages
web
server
types
finally
used
avgest
estimate
distribution
web
server
types
hosting
indexed
pages
used
server
attribute
http
response
determine
server
type
results
standard
deviations
plotted
figure
apache-based
servers
clearly
host
majority
indexed
pages
microsoft-iis
distant
second
server
types
error
margin
measurements
thus
aggregated
category
others
41
11
conclusions
paper
presented
two
new
estimators
aggregate
function
search
engine
corpora
can
expressed
discrete
integrals
estimators
able
overcome
degree
mismatch
problem
thereby
accurate
efficient
time
show
analytically
empirically
estimators
beat
recently
proposed
estimators
designing
estimators
employ
combination
statistical
tools
like
importance
sampling
rao-blackwellization
carefully
analyzing
effect
approximate
weights
bias
importance
sampling
able
design
procedures
mitigate
bias
bias-elimination
technique
approximate
importance
sampling
may
applicable
scenarios
well
references
anagnostopoulos
broder
carmel
sampling
search-engine
results
proc
14th
www
pages
245
256
2005
bar-yossef
berg
chien
fakcharoenphol
weitz
approximating
aggregate
queries
web
pages
via
random
walks
proc
26th
vldb
pages
535
544
2000
bar-yossef
gurevich
efficient
search
engine
measurements
proceedings
16th
international
world
wide
web
conference
www
pages
401
410
2007
bar-yossef
gurevich
mining
search
engine
query
logs
via
suggestion
sampling
proc
34th
vldb
2008
bar-yossef
gurevich
random
sampling
search
engine
index
acm
55
2008
bharat
broder
technique
measuring
relative
size
overlap
public
web
search
engines
proc
7th
www
pages
379
388
1998
bradlow
schmittlein
little
engines
modeling
performance
world
wide
web
search
engines
marketing
science
19
43
62
2000
broder
fontoura
josifovski
kumar
motwani
nabar
panigrahy
tomkins
xu
estimating
corpus
size
via
queries
proc
15th
cikm
2006
casella
robert
rao-blackwellisation
sampling
schemes
biometrika
83
81
94
1996
10
cheney
perry
comparison
size
yahoo
google
indices
available
http://vburton.ncsa.uiuc.edu/indexsize.html,
2005
11
dasgupta
das
mannila
random
walk
approach
sampling
hidden
databases
proc
sigmod
pages
629
640
2007
12
dmoz
open
directory
project
http://dmoz.org.
42
13
dobra
fienberg
large
world
wide
web
web
dynamics
pages
23
44
2004
14
goldreich
sample
samplers
computational
perspective
sampling
survey
eccc
20
1997
15
gulli
signorini
indexable
web
11.5
billion
pages
proc
14th
www
pages
902
903
2005
16
hastings
monte
carlo
sampling
methods
using
markov
chains
applications
biometrika
57
97
109
1970
17
henzinger
heydon
mitzenmacher
najork
measuring
index
quality
using
random
walks
web
proc
8th
www
pages
213
225
1999
18
henzinger
heydon
mitzenmacher
najork
near-uniform
url
sampling
proc
9th
www
pages
295
308
2000
19
hesterberg
advances
importance
sampling
phd
thesis
stanford
university
1988
20
lawrence
giles
searching
world
wide
web
science
5360
280
98
1998
21
lawrence
giles
accessibility
information
web
nature
400
107
109
1999
22
lee
chao
estimating
population
size
via
sample
coverage
closed
capturerecapture
models
biometrics
50
88
97
1994
23
liu
monte
carlo
strategies
scientific
computing
springer
2001
24
marshall
use
multi-stage
sampling
schemes
monte
carlo
computations
symposium
monte
carlo
methods
pages
123
140
1956
25
mccurley
income
inequality
attention
economy
http://mccurley.org/papers/
effective
2007
26
metropolis
rosenbluth
rosenbluth
teller
teller
equations
state
calculations
fast
computing
machines
chemical
physics
21
1087
1091
1953
27
rusmevichientong
pennock
lawrence
giles
methods
sampling
pages
uniformly
world
wide
web
proc
aaai
symp
using
uncertainty
within
computation
2001
28
siegmund
sequential
analysis
tests
confidence
intervals
springer-verlag
1985
29
stuart
ord
kendall
advanced
theory
statistics
vol
distribution
theory
london
hodder
arnold
1994
30
von
neumann
various
techniques
used
connection
random
digits
john
von
neumann
collected
works
volume
oxford
1963
43
search
engine
importance
sampling
estimation
importance
sampling
approximate
degrees
theorem
4.4
restated
ais
sumπ
zπ
cov
distributed
according
trial
distribution
distributed
according
target
distribution
zπ
normalization
constant
proof
since
supp
supp
ais
supp
zπ
supp
supp
supp
supp
renaming
variables
zπ
zπ
cov
zπ
cov
sumπ
importance
sampling
estimator
averages
approximate
importance
sampling
define
ratio
estimator
show
bias
variance
decrease
44
definition
ratio
estimator
suppose
two
estimators
let
mp
nn
independent
instances
respectively
define
mn
mn
n1
ni
ren
mn
nn
ratio
estimator
order
induced
theorem
assume
finite
variance
nn
bias
ren
var
cov
ren
e2
proof
use
delta
method
statistics
see
29
section
10.5
pages
350
371
approximate
ren
cov
mn
nn
mn
mn
mn
ren
var
nn
nn
nn
nn
e2
nn
since
mn
nn
cov
mn
nn
cov
simplify
follows
mn
var
cov
e2
nn
note
bias
decreases
depends
coefficient
variation
standard
deviation
divided
expectation
well
correlation
low
variance
low
correlation
independent
instances
will
needed
order
make
bias
small
next
theorem
quantifies
variance
ratio
estimator
theorem
assume
finite
variance
nn
variance
ren
var
var
cov
var
ren
e2
proof
use
delta
method
statistics
see
29
section
10.5
pages
350
351
approximate
var
ren
var
mn
var
nn
cov
mn
nn
mn
var
ren
nn
e2
mn
nn
mn
nn
simplified
follows
var
var
cov
var
ren
e2
45
similarly
bias
variance
decreases
can
seen
variance
depends
coefficients
variation
coefficient
covariation
lemma
8.1
restated
wse
aris
x1
xn
sumπ
var
wse
cov
ais
wse
sumπ
wse
e2
wse
zπ
cov
proof
theorems
4.4
aris
x1
xn
zπ
cov
sumπ
var
wse
cov
ais
wse
aris
x1
xn
wse
e2
wse
lemma
follows
substituting
zπ
cov
aris
x1
xn
sumπ
expression
lemma
8.2
restated
let
highest
order
term
bias
aris
estimator
zπ
cov
var
aris
x1
xn
sumπ
cov
ais
wse
var
ais
var
wse
ais
wse
e2
ais
e2
wse
46
proof
theorem
var
aris
x1
xn
aris
x1
xn
cov
ais
wse
var
ais
var
wse
ais
wse
ais
wse
restate
first
term
follows
e2
aris
x1
xn
sumπ
sumπ
lemma
follows
47