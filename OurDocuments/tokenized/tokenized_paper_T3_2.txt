tutorial
fairness
in
rankings
and
recommenders
evaggelia
pitoura
pitoura@cs.uoi.gr
university
of
ioannina
ioannina
greece
georgia
koutrika
georgia@athenarc.gr
athena
research
center
athens
greece
among
others
recent
studies
report
that
social
media
has
become
the
main
source
of
online
news
with
more
than
2.4
billion
internet
users
of
which
nearly
64.5
receive
breaking
news
from
social
media
instead
of
traditional
sources
22
thus
to
great
extent
such
systems
play
central
role
in
shaping
our
experiences
and
influencing
our
perception
of
the
world
again
there
are
many
reports
questioning
the
output
of
such
systems
for
instance
known
study
on
search
results
showed
evidence
for
stereotype
exaggeration
in
images
returned
when
people
search
for
professional
careers
16
abstract
with
the
growing
complexity
of
the
available
online
information
search
engines
via
rankings
and
recommender
systems
come
to
the
rescue
providing
suggestions
to
users
about
items
of
potential
interest
from
movies
and
products
to
news
articles
and
even
potential
friends
such
results
and
suggestions
aim
at
covering
the
user
information
needs
and
play
an
important
role
in
guiding
users
decisions
and
in
forming
their
opinions
however
the
same
technology
if
not
used
responsibly
may
lead
to
discrimination
amplify
potential
biases
in
the
original
data
restrict
transparency
and
strengthen
unfairness
for
example
consider
scenarios
in
which
models
based
on
biased
data
produce
results
that
abet
violence
decrease
diversity
or
have
an
adverse
impact
on
economic
policies
while
the
potential
benefits
of
rankings
and
recommenders
are
well-accepted
and
understood
the
importance
of
using
such
systems
in
fair
manner
has
only
recently
attracted
attention
in
this
tutorial
we
cover
recent
advancements
and
highlight
future
research
directions
in
this
increasingly
relevant
research
area
fairness
in
rankings
and
recommenders
in
this
tutorial
we
pay
special
attention
to
the
concept
of
fairness
in
rankings
and
recommender
systems
by
fairness
we
typically
mean
lack
of
discrimination
it
is
not
correct
to
assume
that
insights
achieved
via
computations
on
data
are
unbiased
simply
because
data
was
collected
automatically
or
processing
was
performed
algorithmically
bias
may
come
from
the
algorithm
reflecting
for
example
commercial
or
other
preferences
of
its
designers
or
even
from
the
actual
data
for
example
if
survey
contains
biased
questions
or
if
some
specific
population
is
misrepresented
in
the
input
data
in
this
tutorial
we
review
number
of
definitions
of
fairness
that
aim
at
addressing
discrimination
bias
amplification
and
ensure
fair
treatment
we
organize
these
definitions
around
the
notions
of
individual
and
group
fairness
we
also
present
methods
for
achieving
fairness
in
rankings
and
recommendations
taking
cross-type
view
distinguishing
them
between
pre-processing
in-processing
and
post-processing
approaches
we
conclude
with
discussion
of
the
new
research
directions
that
arise
introduction
currently
algorithmic
systems
driven
by
large
amounts
of
data
are
increasingly
being
used
in
all
aspects
of
society
such
systems
offer
enormous
opportunities
they
accelerate
scientific
discovery
in
all
domains
including
personalized
medicine
and
smart
weather
forecasting
they
automate
tasks
they
help
in
improving
our
life
through
personal
assistants
and
recommendations
they
have
the
potential
of
transforming
society
through
open
government
to
name
just
few
of
their
benefits
often
such
systems
are
being
used
to
assist
or
even
replace
human
decision
making
in
diverse
domains
examples
include
software
systems
used
in
school
admissions
housing
pricing
of
goods
credit
score
estimation
job
applicant
selection
and
sentencing
decisions
in
courts
and
surveillance
prominent
case
is
the
compas
software
used
in
courts
in
the
us
to
assist
bail
and
sentencing
decisions
through
risk
assessment
algorithm
that
predicts
future
crime
the
ubiquitous
use
of
such
systems
may
create
possible
threats
of
economic
loss
social
stigmatization
or
even
loss
of
liberty
for
instance
known
study
by
propublica
found
that
in
compas
the
false
positive
rate
for
african
american
defendants
namely
people
labelled
high-risk
who
did
not
re-offend
was
nearly
twice
as
high
as
that
for
white
defendants
11
another
wellknown
study
shows
that
names
used
predominantly
by
men
and
women
of
colour
are
much
more
likely
to
generate
ads
related
to
arrest
records
34
data-driven
systems
are
also
being
employed
by
search
and
recommendation
engines
social
media
tools
and
news
outlets
tutorial
objectives
this
tutorial
aims
at
presenting
toolkit
of
definitions
models
and
methods
used
for
ensuring
fairness
in
rankings
and
recommendations
our
objectives
are
three-fold
to
provide
solid
framework
on
novel
quickly
evolving
and
impactful
domain
to
highlight
challenges
and
research
paths
for
researchers
and
practitioners
that
work
on
problems
in
the
intersection
of
recommender
systems
and
databases
and
to
show
how
fairness
challenges
manifest
in
other
areas
cloud
computation
and
job
scheduling
and
transfer
findings
from
existing
works
in
these
areas
for
this
purpose
we
organize
our
tutorial
along
the
following
main
axes
motivation
and
background
for
the
need
for
fair
rankings
and
recommendations
ii
modeling
fairness
in
rankings
and
recommendations
iii
ensuring
fair
rankings
and
recommendations
and
iv
fairness
in
computations
algorithms
and
systems
and
open
research
challenges
motivation
and
background
fairness
has
emerged
as
an
important
category
of
research
for
machine
learning
systems
in
many
application
areas
extending
this
concept
to
rankings
and
recommendations
is
tricky
first
there
is
an
essential
tension
between
the
goals
of
fairness
and
those
of
personalization
inherent
in
the
idea
of
personalization
2019
copyright
held
by
the
owner
author
published
in
proceedings
of
the
23rd
international
conference
on
extending
database
technology
edbt
march
30
april
2020
isbn
978
89318
083
on
openproceedings
org
distribution
of
this
paper
is
permitted
under
the
terms
of
the
creative
commons
license
cc-by-nc-nd
4.0
series
issn
2367
2005
kostas
stefanidis
konstantinos.stefanidis@tuni.fi
tampere
university
tampere
finland
651
10.5441
002
edbt
2020.86
is
that
the
best
items
for
one
user
may
be
different
than
those
for
another
however
there
are
contexts
in
which
equity
across
rankings
and
recommendation
outcomes
is
desirable
goal
furthermore
fairness
is
multi-sided
concept
in
which
the
impacts
on
multiple
groups
of
individuals
must
be
considered
in
this
tutorial
we
start
by
presenting
motivating
examples
for
the
need
for
fair
rankings
and
recommendations
from
several
domains
including
justice
ads
image
search
and
others
we
highlight
possible
causes
of
unfairness
such
as
biased
or
incomplete
data
and
algorithmic
inefficiencies
we
point
out
potential
harms
such
as
filter
bubbles
polarization
loss
of
opportunity
and
discrimination
we
consider
number
of
different
dimensions
based
on
which
we
classify
existing
models
and
approaches
firstly
we
distinguish
between
the
multiple
viewpoints
that
fairness
can
have
in
recommendation
systems
namely
fairness
for
the
recommended
items
31
fairness
for
the
users
19
38
fairness
for
groups
of
users
24
29
and
fairness
for
the
item
providers
and
the
recommendation
platform
25
furthermore
we
distinguish
the
existing
methods
for
achieving
fairness
in
rankings
and
recommendations
as
pre-processing
31
in-processing
13
and
post-processing
approaches
15
suggested
data
items
31
users
19
38
group
of
users
27
29
or
item
providers
finally
we
investigate
the
notion
of
fairness
in
sequential
and
multi-round
recommenders
25
33
where
the
goal
is
to
ensure
fairness
in
number
of
interactions
between
the
users
and
the
system
we
also
discuss
fairness
in
the
case
of
link
recommendations
in
networks
and
related
concepts
of
homogeneity
echo
chambers
and
polarity
12
this
part
of
the
tutorial
concludes
with
discourse
on
other
related
concepts
such
as
the
relationship
between
fairness
and
diversity
recommendation
independence
transparency
15
and
feedback
loops
ensuring
fairness
in
this
section
we
present
methods
for
achieving
fairness
in
rankings
and
recommendations
we
first
discuss
the
trade-offs
among
fairness
personalization
and
accuracy
taking
cross-type
view
approaches
can
be
distinguished
as
pre-processing
in-processing
and
post-processing
pre-processing
approaches
target
at
transforming
the
data
so
that
any
underlying
bias
or
discrimination
is
removed
in-processing
approaches
target
at
modifying
existing
or
introducing
new
algorithms
that
result
in
fair
rankings
and
recommendations
by
removing
bias
post-processing
approaches
treat
the
algorithms
for
producing
rankings
and
recommendations
as
black
boxes
without
changing
their
inner
workings
to
ensure
fairness
they
modify
the
output
of
the
algorithm
modeling
fairness
fairness
is
general
term
and
coming
up
with
single
definition
or
model
is
tricky
we
start
this
part
of
the
tutorial
by
reviewing
definitions
of
fairness
which
in
general
ask
for
nondiscrimination
of
users
or
items
based
on
the
values
of
one
or
more
sensitive
or
protected
attributes
such
as
gender
or
race
we
organize
the
definitions
with
respect
to
the
notions
of
individual
fairness
treating
similar
individuals
similarly
10
18
and
group
fairness
treating
different
groups
equally
nondiscrimination
of
sensitive
groups
35
we
present
number
of
widely
used
models
and
definitions
for
fairness
23
36
including
5.1
recommenders
we
first
study
fairness
in
systems
that
produce
recommendations
for
individuals
these
comprise
the
majority
of
existing
recommender
systems
we
start
by
presenting
pre-processing
approaches
that
work
on
modifying
the
input
to
the
recommender
for
example
by
appropriate
sampling
by
adding
more
data
to
the
input
31
or
by
performing
database
repair
28
then
we
focus
on
approaches
for
designing
fairness-aware
algorithms
that
is
recommendation
algorithms
that
produce
fair
recommendations
we
will
present
algorithms
for
fairness-aware
matrix
factorization
38
multi-armed
bandits
13
21
and
deep
learning
recommenders
44
for
instance
we
show
that
when
fairness
with
respect
to
both
consumers
and
to
item
providers
is
important
variants
of
the
well-known
sparse
linear
method
slim
can
be
used
to
negotiate
the
trade-off
between
fairness
and
accuracy
and
improve
the
balance
of
user
and
item
neighborhoods
alternatively
we
can
augment
the
learning
objective
in
matrix
factorization
by
adding
smoothed
variation
of
fairness
metric
38
as
another
example
we
present
methods
that
mitigate
bias
to
increase
fairness
by
incorporating
randomness
in
variational
autoencoders
recommenders
finally
we
present
post-processing
approaches
that
modify
the
output
of
the
recommenders
to
ensure
fairness
15
moving
from
individuals
to
groups
group
recommendations
have
attracted
significant
research
efforts
for
their
importance
in
benefiting
group
of
users
however
maximizing
the
satisfaction
of
each
group
member
while
minimizing
the
unfairness
between
them
is
very
challenging
20
we
study
different
fair-aware
algorithms
for
group
recommenders
20
27
29
32
demographic
or
statistical
parity
35
stating
that
the
proportion
of
each
part
of
protected
class
gender
should
take
the
positive
outcome
at
equal
rates
conditional
statistical
parity
36
which
defines
statistical
parity
given
set
of
legitimate
factors
equalized
odds
stating
that
the
protected
and
unprotected
groups
should
have
equal
rates
for
true
positives
and
false
positives
fairness
through
awareness
10
stating
that
any
two
similar
individuals
should
receive
similar
outcome
counterfactual
fairness
18
stating
that
decision
for
an
individual
is
fair
if
it
is
the
same
in
both
the
actual
world
and
counterfactual
world
where
the
individual
belongs
to
different
demographic
group
calibration-based
fairness
26
stating
that
if
group
receives
predicted
probability
at
least
fraction
of
its
members
should
belong
to
the
predicted
class
next
we
review
how
these
models
of
fairness
have
been
extended
in
the
case
of
ranked
outputs
including
attention-based
and
probability-based
approaches
as
well
as
approaches
based
on
pair-wise
comparisons
37
then
we
look
at
how
definitions
of
algorithmic
fairness
and
fair
ranking
have
been
adopted
in
recommender
systems
31
39
given
that
fairness
is
multi-sided
concept
we
extend
our
taxonomy
under
the
umbrella
of
recommender
systems
considering
that
fairness
can
refer
to
652
5.2
while
the
potential
benefits
of
fairness
are
well-accepted
nowadays
we
still
need
to
study
the
actual
impact
of
fairnessenhancing
algorithms
for
example
extensive
user
studies
are
needed
to
evaluate
the
level
of
acceptance
of
the
fairness-enhanced
results
by
the
users
and
the
long
term
effect
of
these
results
on
their
own
perceptions
and
preferences
extensive
studies
that
exploit
feedback
loops
should
also
be
performed
in
this
line
of
work
so
as
to
investigate
deeper
the
connections
between
the
concepts
of
fairness
explainability
and
personalization
moreover
it
will
be
very
advantageous
to
study
comparatively
the
notions
of
equality
that
ensures
equal
treatment
over
equity
that
ensures
treatment
based
on
needs
operationalizing
equity
is
difficult
task
that
often
depends
on
the
domain
under
study
rankings
in
order
to
guarantee
fair
rankings
in-processing
approaches
work
with
result
generation
procedures
that
allow
the
systematic
control
of
the
degree
of
unfairness
in
the
output
by
exploiting
learning
techniques
satisfying
statistical
parity
while
preserving
relevance
37
43
the
work
in
30
formulates
fairness
constraints
on
rankings
targeting
at
relevance
maximization
in
terms
of
exposure
allocation
learning-based
in-processing
approach
is
also
used
in
41
to
reduce
discrimination
and
inequality
of
opportunity
in
rankings
here
the
method
learns
ranking
function
with
an
additional
objective
that
reduces
disparate
exposure
recent
learning
to
rank
approach
deltr
looks
at
the
average
probability
of
items
from
protected
group
to
be
ranked
at
the
top
position
42
the
post-processing
approach
of
40
aims
at
satisfying
statistical
tests
of
representativeness
when
ranking
items
in
certain
order
so
as
to
ensure
that
the
ratio
of
protected
individuals
that
appear
within
prefix
of
the
ranking
namely
top-k
must
be
above
given
proportion
the
attention
received
by
the
items
in
different
positions
in
the
ranking
is
also
not
the
same
items
ranked
in
first
positions
are
exposed
to
much
more
attention
than
the
lower
ones
tackles
the
problem
of
having
ranking
to
be
presented
as
query
result
where
the
items
in
the
first
positions
have
the
same
or
very
similar
relevance
when
it
happens
there
is
decision
to
be
made
of
which
items
are
being
top-ranked
and
which
are
not
solution
to
this
situation
called
amortized
fairness
considers
that
the
position
index
is
proxy
for
the
level
of
attention
an
item
is
exposed
while
the
output
of
the
prediction
algorithm
corresponds
to
the
item
relevance
accumulated
attention
across
series
of
rankings
should
be
proportional
to
accumulated
relevance
as
indicating
long
term
ranking
fairness
tutorial
information
motivation
and
target
audience
the
tutorial
topic
lies
in
the
core
of
the
conference
interests
the
tutorial
aims
at
researchers
and
students
as
well
as
it
professionals
and
developers
in
searching
ranking
and
recommender
systems
and
the
general
data
management
community
researchers
and
students
will
get
good
introduction
to
the
topic
and
get
inspired
by
challenging
research
problems
furthermore
it
professionals
and
developers
will
learn
appropriate
fairness-aware
techniques
to
promote
fairness
in
their
systems
all
the
materials
that
will
be
used
for
the
tutorial
will
be
publicly
available
prerequisites
the
tutorial
is
carefully
structured
to
accommodate
both
attendees
unfamiliar
with
the
topic
and
more
experienced
participants
by
providing
required
background
knowledge
shared
terminology
and
common
understanding
of
the
basic
fairness-related
concepts
intended
duration
we
are
aiming
for
90
minute
tutorial
link
to
tutorial
resources
https://sites.google.com/view/fair-ranking-recommend
open
issues
and
research
directions
in
this
section
we
present
critical
comparison
of
the
existing
work
on
ensuring
fair
rankings
and
recommendations
and
the
lessons
learnt
in
these
areas
furthermore
we
discuss
open
issues
and
new
research
directions
that
arise
first
we
present
fairness
concepts
studied
in
different
areas
of
computer
science
fairness
is
often
ubiquitous
property
of
computations
algorithms
and
systems
beyond
recommender
systems
for
instance
in
federated
stream
processing
systems
it
is
an
open
challenge
how
to
ensure
global
fairness
on
processing
quality
experienced
by
queries
14
systems
for
processing
big
data
such
as
hadoop
spark
and
massively
parallel
databases
need
to
run
workloads
on
behalf
of
multiple
tenants
simultaneously
the
abundant
disk-based
storage
in
these
systems
is
usually
complemented
by
smaller
but
much
faster
cache
cache
allocation
strategies
are
required
that
speed
up
the
overall
workload
while
being
fair
to
each
tenant
17
then
we
highlight
number
of
possible
research
directions
we
start
with
the
observation
that
even
if
there
exist
several
definitions
and
models
for
representing
fairness
coming
from
different
research
perspectives
these
definitions
and
models
are
many
times
somewhat
incomparable
hindering
consistent
understanding
and
treatment
compiling
existing
definitions
to
produce
new
ones
and
evaluating
their
suitability
in
different
domains
and
applications
appears
to
be
an
open
topic
for
further
research
fairness
in
recommendations
is
multi-sided
achieving
fairness
for
all
parties
involved
is
also
topic
that
needs
to
be
investigated
further
presenters
evaggelia
pitoura
is
prof
at
the
univ
of
ioannina
greece
where
she
also
leads
the
distributed
management
of
data
laboratory
she
received
her
phd
degree
from
purdue
univ
usa
her
research
interests
are
in
the
area
of
data
management
systems
with
recent
emphasis
on
social
networks
and
responsible
data
management
her
publications
include
more
than
150
articles
in
international
journals
including
tods
tkde
pvldb
and
conferences
including
sigmod
icde
www
and
highly-cited
book
on
mobile
computing
her
research
has
been
funded
by
the
ec
and
national
sources
she
has
served
or
serves
on
the
editorial
board
of
acm
tods
vldbj
tkde
dapd
and
as
group
leader
senior
pc
member
or
co-chair
of
many
international
conferences
including
pc
chair
of
edbt
2016
and
icde
2012
she
has
more
than
20
years
experience
in
teaching
prior
tutorials
temporal
graphs
ebiss
17
social
graphs
bigdat
15
data
graphs
summersoc
14
personalization
icde
10
mobile
computing
icde
03
pervasive
computing
icde
00
georgia
koutrika
is
research
director
at
athena
research
center
in
greece
she
has
more
than
15
years
of
experience
in
multiple
roles
at
hp
labs
ibm
almaden
and
stanford
building
innovative
solutions
for
recommendations
data
analytics
and
exploration
her
work
has
been
incorporated
in
commercial
products
described
in
granted
patents
and
18
patent
applications
in
the
us
and
worldwide
and
published
in
more
than
80
papers
in
top-tier
conferences
and
journals
she
is
an
acm
distinguished
speaker
and
associate
editor
for
tkde
and
pvldb
she
has
served
or
653
serves
as
pc
member
or
co-chair
of
many
conferences
including
demo
pc
chair
of
acm
sigmod
2018
and
general
chair
of
acm
sigmod
2016
prior
tutorials
recommender
systems
sigmod
18
edbt
18
icde
15
personalization
icde
10
icde
07
vldb
05
22
martin
2018
how
social
media
has
changed
how
we
consume
news
forbes
2018
https://www.forbes.com/sites/nicolemartin1/2018/11/
30
how-social-media-has-changed-how-we-consume-news
18ae4c093c3c
23
ninareh
mehrabi
fred
morstatter
nripsuta
saxena
kristina
lerman
and
aram
galstyan
2019
survey
on
bias
and
fairness
in
machine
learning
corr
abs
1908.09635
2019
24
eirini
ntoutsi
kostas
stefanidis
kjetil
nørvåg
and
hans-peter
kriegel
2012
fast
group
recommendations
by
applying
user
clustering
in
er
126
140
25
gourab
patro
abhijnan
chakraborty
niloy
ganguly
and
krishna
gummadi
2020
incremental
fairness
in
two-sided
market
platforms
on
smoothly
updating
recommendations
in
aaai
26
geoff
pleiss
manish
raghavan
felix
wu
jon
kleinberg
and
kilian
weinberger
2017
on
fairness
and
calibration
in
advances
in
neural
information
processing
systems
30
guyon
luxburg
bengio
wallach
fergus
vishwanathan
and
garnett
eds
curran
associates
inc
5680
5689
27
dimitris
sacharidis
2019
top-n
group
recommendations
with
fairness
in
sac
1663
1670
28
babak
salimi
luke
rodriguez
bill
howe
and
dan
suciu
2019
interventional
fairness
causal
database
repair
for
algorithmic
fairness
in
sigmod
793
810
29
dimitris
serbos
shuyao
qi
nikos
mamoulis
evaggelia
pitoura
and
panayiotis
tsaparas
2017
fairness
in
package-to-group
recommendations
in
www
371
379
30
ashudeep
singh
and
thorsten
joachims
2018
fairness
of
exposure
in
rankings
in
kdd
2219
2228
31
harald
steck
2018
calibrated
recommendations
in
recsys
154
162
32
maria
stratigi
haridimos
kondylakis
and
kostas
stefanidis
2018
fairgrecs
fair
group
recommendations
by
exploiting
personal
health
information
in
dexa
147
155
33
maria
stratigi
jyrki
nummenmaa
evaggelia
pitoura
and
kostas
stefanidis
2020
fair
sequential
group
recommendations
in
sac
34
latanya
sweeney
2013
discrimination
in
online
ad
delivery
commun
acm
56
2013
44
54
35
virginia
tsintzou
evaggelia
pitoura
and
panayiotis
tsaparas
2019
bias
disparity
in
recommendation
systems
in
rmse
36
sahil
verma
and
julia
rubin
2018
fairness
definitions
explained
in
fairware
37
ke
yang
and
julia
stoyanovich
2017
measuring
fairness
in
ranked
outputs
in
ssdm
22
22
38
sirui
yao
and
bert
huang
2017
beyond
parity
fairness
objectives
for
collaborative
filtering
in
nips
2921
2930
39
sirui
yao
and
bert
huang
2017
new
fairness
metrics
for
recommendation
that
embrace
differences
fat
ml
2017
40
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
megahed
and
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
in
cikm
1569
1578
41
meike
zehlike
and
carlos
castillo
2018
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
corr
abs
1805.08716
2018
42
meike
zehlike
gina-theresa
diehn
and
carlos
castillo
2020
reducing
disparate
exposure
in
ranking
learning
to
rank
approach
in
www
43
richard
zemel
yu
wu
kevin
swersky
toniann
pitassi
and
cynthia
dwork
2013
learning
fair
representations
in
icml
325
333
44
ziwei
zhu
xia
hu
and
james
caverlee
2018
fairness-aware
tensor-based
recommendation
in
cikm
1153
1162
kostas
stefanidis
is
an
assoc
professor
on
data
science
at
the
tampere
university
finland
he
got
his
phd
in
personalized
data
management
from
the
univ
of
ioannina
greece
his
research
interests
lie
in
the
intersection
of
databases
information
retrieval
data
mining
and
the
web
and
include
personalization
and
recommender
systems
and
large-scale
entity
resolution
and
information
integration
his
publications
include
more
than
80
papers
in
peer-reviewed
conferences
and
journals
including
sigmod
icde
and
acm
tods
and
book
on
entity
resolution
in
the
web
of
data
he
has
years
experience
in
teaching
prior
tutorials
recommender
systems
mumia
training
school
14
personalization
icde
10
entity
resolution
icde
17
eswc
16
www
14
cikm
13
references
sihem
amer-yahia
senjuti
basu
roy
ashish
chawla
gautam
das
and
cong
yu
2009
group
recommendation
semantics
and
efficiency
pvldb
2009
754
765
pranjal
awasthi
matthäus
kleindessner
and
jamie
morgenstern
2019
effectiveness
of
equalized
odds
for
fair
classification
under
imperfect
group
information
corr
abs
1906.03284
2019
richard
berk
hoda
heidari
shahin
jabbari
matthew
joseph
michael
kearns
jamie
morgenstern
seth
neel
and
aaron
roth
2017
convex
framework
for
fair
regression
corr
abs
1706.02409
2017
alex
beutel
jilin
chen
tulsee
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
and
cristos
goodrow
2019
fairness
in
recommendation
ranking
through
pairwise
comparisons
in
kdd
2212
2220
asia
biega
krishna
gummadi
and
gerhard
weikum
2018
equity
of
attention
amortizing
individual
fairness
in
rankings
in
sigir
405
414
rodrigo
borges
and
kostas
stefanidis
2019
enhancing
long
term
fairness
in
recommendations
with
variational
autoencoders
in
medes
95
102
robin
burke
2017
multisided
fairness
for
recommendation
corr
abs
1707.00093
2017
robin
burke
nasim
sonboli
and
aldo
ordonez-gauger
2018
balanced
neighborhoods
for
multi-sided
fairness
in
recommendation
in
fat
202
214
elisa
celis
amit
deshpande
tarun
kathuria
and
nisheeth
vishnoi
2016
how
to
be
fair
and
diverse
corr
abs
1610.07183
2016
10
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
and
richard
zemel
2012
fairness
through
awareness
in
innovations
in
theoretical
computer
science
214
226
11
angwin
et
al
2016
machine
bias
propublica
2016
https://www.propublica.
org
article
machine-bias-risk-assessments-in-criminal-sentencing
12
kiran
garimella
gianmarco
de
francisci
morales
aristides
gionis
and
michael
mathioudakis
2018
reducing
controversy
by
connecting
opposing
views
in
ijcai
5249
5253
13
matthew
joseph
michael
kearns
jamie
morgenstern
and
aaron
roth
2016
fairness
in
learning
classic
and
contextual
bandits
in
nips
325
333
14
evangelia
kalyvianaki
marco
fiscato
theodoros
salonidis
and
peter
pietzuch
2016
themis
fairness
in
federated
stream
processing
under
overload
in
sigmod
541
553
15
toshihiro
kamishima
shotaro
akaho
hideki
asoh
and
jun
sakuma
2018
recommendation
independence
in
fat
187
201
16
matthew
kay
cynthia
matuszek
and
sean
munson
2015
unequal
representation
and
gender
stereotypes
in
image
search
results
for
occupations
in
chi
3819
3828
17
mayuresh
kunjir
brandon
fain
kamesh
munagala
and
shivnath
babu
2017
robus
fair
cache
allocation
for
data-parallel
workloads
in
sigmod
219
234
18
matt
kusner
joshua
loftus
chris
russell
and
ricardo
silva
2017
counterfactual
fairness
in
nips
4066
4076
19
jurek
leonhardt
avishek
anand
and
megha
khosla
2018
user
fairness
in
recommender
systems
in
www
101
102
20
xiao
lin
min
zhang
yongfeng
zhang
zhaoquan
gu
yiqun
liu
and
shaoping
ma
2017
fairness-aware
group
recommendation
with
pareto-efficiency
in
recsys
107
115
21
yang
liu
goran
radanovic
christos
dimitrakakis
debmalya
mandal
and
david
parkes
2017
calibrated
fairness
in
bandits
corr
abs
1707.01875
2017
654