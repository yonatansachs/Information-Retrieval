journal
artificial
intelligence
research
submitted
submitted
11
21
published
framework
fairness
systematic
review
existing
fair
ai
solutions
brianna
richardson
juan
gilbert
richardsonb@ufl.edu
juan@ufl.edu
arxiv
2112
05700v1
cs
ai
10
dec
2021
university
florida
gainesville
fl
32601
usa
abstract
world
daily
emerging
scientific
inquisition
discovery
prolific
launch
machine
learning
across
industries
comes
little
surprise
familiar
potential
ml
neither
congruent
expansion
ethics-focused
research
emerged
response
issues
bias
unfairness
stemmed
applications
fairness
research
focuses
techniques
combat
algorithmic
bias
now
supported
ever
large
portion
fairness
research
gone
producing
tools
machine
learning
practitioners
can
use
audit
bias
designing
algorithms
nonetheless
lack
application
fairness
solutions
practice
systematic
review
provides
in-depth
summary
algorithmic
bias
issues
defined
fairness
solution
space
proposed
moreover
review
provides
in-depth
breakdown
caveats
solution
space
arisen
since
release
taxonomy
needs
proposed
machine
learning
practitioners
fairness
researchers
institutional
stakeholders
needs
organized
addressed
parties
influential
implementation
includes
fairness
researchers
organizations
produce
ml
algorithms
machine
learning
practitioners
findings
can
used
future
bridge
gap
practitioners
fairness
experts
inform
creation
usable
fair
ml
toolkits
introduction
today
applications
machine
learning
ml
artificial
intelligence
ai
can
found
nearly
every
domain
jordan
mitchell
2015
medicine
healthcare
goldenberg
et
al
2019
chang
et
al
2018
lin
et
al
2020
munavalli
et
al
2020
banking
finance
sunikka
et
al
2011
choi
lee
2018
moysan
zeitoun
2019
rate
grown
exponentially
last
decade
companies
seizing
opportunity
automate
perfect
procedures
field
healthcare
machine
learning
used
diagnose
treat
prostate
cancer
goldenberg
et
al
2019
perform
robotic
surgeries
chang
et
al
2018
organize
schedule
patients
munavalli
et
al
2020
digitize
electronic
health
record
data
lin
et
al
2020
within
banking
finance
machine
learning
used
personalize
recommendations
consumers
sunikka
et
al
2011
detect
prevent
instances
fraud
choi
lee
2018
provide
faster
personalized
services
use
chatbots
moysan
zeitoun
2019
machine
learning
praised
ability
speed
time-consuming
processes
automate
mundane
procedures
improve
accuracy
performance
tasks
also
praised
ability
remain
neutral
void
human
bias
sandvig
submitted
ai
access
foundation
rights
reserved
richardson
gilbert
2014
calls
neutrality
fallacy
common
misconception
ai
perpetuate
trends
data
often
clouded
human
biases
reason
machine
learning
continues
undergo
heavy
scrutiny
role
plays
furthering
social
inequities
past
years
major
companies
headlined
ai
technologies
cause
harms
consumers
buolamwini
gebru
2018
assessed
commercially-used
facial
recognition
systems
found
darker-skinned
women
misclassified
demographic
group
noble
2018
found
search
engines
perpetuated
stereotypes
contributed
substantially
representational
harms
angwin
et
al
2016
found
biases
automated
recidivism
scores
given
criminal
offenders
circumstances
led
surplus
contributions
solution
space
algorithmic
bias
stakeholders
researchers
alike
zhong
2018
institutions
quick
formulate
ethics
codes
centered
around
concepts
like
fairness
transparency
accountability
trust
jobin
et
al
2019
new
conferences
like
acm
fairness
accountability
transparency
facct
conference1
new
workshops
emerged
centered
concepts
furthermore
machine
learning
conferences
new
tracks
added
focus
algorithmic
bias
research
surplus
recognition
stakeholders
computer
science
researchers
encouraged
commensurate
rise
related
contributions
improve
ethical
concerns
surrounding
ai
one
major
objective
responsible
ai
researchers
creation
fairness
tools
ml
practitioners
across
domains
can
use
application
responsible
ethical
ai
tools
translate
top-tier
research
responsible
ai
space
actionable
procedures
functions
practitioners
can
implement
pipelines
despite
number
currently
existing
tools
still
lack
application
ethical
ai
found
industry
recent
research
suggests
disconnect
fairness
ai
researchers
creating
tools
ml
practitioners
meant
apply
holstein
et
al
2019
law
et
al
2020b
madaio
et
al
2020
law
et
al
2020a
veale
binns
2017
greene
et
al
2019
emphasizes
solution
problem
lies
intersection
technical
design
expertise
responsible
fair
ai
must
undergo
deliberate
design
procedures
match
needs
practitioners
satisfy
technical
dilemmas
found
bias
research
survey
paper
will
highlight
users
expectations
engaging
fair
ai
tooling
will
summarize
fairness
concerns
design
flaws
discussed
literature
first
section
focuses
algorithmic
bias
major
entry
points
bias
perpetuate
need
fairness
research
fairness
tools
section
focuses
solutions
posed
thus
far
fairness
researchers
will
highlight
major
contributions
features
offered
section
will
focus
solutions
working
practice
will
highlight
many
drawbacks
provide
recommendations
fairness
experts
organizations
ml
practitioners
paper
first
kind
provide
comprehensive
review
solution
space
fair
ai
problem
space
algorithmic
bias
first
recorded
case
algorithmic
bias
discrimination
suit
filed
st
george
hospital
medical
school
1980
lowry
macpherson
1988
leaders
admission
program
decided
create
algorithm
mimic
admission
acm
facct
conference
https://facctconference.org/
framework
fairness
figure
taxonomy
currently
existing
forms
bias
process
completing
first
screening
applicants
upon
completion
algorithm
validated
performance
comparing
results
manually
generated
decisions
found
90
95
similarity
years
circulation
staff
began
notice
trends
admission
brought
concerns
attention
school
internal
review
board
irb
irb
agreed
correlation
machine
scores
human
scores
delegitimized
claims
bias
claims
taken
u.k
commission
racial
equity
thorough
analysis
algorithm
commission
confirmed
claims
true
algorithm
placing
value
applicant
names
place
birth
penalizing
individual
non-caucasian
sounding
names
lowry
macpherson
1988
ruling
committee
pivotal
start
algorithmic
bias
research
since
set
precedent
inclusion
bias
system
even
sake
accuracy
impermissible
nonetheless
past
40
years
issue
recurring
biases
continue
emerge
creation
use
machine
learning
legitimizing
unfair
biased
practices
across
multitude
industries
lum
isaac
2016
often-cited
2017
presentation
neurips
crawford
2017
discusses
two
potential
harms
algorithmic
bias
representational
allocative
harms
representational
harms
problems
might
arise
troublesome
ways
certain
populations
represented
feature
space
allocative
harms
problems
arise
decisions
allocated
certain
populations
crawford
2013
literature
demonstrated
bias
can
arise
shapes
forms
task
fairness
experts
organize
confront
biases
richardson
gilbert
friedman
nissenbaum
1996
separates
types
biases
three
categories
preexisting
technical
emergent
biases
define
pre-existing
biases
rooted
institutions
practices
attitudes
technical
biases
stem
technical
constraints
issues
stemming
technical
design
algorithm
lastly
emergent
biases
arise
context
real
use
user
friedman
nissenbaum
1996
section
will
adopt
categorization
classify
influences
bias
found
literature
visual
depiction
forms
biases
will
explained
can
seen
figure
2.0
pre-existing
bias
large
portion
literature
attention
gone
pre-existing
biases
biases
associated
individual
institutions
human
preferences
societal
stereotypes
influence
data
model
said
effect
pre-existing
bias
saying
garbage
garbage
well-known
data
science
community
euphemism
quality
data
give
system
will
quality
results
fairness
researchers
translate
mean
machine
learning
models
serve
feedback
loop
reflecting
biases
fed
barocas
et
al
2019
bias
can
come
assortium
stages
machine
learning
pipeline
prominently
discussed
form
bias
historical
bias
biases
perpetuated
data
result
issues
existed
time
veale
binns
2017
calders
liobaite
2013
olteanu
et
al
2019
barocas
et
al
2019
rovatsos
et
al
2019
suresh
guttag
2019
guszcza
2018
hellstro
et
al
2020
suresh
guttag
2019
defines
historical
biases
arise
misalignment
world
values
model
perpetuates
impact
historical
bias
can
reflected
data
collected
data
collected
quality
data
even
labels
given
samples
suresh
guttag
2019
rovatsos
et
al
2019
barocas
selbst
2018
calders
liobaite
2013
hellstro
et
al
2020
subjectivity
labels
major
concern
reproduces
normalizes
intentional
unintentional
biases
original
labeler
barocas
selbst
2018
kasy
abebe
2021
analyzing
large
collection
ml
publications
geiger
et
al
2019
found
dire
need
normalized
rigorous
standards
evaluating
datasets
data
collection
strategies
prior
machine
learning
processing
aligning
recommendations
crawford
2013
besides
historical
biases
also
exists
data
collection
biases
biases
stem
selection
methods
used
data
sources
olteanu
et
al
2019
data
collection
bias
umbrellas
variety
different
biases
recognized
literature
including
sampling
bias
representation
bias
sampling
bias
consists
issues
arise
non-random
sampling
subgroups
might
lead
over-sampling
certain
populations
mehrabi
et
al
2021
hellstro
et
al
2020
historically
vulnerable
populations
often
undersampled
veale
binns
2017
mester
2017
discusses
two
types
sampling
bias
selection
bias
self-selection
bias
selection
bias
focuses
biases
data
collector
self-selection
bias
stems
individuals
available
willing
participate
data
collection
process
mester
2017
concerns
sampling
bias
stem
fact
inferences
made
mis-balanced
dataset
will
inevitably
lead
disadvantage
populations
missampled
asaro
2019
framework
fairness
representation
bias
also
called
data
bias
olteanu
et
al
2019
another
form
data
collection
bias
consists
bias
arises
insufficient
representation
subgroups
within
dataset
crawford
2017
suresh
guttag
2019
misrepresentation
can
stem
subjective
selection
unfitting
attributes
incomplete
collection
data
calders
liobaite
2013
veale
binns
2017
describes
issue
detail
stating
subgroups
can
contain
nuanced
patterns
difficult
impossible
quantify
dataset
resulting
models
misrepresent
populations
another
prominent
form
pre-existing
bias
data
processing
bias
bias
introduced
data
processing
procedures
olteanu
et
al
2019
selection
methods
across
ml
pipeline
can
incredibly
subjective
many
steps
can
introduce
unintended
biases
model
example
use
sensitive
attributes
within
models
well-known
taboo
data
science
government
regulation
even
gone
far
prohibiting
organizations
access
protected
data
veale
binns
2017
nonetheless
use
proxies
attributes
encode
sensitive
information
prolific
calders
liobaite
2013
corbett-davies
goel
2018
besides
use
sensitive
data
processing
steps
incorporate
bias
including
categorization
data
veale
binns
2017
feature
selection
feature
engineering
veale
binns
2017
barocas
selbst
2018
even
data
evaluated
suresh
guttag
2019
evaluation
bias
can
occur
use
performance
metrics
ill-fitting
given
model
suresh
guttag
2019
final
category
pre-existing
bias
systematic
bias
bias
stems
institutional
governmental
regulations
procedures
regulations
limiting
access
sensitive
data
can
beneficial
limiting
use
sensitive
data
can
also
prevent
identification
de-biasing
proxy
attributes
leading
problematic
models
veale
binns
2017
funding
can
also
produce
bias
manipulating
priorities
institutions
therefore
practitioners
mester
2017
2.0
technical
bias
pre-existing
biases
consider
model
adopting
data
technical
biases
consider
limitations
computers
technology
insufficiencies
might
create
bias
friedman
nissenbaum
1996
baeza-yates
2018
considers
algorithmic
bias
defines
bias
present
data
added
algorithm
overlapping
pre-existing
bias
technical
bias
can
also
stem
processing
procedures
often
selection
features
models
training
procedures
can
introduce
bias
affiliated
practitioner
insufficiency
procedures
characterize
data
example
hyperparameter
tuning
may
attempt
reduce
sparsity
model
end
removing
distinct
patterns
found
sub-populations
veale
binns
2017
hellstro
et
al
2020
furthermore
certain
models
regression
fail
capture
correlation
attributes
subgroups
veale
binns
2017
skitka
et
al
1999
computer
tools
bias
another
form
technical
bias
originates
limitations
statistics
technology
friedman
nissenbaum
1996
simpson
paradox
statistical
issue
arises
aggregation
distinct
subgroup
data
produces
misperformance
one
subgroups
blyth
1972
suresh
guttag
2019
richardson
gilbert
lastly
friedman
nissenbaum
1996
defines
formulation
human
constructs
bias
stems
inability
technology
properly
grasp
human
constructs
situations
fairness
experts
often
insist
practitioners
organizations
reconsider
use
machine
learning
rakova
et
al
2020
selbst
et
al
2019
describes
effects
bias
ripple
effect
trap
describe
use
technology
areas
human
constructs
bias
exists
can
potentially
change
human
values
2.0
emerging
bias
final
category
bias
referred
suresh
guttag
2019
deployment
bias
defined
biases
result
deployment
model
emerging
bias
can
emerge
two
different
forms
via
population
bias
use
bias
population
bias
stems
insufficiency
model
represent
population
society
deployment
olteanu
et
al
2019
can
stem
new
knowledge
emerged
made
model
irrelevant
friedman
nissenbaum
1996
mismatch
sample
population
used
train
model
population
impacted
deployment
friedman
nissenbaum
1996
calders
liobaite
2013
rovatsos
et
al
2019
selbst
et
al
2019
refers
failure
consider
model
works
context
portability
trap
use
bias
umbrellas
variety
different
ways
use
models
can
create
biases
study
conducted
skitka
et
al
1999
participants
interacted
autonomous
aid
provided
recommendations
simulated
flight
task
results
depicted
trends
commission
user
agreed
recommendation
even
contradicted
training
skitka
et
al
1999
neutrality
fallacy
introduced
sandvig
2014
major
pitfall
ai
application
forces
users
believe
tool
correct
even
obvious
sandvig
2014
skitka
et
al
1999
suggests
users
less
willing
challenge
decision
decision
made
algorithm
related
form
use
bias
presentation
bias
bias
stems
model
deployed
baeza-yates
2018
situations
users
make
assumptions
presentation
format
algorithms
may
come
incorrect
conclusions
example
search
algorithm
many
users
attribute
rank
result
relevancy
may
accurate
cases
baeza-yates
2018
belief
bias
also
form
use
bias
occurs
someone
sure
ignore
results
mester
2017
can
also
impact
choose
use
results
model
choose
ignore
veale
et
al
2018
particularly
model
decision
counteracts
beliefs
veale
et
al
2018
discusses
decision-support
design
can
used
prevent
selectional
belief
bias
gamification
another
use
bias
person
learns
manipulate
algorithm
veale
et
al
2018
ciampaglia
et
al
2017
introduces
concept
popularity
bias
preferences
strongly
correlate
popularity
discusses
forms
bias
often
gamified
internet
bots
fake
feedback
final
case
use
bias
curse
knowledge
bias
assume
someone
background
can
use
model
appropriately
mester
2017
concern
appears
various
literature
surrounding
predictive
policing
technologies
asaro
2019
framework
fairness
bennett
moses
chan
2018
ridgeway
2013
critics
voice
concerns
whether
police
understand
limitations
technologies
using
appropriately
guszcza
2018
details
importance
understanding
environment
machine
learning
tools
will
used
creating
environments
mind
solution
space
fairness
technologies
solution
space
algorithmic
bias
consists
diverse
array
solutions
contribute
responsible
ai
space
including
explainability
transparency
interpretability
accountability
cheng
et
al
2021
solution
diverged
different
ethical
dilemma
emerged
machine
learning
production
pipeline
objective
remains
produce
fair
ai
ai
free
unintentional
algorithmic
bias
fairness
defined
mehrabi
et
al
2021
absence
prejudice
favoritism
toward
individual
group
based
inherent
acquired
characteristics
fair
ai
purpose
paper
consists
solutions
combat
algorithmic
bias
often
inclusive
top-tier
solutions
explainability
transparency
interpretability
accountability
research
since
problem
space
algorithmic
bias
large
diverse
concerted
effort
gone
making
fairness
tools
practitioners
can
use
employ
state-of-the
art
fairness
techniques
within
ml
pipelines
solutions
mainly
come
two
forms
software
toolkits
checklists
toolkits
serve
functions
accessible
via
programming
languages
can
used
detect
mitigate
biases
checklists
extensive
guides
created
fairness
experts
ml
ai
practitioners
can
use
ensure
inclusion
ethical
thought
throughout
pipelines
section
won
provide
complete
description
solution
space
will
highlight
diversity
tools
provided
organizations
academic
institutions
3.1
software
toolkits
variety
software
toolkits
currently
exist
overlapping
distinct
characteristics
identify
accessible
via
website
portals
saleiro
et
al
2018
many
exist
installable
packages
can
imported
python
programs
bird
et
al
2020
srinivasan
2020
johnson
et
al
2020
wexler
et
al
2019
languages
used
ml
practitioners
bellamy
et
al
2018
vasudevan
kenthapadi
2020
following
selection
toolkits
provides
diverse
look
available
solutions
3.1
google
toolkits
google
provides
two
relevant
toolkits
fairness
indicators
what-if
toolkit
wit
wexler
et
al
2019
toolkits
function
interactive
widgets
users
can
provide
model
performance
fairness
metrics
choice
attributes
slicing
evaluating
will
take
place
richardson
et
al
2021
toolkits
embedded
tensorflow
fairness
evaluation
visualization
toolkit
https://github.com/tensorflow/fairnessindicators
richardson
gilbert
within
tensorflow
package
python
users
can
build
custom
prediction
functions
model
exists
outside
tensorflow
unlike
toolkits
google
toolkits
require
users
provide
model
seng
et
al
2021
additionally
toolkits
allow
model
comparison
models
fairness
indicators
works
binary
multiclass
problems
allows
intersectional
analysis
wit
works
binary
regression
problems
within
fairness
indicators
users
can
create
visualizations
compare
models
across
different
performance
fairness
metrics
users
can
select
deselect
performance
metrics
focus
wit
includes
features
overview
performance
chart
data
point
editor
features
overview
provides
visualizations
depict
distribution
feature
summary
statistics
performance
chart
provides
simulated
results
depicting
outcome
select
fairness
transformations
including
option
customize
fairness
solution
lastly
data
point
editor
provides
custom
visualizations
data
points
data
allows
users
compare
counterfactual
points
make
modifications
points
see
results
change
plot
partial
dependence
plots
determine
model
sensitivity
feature
3.1
uchicago
aequitas
aequitas
audit
report
toolkit
can
accessed
via
command
line
python
package
web
application
saleiro
et
al
2018
aequitas
tool
built
classification
models
allows
comparison
models
aequitas
provide
fairness
mitigation
techniques
evaluate
models
based
commonly
used
fairness
criteria
users
can
label
sensitive
attributes
aequitas
will
calculate
group
metrics
sensitive
subgroups
provide
disparity
scores
reflect
degree
unfairness
aequitas
also
produces
plots
record
fairness
disparities
group
metrics
within
visualizations
users
can
choose
groups
colored
based
whether
subgroup
disparities
pass
set
threshold
fair
unfair
saleiro
et
al
2018
tool
built
two
types
users
mind
data
scientists
ai
researchers
building
ai
tools
policymakers
approving
use
practice
saleiro
et
al
2018
3.1
ibm
ai
fairness
360
ai
fairness
360
aif360
toolkit
provides
fairness
detection
mitigation
strategies
bellamy
et
al
2018
can
used
python
fairness
metrics
provided
include
general
performance
metrics
group
fairness
metrics
individual
fairness
metrics
users
also
assortment
mitigation
strategies
can
apply
models
mitigation
strategies
can
applied
across
variety
stages
pipeline
including
pre-processing
processing
post-processing
order
apply
strategies
users
can
distinguish
privileged
unprivileged
subgroups
complete
analysis
assist
users
learning
website
includes
wide
selection
tutorials
web
demos
bellamy
et
al
2018
toolkit
provide
visualization
functions
provide
guidance
functions
can
used
conjunction
local
interpretable
model-agnostic
explanations
toolkit
aka
lime
ribeiro
et
al
2016
similar
aequitas
aif360
built
business
users
ml
developers
mind
bellamy
et
al
2018
framework
fairness
3.1
linkedin
fairness
toolkit
lift
lift
recently
released
toolkit
provides
fairness
detection
strategies
measuring
fairness
across
variety
metrics
vasudevan
kenthapadi
2020
provide
assortment
metrics
segregated
based
whether
data
model
outputs
similar
ai
fairness
360
produce
visualizations
unlike
previous
toolkits
built
applied
scala
spark
programs
furthermore
toolkit
unique
flexibility
scalability
overcoming
previous
issues
measuring
fairness
large
datasets
vasudevan
kenthapadi
2020
3.1
solutions
assortment
toolkits
exist
including
microsoft
fairlearn
bird
et
al
2020
ml
fairness
gym
srinivasan
2020
scikit
fairness
tool
johnson
et
al
2020
pymetrics
audit-ai
new
ones
arise
often
toolkits
may
differ
fairness
metrics
consider
fairness
mitigation
strategies
may
employ
highlights
features
can
seen
toolkits
interactive
provide
visual
support
allow
intersectional
analysis
focus
detection
mitigation
alone
etc
3.2
checklist
unlike
toolkits
checklists
mostly
exist
documentation
meant
guide
readers
incorporating
ethical
responsible
ai
throughout
lifecycle
projects
specifically
meant
data
scientists
machine
learning
engineers
building
tools
checklists
generally
written
relevant
parties
mind
toolkits
provide
statistical
support
comes
strategies
detect
mitigate
checklists
engage
developers
questions
tasks
effectively
ensure
ethical
thought
occurs
idea
formulation
auditing
deployment
patil
et
al
2018
3.2
deon
deon
fairness
checklist
created
drivendata
data
scientist-led
company
utilizes
crowdsourcing
cutting-edge
technology
tackle
predictive
problems
societal
impact
checklist
described
default
toolkit
customizations
made
practitioners
domain-specific
concerns
deon
split
five
areas
data
collection
data
storage
analysis
modeling
deployment
assist
users
utilizing
checklist
checklist
item
accompanied
use
cases
exemplify
relevant
concerns
one
deon
unique
features
command
line
interface
practitioners
can
use
interact
checklist
3.2
microsoft
ai
fairness
checklist
madaio
et
al
2020
produced
ai
fairness
checklist
unlike
checklists
checklist
co-designed
iterative
feedback
practitioners
checklist
split
different
parts
envision
define
prototype
build
launch
evolve
envision
define
prototype
provide
ethical
considerations
fit
best
initial
planning
pymetric
audit-ai
https://github.com/pymetrics/audit-ai
driven
data
deon
tool
https://deon.drivendata.org/
richardson
gilbert
designing
stages
project
build
provides
guidance
ai
creation
phases
launch
evolve
provide
guidance
deployment
stages
product
also
comes
comprehensive
preamble
introduces
complexity
fairness
provides
instructions
checklist
used
encourages
practitioners
teams
personalize
customize
checklist
best
fit
environment
madaio
et
al
2020
3.2
legal
ethics
checklist
lifshitz
mcmaster
2020
provides
unique
ethics
checklist
focuses
legal
considerations
noted
creation
ai
unlike
related
checklists
checklist
sectioned
stages
ai
lifecycle
legal
priorities
including
human
agency
oversight
security
safety
privacy
data
governance
transparency
accessibility
etc
checklist
provides
unique
viewpoint
lawyer
institutions
practitioners
use
avoid
running
legal
issues
lifshitz
mcmaster
2020
3.2
ibm
ai
factsheets
ai
factsheets
unique
guide
provides
methodology
incorporating
responsibility
transparency
ai
pipeline
via
documentation
ai
factsheets
described
checklist
provide
guidance
create
documentation
depicts
responsible
ai
practices
creation
factsheet
template
domain-dependent
arnold
et
al
2019
provides
detailed
methodology
factsheet
can
customized
project
satisfying
components
factsheets
institutions
teams
can
engender
trust
consumers
increasing
transparency
ensuring
products
satisfy
necessary
ethical
considerations
arnold
et
al
2019
checklists
provide
overview
differing
features
can
found
across
checklist
landscape
focus
legality
domain
specific
others
built
ai
pipeline
mind
checklists
guides
produced
well
bradley
et
al
2020
manders-huits
zimmer
2009
gebru
et
al
2018
mitchell
et
al
2019
similarly
structured
intended
assist
practitioners
implementing
fair
responsible
practices
pipelines
shortcomings
solutions
recommendations
future
solutions
posed
pivotal
starting
point
fairness
research
much
work
done
addressing
pitfalls
contributions
well
section
aims
discuss
prominent
issues
arisen
literature
corresponding
recommendations
made
future
work
many
recommendations
made
fairness
researchers
designers
also
recommendations
organizations
ml
practitioners
4.1
recommendations
fairness
experts
towards
goal
universally
ethical
ai
much
responsibility
lies
hands
fair
ai
researchers
define
design
translate
fairness
palatable
procedure
can
easily
applied
practitioners
institutions
requires
process
producing
fair
ai
undergo
human-centered
research
design
procedure
nonetheless
10
framework
fairness
previous
research
found
exist
major
gaps
practitioners
fairness
experts
indicating
lack
communication
producing
fair
ai
intended
apply
holstein
et
al
2019
law
et
al
2020b
madaio
et
al
2020
law
et
al
2020a
veale
binns
2017
results
works
thematically
categorized
accompanied
suggestions
improvement
4.1
conflicting
fairness
metrics
along
identifying
23
different
types
biases
different
categories
discrimination
mehrabi
et
al
2021
identified
10
different
types
fairness
metrics
due
vast
quantity
similarity
metrics
many
authors
tried
categorize
related
bins
mehrabi
et
al
2021
identified
three
different
types
fairness
metrics
individual
group
subgroup
individual
fairness
gives
similar
predictions
similar
individuals
group
fairness
treats
different
groups
equally
subgroup
fairness
attempts
achieve
balance
group
individual
fairness
mehrabi
et
al
2021
barocas
et
al
2019
also
identified
19
proposed
fairness
metrics
also
categorized
three
categories
independence
separation
sufficiency
barocas
et
al
2019
defines
categories
properties
joint
distribution
sensitive
attribute
target
variable
classifier
score
random
variable
satisfies
independence
random
variable
satisfies
separation
random
variable
satisfies
sufficiency
barocas
et
al
2019
lastly
comprehensive
review
explanation
fairness
metrics
verma
rubin
2018
isolated
20
different
fairness
metrics
categorized
different
categories
statistical
measures
similarity
measures
causal
reasoning
statistical
measures
depend
true
positive
false
positive
false
negative
true
negative
similarity
based
measures
attempt
address
issues
ignored
statistical
measures
focusing
insensitive
attributes
well
causal
reasoning
uses
causal
graphs
draw
relations
attributes
determine
influence
outcome
allow
users
understand
exactly
bias
coming
whether
permissible
verma
rubin
2018
author
recognized
legitimate
patterns
within
metrics
works
highlight
major
issue
exists
many
metrics
measuring
unfairness
differences
delineate
mehrabi
et
al
2021
furthermore
major
trade-offs
exist
fairness
metrics
making
selection
metrics
highly
situationdependent
binns
2018
verma
rubin
2018
concludes
many
metrics
advanced
require
expert-level
input
produces
implicit
biases
furthermore
many
works
identified
conflicts
metrics
make
incompatible
forces
individuals
choose
berk
et
al
2021
friedler
et
al
2021
kleinberg
et
al
2017
mittelstadt
2019
rovatsos
et
al
2019
different
metrics
may
emphasize
different
aspects
performance
japkowicz
shah
2011
much
work
done
comparing
contrasting
metrics
garcia-gathright
et
al
2018
verma
rubin
2018
chouldechova
2017
corbett-davies
goel
2018
binns
2018
fish
et
al
2016
kilbertus
et
al
2017
simoiu
et
al
2017
cramer
et
al
2019
zliobaite
2015
decision
one
taken
lightly
act
choosing
fairness
metric
political
valorizes
one
point
view
silencing
another
bowker
11
richardson
gilbert
star
1999
friedler
et
al
2021
according
rovatsos
et
al
2019
task
choosing
right
metric
currently
lies
hands
practitioners
unfamiliar
fairness
research
heavy
expectation
considering
fact
society
whole
decided
ethical
standards
prioritize
support
issue
verma
rubin
2018
suggests
work
needed
clarify
definitions
appropriate
situations
friedler
et
al
2021
states
fairness
experts
must
explicitly
state
priorities
fairness
metric
ensure
practitioners
making
informed
choices
furthermore
friedler
et
al
2018
emphasizes
new
measures
fairness
introduced
metric
behaves
fundamentally
differently
already
proposed
4.1
metric-specific
pitfalls
addition
conflicting
nature
fairness
metrics
exist
several
pitfalls
recognized
researchers
kilbertus
et
al
2017
discussed
insufficiency
observational
criteria
criteria
often
used
sensitive
attributes
within
toolkits
emphasize
methodologies
unable
confirm
protected
attributes
direct
causal
influence
results
instead
propose
two
new
metrics
causal
reasoning
proxy
discrimination
unresolved
discrimination
kilbertus
et
al
2017
proxy
discrimination
can
potentially
inferred
causal
graph
exists
path
protected
attribute
predicted
attribute
includes
proxy
variable
unresolved
discrimination
can
inferred
causal
graph
path
protected
attribute
predicted
attribute
includes
resolving
variable
kilbertus
et
al
2017
furthermore
friedler
et
al
2018
found
many
fairness
metrics
lack
robustness
simply
modifying
dataset
composition
changing
train-test
splits
results
study
depicted
many
fairness
criteria
lacked
stability
proposed
measures
depict
stability
success
used
report
fairness
hoffmann
2019
discusses
lack
intersectional
analysis
fairness
criteria
many
metrics
provide
analysis
sensitive
attributes
one-dimensionally
hoffmann
2019
emphasizes
proposed
metrics
strategically
identify
multi-dimensional
correlations
attributes
outcomes
wagstaff
2012
emphasizes
use
abstract
metrics
distract
problems
specific
datasets
applications
furthermore
impact
metrics
inferred
scores
furthermore
kasy
abebe
2021
demonstrates
many
standard
metrics
legitimize
focus
merit
deterring
individuals
questioning
legitimacy
status
quo
additionally
concern
assumptions
made
fairness
experts
producing
fairness
metrics
many
metrics
available
fairness
tooling
depend
access
sensitive
attributes
many
practitioners
holstein
et
al
2019
law
et
al
2020a
rovatsos
et
al
2019
additionally
legal
restrictions
fairness
definitions
xiang
raji
2019
legal
access
sensitive
data
practitioners
concerned
public
scrutinize
use
sensitive
attributes
even
detection
bias
rovatsos
et
al
2019
furthermore
studying
political
philosophy
connects
fairness
machine
learning
binns
2018
notes
12
framework
fairness
risks
incomplete
fairness
analysis
arise
users
forced
adhere
static
set
prescribed
protected
classes
instead
thorough
analysis
identify
discrimination
combat
issue
law
et
al
2020a
suggests
coarse-grained
demographic
info
utilized
fairness
tools
proposed
practitioners
furthermore
fairness
experts
utilize
promote
fairness
metrics
rely
sensitive
attributes
holstein
et
al
2019
law
et
al
2020a
rovatsos
et
al
2019
4.1
oversimplification
fairness
major
concern
literature
emphasis
technical
solutions
algorithmic
bias
socio-technical
problem
many
authors
emphasize
need
supplement
statistical
definitions
social
practices
veale
et
al
2018
madaio
et
al
2020
verma
rubin
2018
fazelpour
lipton
2020
jacobs
wallach
2021
birhane
2021
madaio
et
al
2020
called
sole
use
technical
solutions
ethics
washing
selbst
et
al
2019
describes
failure
account
fact
fairness
solely
achieved
mathematical
formulation
formalism
trap
fazelpour
lipton
2020
details
practice
ethics
washing
can
lead
misguided
strategies
mitigating
bias
harcourt
2007
emphasizes
perceived
success
technical
solutions
stalls
pursuits
achieve
actual
fairness
aid
social
practices
birhane
2021
calls
fundamental
shift
towards
considering
relational
factors
impact
machine
learning
societal
standpoint
fazelpour
lipton
2020
recommends
mathematical
assessment
supplemented
social
assessment
tools
like
thorough
analysis
data
collection
strategies
understanding
assumptions
made
different
ml
algorithms
furthermore
toolkits
checklists
fair
evaluation
avoid
solely
providing
mathematical
solutions
4.1
misguided
fairness
objectives
addition
issue
ethics
washing
concerns
arisen
concerning
trajectory
fairness
research
hoffmann
2019
critiques
fairness
research
focus
avoiding
disadvantage
instead
understanding
advantage
stems
support
better
understanding
biases
exist
data
veale
binns
2017
proposes
energy
placed
data
exploration
use
unsupervised
learning
strategy
identify
hidden
patterns
futhermore
many
fairness
metrics
strategies
rely
assumptions
world
may
problematic
example
hu
kohler-hausmann
2020
discusses
use
social
concept
sex
can
perpetuate
problematic
harmful
sex
discrimination
4.1
making
fair
ai
applicable
works
done
holstein
et
al
2019
veale
et
al
2018
rakova
et
al
2020
richardson
et
al
2021
novel
inclusion
practitioner
feedback
fairness
literature
common
theme
emerged
papers
lack
applicability
practitioners
held
toward
fairness
tools
support
practitioners
interviewed
holstein
et
al
2019
emphasized
need
domain-specific
procedures
metrics
participants
requested
fairness
experts
pool
knowledge
resources
domain
easy
access
additionally
practitioners
study
veale
et
al
2018
concerns
13
richardson
gilbert
scalability
fairness
analysis
backed
data
scientists
verma
rubin
2018
practitioners
interviewed
rakova
et
al
2020
felt
metrics
academic-based
research
vastly
different
objectives
metrics
industry
required
industry
researchers
additional
work
translate
work
using
insufficient
metrics
supplement
workload
practitioners
felt
fairness
research
academia
fit
smoothly
industry
pipelines
required
significant
amount
energy
fully
implement
rakova
et
al
2020
study
richardson
et
al
2021
practitioners
opportunity
interact
fairness
tools
provide
comments
feedback
authors
summarized
key
themes
practitioners
regarding
features
design
considerations
make
tools
applicable
following
lists
contains
features
requested
fairness
toolkits
applicable
diverse
range
predictive
tasks
model
types
data
types
holstein
et
al
2019
can
detect
mitigate
bias
holstein
et
al
2019
olteanu
et
al
2019
mehrabi
et
al
2021
can
intervene
different
stages
ml
ai
life
cycle
bellamy
et
al
2018
holstein
et
al
2019
veale
binns
2017
fairness
performance
criteria
agnostic
corbett-davies
goel
2018
barocas
et
al
2019
verma
rubin
2018
diverse
explanation
types
ribeiro
et
al
2016
dodge
et
al
2019
arya
et
al
2019
binns
et
al
2018
provides
recommendations
next
steps
holstein
et
al
2019
well-supported
demos
tutorials
holstein
et
al
2019
results
studies
collecting
practitioner
feedback
supplement
many
data
scientists
social
scientists
confirmed
study
conducted
corbett-davies
et
al
2017
utilizing
fairness
metrics
recidivism
use
case
results
depicted
metrics
significant
trade-offs
public
safety
exemplified
importance
domain-specific
guides
details
comes
fairness
procedures
similar
landscape
summaries
algorithmic
bias
rovatsos
et
al
2019
reisman
et
al
2018
also
discuss
concerns
algorithmic
bias
differ
substantially
domain
application
yet
domains
lack
thorough
specific
algorithmic
bias
guidance
lastly
studying
use
fair
ai
checklists
madaio
et
al
2020
cramer
et
al
2018
emphasized
importance
aligning
checklists
team
workflows
4.1
designing
usable
fair
ai
concerns
applicability
emphasize
over-arching
importance
human-centered
design
procedures
creation
fair
ai
tools
efforts
made
collect
practitioner
feedback
incorporate
feedback
creation
fair
ai
richardson
et
al
2021
holstein
et
al
2019
rakova
et
al
2020
14
framework
fairness
major
component
usability
comes
fair
ai
tools
integration
affordances
support
ai
fairness
according
robert
et
al
2020
affordances
include
transparency
explainability
voice
visualization
work
define
transparency
making
underlying
ai
mechanics
visible
known
employee
explainability
describing
ai
decision
actions
employee
human
terms
voice
providing
employees
opportunity
communicate
provide
feedback
ai
visualization
representing
information
employees
via
images
diagrams
animations
robert
et
al
2020
26
work
one
kind
discuss
importance
features
strengthening
fairness
tools
results
studies
interviewed
studied
practitioners
emphasized
importance
ability
interact
fairness
toolkits
richardson
et
al
2021
holstein
et
al
2019
cramer
et
al
2019
using
tool
compare
models
methods
richardson
et
al
2021
providing
understandable
visualizations
richardson
et
al
2021
veale
et
al
2018
law
et
al
2020a
ribeiro
et
al
2016
dodge
et
al
2019
arya
et
al
2019
law
et
al
2020b
veale
binns
2017
receiving
feedback
tools
ribeiro
et
al
2016
richardson
et
al
2021
solutions
aid
users
understanding
bias
issues
reside
interpretability
explainability
research
hutchinson
mitchell
2018
emphasizes
importance
utilizing
explanations
fairness
pursuits
study
ribeiro
et
al
2016
participants
opportunity
interact
lime
local
interpretable
model-agnostic
explanations
toolkit
results
study
depict
importance
explanations
tool
users
decide
whether
trust
classifier
determine
fix
classifier
ribeiro
et
al
2016
without
concrete
explanations
users
either
willfully
ignorant
unable
rely
classifier
ribeiro
et
al
2016
furthermore
study
dodge
et
al
2019
found
different
fairness
problems
better
explained
different
types
explanations
users
exposed
different
types
fairness
explanations
exhibited
different
opinions
model
explanations
considered
inherently
less
fair
meaning
users
tended
view
models
biased
others
enhanced
participants
confidence
classifier
dodge
et
al
2019
authors
arya
et
al
2019
similar
study
introducing
ai
explainability
360
toolkit
compared
contrasted
different
explainer
types
including
data
directly
interpretable
local
post-hoc
global
post-hoc
explainers
provide
taxonomy
explanations
provides
guidance
explainers
best
users
future
work
needs
done
arya
et
al
2019
law
et
al
2020b
ran
simulation
practitioners
depict
plethora
visualizations
used
without
overloading
participants
depending
use
case
proposed
different
techniques
organizing
visualizations
one
technique
referred
recommendation
list
provides
summary
fairness
results
best
situations
many
performance
metrics
technique
visual
cues
provided
high
comprehensiveness
metrics
law
et
al
2020b
despite
indubitable
strengths
fairness
toolkits
lakkaraju
bastani
2019
kaur
et
al
2020
also
gave
risks
came
use
visualizations
explanations
lakkaraju
bastani
2019
found
explanations
black-box
model
still
manipulated
hide
issues
unfairness
simply
omitting
features
users
considered
problematic
generated
explanations
increased
user
trust
15
richardson
gilbert
nearly
10
fold
lakkaraju
bastani
2019
furthermore
kaur
et
al
2020
depicted
importance
training
users
providing
toolkits
visualizations
unfamiliar
users
internalizing
tutorials
made
incorrect
assumptions
data
model
successfully
uncover
issues
dataset
low
confidence
interacting
tool
nonetheless
reported
high
trust
fairness
tools
provided
visualizations
publicly
available
kaur
et
al
2020
findings
considered
creation
deployment
toolkits
4.1
avoid
over-reliance
fair
ai
depicted
work
kaur
et
al
2020
many
practitioners
overly
trusted
toolkits
despite
completely
understanding
results
toolkit
particular
concern
users
asked
select
dropdown
list
functionalities
toolkit
grossly
overestimated
toolkit
results
depicted
misalignment
practitioners
understanding
toolkits
toolkits
intended
use
kaur
et
al
2020
practitioners
interviewed
veale
et
al
2018
depicted
similar
hesitations
interacting
fairness
tooling
suspected
under-rely
4.1
communicating
fairness
stakeholders
across
several
interview
studies
researchers
collected
practitioner
feedback
practitioners
requested
help
communicating
fairness
concerns
stakeholders
holstein
et
al
2019
veale
et
al
2018
law
et
al
2020a
practitioners
discussed
objectives
stakeholders
different
requested
resources
provided
communicating
fairness
trade-off
relates
objectives
veale
et
al
2018
law
et
al
2020a
also
requested
resources
understandable
laymen
business-oriented
individuals
can
understand
cause
bias
importance
fairness
considerations
law
et
al
2020a
garcia-gathright
et
al
2018
one
practitioner
said
fairness
toolkits
incorporate
assumption
will
tech
resistance
many
stakeholders
practitioners
will
reject
output
fairness
toolkits
effectively
communicated
veale
et
al
2018
prior
discussions
different
explanations
differing
users
definitely
useful
explaining
fairness
issues
arya
et
al
2019
fairness
experts
consider
creating
guides
communicating
fairness
results
4.1
burden
fairness
common
sentiment
came
discussing
utilizing
fairness
tools
practitioners
feeling
overwhelmed
whether
users
interacting
toolkit
law
et
al
2020b
richardson
et
al
2021
checklist
cramer
et
al
2019
many
felt
either
qualified
holstein
et
al
2019
ethics
difficult
operationalize
madaio
et
al
2020
ethics
presented
questions
answers
binns
2018
checklists
tools
overwhelming
cramer
et
al
2019
richardson
et
al
2021
law
et
al
2020b
concerns
require
support
institutions
many
works
discussed
fairness
experts
relieve
burden
16
framework
fairness
since
exists
many
avenues
bias
many
feasible
solutions
mehrabi
et
al
2021
olteanu
et
al
2019
cramer
et
al
2018
suggests
fairness
experts
create
taxonomies
existing
biases
can
help
practitioners
recognize
counter
biases
overcome
information
overload
might
accompany
visual
toolkit
law
et
al
2020b
law
et
al
2020a
richardson
et
al
2021
suggests
toolkits
filter
biases
users
behalf
alarming
outcomes
can
stand
solution
presents
selection
biases
fairness
developers
important
outcomes
may
differ
different
contexts
results
cramer
et
al
2019
suggests
checklist
avoided
self-serve
tool
since
often
found
overwhelming
users
instead
interactive
interface
might
beneficial
cramer
et
al
2019
goal
fairness
practitioners
incorporate
fairness
pre-existing
workflows
way
disruption
chaos
minimized
objective
mind
fairness
considered
substantially
less
overwhelming
practitioners
4.1
10
supplemental
resources
practitioners
major
challenge
presented
fairness
experts
translating
principles
ethics
codes
actionable
items
practitioners
institutions
can
implement
mittelstadt
2019
includes
providing
guidance
users
detect
foresee
bias
madaio
et
al
2020
law
et
al
2020a
garcia-gathright
et
al
2018
mehrabi
et
al
2021
cramer
et
al
2018
determine
bias
might
stem
holstein
et
al
2019
garcia-gathright
et
al
2018
determine
respond
biases
holstein
et
al
2019
veale
et
al
2018
law
et
al
2020a
garcia-gathright
et
al
2018
maintain
fairness
deployment
veale
et
al
2018
practitioners
requesting
taxonomies
potential
harms
biases
madaio
et
al
2020
cramer
et
al
2018
easy-to-digest
summaries
explaining
biases
garcia-gathright
et
al
2018
cramer
et
al
2018
guidelines
best
practices
throughout
ml
pipeline
holstein
et
al
2019
users
also
requesting
plethora
additional
resources
like
community
forums
fairness
domain-specific
guides
plenty
tutorials
exemplifying
incorporate
fairness
holstein
et
al
2019
madaio
et
al
2020
richardson
et
al
2021
gray
chivukula
2019
fairness
experts
much
possible
provide
resources
practitioners
support
use
fairness
tools
4.2
recommendations
institutions
considerable
portion
recommendations
given
fairness
researchers
organizations
also
large
portion
responsibility
ensuring
success
fairness
implementation
several
works
provide
critique
actionable
steps
institutions
organizations
use
assist
practitioners
implementation
fairness
toolkits
4.2
prioritize
fairness
plethora
institutions
created
ethical
principles
guidelines
centered
fairness
accountability
transparency
ai
doubt
organizations
recognized
importance
prioritizing
ethics
jobin
et
al
2019
nonetheless
application
fairness
practice
rare
ethical
codes
put
action
engineers
practitioners
frankel
1989
emphasizes
ethics
codes
fail
put
val17
richardson
gilbert
ues
practice
political
tools
meant
manipulate
public
one
easiest
ways
promote
culture
bias
awareness
organizational
level
prioritize
fairness
global
objective
garcia-gathright
et
al
2018
stark
hoffmann
2019
organizations
can
considering
fairness
hiring
practices
garcia-gathright
et
al
2018
prioritizing
bias
correction
prioritize
privacy
accessibility
madaio
et
al
2020
garcia-gathright
et
al
2018
frankel
1989
providing
practitioners
resources
teams
can
provide
actionable
steps
madaio
et
al
2020
mittelstadt
2019
changing
organizational
structure
embrace
inclusion
fairness
considerations
cramer
et
al
2019
practitioners
interviewed
rakova
et
al
2020
provided
plethora
recommendations
organizations
intending
prioritize
fairness
organizations
provide
reward
systems
incentivize
practitioners
continue
educating
ethical
ai
furthermore
allow
practitioners
work
closely
marginalized
communities
ensure
data
representative
products
harmful
lastly
practitioners
emphasized
importance
rejecting
ai
system
found
perpetuate
harms
individuals
rakova
et
al
2020
reisman
et
al
2018
recommended
organizations
practice
transparency
public
method
ensuring
accountability
therefore
fairness
suggest
public
given
notice
technology
might
impact
lives
reisman
et
al
2018
fairness
research
also
emphasizes
importance
public
transparency
institutions
comes
existence
auditing
ai
richardson
et
al
2020
4.2
operationalize
fairness
one
method
exhibit
prioritization
fairness
operationalization
fairness
exist
now
published
ethics
codes
difficult
operationalize
due
abstract
nature
madaio
et
al
2020
stark
hoffmann
2019
operationalizing
organizations
force
consider
want
enact
fairness
robert
et
al
2020
frankel
1989
relieve
burden
ethical
responsibility
placed
practitioner
stark
hoffmann
2019
making
big
decision
things
like
type
demographics
biases
stakeholders
intend
consider
efforts
cramer
et
al
2018
will
require
discussions
top-players
within
organizations
deciding
prioritization
fairness
may
displace
priorities
requires
consultation
legal-consultants
can
ensure
new
policies
satisfy
government
regulations
veale
et
al
2018
mittelstadt
2019
furthermore
leaders
must
make
decision
much
bias
permissible
prepared
handle
consequences
decisions
barocas
selbst
2018
barocas
et
al
2019
4.2
avoid
dividing
practitioner
loyalty
according
recent
research
many
practitioners
feel
divided
desire
create
ethicallysound
products
obligation
role
employees
stark
hoffmann
2019
mittelstadt
2019
felt
though
conversations
ethics
taboo
impact
goals
career
advancement
madaio
et
al
2020
rakova
et
al
2020
many
18
framework
fairness
felt
like
individual
battle
sole
advocate
battling
fairness
holstein
et
al
2019
madaio
et
al
2020
studying
ethics
engineering
becomes
obvious
concerns
nothing
new
field
engineering
works
frankel
1989
davis
1991
discuss
ethical
dilemma
impacted
engineers
decades
studying
code
ethics
taught
engineers
clear
engineers
prioritize
ethical
obligation
institutional
obligations
call
davis
1991
nonetheless
rhetoric
rarely
implemented
attempt
follow
codes
often
referred
whistleblowers
institutions
can
prevent
creating
clear
guidelines
practitioners
encounter
issues
rewarding
individuals
recognize
biases
follow
appropriate
steps
fixing
frankel
1989
4.2
create
organizational
structure
around
ethics
interviewed
practitioners
stated
concerns
based
lack
fairness
infrastructure
within
organizations
many
practitioners
stated
fairness
mitigation
done
independently
practitioners
often
efforts
went
uncompensated
madaio
et
al
2020
rakova
et
al
2020
others
said
unsure
share
fairness
issues
even
responsibility
veale
et
al
2018
rakova
et
al
2020
stark
hoffmann
2019
lastly
practitioners
concerns
handle
fairness
issues
different
parts
ai
product
owned
handled
different
teams
cramer
et
al
2018
comments
suggest
dire
need
organizations
establish
clear
delegations
tasks
around
fairness
practitioners
even
recommended
fairness
teams
internal
review
boards
created
used
resource
practitioners
auditors
ai
technologies
rakova
et
al
2020
robert
et
al
2020
additional
support
teams
formal
path
communication
dictates
clearly
practitioners
hierarchy
communicating
relevant
issues
veale
et
al
2018
issues
arise
relates
fairness
organizations
prepared
plans
issues
will
dealt
rakova
et
al
2020
organizations
consider
use
internal
external
investigation
committees
especially
handling
sensitive
protected
attributes
rakova
et
al
2020
veale
binns
2017
critical
teams
given
open
access
data
models
procedures
order
optimize
transparency
reproducibility
haibe-kains
et
al
2020
additionally
education
infrastructure
focusing
educating
practitioners
recognizing
combating
algorithmic
bias
made
available
practitioners
product
managers
ensure
baseline
understanding
amongst
practitioners
rakova
et
al
2020
prioritization
fairness
organizations
critical
practitioners
global
image
consumers
products
services
organizations
can
prioritize
fairness
operationalizing
defining
explicit
infrastructure
ensuring
protection
employees
interested
implementing
4.3
recommendations
ml
practitioners
despite
predominant
portion
work
needs
done
fairness
experts
practitioners
still
recommendations
literature
ml
practitioners
19
richardson
gilbert
order
ensure
success
fairness
effort
must
also
made
practitioners
educate
properly
implement
fairness
made
available
section
briefly
provides
recommendations
practitioners
can
correctly
implement
fairness
pipelines
4.3
implement
fairness
throughout
pipeline
stated
stark
hoffmann
2019
ethics
considered
final
checkpoint
considered
start
project
reflected
within
every
stage
process
currently
practitioners
engage
fairness
ad-hocly
final
performance
check
holstein
et
al
2019
madaio
et
al
2020
rakova
et
al
2020
literature
suggests
fairness
much
easier
implement
considered
idea
formulation
stage
project
cramer
et
al
2018
2019
friedman
et
al
2002
task
becomes
much
difficult
model
deployed
running
unintended
biases
will
become
recursive
therefore
much
difficult
manage
cramer
et
al
2018
furthermore
practitioners
make
effort
including
wide
variety
experts
lay-persons
design
implementation
evaluation
ml
wagstaff
2012
practitioners
make
sure
assessing
addressing
bias
becomes
normal
procedure
throughout
pipeline
projects
4.3
iteratively
implement
fairness
fairness
considered
throughout
process
sometimes
may
overwhelming
immediately
counteract
every
instance
unfairness
found
cramer
et
al
2018
allowed
practitioners
engage
fairness
checklists
noticed
concerns
practitioners
checklists
might
conflict
rapid
delivery
development
procedures
existed
within
organizations
cramer
et
al
2018
concluded
addressing
algorithmic
bias
need
done
short-term
narrow
steps
plans
iterative
improvement
fairness
issues
cramer
et
al
2019
also
suggested
method
reduce
overload
felt
practitioners
therefore
practitioners
iteratively
implement
small
fixes
fairness
issues
deployments
try
detect
handle
every
issue
immediately
related
works
work
first
kind
provide
comprehensive
review
solution
space
fairness
ai
related
reviews
section
summative
breakdown
biases
exists
machine
learning
similar
friedman
nissenbaum
1996
mehrabi
et
al
2021
olteanu
et
al
2019
hutchinson
mitchell
2018
hellstro
et
al
2020
mehrabi
et
al
2021
provides
comprehensive
list
23
types
biases
organized
format
friedman
nissenbaum
1996
olteanu
et
al
2019
provided
hierarchy
biases
creating
structure
biases
organized
olteanu
et
al
2019
summarizes
different
biases
associated
social
data
friedman
nissenbaum
1996
defines
new
set
biases
around
data
machine
learning
work
adopts
part
friedman
nissenbaum
1996
hierarchical
structure
utilizes
thematic
analysis
categorize
20
framework
fairness
different
biases
introduced
across
comprehensive
list
works
hutchinson
mitchell
2018
discusses
history
fairness
including
differing
notions
fairness
gaps
fairness
work
lastly
hellstro
et
al
2020
creates
taxonomy
bias
discusses
relation
forms
bias
provides
examples
support
differing
definitions
taxonomy
however
depicted
based
chronological
influences
bias
knowledge
reviews
exists
similar
section
works
user-focused
comparative
analysis
different
toolkits
richardson
et
al
2021
paper
allows
practitioners
engage
two
toolkits
uchicago
aequitas
saleiro
et
al
2018
google
what-if
wexler
et
al
2019
collects
feedback
seng
et
al
2021
similar
work
compares
six
different
toolkits
scikit
fairness
tool
johnson
et
al
2020
ibm
fairness
360
bellamy
et
al
2018
uchicago
aequitas
saleiro
et
al
2018
google
what-if
wexler
et
al
2019
pymetrics
audit-ai
microsoft
fairlearn
bird
et
al
2020
knowledge
work
reviews
fairness
toolkits
checklists
furthermore
contributions
section
entirely
novel
richardson
et
al
2021
brief
literature
review
related
works
take
practitioner
feedback
consideration
paper
first
kind
combine
recommendations
practitioners
fairness
experts
institutions
alike
presents
comprehensive
review
necessary
work
fairness
research
conclusion
exists
seemingly
endless
list
potential
influences
bias
comes
machine
learning
algorithms
order
respond
prolific
nature
bias
fairness
researchers
produced
wide
variety
tools
practitioners
can
use
mainly
form
software
toolkits
checklists
companies
academic
institutions
independent
researchers
alike
created
versions
resources
unique
features
developers
thought
critical
nonetheless
still
exists
disconnect
fairness
developers
practitioners
prevents
use
resources
practice
literature
suggests
fairness
practice
can
normalized
effort
fairness
developers
institutions
create
ai
practitioners
build
ai
review
provides
summary
algorithmic
bias
issues
arisen
literature
highlights
unique
contributions
fairness
solution
space
summarizes
recommendations
normalization
fairness
practices
knowledge
exists
work
provides
comprehensive
review
problem
solution
space
fairness
research
literature
suggests
still
much
work
done
review
highlights
major
needs
acknowledgments
authors
wish
thank
hans-martin
adorf
don
rosenthal
richard
franier
peter
cheeseman
monte
zweben
assistance
advice
also
thank
ron
musick
anonymous
reviewers
comments
space
telescope
science
institute
operated
association
universities
research
astronomy
nasa
21
richardson
gilbert
references
angwin
larson
mattu
kirchner
2016
machine
bias
tech
rep
propublica
arnold
piorkowski
reimer
richards
tsay
varshney
bellamy
hind
houde
mehta
mojsilovic
nair
ramamurthy
olteanu
2019
factsheets
increasing
trust
ai
services
supplier
declarations
conformity
ibm
journal
research
development
63
arya
bellamy
chen
dhurandhar
hind
hoffman
houde
liao
luss
mojsilovic
mourad
pedemonte
raghavendra
richards
sattigeri
shanmugam
singh
varshney
wei
zhang
2019
one
explanation
fit
toolkit
taxonomy
ai
explainability
techniques
arxiv
abs
1909.03012
asaro
2019
ai
ethics
predictive
policing
models
threat
ethics
care
ieee
technology
society
magazine
38
40
53
baeza-yates
2018
bias
web
communications
acm
61
54
61
barocas
hardt
narayanan
2019
fairness
machine
learning
fairmlbook
org
barocas
selbst
2018
big
data
disparate
impact
ssrn
electronic
journal
104
671
732
bellamy
dey
hind
hoffman
houde
kannan
lohia
martino
mehta
mojsilovic
nagar
ramamurthy
richards
saha
sattigeri
singh
varshney
zhang
2018
ai
fairness
360
extensible
toolkit
detecting
understanding
mitigating
unwanted
algorithmic
bias
advances
neural
information
processing
systems
2017
decem
nips
5681
5690
bennett
moses
chan
2018
algorithmic
prediction
policing
assumptions
evaluation
accountability
policing
society
28
806
822
berk
heidari
jabbari
kearns
roth
2021
fairness
criminal
justice
risk
assessments
state
art
sociological
methods
research
50
44
binns
2018
fairness
machine
learning
lessons
political
philosophy
journal
machine
learning
research
81
11
binns
van
kleek
veale
lyngs
zhao
shadbolt
2018
reducing
human
percentage
perceptions
justice
algorithmic
decisions
conference
human
factors
computing
systems
proceedings
2018april
bird
dudı
edgar
horn
lutz
milan
sameki
wallach
walker
2020
fairlearn
toolkit
assessing
improving
fairness
ai
tech
rep
microsoft
birhane
2021
algorithmic
injustice
relational
ethics
approach
patterns
100205
22
framework
fairness
blyth
1972
simpson
paradox
sure-thing
principle
journal
american
statistical
association
67
338
366
bowker
star
1999
sorting
things
classification
consequences
mit
press
bradley
ambrose
bernstein
deloatch
dreisigmeyer
gonzales
grubb
haralampus
hawes
johnson
kopp
krebs
marsico
morgan
osatuke
vidrine
2020
data
ethics
framework
tech
rep
united
kingdom
department
digital
culture
media
sport
buolamwini
gebru
2018
gender
shades
intersectional
accuracy
disparities
commercial
gender
classification
proceedings
machine
learning
research
vol
81
pp
15
calders
liobaite
2013
unbiased
computational
processes
can
lead
discriminative
decision
procedures
studies
applied
philosophy
epistemology
rational
ethics
vol
pp
43
57
springer
international
publishing
chang
raheem
rha
2018
novel
robotic
systems
future
directions
cheng
varshney
liu
2021
socially
responsible
ai
algorithms
issues
purposes
challenges
journal
artificial
intelligence
research
71
1137
1181
choi
lee
2018
artificial
intelligence
approach
financial
fraud
detection
iot
environment
survey
implementation
chouldechova
2017
fair
prediction
disparate
impact
study
bias
recidivism
prediction
instruments
big
data
153
163
ciampaglia
nematzadeh
menczer
flammini
2017
algorithmic
popularity
bias
hinders
promotes
quality
scientific
reports
corbett-davies
goel
2018
measure
mismeasure
fairness
critical
review
fair
machine
learning
arxiv
abs
1808.00023
corbett-davies
pierson
feller
goel
huq
2017
algorithmic
decision
making
cost
fairness
proceedings
23rd
acm
sigkdd
international
conference
knowledge
discovery
data
mining
new
york
ny
usa
acm
cramer
garcia-gathright
springer
reddy
2018
assessing
addressing
algorithmic
bias
practice
interactions
25
58
63
cramer
reddy
bouyer
garcia-gathright
springer
2019
translation
tracks
data
algorithmic
bias
effort
practice
conference
human
factors
computing
systems
proceedings
association
computing
machinery
crawford
2013
hidden
biases
big
data
harvard
business
review
crawford
2017
trouble
bias
davis
1991
thinking
like
engineer
place
code
ethics
practice
profession
philosophy
public
affairs
20
150
167
dodge
liao
zhang
bellamy
dugan
2019
explaining
models
empirical
study
explanations
impact
fairness
judgment
23
richardson
gilbert
international
conference
intelligent
user
interfaces
proceedings
iui
vol
part
f1476
pp
275
285
association
computing
machinery
fazelpour
lipton
2020
algorithmic
fairness
non-ideal
perspective
aies
2020
proceedings
aaai
acm
conference
ai
ethics
society
57
63
fish
kun
lelkes
2016
confidence-based
approach
balancing
fairness
accuracy
16th
siam
international
conference
data
mining
2016
sdm
2016
144
152
frankel
1989
professional
codes
impact
journal
business
ethics
109
115
friedler
scheidegger
venkatasubramanian
choudhary
hamilton
roth
2018
comparative
study
fairness-enhancing
interventions
machine
learning
fat
2019
proceedings
2019
conference
fairness
accountability
transparency
329
338
friedler
scheidegger
venkatasubramanian
2021
im
possibility
fairness
communications
acm
64
136
143
friedman
kahn
borning
2002
value
sensitive
design
theory
methods
university
washington
technical
report
12
friedman
nissenbaum
1996
bias
computer
systems
acm
transactions
information
systems
14
330
347
garcia-gathright
springer
cramer
2018
assessing
addressing
algorithmic
bias
get
proceedings
ofthe
aaai2018
spring
symposium
designing
user
experience
ofartificial
intelligence
gebru
morgenstern
vecchione
vaughan
wallach
daume
iii
crawford
2018
datasheets
datasets
arxiv
geiger
yu
yang
dai
qiu
tang
huang
2019
garbage
garbage
machine
learning
application
papers
social
computing
report
human-labeled
training
data
comes
proceedings
2020
conference
fairness
accountability
transparency
pp
325
336
association
computing
machinery
inc
goldenberg
nir
salcudean
2019
new
era
artificial
intelligence
machine
learning
prostate
cancer
nature
reviews
urology
16
391
403
gray
chivukula
2019
ethical
mediation
ux
practice
proceedings
2019
chi
conference
human
factors
computing
systems
association
computing
machinery
greene
hoffman
stark
2019
better
nicer
clearer
fairer
critical
assessment
movement
ethical
artificial
intelligence
machine
learning
proceedings
52nd
hawaii
international
conference
system
sciences
guszcza
2018
smarter
together
artificial
intelligence
needs
human-centric
design
tech
rep
22
deloitte
insights
24
framework
fairness
haibe-kains
adam
hosny
khodakarami
shraddha
kusko
sansone
tong
wolfinger
mason
jones
dopazo
furlanello
waldron
wang
mcintosh
goldenberg
kundaje
greene
broderick
hoffman
leek
korthauer
huber
brazma
pineau
tibshirani
hastie
ioannidis
quackenbush
aerts
2020
transparency
reproducibility
artificial
intelligence
nature
586
7829
e14
e16
harcourt
2007
prediction
profiling
policing
punishing
actuarial
age
university
chicago
press
chicago
il
hellstro
dignum
bensch
2020
bias
machine
learning
good
ceur
workshop
proceedings
vol
2659
pp
10
ceur-ws
hoffmann
2019
fairness
fails
data
algorithms
limits
antidiscrimination
discourse
information
communication
society
22
900
915
holstein
wortman
vaughan
daume
iii
dudı
wallach
2019
improving
fairness
machine
learning
systems
industry
practitioners
need
chi
conference
human
factors
computing
systems
acm
hu
kohler-hausmann
2020
sex
got
machine
learning
proceedings
2020
conference
fairness
accountability
transparency
fat
20
513
new
york
ny
usa
association
computing
machinery
hutchinson
mitchell
2018
50
years
test
un
fairness
lessons
machine
learning
proceedings
2019
conference
fairness
accountability
transparency
pp
49
58
association
computing
machinery
inc
jacobs
wallach
2021
measurement
fairness
proceedings
2021
acm
conference
fairness
accountability
transparency
facct
21
375
385
new
york
ny
usa
association
computing
machinery
japkowicz
shah
2011
evaluating
learning
algorithms
classification
perspective
cambridge
university
press
jobin
ienca
vayena
2019
global
landscape
ai
ethics
guidelines
nature
machine
intelligence
389
399
johnson
bartola
angell
keith
witty
giguere
brun
2020
fairkit
fairkit
wall
fairest
supporting
data
scientists
training
fair
models
arxiv
abs
2012.09951
jordan
mitchell
2015
machine
learning
trends
perspectives
prospects
kasy
abebe
2021
fairness
equality
power
algorithmic
decision-making
proceedings
2021
acm
conference
fairness
accountability
transparency
vol
11
pp
576
586
association
computing
machinery
inc
kaur
nori
jenkins
caruana
wallach
wortman
vaughan
2020
interpreting
interpretability
understanding
data
scientists
use
interpretability
tools
machine
learning
chi
conference
human
factors
computing
systems
25
richardson
gilbert
kilbertus
rojas-carulla
parascandolo
hardt
janzing
scho
lkopf
2017
avoiding
discrimination
causal
reasoning
proceedings
2017
advances
neural
information
processing
systems
vol
30
kleinberg
mullainathan
raghavan
2017
inherent
trade-offs
fair
determination
risk
scores
leibniz
international
proceedings
informatics
lipics
67
23
lakkaraju
bastani
2019
fool
manipulating
user
trust
via
misleading
black
box
explanations
aies
2020
proceedings
aaai
acm
conference
ai
ethics
society
79
85
law
malik
du
sinha
2020a
designing
tools
semi-automated
detection
machine
learning
biases
interview
study
proceedings
chi
2020
workshop
detection
design
cognitive
biases
people
computing
systems
law
malik
du
sinha
2020b
impact
presentation
style
human-in-the-loop
detection
algorithmic
bias
graphics
interface
lifshitz
mcmaster
2020
legal
ethics
checklist
ai
systems
scitech
lawyer
17
28
34
lin
chen
chiang
hribar
2020
applications
artificial
intelligence
electronic
health
record
data
ophthalmology
translational
vision
science
technology
lowry
macpherson
1988
blot
profession
british
medical
journal
clinical
research
ed
296
6623
657
658
lum
isaac
2016
predict
serve
significance
13
14
19
madaio
stark
wortman
vaughan
wallach
2020
co-designing
checklists
understand
organizational
challenges
opportunities
around
fairness
ai
chi
conference
human
factors
computing
systems
honolulu
acm
manders-huits
zimmer
2009
values
pragmatic
action
challenges
introducing
ethical
intelligence
technical
design
communities
international
review
information
ethics
10
37
44
mehrabi
morstatter
saxena
lerman
galstyan
2021
survey
bias
fairness
machine
learning
acm
comput
surv
54
mester
2017
statistical
bias
types
explained
examples
mitchell
wu
zaldivar
barnes
vasserman
hutchinson
spitzer
raji
gebru
2019
model
cards
model
reporting
proceedings
2019
conference
fairness
accountability
transparency
pp
220
229
association
computing
machinery
inc
mittelstadt
2019
ai
ethics
principled
fail
nature
machine
intelligence
moysan
zeitoun
2019
chatbots
lever
redefine
customer
experience
banking
journal
digital
banking
242
249
26
framework
fairness
munavalli
rao
srinivasan
van
merode
2020
intelligent
realtime
scheduler
out-patient
clinics
multi-agent
system
model
health
informatics
journal
26
2383
2406
noble
2018
algorithms
oppression
search
engines
reinforce
racism
first
edition
nyu
press
olteanu
castillo
diaz
kıcıman
2019
social
data
biases
methodological
pitfalls
ethical
boundaries
frontiers
big
data
13
patil
mason
loukides
2018
oaths
checklists
rakova
yang
cramer
chowdhury
2020
responsible
ai
meets
reality
practitioner
perspectives
enablers
shifting
organizational
practices
reisman
schultz
crawford
whittaker
2018
algorithmic
impact
assessments
practical
framework
public
agency
accountability
tech
rep
ai
now
ribeiro
singh
guestrin
2016
trust
explaining
predictions
classifier
proceedings
22nd
acm
sigkdd
international
conference
knowledge
discovery
data
mining
pp
1135
1144
richardson
garcia-gathright
spotify
way
jennifer
thom
henriette
cramer
garcia-gathright
thom
cramer
2021
towards
fairness
practice
practitioner-oriented
rubric
evaluating
fair
ml
toolkits
towards
fairness
practice
practitioner-oriented
rubric
evaluating
fair
ml
toolkits
chi
conference
human
factors
computing
systems
chi
21
yokohama
japan
richardson
prioleau
alikhademi
gilbert
2020
public
accountability
understanding
sentiments
towards
artificial
intelligence
across
dispositional
identities
ieee
2020
international
symposium
technology
society
ridgeway
2013
pitfalls
preduction
tech
rep
271
nij
journal
robert
pierce
marquis
kim
alahmad
2020
designing
fair
ai
human-computer
interaction
rovatsos
mittelstadt
koene
2019
landscape
summary
bias
algorithmic
decision-making
bias
algorithmic
decision-making
can
identify
can
mitigate
tech
rep
centre
data
ethics
innovation
saleiro
kuester
stevens
anisfeld
hinkson
london
ghani
2018
aequitas
bias
fairness
audit
toolkit
arxiv
abs
1811.05577
sandvig
2014
seeing
sort
aesthetic
industrial
defense
algorithm
journal
new
media
caucus
10
selbst
boyd
friedler
venkatasubramanian
vertesi
2019
fairness
abstraction
sociotechnical
systems
fat
2019
proceedings
2019
conference
fairness
accountability
transparency
pp
59
68
new
york
ny
usa
association
computing
machinery
inc
27
richardson
gilbert
seng
lee
singh
2021
landscape
gaps
open
source
fairness
toolkits
proceedings
2021
chi
conference
human
factors
computing
systems
acm
simoiu
corbett-davies
goel
2017
problem
infra-marginality
outcome
tests
discrimination
annals
applied
statistics
11
1193
1216
skitka
mosier
burdick
1999
automation
bias
decision-making
international
journal
human
computer
studies
51
991
1006
srinivasan
2020
ml-fairness-gym
tool
exploring
long-term
impacts
machine
learning
systems
google
ai
blog
stark
hoffmann
2019
data
new
popular
metaphors
professional
ethics
emerging
data
culture
journal
cultural
analytics
sunikka
bragge
kallio
2011
effectiveness
personalized
marketing
online
banking
comparison
search
experience
offerings
journal
financial
services
marketing
16
183
194
suresh
guttag
2019
framework
understanding
unintended
consequences
machine
learning
arxiv
abs
1901.10002
vasudevan
kenthapadi
2020
lift
scalable
framework
measuring
fairness
ml
applications
proceedings
29th
acm
international
conference
information
knowledge
management
veale
binns
2017
fairer
machine
learning
real
world
mitigating
discrimination
without
collecting
sensitive
data
big
data
society
veale
van
kleek
binns
2018
fairness
accountability
design
needs
algorithmic
support
high-stakes
public
sector
decision-making
conference
human
factors
computing
systems
proceedings
2018
april
verma
rubin
2018
fairness
definitions
explained
ieee
acm
international
workshop
software
fairness
18
wagstaff
2012
machine
learning
matters
proceedings
29th
international
conference
machine
learning
vol
pp
529
534
wexler
pushkarna
bolukbasi
wattenberg
vı
wilson
2019
what-if
tool
interactive
probing
machine
learning
models
ieee
transactions
visualization
computer
graphics
26
56
65
xiang
raji
2019
legal
compatibility
fairness
definitions
arxiv
zhong
2018
tutorial
fairness
machine
learning
zliobaite
2015
relation
accuracy
fairness
binary
classification
2nd
workshop
fairness
accountability
transparency
machine
learning
28