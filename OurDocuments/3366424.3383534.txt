         FairSearch: A Tool For Fairness in Ranked Search Results
                                Meike Zehlike                                                                            Tom Sühr
                   Humboldt Universität zu Berlin                                                            Technische Universität Berlin
                 Max Planck Inst. for Software Systems                                                        tom.suehr@googlemail.com
                     meikezehlike@mpi-sws.org

                               Carlos Castillo                                                                         Ivan Kitanovski
                          Universitat Pompeu Fabra                                               Faculty of Computer Science and Engineering
                              chato@acm.org                                                          University Saint Cyril and Methodius
                                                                                                        ivan.kitanovski@finki.ukim.mk
ABSTRACT                                                                         relevant content. Ranking algorithms automatically score and sort
Ranked search results and recommendations have become the main                   these contents for us, typically by decreasing probability of an
mechanism by which we find content, products, places, and people                 item being relevant [6]. Therefore, more often than not, algorithms
online. With hiring, selecting, purchasing, and dating being increas-            choose not only the products we are offered and the news we
ingly mediated by algorithms, rankings may determine business                    read, but also the people we meet, or whether we get a loan or an
opportunities, education, access to benefits, and even social success.           invitation to a job interview. With hiring, selecting, purchasing, and
It is therefore of societal and ethical importance to ask whether                dating being increasingly mediated by algorithms, rankings may
search results can demote, marginalize, or exclude individuals of                determine business opportunities, education, access to benefits, and
unprivileged groups or promote products with undesired features.                 even social success. It is therefore of societal and ethical importance
    In this paper we present FairSearch, the first fair open source              to ask whether search algorithms produce results that can demote,
search API to provide fairness notions in ranked search results. We              marginalize, or exclude individuals of unprivileged groups (e.g.,
implement two well-known algorithms from the literature, namely                  racial or gender discrimination) or promote products with undesired
FA*IR (Zehlike et al., 2017) and DELTR (Zehlike and Castillo, 2018)              features (e.g., gendered books) [2, 4, 5, 8].
and provide them as stand-alone libraries in Python and Java. Ad-                    This paper operates on the concept of a historically and currently
ditionally we implement interfaces to Elasticsearch for both algo-               disadvantaged protected group, and the concern of disparate impact,
rithms, a well-known search engine API based on Apache Lucene.                   i.e., a loss of opportunity for said group independently of whether
The interfaces use the aforementioned Java libraries and enable                  they are treated differently. In rankings disparate impact translates
search engine developers who wish to ensure fair search results                  into differences in exposure [7] or inequality of attention across
of different styles to easily integrate DELTR and FA*IR into their               groups, which are to be understood as systematic differences in
existing Elasticsearch environment.                                              access to economic or social opportunities.
                                                                                     In this paper we present FairSearch, the first fair open source
CCS CONCEPTS                                                                     search    API that implements two well-known methods from the
                                                                                 literature, namely FA*IR [9] and DELTR [10]. For both algorithms
• Information systems → Learning to rank; • Applied com-
                                                                                 the implementation is provided as a stand-alone Java and Python
puting → Law, social and behavioral sciences;
                                                                                 library, as well as interfaces for Elasticsearch,1 a popular, well-
                                                                                 tested search engine, which is used by many big brands such as
KEYWORDS
                                                                                 Amazon, Netflix and Facebook. Our goal with FairSearch is to pro-
Ranking, Algorithmic Fairness, Disparate Impact                                  vide various approaches for fair ranking algorithms, with a broad
ACM Reference Format:                                                            spectrum of justice definitions to satisfy many possible fairness
Meike Zehlike, Tom Sühr, Carlos Castillo, and Ivan Kitanovski. 2020. FairSearch: policies in various business situations. By providing the algorithms
A Tool For Fairness in Ranked Search Results. In Companion Proceedings of        as stand-alone libraries in Python and Java and for Elasticsearch
the Web Conference 2020 (WWW ’20 Companion), April 20–24, 2020, Taipei,          we make the on-going research on fair machine learning accessible
Taiwan. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3366424.        and ready-to-use for a broad community of professional developers
3383534
                                                                                 and researchers, particularly those working in the realm of human-
1 INTRODUCTION                                                                   centric and socio-technical systems, as well as sharing economy
                                                                                 platforms.
With the volume of information increasing at a frenetic pace, ranked
search results have become the main mechanism by which we find                   2 THEORETICAL BACKGROUND
                                                                                           This section explains the math behind FA*IR and DELTR and gives
This paper is published under the Creative Commons Attribution 4.0 International
(CC BY 4.0) license. Authors reserve their rights to disseminate the work on their         examples for their application domain. DELTR [10]constitutes a so-
personal and corporate Web sites with the appropriate attribution.                         called in-processing approach, that incorporates a fairness term into
WWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan
                                                                                           its learning objective. This way it can learn to ignore the protected
© 2020 IW3C2 (International World Wide Web Conference Committee), published
under Creative Commons CC BY 4.0 License.                                                  feature as well as non-protected ones that serve as proxies, such as
ACM ISBN 978-1-4503-7024-0/20/04.
https://doi.org/10.1145/3366424.3383534                                                    1 https://www.elastic.co/




                                                                                     172
WWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan                                                                                                       Zehlike et al.

                                                                                         aa k
                                                                                         p aa 1               2    3    4    5     6   7    8    9    10    11     12
                                                                                            a
                                                                                                  0.1    0    0    0    0    0     0   0    0    0     0     0         0
                                                                                                  0.3    0    0    0    0    0     0   1    1    1     1     1         2
                                                                                                  0.5    0    0    0    1    1     1   2    2    3     3     3         4
  (a) Case where all non-protected elements appear first in the training set
                                                                                                  0.7    0    1    1    2    2     3   3    4    5     5     6         6
                                                                                     Table 1: Example values of the minimum number of pro-
                                                                                     tected items that must appear in the top k positions to pass
                                                                                     the ranked group fairness test with α = 0.1. We call this an
                                                                                     MTable. Table from [9]
                                                                                     given ranking of length k, the ratio of protected items does not
                                                                                     fall far below a given p at any ranking position. FA*IR translates
      (b) Case where all protected elements appear first in the training set

Figure 1: Depiction of test results using synthetic data. Top:                       this constraint into a statistical significance test, using the binomial
DELTR reduces disparate exposure. Bottom: asymmetry in                               cumulative distribution function F with parameters p, k and α and
DELTR, which does not change rankings if protected ele-                              declares a ranking as fairly representing the protected group if, for
ments already appear in the first positions.                                         each k the following constraint holds:
ZIP code. FA*IR [9] belongs to the class of post-processing proce-                                               F (τp ; k, p) > α,
dures and re-ranks a given search engine result to meet predefined                   where τp is the actual number of protected items in the ranking
fairness constraints.                                                                under test. This constraint can now be used to calculate the min-
                                                                                     imum number of protected items at each ranking position such
2.1      DELTR: A Learning-To-Rank Approach                                          that the constraint holds (see table 1 with different examples of p).
In traditional learning-to-rank (LTR) systems a ranking function f                   As an example consider the ranking in table 2 that corresponds
is learned by minimizing a loss function L, that measures the error                  to a job candidate search for an “economist” in the XING dataset
between predictions ŷ made by f and the training judgments y. For                   used in [9]. We observe that the proportion of male and female
DELTR the loss function of ListNet [3], a well-known LTR algorithm
is extended by a term U , which measures the “unfairness” of a                                    Position                  top 10     top 10     top 40    top 40
predicted ranking. This way a new loss function L DELTR = L(y, ŷ) +                       1 2 3 4 5 6 7 8 9 10              male      female      male     female
γU (ŷ) simultaneously optimizes f for relevance and fairness. U is                         f m m m m m m m m m              90%        10%          73%         27%
defined to be a measure of disparate exposure across different social
groups in a probabilistic ranking Pŷ . This means discrepancies in                  Table 2: Example of non-uniformity of the top-10 vs. the top-
the probability to appear at the top position, received by items of                  40 results for query “economist” in XING (Jan 2017). Table
the protected group G 1 vs items of the non-protected group G 0 are                  from [9]
measured:                                                                            candidates keeps changing throughout the top k positions, which in
                                                                     2
        U (ŷ) = max 0, Exposure(G 0 |Pŷ ) − Exposure(G 1 |Pŷ )                    this case disadvantages women by preferring men at the top-10 po-
                                                                                     sitions. Suppose that the required proportion of female candidates
Figure 1 shows how DELTR works on a synthetic dataset which
                                                                                     is p = 0.3, this translates into having at least one female candidate
has a total size of 50 items and each item x i is represented by
                                                                                     in the top-10 positions. Hence the ranking in table 2 will be ac-
two features: their protection status and a score between 0 and 1:
                                                                                     cepted as fair. However, if the required proportion is p = 0.5 this
x i = (x i,1 , x i,2 ). The attribute x i,1 is 1 if the item belongs to the
                                                                                     translates into needing at least one female candidate in the top-4,
protected group G 1 , and 0 otherwise. The scores x i,2 are distributed
                                                                                     two in the top-7 and three in the top-9 positions. In this case the
uniformly at random over two non-overlapping intervals. Training
                                                                                     ranking will be reordered by FA*IR to meet the fairness constraints.
documents are ordered by decreasing scores, hence the top element
                                                                                     Furthermore, our library implements the best possible adjustment
is the one that has the highest score.
                                                                                     of the desired significance level α. This is necessary, because the
    We first consider a scenario in which all protected elements have
                                                                                     test for a representation like in table 1 is a multi-hypothesis test.
strictly smaller scores than all non-protected ones (Figure 1a). A
standard learning to rank algorithm in this case places all non-                     3    FAIRSEARCH: THE DELTR PLUGIN
protected elements above all protected elements, giving them a                       For the integration of DELTR into Elasticsearch we use the Elas-
larger exposure. Instead, DELTR with increasing values of γ reduces                  ticsearch Learning to Rank (LTR-ES) plugin 2 . The integration ar-
the disparate exposure, while still considering the discrepancy in                   chitecture is depicted on Figure 3. The logic consists of two phases:
the score values. Figure 1b shows the asymmetry of the method: if                    training and ranking.
the protected elements already receive larger predicted exposure                     Training. To apply DELTR at run-time for retrieval, LTR-ES needs
than the non-protected by ranker f , DELTR will behave like a                        a previously trained model that is uploaded into its model storage.
standard LTR approach.                                                               Since training models is a very CPU intensive task that involves a
2.2      FA*IR: A Re-Ranking Approach                                                lot of supervision and verification, it happens offline in a DELTR
Being a post-processing method, FA*IR [9] assumes that a ranking                     wrapper, which calls our stand-alone DELTR Python library to
function has already been trained and a ranked search result is                      train a LTR-ES suitable model. The wrapper has to be provided
available. Its ranked group fairness constraint guarantees that in a                 2 https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/




                                                                               173
FairSearch: A Tool For Fairness in Ranked Search Results                            WWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan




             (a) Architecture of the FA*IR Elasticsearch Plugin                                       (b) Demo Application

Figure 2: (a) Architecture of the FA*IR Elasticsearch Plugin and (b) a Demo Webapp with the FA*IR Elasticsearh Plugin; red
indicates protected items
with a training set, the training parameters and a name for the          interface, which it applies our previously learned weights to the
model. After training the wrapper calls the LTR-ES upload API,           document features of the top N results to produce the final ranking.
which stores the serialized model inside Elasticsearch’s LTR plugin,        In the Rescorer, we have to specify two key parameters:
making it available for up-coming retrieval tasks. Upon upload the            • window_size - the number of elements to re-score (usually
wrapper specifies model_name, type (always DELTR), the model                    N)
itself and the feature_set it was trained against. feature_set                • model - the model name.
specifies query-dependent features, that tell LTR-ES which docu-
ment features to use when applying the model.                            POST s o m e i n d e x / _ s e a r c h
Ranking. Elasticsearch ranks retrieved documents by applying             { " query " : {
                                                                         " match " : {
re-scoring methods, because executing a query on the entire Elas-
                                                                         " _ a l l " : " J o n Snow " } } ,
ticsearch cluster is very expensive. The system first executes a         " rescore " : {
baseline relevance query on the entire index and returns the top N       " window_size " : 1000 ,
results. The Rescorer then modifies the scores for the top N results     " query " : {
and returns the new list. DELTR implements Elastic’s Rescorer            " rescore_query " : {
                                                                         " sltr " : {
                                                                         " params " : {
                                                                         " keywords " : " J o n Snow " } ,
                                                                         " model " : " d e l t r _ m o d e l " , } } } } }

                                                                         The above code constitutes a sample rescore query using DELTR, in
                                                                         which we limit the result set to documents that match “Jon Snow”.
                                                                         All results are scored based on Elasticsearch’s default similarity
                                                                         (BM25). On top of those already somewhat relevant results we apply
                                                                         our DELTR model to get the best and fairest ranking of the top 1000
                                                                         documents.
                                                                         4     FAIRSEARCH: THE FA*IR PLUGIN
                                                                         The FA*IR plugin enables Elasticsearch to process a search query
                                                                         and re-rank the result using FA*IR with parameters k, p and α. It
                                                                         extends the Elasticsearch API by two new endpoints and a fair
                                                                         rescorer JSON object, that contains the parameters for FA*IR. The
                                                                         two new endpoints create a new or request an existing MTable, an
                                                                         integer array that implements table 1. Once generated, MTables are
                                                                         persisted within Elasticsearch for further usage to avoid additional
                                                                         computational costs at search time. Figure 2a shows the control
Figure 3: Architecture of the Elasticsearch plugin integra-              flow inside the plugin. A FA*IR query is passed to Elasticsearch, and
tion for DELTR




                                                                   174
WWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan                                                                                               Zehlike et al.

    Algorithm 1: Construct MTable
     INPUT: Ranking size k, minimum proportion p,
      significance α;
     OUTPUT: MTable M ∈ Nk
     M ← 0k ;
     αc ← adjustAlpha(k, p, α);
     for i := 1 to k do
         Mi ← inverseCDF (i, p, αc );
     end
     return M;

Elastic returns the standard result ranking to the plugin. The plugin
                                                                                Figure 4: Re-ranking an Elasticsearch result according to a
then re-ranks the result according to the respective MTable that
                                                                                MTable; Shields indicate protected items
matches the input parameters p, k and α. Note that the execution of
an unaware search query with all built-in features is still possible.           6    DEMONSTRATION
                                                                                All libraries and plugins are available at https://github.com/fair-search.
POST s o m e i n d e x / _ s e a r c h
                                                                                Our demo will consist of two main parts: First we will explain the
{ " from " : 0 ,
" size " : k ,
                                                                                architecture of FA*IR and DELTR by use of the figures in this paper.
" q u e r y " : { " match " : { " body " : q } } ,                              Next we will have a live coding session. For FA*IR we will code
" rescore " : {                                                                 a mini example that is going to setup the algorithm in an Elastic-
" window_size " : k ,                                                           search instance. It will show how to integrate the parameters p
" fair_rescorer " : {                                                           and α and how to further interact with the Elasticsearch plugin via
" protected_key " : " gender " ,                                                search queries. An introduction into the FA*IR python library and
" protected_value " : " f " ,                                                   Elasticsearch plugin is available on YouTube [11]. For DELTR we
" s i g n i f i c a n c e _ l e v e l " : alpha ,                               will use the synthetic dataset from section 2.1 to train a fair model.
" min_proportion_protected " : p } } }                                          We will show how to upload this model into Elasticsearch using the
                                                                                DELTR-Wrapper and how it is used when issuing a search query.
The components communicate via a REST API for HTTP requests                        Second using the results from the live coding session we will
and the above code represents a HTTP request to the plugin. With                observe how the algorithms influence ranking results on a demo
this Elasticsearch executes a regular search using the specified                website (Figure 2b) for job candidate search, which operates on a
query object, the match object and query terms q. The result is                 resume dataset [1]. Lastly we will demonstrate how different input
re-ranked by the plugin using FA*IR, if the fairness constraints                parameters for DELTR and FA*IR will affect the results and give
named in p, k and α are not met. First the MTable Handler will                  intuition on best practice choices for the parameters. These two
check if a MTable for parameters k, p, α already exists (right side             parts are also shown in the YouTube tutorial.
of Figure 2a). If not, the plugin calls the MTable Generator to                    We require a large screen, so that attendees will be able to follow
create it using algorithm 1 and stores it to MTable Storage as                  the coding examples from a distance.
key-value pairs with key (k, p, α). We note that the MTable handler
in Figure 2a is a simplification of Java classes and interfaces for the         REFERENCES
purpose of presentation. The FA*IR ranker (Figure 2a) re-ranks the               [1] 2018. Resumes Dataset with Labels. (2018).            https://www.kaggle.com/
                                                                                     iammhaseeb/resumes-dataset-with-labels Accessed: 2018-11-02.
Elasticsearch results according to the requested MTable (Figure 4)               [2] Toon Calders and Indrė Žliobaitė. 2013. Why unbiased computational processes
and returns them through a HTTP response in JSON format like a                       can lead to discriminative decision procedures. In Discrimination and Privacy in
                                                                                     the Information Society. Springer, 43–57.
standard Elasticsearch result.                                                   [3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning
                                                                                     to rank: from pairwise approach to listwise approach. In Proceedings of the 24th
5   CONCLUSION                                                                       international conference on Machine learning. ACM, 129–136.
In this paper we presented FairSearch, the first open source API                 [4] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
                                                                                     Zemel. 2012. Fairness through awareness. In Proc. of ITCS. ACM Press, 214–226.
for search engines to provide fair search results. We implemented                [5] Moritz Hardt. 2014. How big data is unfair: Understanding sources of unfairness
our previously published methods as stand-alone libraries in Python                  in data driven decision making. (2014).
                                                                                 [6] Stephen E Robertson. 1977. The probability ranking principle in IR. Journal of
and Java and embedded those into a plugins for Elasticsearch. While                  documentation 33, 4 (1977), 294–304.
the plugins are intended to be off-the-shelf implementations for                 [7] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings.
Elasticsearch engineers, the stand-alone libraries allow great flexi-                In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
                                                                                     Discovery & Data Mining. ACM, 2219–2228.
bility for those who use other technology such as Solr. This way                 [8] Latanya Sweeney. 2013. Discrimination in online ad delivery. Queue 11, 3 (2013),
we hope that fairness-aware algorithms will make their way faster                    10.
into productive code and business environments to avoid bad social               [9] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Mega-
                                                                                     hed, and Ricardo Baeza-Yates. 2017. FA*IR: A fair top-k ranking algorithm. In
consequences such as discrimination in search results.                               Proc. of the 2017 ACM on Conference on Information and Knowledge Management.
                                                                                     ACM, 1569–1578.
Acknowledgments. This project was realized with a research                      [10] Meike Zehlike and Carlos Castillo. 2018. Reducing disparate exposure in ranking:
grant from Data Transparency Lab. Castillo is partially funded                       A learning to rank approach. arXiv preprint arXiv:1805.08716 (2018).
by La Caixa project LCF/PR/PR16/11110009. Zehlike is funded by                  [11] Meike Zehlike and Tom Sühr. 2019. FA*IR in FairSearch – Tutorial. (05 01 2019).
                                                                                     https://youtu.be/UXxTijlb5SY
the MPI-SWS.




                                                                          175
