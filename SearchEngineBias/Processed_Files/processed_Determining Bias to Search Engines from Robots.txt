determining
bias
search
engines
robots
txt
yang
sun
ziming
zhuang
isaac
councill
lee
giles
information
sciences
technology
pennsylvania
state
university
university
park
pa
16801
usa
ysun
zzhuang
icouncill
giles
ist
psu
edu
abstract
search
engines
largely
rely
robots
crawlers
spiders
collect
information
web
crawling
activities
can
regulated
server
side
deploying
robots
exclusion
protocol
file
called
robots
txt
ethical
robots
will
follow
rules
specified
robots
txt
websites
can
explicitly
specify
access
preference
robot
name
biases
may
lead
rich
get
richer
situation
popular
search
engines
ultimately
dominate
web
preferred
access
resources
inaccessible
others
issue
seldom
addressed
although
robots
txt
convention
become
de
facto
standard
robot
regulation
search
engines
become
indispensable
tool
information
access
propose
metric
evaluate
degree
bias
specific
robots
subjected
investigated
593
websites
covering
education
government
news
business
domains
collected
925
distinct
robots
txt
files
results
content
statistical
analysis
data
confirm
robots
popular
search
engines
information
portals
google
yahoo
msn
generally
favored
websites
sampled
results
also
show
strong
correlation
search
engine
market
share
bias
toward
particular
search
engine
robots
introduction
without
robots
probably
search
engines
web
search
engines
digital
libraries
many
web
applications
offline
browsers
internet
marketing
software
intelligent
searching
agents
heavily
depend
robots
acquire
documents
robots
also
called
spiders
crawlers
bots
harvesters
self-acting
agents
navigate
around-the-clock
hyperlinks
web
harvesting
topical
resources
zero
costs
greatly
abbreviated
version
paper
appeared
poster
proceedings
16th
international
world
wide
web
conference
2007
human
management
14
highly
automated
nature
robots
rules
must
made
regulate
crawling
activities
order
prevent
undesired
impact
server
workload
access
non-public
information
robots
exclusion
protocol
proposed
12
provide
advisory
regulations
robots
follow
file
called
robots
txt
contains
robot
access
policies
deployed
root
directory
website
accessible
robots
ethical
robots
read
file
obey
rules
visit
website
robots
txt
convention
adopted
community
since
late
1990s
continued
serve
one
predominant
means
robot
regulation
however
despite
criticality
robots
txt
convention
content
providers
harvesters
little
work
done
investigate
usage
detail
especially
scale
web
importantly
websites
may
favor
disfavor
certain
robots
assigning
different
access
policies
bias
can
lead
rich
get
richer
situation
whereby
popular
search
engines
granted
exclusive
access
certain
resources
turn
make
even
popular
considering
fact
users
often
prefer
search
engine
broad
exhaustive
information
coverage
rich
get
richer
phenomenon
may
introduce
strong
influence
users
choice
search
engines
will
eventually
reflected
search
engine
market
share
hand
since
often
believed
although
exaggeration
searchable
exist
phenomenon
may
also
introduce
biased
view
information
web
1.1
related
work
contributions
1999
study
usage
robots
txt
10
uk
universities
colleges
investigated
163
websites
53
robots
txt
robots
txt
files
examined
terms
file
size
use
robots
exclusion
protocol
within
uk
university
domains
2002
drott
studied
usage
robots
txt
aid
indexing
protect
information
60
samples
fortune
global
500
company
web
sites
manually
examined
work
concluding
robots
txt
files
widely
used
sampled
group
sites
appear
redundant
exclude
robots
directories
locked
anyway
investigation
shows
contrary
result
may
due
difference
sample
size
domain
time
work
addresses
legal
aspects
obeying
robots
txt
overview
web
robots
robots
txt
usage
given
none
aforementioned
work
investigates
content
robots
txt
terms
biases
towards
different
robots
addition
sample
sizes
previous
studies
tended
relatively
small
considering
size
web
paper
present
first
quantitative
study
biases
conduct
comprehensive
survey
robots
txt
usage
web
implementing
specialized
robots
txt
crawler
collect
real-world
data
considerable
amount
unique
websites
different
functionalities
covering
domains
education
government
news
business
investigate
following
questions
type
text
plain
placed
root
directory
web
server
line
robots
txt
file
format
ield
optionalspace
value
optionalspace
three
types
case-insensitive
tags
ield
specify
rules
user-agent
allow
disallow
another
unofficial
directive
crawldelay
also
used
many
websites
limit
frequency
robot
visits
robots
txt
file
starts
one
ser
agent
fields
specifying
robots
rules
apply
followed
number
disallow
allow
fields
indicating
actual
rules
regulate
robot
comments
allowed
anywhere
file
consist
optional
whitespaces
comments
started
comment
character
terminated
linkbreak
sample
robots
txt
listed
robots
txt
file
botseer3
user-agent
disallow
robots
disallow
src
disallow
botseer
disallow
uastring
disallow
srcseer
disallow
robotstxtanalysis
disallow
whois
robot
bias
exist
bias
measured
quantitatively
implication
bias
contributions
user-agent
googlebot
disallow
robotstxtanalysis
disallow
uastring
propose
quantitative
metric
automatically
measure
robot
biases
applying
metric
large
sample
websites
present
findings
favored
disfavored
robots
rest
paper
organized
follows
section
briefly
introduce
robots
exclusion
protocol
section
present
data
collection
study
section
propose
bias
metric
demonstrate
applied
measure
degree
robot
bias
section
present
observations
robots
txt
usage
discuss
implications
section
conclude
paper
plans
future
work
robots
exclusion
protocol
robots
exclusion
protocol2
convention
allows
website
administrators
indicate
visiting
robots
parts
site
visited
robots
txt
file
website
robots
free
crawl
content
format
robots
exclusion
protocol
described
12
file
named
robots
txt
internet
media
http://www.robotstxt.org/wc/norobots.html
user-agent
botseer
disallow
shows
googlebot
visit
robotstxtanalysis
uastring
botseer
can
visit
directory
file
server
robots
follow
rules
ser
agent
visit
directories
files
matching
robots
src
botseer
uastring
srcseer
robotstxtanalysis
whois
robot
bias
propose
measure
favoribility
robots
across
sample
robots
txt
files
measure
degree
specific
robots
favored
disfavored
set
websites
formal
definition
robot
bias
favored
disfavored
described
3.1
getbias
algorithm
definition
favored
robot
robot
allowed
access
directories
universal
robot
according
http://botseer.ist.psu.edu/robots.txt
robots
txt
file
website
universal
robot
robot
matched
specific
user-agent
names
robots
txt
file
words
universal
robot
represents
robots
appear
name
robots
txt
file
let
set
robots
txt
files
dataset
given
robots
txt
file
let
denote
set
named
robots
given
robots
txt
file
named
robot
define
getbias
algorithm
specified
algorithm
getbias
measures
degree
named
robot
favored
disfavored
given
robots
txt
file
algorithm
getbias
return
end
construct
dir
bias
dir
allowed
du
end
10
end
11
dir
12
allowed
13
dr
14
end
15
end
16
bias
dr
du
17
return
bias
let
dir
set
directories
appear
robots
txt
file
specific
website
dir
used
estimation
actual
directory
structure
website
robot
exclusion
protocol
considers
directory
website
match
directories
robots
txt
allowed
directory
default
du
dir
set
directories
universal
robot
allowed
visit
rules
specified
ser
agent
universal
robot
can
access
everything
default
dr
dir
set
directories
given
robot
allowed
visit
du
dr
number
directories
du
dr
given
robot
algorithm
first
counts
many
directories
dir
allowed
calculates
bias
score
robot
difference
number
directories
dir
allowed
robot
number
directories
allowed
universal
robot
getbias
algorithm
bias
universal
robot
treated
reference
point
getbias
returns
bias
scores
favored
robots
returned
getbias
positive
values
higher
score
robot
means
robot
favored
contrary
bias
scores
disfavored
robots
returned
getbias
negative
values
consistent
bias
definition
thus
bias
robot
robots
txt
file
can
represented
categorical
variable
three
categories
favored
disfavored
bias
example
consider
robots
txt
file
http://botseer.ist.psu.edu
section
dir
robots
src
botseer
uastring
srcseer
robotstxtanalysis
whois
according
algorithm
du
null
dbotseer
robots
src
botseer
uastring
srcseer
robotstxtanalysis
whois
dgoogle
robots
src
botseer
srcseer
whois
thus
du
dbotseer
dgoogle
according
algorithm
biasu
du
du
biasbotseer
dbotseer
du
biasgooglebot
dgoogle
du
thus
robots
googlebot
botseer
favored
website
categorized
favored
robots
will
categorized
bias
3.2
measuring
overall
bias
based
bias
score
file
propose
favorability
order
evaluate
degree
specific
robot
favored
disfavored
set
robots
txt
files
let
total
number
robots
txt
files
dataset
favorability
robot
can
defined
pf
avor
pdisf
avor
nf
avor
ndisf
avor
nf
avor
ndisf
avor
number
times
robot
favored
disfavored
respectively
pf
avor
proportion
robots
txt
files
robot
favored
pdisf
avor
proportion
robots
txt
files
robot
disfavored
proportions
robots
txt
files
favor
disfavor
specific
robot
simple
measures
survey
statistics
however
dataset
two
proportions
isolation
accurate
reflecting
overall
biases
sample
since
two
events
favor
disfavor
bias
means
pf
avor
pdisf
avor
event
reflects
one
aspect
bias
example
robot
named
ia
archiver
favored
0.24
websites
dataset
proportion
sites
favor
momspider
0.21
alternatively
proportions
sites
disfavor
ia
archiver
momspider
1.9
respectively
consider
favored
proportion
will
reach
conclusion
ia
archiver
favored
momspider
difference
proportions
sites
favor
disfavor
specific
robot
thus
treats
cases
unison
example
ia
archiver
1.66
momspider
0.21
thus
momspider
favored
ia
archiver
no-bias
robot
bias
measure
can
eliminate
misleading
cases
still
intuitively
understandable
favored
robots
positive
numbers
disfavored
robots
negative
numbers
3.3
examining
favorability
favorability
actually
ranking
function
robots
evaluate
accuracy
ranking
function
run
ranking
performance
test
based
kendall
rank
correlation
method
11
rank
correlation
method
briefly
described
details
ranking
performance
evaluation
using
partial
order
can
found
robots
txt
file
let
ma
bias
measure
function
robots
appearing
let
ri
rj
two
named
robots
denote
ri
ma
rj
ri
ranked
higher
rj
measure
ma
thus
two
measure
functions
ma
mb
kendall
can
defined
based
number
pf
concordant
pairs
number
qf
discordant
pairs
pair
ri
rj
concordant
ma
mb
agree
order
ri
rj
discordant
disagree
case
kendall
can
defined
τf
ma
mb
pf
qf
pf
qf
given
measure
ma
mb
τf
ma
mb
represents
well
two
ranking
measures
agree
file
let
ma
represent
actual
ranking
function
robots
although
know
actual
ranking
function
partial
ranking
robots
robots
txt
file
based
bias
score
defined
previously
thus
computing
τf
ma
mb
robots
txt
files
will
show
well
measure
mb
agrees
ma
actual
ranking
robots
file
calculate
τf
ma
mb
robots
txt
files
dataset
τf
ma
mb
given
robots
txt
file
consider
file
concordant
file
ma
mb
otherwise
file
discordant
file
counting
concordant
files
discordant
files
dataset
can
compute
average
ma
mb
note
thus
ma
mb
2q
rank
robots
using
δp
favorability
ranked
lists
compared
actual
ranking
using
method
introduced
average
value
0.957
believe
accurate
enough
measure
overall
bias
robot
data
collection
observe
potential
robot
bias
web
work
studies
wide
range
websites
different
domains
different
physical
locations
data
collection
described
detail
4.1
data
sources
open
directory
project
largest
comprehensive
human-maintained
web
directory
primary
source
collect
initial
urls
feed
crawler
dmoz
open
directory
project
classifies
large
url
collection
different
categories
enables
us
collect
data
different
domains
physical
locations
collection
open
directory
project
covers
three
domains
education
news
government
university
domain
broken
american
european
asian
university
domains
since
directory
structure
business
domain
dmoz
complicated
significant
amount
overlaps
use
2005
fortune
top
1000
company
list
data
source
certain
limitations
inherent
data
collection
first
website
collection
dmoz
limited
countries
especially
non-english
websites
majority
websites
usa
second
dmoz
entries
organized
human
editors
might
errors
finally
collect
business
websites
fortune
1000
list
contains
data
mostly
large
corporations
data
domain
may
representative
small
businesses
intend
address
limitations
future
research
4.2
crawling
robots
txt
implemented
specialized
focused
crawler
study
crawler
starts
crawling
metadata
website
obtained
dmoz
including
functional
classification
name
website
physical
location
affiliated
organization
crawler
checks
existence
robots
txt
domain
downloads
existing
robots
txt
files
offline
analysis
parsing
filtering
module
also
integrated
crawler
eliminate
duplicates
ensuring
retrieved
pages
within
target
domain
besides
root
level
directory
crawler
also
examines
possible
locations
robots
txt
file
subdirectories
website
level
inspected
results
show
cases
robots
txt
placed
root
directory
according
robots
exclusion
protocol
misspelled
filenames
also
examined
crawler
rare
cases
filename
robot
txt
will
ignored
robots
used
instead
robots
txt
order
observe
temporal
properties
crawler
performed
crawls
set
websites
dec
2005
oct
2006
order
analyze
temporal
properties
downloaded
robots
txt
files
archived
according
date
crawl
4.3
statistics
crawled
investigated
593
unique
websites
including
600
government
websites
2047
newspaper
websites
1487
usa
university
websites
1420
european
university
websites
1039
asian
university
websites
1000
company
websites
number
websites
robots
txt
files
domain
crawls
shown
table
government
newspaper
usa
univ
european
univ
company
asian
univ
total
collected
robots
txt
files
websites
dec
jan
may
sep
oct
2005
2006
2006
2006
2006
600
248
257
263
262
264
2047
859
868
876
937
942
1487
615
634
650
678
683
1420
497
510
508
524
537
1000
1039
7593
figure
probability
website
robots
txt
domain
user-agent
field
used
2744
times
means
93.8
robots
txt
files
rules
universal
robots
72.4
named
robots
appeared
twice
frequently
appearing
robots
dataset
shown
figure
303
306
319
341
339
140
248
149
165
160
2662
2823
2765
2907
2925
table
number
robots
txt
found
domain
crawl
better
describe
usage
robots
txt
websites
different
domains
figure
illustrates
proportion
websites
robots
txt
domain
overall
except
case
asian
university
websites
usage
robots
txt
increased
46.02
newspaper
websites
currently
implemented
robots
txt
files
newspaper
domain
domain
robots
exclusion
protocol
frequently
adopted
45.93
usa
university
websites
sample
adopt
robots
exclusion
protocol
significantly
european
37.8
asian
15.4
sites
since
search
engines
intelligent
searching
agents
become
important
accessing
web
information
result
expected
robots
exclusion
protocol
frequently
adopted
government
newspaper
university
websites
usa
used
extensively
protect
information
offered
public
balance
workload
websites
detailed
robots
txt
usage
report
can
found
16
results
1056
named
robots
found
dataset
universal
robot
frequently
used
robot
figure
frequently
used
robot
names
robots
txt
files
height
bar
represents
number
times
robot
appeared
dataset
5.1
history
bias
distribution
many
times
robot
used
see
figure
change
significantly
past
11
months
thus
show
bias
results
latest
crawl
since
much
changed
since
robots
appeared
twice
dataset
ranking
scores
ranked
middle
list
almost
indistinguishable
consider
top
ranked
favored
bottom
ranked
disfavored
robots
10
favored
robots
10
figure
distribution
robot
used
disfavored
robots
listed
table
sample
size
nf
avor
number
times
robot
favored
ndisf
avor
number
times
robot
dis
favored
categorical
stann
dard
deviation
13
categorical
standard
deviation
gives
variance
using
estimate
favorability
robots
web
bias
measure
shows
highly
favored
robots
well-known
search
engines
organizations
google
yahoo
msn
favored
much
remaining
robots
please
note
robots
disfavored
category
favorability
show
significant
difference
due
rare
appearances
sampled
robots
txt
files
hand
disfavored
robots
email
collectors
cherrypicker
emailsiphon
off-line
browsers
wget
webzip
privacy
perspective
reasonable
webmasters
exclude
robots
whose
major
purpose
collect
private
information
also
webmasters
typically
want
websites
copied
entirely
others
however
even
robots
well-known
companies
can
disfavored
msiecrawler
microsoft
ia
archiver
alexa
msiecrawler
robot
embedded
internet
explorer
ie
ie
users
bookmark
page
offline
msiecrawler
downloads
page
links
related
including
links
images
javascript
style
sheets
user
next
online
ia
archiver
crawler
archive
org
alexa
com
list
detailed
description
known
robots
appeared
paper
can
found
web4
also
find
robot
biases
different
domains
vary
http://botseer.ist.psu.edu/namedrobots.html
favored
robots
sample
size
2925
robot
nf
avor
ndisf
name
google
877
25
0.2913
0.0084
yahoo
631
34
0.2041
0.0075
msn
349
0.1162
0.0059
scooter
341
15
0.1104
0.0058
lycos
91
0.0294
0.0031
netmechanic
84
10
0.0253
0.0029
htdig
15
0.0041
0.0012
teoma
13
0.0034
0.0011
oodlebot
0.0027
0.0010
momspider
0.0021
0.0008
disfavored
robots
sample
size
2925
robot
nf
avor
ndisf
name
msiecrawler
85
0.0291
0.0031
ia
archiver
55
0.0164
0.0023
cherrypicker
37
0.0126
0.0021
emailsiphon
34
0.0106
0.0019
roverbot
27
0.0085
0.0017
psbot
23
0.0079
0.0016
webzip
21
0.0072
0.0016
wget
22
0.0072
0.0016
linkwalker
20
0.0062
0.0015
asterias
18
0.0062
0.0015
table
top
10
favored
disfavored
robots
standard
deviation
significantly
google
always
favored
robot
top
favored
robots
vary
different
domains
yahoo
slurp
yahoo
robot
msn
also
favored
domains
significantly
favored
robots
top
favored
robots
mostly
open
source
crawlers
crawlers
well-known
organizations
disfavored
robot
lists
vary
widely
different
domains
robots
still
email
collectors
offline
browsers
differences
due
different
behaviors
robots
different
domains
emailsiphon
may
crawl
business
websites
often
others
collect
business
contacts
5.2
search
engine
market
vs
robot
bias
order
study
impact
rich
get
richer
effect
calculate
correlation
robot
bias
search
engine
market
share
specific
companies
market
share
google
yahoo
msn
ask
past
11
months
favorability
corresponding
robots
considered
two
independent
variables
pearson
product-moment
correlation
coefficient
15
pmcc
two
variables
measure
tendency
two
variables
measured
object
organism
increase
decrease
together
dataset
pearson
correlation
market
share
four
companies
corresponding
robots
0.930
p-value
0.001
search
engine
market
share
robot
bias
september
2006
shown
figure
web
robots
behavior
will
undertaken
order
better
understand
live
robots
interpret
robots
exclusion
protocol
references
fortune
magazine
http://money.cnn.com/magazines/fortune/fortune500,
2005
boonk
groot
brazier
oskamp
agent
exclusion
websites
proceedings
4th
workshop
law
electronic
agents
2005
chakrabarti
van
den
berg
dom
focused
crawling
new
approach
topic-specific
web
resource
discovery
proc
8th
www
conference
pages
545
562
1999
cho
garcia-molina
page
efficient
crawling
url
ordering
proceedings
7th
international
world
wide
web
conference
1998
chun
world
wide
web
robots
overview
online
information
review
23
135
142
1999
figure
search
engine
market
share
vs
robot
bias
conclusions
presented
comprehensive
survey
robot
biases
web
careful
content
statistical
analysis
large
sample
robots
txt
files
results
show
robots
popular
search
engines
information
portals
google
yahoo
msn
generally
favored
websites
sampled
implies
rich
get
richer
bias
toward
popular
search
engines
also
shows
strong
correlation
search
engine
market
share
bias
toward
corresponding
robots
study
indicates
usage
robots
txt
increased
past
11
months
662
robots
txt
files
found
first
crawl
925
files
found
last
crawl
observe
46.02
newspaper
websites
currently
implemented
robots
txt
files
newspaper
domain
domain
robots
exclusion
protocol
frequently
adopted
45.93
usa
university
websites
sample
adopt
robots
exclusion
protocol
significantly
european
37.8
asian
15.4
sites
future
work
will
try
break
analysis
geographical
region
investigate
robot
favorability
country
future
work
will
pursue
deeper
larger
scale
analysis
robots
behavior
regulations
investigating
metrics
robot
bias
experimental
investigation
http://www.netratings.com
dmoz
open
directory
project
http://dmoz.org,
2005
drott
indexing
aids
corporate
websites
use
robots
txt
meta
tags
information
processing
management
38
209
219
2002
eichmann
ethical
web
agents
computer
networks
isdn
systems
28
127
136
1995
joachims
optimizing
search
engines
using
clickthrough
data
kdd
02
proceedings
eighth
acm
sigkdd
international
conference
knowledge
discovery
data
mining
pages
133
142
new
york
ny
usa
2002
acm
press
10
kelly
peacock
webwatching
uk
web
communities
final
report
webwatch
project
british
library
research
innovation
report
1999
11
kendall
rank
correlation
methods
hafner
1955
12
koster
method
web
robots
control
internet
draft
internet
engineering
task
force
ietf
1996
13
ott
longnecker
introduction
statistical
methods
data
analysis
duxbury
press
5th
edition
2000
14
pant
srinivasan
menczer
crawling
web
chapter
web
dynamics
springer-verlag
2004
15
sokal
rohlf
biometry
freeman
new
york
2001
16
sun
zhuang
giles
large-scale
study
robots
txt
www
07
proceedings
16th
international
conference
world
wide
web
pages
1123
1124
new
york
ny
usa
2007
acm
press