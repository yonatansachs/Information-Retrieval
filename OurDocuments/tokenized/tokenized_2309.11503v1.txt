fairness
vs
personalization
towards
equity
in
epistemic
utility
jennifer
chien
uc
san
diego
usa
david
danks
uc
san
diego
usa
the
applications
of
personalized
recommender
systems
are
rapidly
expanding
encompassing
social
media
online
shopping
search
engine
results
and
more
these
systems
oÔ¨Äer
more
eÔ¨Écient
way
to
navigate
the
vast
array
of
items
available
however
alongside
this
growth
there
has
been
increased
recognition
of
the
potential
for
algorithmic
systems
to
exhibit
and
perpetuate
biases
risking
arxiv
2309
11503v1
cs
ir
sep
2023
unfairness
in
personalized
domains
in
this
work
we
explicate
the
inherent
tension
between
personalization
and
conventional
implementations
of
fairness
as
an
alternative
we
propose
equity
to
achieve
fairness
in
the
context
of
epistemic
utility
we
provide
mapping
between
goals
and
practical
implementations
and
detail
policy
recommendations
across
key
stakeholders
to
forge
path
towards
achieving
fairness
in
personalized
systems
additional
key
words
and
phrases
fairness
personalization
recommender
systems
equity
epistemic
utility
epistemic
harms
policy
acm
reference
format
jennifer
chien
and
david
danks
2023
fairness
vs
personalization
towards
equity
in
epistemic
utility
september
2023
11
pages
introduction
personalized
algorithmic
systems
are
being
increasingly
deployed
in
broad
range
of
sectors
these
algorithms
hold
the
potential
to
provide
more
appropriate
outputs
on
an
individual
basis
by
personalizing
based
on
preferences
values
needs
or
environmental
conditions
most
prominently
recommender
systems
are
now
widely
used
to
provide
the
most
useful
recommendations
to
each
individual
in
the
given
domain
at
the
same
time
we
have
seen
signiÔ¨Åcant
increase
in
the
recognition
that
algorithms
can
exhibit
biases
and
produce
unfair
or
unjust
outcomes
there
are
many
diÔ¨Äerent
potential
sources
of
bias
in
algorithms
and
models
and
hence
many
diÔ¨Äerent
responses
may
be
appropriate
or
required
algorithm
development
and
deployment
eÔ¨Äorts
now
typically
recognize
the
possibility
of
algorithmic
bias
and
the
need
to
often
do
something
to
mitigate
it
we
contend
that
these
two
desiderata
for
algorithms
personalization
and
fairness
stand
in
signiÔ¨Åcant
tension
at
high
level
personalization
is
fundamentally
about
treating
each
individual
in
distinct
ways
the
goal
is
to
not
give
the
same
output
for
each
individual
but
rather
to
tailor
the
outputs
to
their
speciÔ¨Åc
situation
in
contrast
fairness
is
fundamentally
about
treating
individuals
similarly
the
algorithm
ought
to
be
the
same
in
some
sense
for
everyone
of
course
this
high-level
gloss
on
the
tension
is
far
too
quick
for
example
fairness
allows
for
diÔ¨Äerential
treatment
or
outcomes
as
long
as
it
is
based
on
morally
or
legally
defensible
grounds
nonetheless
these
high-level
observations
point
towards
tension
that
we
contend
continues
to
hold
when
we
look
deeper
more
speciÔ¨Åcally
we
analyze
fairness
in
the
context
of
personalized
systems
using
the
notion
of
epistemic
utility
essentially
the
beneÔ¨Åt
that
an
agent
receives
by
an
epistemic
improvement
reduction
in
uncertainty
and
provide
both
practical
and
policy
guidance
about
how
to
achieve
fair
personalized
systems
authors
addresses
jennifer
chien
jjchien@ucsd.edu
uc
san
diego
usa
david
danks
ddanks@ucsd.edu
uc
san
diego
usa
2023
manuscript
submitted
to
acm
manuscript
submitted
to
acm
jennifer
chien
and
david
danks
preliminaries
recommender
system
we
consider
recommender
system
as
that
which
uses
an
algorithmic
approach
to
provide
an
ordering
or
prioritization
for
items
based
on
relevance
to
user
this
form
of
personalization
is
often
based
on
their
preferences
past
behavior
or
similarities
with
other
users
with
the
overarching
goal
of
improving
user
experience
and
engagement
group
fairness
algorithmic
fairness
is
typically
framed
as
metric
parity1
across
two
or
more
relevantly
comparable
entities
such
that
the
diÔ¨Äerence
between
them
is
deemed
socially
legally
and
or
culturally
irrelevant
for
instance
group
fairness
may
be
deÔ¨Åned
as
demographic
parity
meaning
equality
in
the
likelihood
of
given
outcome
conditioned
on
the
group
gender
age
disability
formally
ùë¶ÀÜ
ùëîùëñ
ùë¶ÀÜ
ùëîùëñ
where
is
the
total
number
of
groups
and
ùë¶ÀÜ
is
the
likelihood
of
given
outcome
epistemic
utility
we
deÔ¨Åne
epistemic
utility
as
the
value
or
usefulness
of
information
in
terms
of
improving
knowledge
understanding
or
beliefs
it
encompasses
the
epistemic
beneÔ¨Åts
gained
from
acquiring
accurate
reliable
and
relevant
information
these
beneÔ¨Åts
could
of
course
enhance
decision-making
prediction
problem-solving
or
overall
intellectual
development
but
epistemic
utility
refers
only
to
the
beneÔ¨Åts
in
terms
of
the
agent
epistemic
state
even
if
those
improvements
do
not
immediately
lead
to
improved
outcomes
it
thereby
captures
the
intrinsic
value
of
acquiring
knowledge
and
understanding
it
underscores
the
role
of
information
as
valuable
resource
for
intellectual
growth
social
mobility
and
production
and
captures
the
motivational
value
of
information
for
people
behavior
related
work
this
work
is
most
closely
related
to
eÔ¨Äorts
to
understand
fairness
in
recommender
systems
many
of
which
deÔ¨Åne
fairness
as
parity
in
some
performance
measure
across
some
sensitive
attribute
10
methods
for
individual
notions
of
fairness
similarly
require
domain-speciÔ¨Åc
similarity
function
and
focus
on
trade-oÔ¨Äs
between
individual
and
group
fairness
11
12
in
contrast
we
start
from
the
position
that
fairness
requires
equity
across
all
individuals
rather
than
being
deÔ¨Åned
solely
through
comparisons
this
approach
enables
us
to
account
for
cases
of
unfairness
where
everyone
is
worse
oÔ¨Ä
including
situations
with
non-comparative
injustices
not
only
those
in
which
one
group
is
more
speciÔ¨Åcally
prior
work
has
aimed
to
ensure
proportional
representation
and
exposure
often
in
e-commerce
settings
measures
such
as
disparate
exposure
and
visibility
still
require
access
to
sensitive
or
group
attributes
10
calibration
in
which
the
expected
proportions
of
predicted
classes
match
those
observed
in
available
data
implements
group
fairness
without
explicit
knowledge
of
sensitive
attribute
status
13
14
15
16
17
however
its
application
in
domains
such
as
news
can
contribute
to
hegemonic
regimes
of
representation
making
it
limited
to
applications
where
less
diversity
is
valued
by
users
17
additionally
fairness
deÔ¨Ånitions
that
target
disparities
in
utility
compare
average
sensitive
attribute
group
performance
rather
than
providing
assurances
for
individuals
or
the
entire
population
as
whole
11
18
19
20
concerns
about
unfairness
in
personalized
algorithms
is
connected
to
research
on
epistemic
injustice
21
the
core
concern
in
this
latter
work
is
that
epistemic
limitations
may
impair
an
agent
ability
to
recognize
or
respond
to
within
some
tolerance
as
is
legally
permissible
the
four-Ô¨Åfths
rule
manuscript
submitted
to
acm
fairness
vs
personalization
towards
equity
in
epistemic
utility
injustice
for
example
if
an
individual
does
not
have
the
concept
of
racial
discrimination
then
they
might
fail
to
detect
its
occurrence
despite
suÔ¨Äering
harms
from
it
that
is
the
injustice
is
not
only
about
what
happens
to
the
individual
but
also
about
their
epistemic
inability
to
recognize
understand
and
respond
to
it
personalized
systems
can
readily
lead
to
diÔ¨Äerent
individuals
having
diÔ¨Äerent
information
in
fact
that
is
one
goal
of
such
systems
and
so
we
expect
that
individuals
would
develop
diÔ¨Äerent
concepts
as
result
hence
personalized
systems
can
signiÔ¨Åcantly
raise
the
risks
of
epistemic
injustice
research
on
over-personalization
and
information
access
is
similarly
closely
related
while
the
inÔ¨Çuence
of
personalization
in
Ô¨Ålter
bubbles
and
echo
chambers
has
been
denied
for
purely
technical
systems
22
23
24
when
considering
human
interactions
the
inÔ¨Çuence
becomes
somewhat
complex
this
is
in
part
due
to
signiÔ¨Åcant
variability
in
how
users
engage
with
site
accept
or
reject
information
and
perceive
the
impartiality
of
the
content
they
receive
25
26
27
28
29
our
research
adopts
sociotechnical
approach
to
deÔ¨Åne
and
scope
the
problem
taking
into
account
the
concept
of
epistemic
utility
and
making
reasonable
assessments
of
human
eÔ¨Äort
core
tension
personalization
and
standard
approaches
to
fairness
fairness
is
often
deÔ¨Åned
as
metric
parity
across
similar
groups
or
individuals
where
determining
similarity
across
units
remains
an
open
question
in
search
settings
one
might
start
by
grouping
together
individuals
with
similar
values
and
interests
as
on
fairness
grounds
they
may
be
expected
to
have
similar
disambiguation
or
content
recommendation
experience
however
determining
values
can
be
diÔ¨Écult
to
operationalize
because
explicit
and
implicit
preferences
may
be
conÔ¨Çicting
or
diÔ¨Écult
to
collect
30
in
addition
the
level
of
granularity
by
which
to
measure
such
values
remains
unknown
in
this
section
we
construct
arguments
for
why
user
similarity
of
values
is
diÔ¨Écult
impractical
or
perhaps
even
impossible
to
determine
thereby
making
similarity-based
fairness
deÔ¨Ånitions
diÔ¨Écult
to
deploy
or
conceptually
impossible
sensitive
attributes
are
problematic
and
coarse
proxies
for
values
the
simplest
approach
to
constructing
similar
groups
for
fairness
considerations
could
use
sensitive
attributes
race
ethnicity
gender
age
religious
aÔ¨Éliation
disability
etc
however
this
approach
assumes
that
individuals
with
the
same
sensitive
attributes
not
only
face
similar
struggles
lived
experiences
discrimination
and
prejudice
but
also
have
relevantly
similar
values
in
practice
this
approach
would
likely
result
in
the
treatment
of
minority
groups
as
monoliths
perhaps
even
as
homogeneous
other
this
is
problematic
not
only
in
its
instrumental
failures
of
insuÔ¨Écient
speciÔ¨Åcity
and
complexity
but
also
in
its
intrinsic
treatment
of
single
sensitive
attribute
as
representative
of
all
values
additionally
conditional
groupings
only
ensure
parity
guarantees
for
the
average
member
of
group
not
for
each
individual
nor
necessarily
an
individual
that
exists
therefore
enabling
unfairness
for
anyone
that
doesn
satisfy
such
assumptions
taking
an
intersectional
approach
considering
subgroups
of
sensitive
attributes
may
better
approximate
similarity
of
values
consider
for
example
race
and
gender
meaning
groups
such
as
black
women
and
white
men
this
still
considers
an
entire
group
of
people
homogeneously
thereby
rendering
the
plight
of
for
example
all
black
women
into
singular
experience
in
the
extreme
considering
all
possible
sub-groups
across
all
sensitive
attributes
faces
practical
challenges
as
each
subgroup
becomes
impractically
small
for
data
availability
purposes
thereby
reifying
sensitive
attributes
in
constant
struggle
for
relative
popularity
and
public
consciousness
31
user
search
history
is
noisy
underdetermines
values
and
can
vary
temporally
an
alternative
approach
would
be
to
use
the
individual
behavior
to
determine
the
relevant
group
for
fairness
considerations
for
instance
in
the
context
manuscript
submitted
to
acm
jennifer
chien
and
david
danks
of
search
engine
we
could
condition
on
exact
search
history
unfortunately
this
approach
faces
three
open
challenges
which
we
highlight
in
the
context
of
search
though
the
problems
naturally
generalize
to
other
systems
that
use
observed
behavior
to
personalize
on
the
basis
of
unobserved
inferred
features
first
not
all
search
behaviors
are
equally
informative
for
example
search
query
may
be
made
due
to
diÔ¨Äerent
situational
demands
homework
third
party
question
or
may
arise
from
very
diÔ¨Äerent
motivations
devil
advocate
vs
searching
for
validative
or
supporting
arguments
hence
speciÔ¨Åc
history
of
search
queries
may
or
may
not
be
informative
about
an
individual
values
the
history
alone
is
not
suÔ¨Écient
second
even
if
search
behaviors
are
genuine
they
are
noisy
indicators
for
the
totality
of
an
individual
interests
or
values
for
example
the
individual
agent
might
not
search
for
topic
they
already
have
suÔ¨Écient
information
on
but
it
may
still
be
core
value
more
generally
consider
two
users
with
identical
search
histories
at
any
given
point
in
time
conditioning
on
search
history
would
imply
that
they
should
receive
similar
experiences
however
it
is
highly
unlikely
that
these
individuals
have
the
exact
same
sets
of
values
and
interests
that
is
multiple
sets
of
values
and
interests
could
be
consistent
with
the
observed
behavior
the
space
of
values
are
underdetermined
by
the
evidence
therefore
since
queries
subject
to
the
limitations
of
observational
data
and
missing
contextual
information
conditioning
on
search
history
alone
has
the
potential
to
greatly
under-specify
an
individual
values
finally
search
history
provides
no
guarantees
of
stability
over
time
consider
again
two
individuals
with
the
same
user
history
adding
diÔ¨Äerent
element
to
each
user
search
history
could
lead
them
to
be
placed
in
fundamentally
diÔ¨Äerent
groups
more
generally
at
any
point
in
time
there
are
inÔ¨Ånitely
many
ways
that
search
history
could
progress
thereby
making
it
impossible
to
guarantee
fairness
over
time
for
these
two
individuals
let
alone
all
fairness
groupings
not
sensitive
to
the
time
and
search
history
therefore
may
not
ever
converge
to
fair
groupings
in
the
limit
this
echoes
similar
problem
in
fairness
research
where
fairness
constraints
at
each
round
of
deployment
do
not
ensure
long-term
aggregate
parity
in
the
same
metric
32
while
this
has
not
rendered
particular
fairness
metrics
obsolete
this
instability
points
to
ambiguity
in
the
extent
to
which
fairness
must
provide
guarantees
temporally
different
approach
5.1
personalization
delving
further
into
the
notion
of
system
tailored
to
each
person
needs
let
us
deÔ¨Åne
personalization
given
search
engine
companies
may
strive
to
deliver
service
that
maximizes
epistemic
utility
and
convenience
in
order
to
avoid
wasting
time
resources
or
eroding
consumer
patience
search
engine
providers
may
optimize
to
disambiguate
queries
as
eÔ¨Éciently
as
possible
increases
in
eÔ¨Éciency
can
be
made
by
leveraging
all
accessible
information
about
user
this
may
include
search
history
publicly
available
data
social
media
proÔ¨Åles
and
inferred
or
disclosed
demographic
information
to
accurately
disambiguate
user
query
this
motivates
the
following
deÔ¨Ånition
deÔ¨Ånition
5.1
inÔ¨Ånite
personalization
consider
platform
that
recommends
relevant
content
to
user
tailored
to
their
previous
history
at
time
since
the
space
of
all
possible
information
is
very
large
for
each
piece
of
content
ùëêùëñ
the
system
calculates
probabilities
ùëêùëñ
estimating
the
likelihood
of
relevance
we
deÔ¨Åne
inÔ¨Ånite
personalization
as
the
phenomena
in
which
system
is
allowed
to
produce
extremely
Ô¨Åne-grained
personalization
formally
there
exists
some
ùëêùëñ
such
that
ùëêùëñ
and
ùëêùëñ
meaning
that
there
exists
some
content
such
that
it
is
arbitrarily
improbable
to
recommend
particular
piece
of
content
ùëêùëñ
to
user
even
if
user
were
to
add
queries
to
their
search
history
manuscript
submitted
to
acm
fairness
vs
personalization
towards
equity
in
epistemic
utility
for
instance
user
with
search
history
exclusively
composed
of
cats
inputs
bengal
toys
into
search
engine
and
receives
recommendations
about
bengal
cat
toys
we
consider
the
system
to
be
exhibiting
inÔ¨Ånite
personalization
if
the
user
can
take
queries
about
dogs
or
anything
else
and
still
not
receive
recommendation
for
bengal
dog
toys
when
searching
bengal
toys
having
deÔ¨Åned
personalization
and
it
eÔ¨Äects
in
the
extreme
case
we
now
deÔ¨Åne
fairness
as
equity
and
provide
motivation
for
the
outcome
we
target
5.2
equity
in
ensuring
parity
across
similar
groups
fairness
guarantees
are
only
made
across
those
deemed
suÔ¨Éciently
similar
an
equity-based
approach
however
centers
around
every
individual
achieving
their
desired
outcome
such
approaches
promote
social
cohesion
collective
liberation
and
address
long-standing
inequities
such
as
poverty
public
health
social
mobility
and
overall
well-being
moreover
they
tackle
concerns
regarding
temporal
stability
and
robustness
by
establishing
consistent
criterion
for
each
deployment
phase
equity-based
approaches
lend
themselves
to
audits
legislation
and
transparency
as
they
deÔ¨Åne
singular
outcome
requirement
for
all
users
an
open
challenge
however
is
what
metrics
to
deÔ¨Åne
equity
with
respect
to
as
each
metric
targets
diÔ¨Äerent
kind
of
unfairness
equity
across
quality
of
information
targets
unfairness
when
one
person
gets
more
utility
from
their
query
compared
to
diÔ¨Äerent
query
of
another
person
for
example
this
type
of
equity
requires
that
we
equalize
the
number
of
recommendations
and
quality
of
information
provided
for
any
given
query
however
this
is
infeasible
as
we
cannot
control
the
quantity
of
information
known
for
given
topic
thus
given
that
there
are
inÔ¨Ånitely
many
distinct
queries
any
user
can
make
ensuring
that
each
query
has
the
same
number
of
relevant
articles
would
require
perfect
knowledge
equity
in
speed
of
access
to
information
targets
unfairness
when
one
person
receives
the
relevant
information
faster
than
another
this
equalizes
the
utility
for
every
user
by
either
slowing
down
or
acting
intentionally
uncooperatively
for
queries
that
are
more
easily
disambiguated
or
speeding
up
the
service
for
slower
queries
both
are
infeasible
the
former
fails
to
maximize
overall
utility
and
sacriÔ¨Åces
function
in
the
name
of
fairness
this
in
turn
renders
adoption
by
stakeholders
subject
to
capitalistic
pressures
even
more
unlikely
the
latter
is
also
infeasible
as
we
cannot
directly
control
disambiguation
rate
and
therefore
speed
up
some
queries
over
others
more
generally
information
is
an
inÔ¨Ånitely
shareable
resource
so
providing
information
to
one
person
does
not
deprive
someone
else
the
beneÔ¨Åt
of
that
same
information
therefore
speed
of
access
to
information
is
poor
target
for
an
equity-based
intervention
in
contrast
equity
in
epistemic
utility
targets
unfairness
in
personalized
access
to
the
information
resources
this
is
motivated
by
the
intuition
that
although
we
cannot
directly
control
the
availability
of
information
or
disambiguation
rate
everyone
should
have
some
baseline
access
to
all
information
thus
we
propose
an
upper
bound
on
the
number
of
queries
to
access
to
any
piece
of
information
this
means
that
if
the
information
does
exist
all
users
should
be
able
to
access
it
by
exerting
reasonable
amount
of
eÔ¨Äort
this
does
not
require
that
all
queries
have
the
same
accessibility
or
disambiguation
nor
that
all
users
be
provided
the
exact
same
service
it
does
provide
soft
guarantee
that
everyone
is
able
to
achieve
some
base-level
epistemic
utility
without
requiring
the
grouping
or
comparison
of
individuals
on
the
basis
of
their
values
needs
or
histories
epistemic
utility
particularly
through
the
internet
is
recognized
as
universal
human
right
by
article
19
of
the
universal
declaration
of
human
rights
33
stating
that
everyone
has
the
right
to
freedom
to
seek
receive
and
impart
information
and
ideas
through
any
media
and
regardless
of
frontiers
manuscript
submitted
to
acm
jennifer
chien
and
david
danks
deÔ¨Ånition
5.2
ùúñ-equity
fairness
given
user
with
history
we
deÔ¨Åne
ùúñ-equity
fairness
as
the
upper
bound
of
the
number
of
additions
to
the
search
history
such
that
for
any
given
content
ùëêùëñ
adding
queries
to
the
history
will
result
in
ùëêùëñ
enabling
the
user
to
obtain
the
relevant
content
here
we
consider
the
case
in
which
the
relevant
content
does
not
exist
as
out
of
scope
and
focus
purely
on
access
embracing
equity
as
fundamental
principle
of
fairness
presents
range
of
complex
challenges
these
challenges
encompass
deÔ¨Åning
baseline
level
of
equity
for
all
users
determining
the
pertinent
factors
that
contribute
to
establishing
such
baseline
and
deliberating
whether
these
considerations
should
be
domain-speciÔ¨Åc
or
universally
applicable
rather
than
proposing
singular
standard
we
propose
adopting
heuristic
operationalization
that
prioritizes
those
who
are
most
disadvantaged
in
terms
of
knowledge
and
understanding
our
intention
is
not
to
argue
for
strictly
prioritarian
perspective
but
that
the
incremental
advancements
can
achieve
the
ultimate
goal
of
equity
in
the
long-term
thus
we
advocate
for
equity
as
guiding
concept
to
achieve
equality
by
inductively
reaching
fairness
in
the
limit
having
established
deÔ¨Ånitions
inÔ¨Ånite
personalization
and
ùúñ-equity
fairness
next
we
Ô¨Çesh
out
the
tension
between
the
two
inherent
conflict
and
trade-offs
inÔ¨Ånite
personalization
we
posit
comes
at
cost
to
ùúñ-equity
fairness
more
explicitly
in
inÔ¨Ånite
personalization
the
relevance
probability
of
some
information
ùëêùëñ
to
user
can
be
less
than
this
is
due
to
the
eÔ¨Äects
of
personalization
indexing
and
re-ranking
for
eÔ¨Écient
navigation
across
the
vast
space
of
information
however
this
violates
ùúñ-equity
fairness
as
some
information
will
be
eÔ¨Äectively
rendered
unreachable
within
Ô¨Åxed
window
of
queries
we
provide
some
examples
of
trade-oÔ¨Äs
below
search
engine
personalization
prioritizes
results
relative
to
user
input
query
and
prior
history
for
any
given
topic
orthogonal
to
user
past
history
personalization
may
rank
desired
items
far
beyond
the
average
expected
or
typical
number
of
results
any
given
user
explores
rendering
it
practically
infeasible
to
reach
social
media
serves
many
functions
dissemination
of
news
and
information
entertainment
and
social
connection
personalization
in
the
extreme
may
contribute
to
propagation
of
mis
and
disinformation
echo
chambers
Ô¨Ålter
bubbles
radicalization
and
social
disconnection
github
copilot
developed
by
openai
focuses
on
code
completion
providing
suggestions
for
code
lines
or
entire
functions
directly
integrated
into
interactive
development
environments
here
personalization
has
the
potential
to
provide
codebase-speciÔ¨Åc
suggestions
and
completion
but
may
also
infringe
on
coder
ability
construct
novel
functionality
google
bard
is
language
model
for
synthesizing
insights
for
settings
with
wide
range
of
opinions
perspectives
or
no
right
answer
as
an
augmentation
to
search-engine
information
retrieval
this
may
facilitate
more
eÔ¨Écient
ordering
and
disambiguation
of
search
results
but
may
also
infringe
on
access
to
seemingly
irrelevant
information
chatgpt
is
general-purpose
large
language
model
llm
designed
to
engage
in
human-like
conversations
and
answer
wide
range
of
questions
personalization
in
an
extreme
case
may
lead
to
conversational
loops
or
conversations
that
remain
within
the
scope
of
single
topic
rather
than
having
the
Ô¨Çexibility
to
move
between
topics
despite
user
prompt
or
request
here
we
list
several
examples
that
use
llms
due
to
their
relevance
to
personalization
information
synthesis
and
curation
and
appeals
to
human
language
usability
we
acknowledge
that
there
are
many
open
relevant
problems
related
to
personalization
such
as
contribution
or
exacerbation
of
representational
harms
for
the
scope
of
this
work
we
focus
on
how
such
systems
can
impact
access
to
information
manuscript
submitted
to
acm
fairness
vs
personalization
towards
equity
in
epistemic
utility
the
key
takeaway
of
this
list
is
not
that
personalization
necessitates
unfairness
or
inequity
nor
that
fairness
is
not
achievable
in
personalized
systems
rather
to
emphasize
that
personalization
without
consideration
of
fairness
can
readily
lead
to
systems
that
deny
some
individuals
base-level
of
expected
epistemic
utility
policy
goals
and
examples
of
implementations
while
this
conceptual
analysis
provides
guidance
about
how
best
to
think
about
fairness
in
the
context
of
personalized
algorithms
it
does
not
provide
guidance
about
how
to
achieve
fairer
less
biased
systems
as
we
turn
to
more
practical
recommendations
we
start
by
considering
key
stakeholders
and
their
associated
roles
table
this
creates
lexicon
and
clariÔ¨Åes
the
expected
function
of
each
stakeholder
each
of
these
stakeholders
has
wide
range
of
actions
available
that
can
lead
to
fairer
personalized
systems
see
table
these
actions
include
wide
range
of
governance
mechanisms
ranging
from
hard
regulation
to
soft
social
norms
we
emphasize
that
there
is
no
single
response
to
Ô¨Åx
unfairness
in
personalized
systems
inevitably
many
interdependent
actions
will
likely
need
to
be
taken
in
order
to
make
progress
on
this
problem
moreover
these
are
by
no
means
intended
to
be
exhaustive
but
rather
provide
grounded
starting
point
that
emphasizes
the
necessity
of
cross-stakeholder
collaborations
manuscript
submitted
to
acm
jennifer
chien
and
david
danks
stakeholder
role
government
develop
guidelines
principles
procedures
regulations
and
standards
for
safe
sustainable
deployment
may
hold
the
power
to
create
structural
supporting
systems
directly
within
the
government
or
Ô¨Ånancially
support
external
systems
of
enforcement
of
such
regulations
may
also
enact
other
means
of
operationalizing
policy
such
as
hosting
interdisciplinary
research
summits
or
sending
representatives
to
build
collaborations
with
other
stakeholders
to
better
inform
outputs
this
stakeholder
may
also
set
roles
and
norms
for
other
stakeholder
responsibilities
and
consequences
civil
society
conduct
evaluations
and
investigations
of
algorithms
and
software
developed
by
others
this
stakeholder
may
serve
as
an
external
third-party
entity
for
conducting
unbiased
evaluations
of
performance
and
compliance
with
policy
may
be
responsible
for
developing
novel
methods
of
measurement
industry
implement
technical
methods
tools
and
technological
innovations
usually
manifesting
as
consumer-facing
service
or
product
may
be
subject
to
practical
implementation
constraints
Ô¨Ånancial
competition
or
legislative
while
particular
product
may
be
theoretically
feasible
it
may
not
be
practically
viable
or
sustainable
for
company
to
produce
responsibilities
may
also
include
foreseeing
and
adequately
mitigating
harms
of
deployed
products
on
those
directly
and
indirectly
aÔ¨Äected
academia
broad
develop
and
implement
technical
theoretical
and
conceptual
knowledge
without
direct
consideration
of
proÔ¨Åts
results
can
include
work
that
builds
upon
understanding
implications
or
evaluation
methods
downstream
impacts
sociotechnical
approaches
this
stakeholder
has
the
potential
to
act
as
an
unbiased
actor
in
developing
best-practices
for
sustainable
long-term
practices
potentially
at
the
direct
cost
of
proÔ¨Åts
considering
environmental
social
and
cultural
norms
and
inÔ¨Çuences
general
public
provide
input
feedback
criticism
and
opinions
on
direction
and
alignment
with
values
for
research
and
innovation
via
implicit
and
explicit
signals
on
the
order
of
communities
groups
and
or
individuals
public
support
or
lack
thereof
can
be
collected
across
multitude
of
avenues
and
granularities
including
Ô¨Ånancial
support
interest
engagement
collective
action
protests
organizations
and
projects
table
stakeholder
roles
we
define
the
expected
roles
of
stakeholders
for
clarity
of
responsibilities
enumerated
in
table
we
note
that
these
responsibilities
are
not
restrictive
there
are
many
ways
in
which
one
stakeholder
may
overlap
or
even
take
over
the
responsibilities
of
another
for
instance
if
company
develops
policy
for
user
privacy
while
it
competitors
do
not
self-imposed
constraints
on
data
collection
may
impact
their
product
performance
however
as
this
design
value
receives
public
support
this
can
become
standard
competitive
service
without
the
intervention
of
government
or
civil
society
manuscript
submitted
to
acm
fairness
vs
personalization
towards
equity
in
epistemic
utility
goal
example
implementation
operationalization
mitigation
industry
academia
develop
algorithms
that
facilitate
connections
between
disjoint
parts
of
the
data
manifold
this
might
be
analogous
to
connecting
diÔ¨Äerent
parts
of
the
internet
together
through
links
but
done
by
creating
arbitrary
injections
that
fabricate
similarities
covariances
or
links
between
disjoint
parts
of
the
training
data
government
construct
structural
incentives
Ô¨Ånancial
prestige
recognition
standards
for
achieving
baseline
levels
of
interconnectivity
to
ensure
equitable
access
audit
industry
academia
civil
society
develop
measures
that
quantify
epistemic
utility
for
users
using
range
of
behaviors
and
data
tool
use
experienced
frustration
time
to
complete
task
etc
produce
formal
domain-speciÔ¨Åc
representations
of
distributions
of
utility
such
as
non-uniform
distributions
being
indicators
of
threats
to
fairness
measure
utility
and
study
barriers
to
utility
for
non-users
and
those
who
have
left
the
platform
civil
society
report
the
utility
distributions
for
non-users
to
compare
against
those
that
are
already
using
the
service
academia
deÔ¨Åning
and
measuring
downstream
impact
may
be
essential
to
thorough
quantiÔ¨Åcation
of
disparities
and
unfairness
downstream
impacts
such
as
social
inÔ¨Çuences
on
platform
usage
can
be
inÔ¨Çuenced
by
an
individual
experience
government
regulation
for
timing
and
structure
of
fairness
audits
for
personalized
systems
including
guidance
on
composition
of
internal
and
external
stakeholders
transparency
industry
civil
society
disclosure
of
aforementioned
distributions
of
utility
across
various
groups
of
users
reports
should
include
comments
about
whether
these
distributions
are
problematic
what
are
the
proposed
solutions
and
on
what
timeline
government
clear
consequences
and
or
reparations
for
users
aÔ¨Äected
throughout
improvement
and
for
continued
negligence
general
public
active
engagement
and
feedback
on
reports
to
inform
appropriate
consequences
and
informed
public
dis
favor
individual
control
industry
academia
design
control
mechanisms
over
the
degree
of
exploration
to
query
when
serving
recommendations
this
may
include
prompting
users
for
additional
input
before
producing
prediction
alternatively
allowing
users
to
reset
their
history
or
set
manual
preferences
over
personalization
Ô¨Ålters
government
regulation
and
enforcement
of
satisfaction
and
compliance
general
public
participation
and
feedback
on
norm
setting
of
what
features
should
be
available
how
accessible
services
should
be
determine
embedded
values
such
as
degree
of
reasonableness
that
are
feasible
and
realistic
for
all
users
education
awareness
industry
academia
government
general
public
workshops
and
training
sessions
can
facilitate
comprehension
of
the
dangers
limitations
and
better
calibrate
users
to
appropriate
expectations
of
functionality
these
workshops
can
include
education
on
rights
and
how
to
advocate
for
them
for
instance
if
companies
fail
to
comply
where
can
people
Ô¨Åle
their
grievances
so
they
can
be
aggregated
and
collectively
analyzed
industry
academia
civil
society
development
of
interactive
tools
that
enable
visualization
and
comparison
of
an
individual
utility
compared
to
the
general
population
or
those
relevantly
comparable
manuscript
submitted
to
acm
civil
society
government
regulation
and
continued
measurement
to
ensure
such
tools
Ô¨Çag
anomalous
data
and
ensure
improvement
within
some
time-frame
create
and
publicly
release
of
reports
of
corrections
adjustments
and
investigations
made
to
remedy
grievances
table
example
policy
interventions
and
goals
by
stakeholder
although
we
separate
actions
by
stakeholders
we
emphasize
that
these
goals
require
collaborations
and
contributions
across
multiple
stakeholders
to
achieve
such
solutions
10
jennifer
chien
and
david
danks
references
zeynep
tufekci
how
recommendation
algorithms
run
the
world
apr
2019
url
https://www.wired.com/story/how-recommendation-
gurkan
solmaz
jesmin
jahan
tithi
and
juan
miguel
de
joya
why
algorithmic
fairness
oct
2020
url
https://selects.acm.org/selectio
elizabeth
anne
watkins
michael
mckenna
and
jiahao
chen
the
four-Ô¨Åfths
rule
is
not
disparate
impact
woeful
tale
of
epistemic
trespassing
in
algorithmic
fairness
in
arxiv
preprint
arxiv
2202.09519
2022
cynthia
dwork
et
al
fairness
through
awareness
in
proceedings
of
the
3rd
innovations
in
theoretical
computer
science
conference
2012
pp
214
226
rishabh
mehrotra
et
al
towards
fair
marketplace
counterfactual
evaluation
of
the
trade-oÔ¨Ä
between
relevance
fairness
satisfaction
in
recommendation
systems
in
proceedings
of
the
27th
acm
international
conference
on
information
and
knowledge
management
2018
pp
2243
2251
nasim
sonboli
et
al
opportunistic
multi-aspect
fairness
through
personalized
re-ranking
in
proceedings
of
the
28th
acm
conference
on
user
modeling
adaptation
and
personalization
2020
pp
239
247
weiwen
liu
et
al
personalized
fairness-aware
re-ranking
for
microlending
in
proceedings
of
the
13th
acm
conference
on
recommender
systems
2019
pp
467
471
ludovico
boratto
gianni
fenu
and
mirko
marras
interplay
between
upsampling
and
regularization
for
provider
elizabeth
g√≥mez
et
al
the
winner
takes
it
all
geographic
imbalance
and
provider
un
fairness
in
educational
fairness
in
recommender
systems
in
user
modeling
and
user-adapted
interaction
31.3
2021
pp
421
455
recommender
systems
in
proceedings
of
the
44th
international
acm
sigir
conference
on
research
and
development
in
information
retrieval
2021
pp
1808
1812
10
abhisek
dash
et
al
when
the
umpire
is
also
player
bias
in
private
label
product
recommendations
on
ecommerce
marketplaces
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
2021
pp
873
884
11
ziwei
zhu
xia
hu
and
james
caverlee
fairness-aware
tensor-based
recommendation
in
proceedings
of
the
27th
acm
international
conference
on
information
and
knowledge
management
2018
pp
1153
1162
12
bora
edizel
et
al
fairecsys
mitigating
algorithmic
bias
in
recommender
systems
in
international
journal
of
data
science
and
analytics
2020
pp
197
213
13
himan
abdollahpouri
et
al
calibrated
recommendations
as
minimum-cost
Ô¨Çow
problem
in
proceedings
of
the
sixteenth
acm
international
conference
on
web
search
and
data
mining
2023
pp
571
579
14
himan
abdollahpouri
et
al
user-centered
evaluation
of
popularity
bias
in
recommender
systems
in
proceedings
of
the
29th
acm
conference
on
user
modeling
adaptation
and
personalization
2021
pp
119
129
15
bruna
wundervald
cluster-based
quotas
for
fairness
improvements
in
music
recommendation
systems
in
16
diego
corr√™a
da
silva
marcelo
garcia
manzato
and
frederico
ara√∫jo
dur√£o
exploiting
personalized
calibra
17
yashar
deldjoo
et
al
fairness
in
recommender
systems
research
landscape
and
future
directions
in
user
international
journal
of
multimedia
information
retrieval
10.1
2021
pp
25
32
tion
and
metrics
for
fairness
recommendation
in
expert
systems
with
applications
181
2021
115112
modeling
and
user-adapted
interaction
2023
pp
50
18
yashar
deldjoo
et
al
recommender
systems
fairness
evaluation
via
generalized
cross
entropy
in
arxiv
preprint
arxiv
1908.06708
2019
manuscript
submitted
to
acm
fairness
vs
personalization
towards
equity
in
epistemic
utility
19
11
yashar
deldjoo
tommaso
di
noia
and
felice
antonio
merra
adversarial
machine
learning
in
recommender
systems
aml-recsys
in
proceedings
of
the
13th
international
conference
on
web
search
and
data
mining
2020
pp
869
872
20
yunqi
li
et
al
user-oriented
fairness
in
recommendation
in
proceedings
of
the
web
conference
2021
2021
pp
624
632
21
miranda
fricker
epistemic
injustice
power
and
the
ethics
of
knowing
oxford
university
press
2007
22
c√©dric
courtois
laura
slechten
and
lennert
coenen
challenging
google
search
Ô¨Ålter
bubbles
in
social
and
political
information
disconforming
evidence
from
digital
methods
case
study
in
telematics
and
informatics
35.7
2018
pp
2006
2015
23
william
dutton
et
al
searching
through
Ô¨Ålter
bubbles
echo
chambers
in
society
and
the
internet
how
24
efrat
nechushtai
and
seth
lewis
what
kind
of
news
gatekeepers
do
we
want
machines
to
be
filter
bub
networks
of
information
and
communication
are
changing
our
lives
2019
228
bles
fragmentation
and
the
normative
dimensions
of
algorithmic
recommendations
in
computers
in
human
behavior
90
2019
pp
298
307
25
tawanna
dillahunt
christopher
brooks
and
samarth
gulati
detecting
and
visualizing
Ô¨Ålter
bubbles
in
google
and
bing
in
proceedings
of
the
33rd
annual
acm
conference
extended
abstracts
on
human
factors
in
computing
systems
2015
pp
1851
1856
26
axel
ekstr√∂m
diederick
niehorster
and
erik
olsson
self-imposed
Ô¨Ålter
bubbles
selective
attention
and
exposure
in
online
search
in
computers
in
human
behavior
reports
2022
100226
27
mykola
makhortykh
and
mari√´lle
wijermars
can
Ô¨Ålter
bubbles
protect
information
freedom
discussions
of
algorithmic
news
recommenders
in
eastern
europe
in
digital
journalism
2021
pp
25
28
jaime
teevan
how
people
recall
recognize
and
reuse
search
results
in
acm
transactions
on
information
systems
tois
26.4
2008
pp
27
29
kelly
shelton
council
post
the
value
of
search
results
rankings
nov
2017
url
https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/th
30
tessa
es
charlesworth
and
mahzarin
banaji
patterns
of
implicit
and
explicit
attitudes
iv
change
and
stability
from
2007
to
2020
in
psychological
science
33.9
2022
pp
1347
1371
31
youjin
kong
are
intersectionally
fair
ai
algorithms
really
fair
to
women
of
color
philosophical
analysis
in
2022
acm
conference
on
fairness
accountability
and
transparency
2022
pp
485
494
32
lily
hu
and
yiling
chen
short-term
intervention
for
long-term
fairness
in
the
labor
market
in
proceedings
of
the
2018
world
wide
web
conference
2018
pp
1389
1398
33
url
https://www.unesco.org/en/right-information.
manuscript
submitted
to
acm
this
figure
acm-jdslogo
png
is
available
in
png
format
from
http://arxiv.org/ps/2309.11503v1