research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
fairness
of
exposure
in
rankings
ashudeep
singh
thorsten
joachims
cornell
university
cornell
university
ithaca
ny
ithaca
ny
ashudeep@cs.cornell.edu
tj@cs.cornell.edu
abstract
anything
that
is
not
being
ranked
today
products
jobs
job
seek
rankings
are
ubiquitous
in
the
online
world
today
as
we
have
ers
opinions
potential
romantic
partners
nevertheless
one
of
the
transitioned
from
finding
books
in
libraries
to
ranking
products
guiding
technical
principles
behind
the
optimization
of
ranking
sys
jobs
job
applicants
opinions
and
potential
romantic
partners
there
tems
still
dates
back
to
four
decades
ago
namely
the
probability
is
substantial
precedent
that
ranking
systems
have
responsibility
ranking
principle
prp
28
it
states
that
the
ideal
ranking
should
not
only
to
their
users
but
also
to
the
items
being
ranked
to
address
order
items
in
the
decreasing
order
of
their
probability
of
relevance
these
often
conflicting
responsibilities
we
propose
conceptual
and
since
this
is
the
ranking
that
maximizes
utility
of
the
retrieval
sys
computational
framework
that
allows
the
formulation
of
fairness
tem
to
the
user
for
broad
range
of
common
utility
measures
in
constraints
on
rankings
in
terms
of
exposure
allocation
as
part
of
information
retrieval
but
is
this
uncompromising
focus
on
utility
this
framework
we
develop
efficient
algorithms
for
finding
rankings
to
the
users
still
appropriate
when
we
are
not
ranking
books
in
that
maximize
the
utility
for
the
user
while
provably
satisfying
library
but
people
products
and
opinions
specifiable
notion
of
fairness
since
fairness
goals
can
be
application
there
are
now
substantial
arguments
and
precedent
that
many
specific
we
show
how
broad
range
of
fairness
constraints
can
be
of
the
ranking
systems
in
use
today
have
responsibility
not
only
to
implemented
using
our
framework
including
forms
of
demographic
their
users
but
also
to
the
items
that
are
being
ranked
in
particular
parity
disparate
treatment
and
disparate
impact
constraints
we
the
scarce
resource
that
ranking
systems
allocate
is
the
exposure
illustrate
the
effect
of
these
constraints
by
providing
empirical
of
items
to
users
and
exposure
is
largely
determined
by
position
in
results
on
two
ranking
problems
the
ranking
and
so
is
job
applicant
chances
to
be
interviewed
by
an
employer
an
airbnb
host
ability
to
rent
out
their
property
ccs
concepts
or
writer
to
be
read
this
exposes
companies
operating
with
sensitive
data
to
legal
and
reputation
risks
and
disagreements
information
systems
probabilistic
retrieval
models
retrieval
about
fair
allocation
of
exposure
have
already
led
to
high-profile
effectiveness
presentation
of
retrieval
results
legal
challenges
such
as
the
european
union
antitrust
violation
fine
keywords
on
google
30
and
it
has
sparked
policy
debate
about
search
neutrality
14
it
is
unlikely
that
there
will
be
universal
definition
fairness
in
rankings
fairness
algorithmic
bias
position
bias
equal
of
fairness
that
is
appropriate
across
all
applications
but
we
give
opportunity
three
concrete
examples
where
ranking
system
may
be
perceived
acm
reference
format
as
unfair
or
biased
in
its
treatment
of
the
items
that
are
being
ashudeep
singh
and
thorsten
joachims
2018
fairness
of
exposure
in
ranked
and
where
the
ranking
system
may
want
to
impose
fairness
rankings
in
kdd
18
the
24th
acm
sigkdd
international
conference
on
constraints
that
guarantee
some
notion
of
fairness
knowledge
discovery
data
mining
august
19
23
2018
london
united
the
main
contribution
of
this
paper
is
conceptual
and
compu
kingdom
acm
new
york
ny
usa
10
pages
https://doi.org/10.1145/
tational
framework
for
formulating
fairness
constraints
on
rank
3219819.3220088
ings
and
the
associated
efficient
algorithms
for
computing
utility
maximizing
rankings
subject
to
such
fairness
constraints
this
introduction
framework
provides
flexible
way
for
balancing
fairness
to
the
rankings
have
become
one
of
the
dominant
forms
with
which
items
being
ranked
with
the
utility
the
rankings
provide
to
the
online
systems
present
results
to
the
user
far
surpassing
their
con
users
in
this
way
we
are
not
limited
to
single
definition
of
fair
ception
in
library
science
as
tool
for
finding
books
in
library
the
ness
since
different
application
scenarios
probably
require
different
prevalence
of
rankings
now
ranges
from
search
engines
and
online
trade-offs
between
the
rights
of
the
items
and
what
can
be
consid
stores
to
recommender
systems
and
news
feeds
consequently
it
ered
an
acceptable
loss
in
utility
to
the
user
we
show
that
broad
is
no
longer
just
books
that
are
being
ranked
but
there
is
hardly
range
of
fairness
constraints
can
be
implemented
in
our
framework
permission
to
make
digital
or
hard
copies
of
all
or
part
of
this
work
for
personal
or
using
its
expressive
power
to
link
exposure
relevance
and
impact
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
in
particular
we
show
how
to
implement
forms
of
demographic
for
profit
or
commercial
advantage
and
that
copies
bear
this
notice
and
the
full
citation
on
the
first
page
copyrights
for
components
of
this
work
owned
by
others
than
the
parity
disparate
treatment
and
disparate
impact
constraints
the
author
must
be
honored
abstracting
with
credit
is
permitted
to
copy
otherwise
or
ranking
algorithm
we
develop
provides
provable
guarantees
for
republish
to
post
on
servers
or
to
redistribute
to
lists
requires
prior
specific
permission
optimizing
expected
utility
while
obeying
the
specified
notion
of
and
or
fee
request
permissions
from
permissions@acm.org
kdd
18
august
19
23
2018
london
united
kingdom
fairness
in
expectation
2018
copyright
held
by
the
owner
author
publication
rights
licensed
to
associa
to
motivate
the
need
and
range
of
situations
where
one
may
tion
for
computing
machinery
want
to
trade-off
utility
for
some
notion
of
fairness
we
start
with
acm
isbn
978
4503
5552
18
08
15.00
https://doi.org/10.1145/3219819.3220088
presenting
the
following
three
application
scenarios
they
make
2219
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
ùëé1
ùëé2
ùëé6
ùëé5
0.82
0.81
0.77
0.78
ùëé4
ùëé3
0.79
0.80
ùëé1
ùëé2
0.71
0.81
ùëé3
ranking
0.03
difference
in
avg
relevance
0.32
difference
in
avg
exposure
ùëé4
0.78
ùëé5
0.39
figure
an
image
search
result
page
for
the
query
ceo
ùëé6
showing
disproportionate
number
of
male
ceos
relevance
prob
of
interview
exposure
figure
job
seeker
example
to
illustrate
how
small
differ
example
fairly
representing
distribution
of
results
some
ence
in
relevance
can
lead
to
large
difference
in
exposure
times
the
results
of
query
are
used
as
statistical
sample
either
an
opportunity
for
the
group
of
females
explicitly
or
implicitly
for
example
user
may
expect
that
an
im
age
search
for
the
query
ceo
on
search
engine
returns
roughly
use
of
the
concept
of
protected
groups1
where
fairness
is
related
to
the
right
number
of
male
and
female
executives
reflecting
the
true
the
differences
in
how
groups
are
treated
however
we
later
discuss
distribution
of
male
vs
female
ceos
in
the
world
if
search
engine
how
this
extends
to
individual
fairness
by
considering
groups
of
size
returns
highly
disproportionate
number
of
males
as
compared
to
one
the
three
examples
illustrate
how
fairness
can
be
related
to
females
like
in
the
hypothetical
results
in
figure
then
the
search
biased
allocation
of
opportunity
misrepresentation
of
real-world
engine
may
be
perceived
as
biased
in
fact
study
detected
the
distributions
and
fairness
as
freedom
of
speech
principle
presence
of
gender
bias
in
image
search
results
for
variety
of
occupations
19
biased
information
environment
may
affect
example
fairly
allocating
economic
opportunity
consider
users
perceptions
and
behaviors
and
it
was
shown
that
such
biases
web-service
that
connects
employers
users
to
potential
employees
indeed
affect
people
belief
about
various
occupations
19
note
items
the
following
example
demonstrates
how
small
differences
that
the
probability
ranking
principle
does
not
necessarily
produce
in
item
relevance
can
cause
large
difference
in
exposure
and
results
that
represent
the
relevance
distribution
in
an
unbiased
way
therefore
economic
opportunity
across
groups
in
this
case
the
web
this
means
that
even
if
users
relevance
distribution
agrees
with
service
uses
ranking-based
system
to
present
set
of
applicants
the
true
distribution
of
female
ceos
the
optimal
ranking
accord
for
software
engineering
position
to
relevant
employers
figure
ing
to
the
probability
ranking
principle
may
still
look
like
that
in
the
set
contains
males
and
females
the
male
applicants
have
figure
instead
of
solely
relying
on
the
prp
it
seems
reasonable
relevance
of
0.80
0.79
0.78
respectively
for
the
employers
while
to
distribute
exposure
proportional
to
relevance
even
if
this
may
the
female
applicants
have
relevance
of
0.77
0.76
0.75
respectively
mean
drop
in
utility
to
the
users
here
we
follow
the
standard
probabilistic
definition
of
relevance
where
0.77
means
that
77
of
all
employers
issuing
the
query
find
example
giving
speakers
fair
access
to
willing
listeners
rank
that
applicant
relevant
the
probability
ranking
principle
suggests
ing
systems
play
an
increasing
role
as
medium
for
speech
creating
ranking
these
applicants
in
the
decreasing
order
of
relevance
connection
between
bias
and
fairness
in
rankings
and
principles
the
males
at
the
top
positions
followed
by
the
females
what
does
behind
freedom
of
speech
14
while
the
ability
to
produce
speech
this
mean
for
exposure
between
the
two
groups
if
we
consider
and
make
this
speech
available
on
the
internet
has
certainly
created
standard
exposure
drop-off
position
bias
of
log
new
opportunities
to
exercise
freedom
of
speech
for
speaker
there
where
is
the
position
in
the
ranking
as
commonly
used
in
the
remains
the
question
whether
or
not
free
speech
makes
its
way
to
discounted
cumulative
gain
dcg
measure
the
female
applicants
the
interested
listeners
hence
the
study
of
the
medium
becomes
will
get
30
less
exposure
even
though
the
average
difference
necessary
search
engines
are
the
most
popular
mediums
of
this
in
relevance
between
male
and
female
applicants
is
just
0.03
see
kind
and
therefore
have
an
immense
capability
of
influencing
user
figure
is
this
winner-take-all
allocation
of
exposure
fair
in
this
attention
through
their
editorial
policies
which
has
sparked
pol
context
even
if
the
winner
just
has
tiny
advantage
in
relevance
icy
debate
around
search
neutrality
13
14
16
while
no
unified
it
seems
reasonable
to
distribute
exposure
more
evenly
even
if
definition
of
search
neutrality
exists
many
argue
that
search
en
this
may
mean
small
drop
in
utility
to
the
employers
gines
should
have
no
editorial
policies
other
than
that
their
results
are
comprehensive
impartial
and
solely
ranked
by
relevance
26
groups
that
are
protected
from
discrimination
by
law
based
on
sex
race
age
disability
but
does
ranking
solely
on
relevance
necessarily
imply
the
proba
color
creed
national
origin
or
religion
we
use
broader
meaning
of
protected
groups
bility
ranking
principle
or
are
there
other
relevance-based
ranking
here
that
suits
our
domain
note
that
this
tiny
advantage
may
come
from
of
the
employers
being
gender
principles
that
lead
to
medium
with
more
equitable
distribution
biased
but
this
is
not
problem
we
are
addressing
here
of
exposure
and
access
to
willing
listeners
2220
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
related
work
of
designing
fair
scoring
functions
that
satisfy
desirable
fairness
before
introducing
the
algorithmic
framework
for
formulating
constraints
broad
range
of
fairness
constraints
on
rankings
we
first
survey
most
of
the
fairness
constraints
defined
in
the
previous
work
three
related
strands
of
prior
work
first
this
paper
draws
on
con
reflect
parity
constraints
restricting
the
fraction
of
items
with
each
cepts
for
algorithmic
fairness
of
supervised
learning
in
the
presence
attribute
in
the
ranking
the
framework
we
propose
goes
beyond
of
sensitive
attributes
second
we
relate
to
prior
work
on
algorith
such
parity
constraints
as
we
propose
general
algorithmic
frame
mic
fairness
for
rankings
finally
we
contrast
fairness
with
the
work
for
efficiently
computing
optimal
probabilistic
rankings
for
well-studied
area
of
diversified
ranking
in
information
retrieval
large
class
of
possible
fairness
constraints
concurrent
and
independent
work
by
biega
et
al
formulates
2.1
algorithmic
fairness
fairness
for
rankings
similar
to
the
special
case
of
our
framework
discussed
in
section
4.2
aiming
to
achieve
amortized
fairness
as
algorithmic
techniques
especially
machine
learning
find
wide
of
attention
by
making
exposure
proportional
to
relevance
they
spread
applications
there
is
much
interest
in
understanding
its
focus
on
individual
fairness
which
in
our
framework
amounts
to
societal
impacts
while
algorithmic
decisions
can
counteract
ex
the
special
case
of
protected
groups
of
size
one
the
two
approaches
isting
biases
algorithmic
and
data-driven
decision
making
affords
not
only
differ
in
expressive
power
but
algorithmically
they
solve
new
mechanisms
for
introducing
unintended
bias
there
have
an
integer
linear
program
to
generate
series
of
rankings
while
been
numerous
attempts
to
define
notions
of
fairness
in
the
super
our
approach
provides
provably
efficient
solution
via
standard
vised
learning
setting
the
individual
fairness
perspective
states
linear
program
and
the
birkhoff-von
neumann
decomposition
that
two
individuals
similar
with
respect
to
task
should
be
clas
sified
similarly
12
individual
fairness
is
hard
to
define
precisely
because
of
the
lack
of
agreement
on
task-specific
similarity
metrics
2.3
information
diversity
in
retrieval
for
individuals
there
is
also
group
fairness
perspective
for
super
at
first
glance
fairness
and
diversity
in
rankings
may
appear
re
vised
learning
that
implies
constraints
like
demographic
parity
and
lated
since
they
both
lead
to
more
diverse
rankings
however
their
equalized
odds
demographic
parity
posits
that
decisions
should
motivation
and
mechanisms
are
fundamentally
different
like
the
be
balanced
around
sensitive
attribute
like
gender
or
race
37
prp
diversified
ranking
is
entirely
beholden
to
maximizing
utility
however
it
has
been
shown
that
demographic
parity
causes
loss
to
the
user
while
our
approach
to
fairness
balances
the
needs
of
in
the
utility
and
infringes
individual
fairness
12
since
even
users
and
items
in
particular
both
the
prp
and
diversified
ranking
perfect
predictor
typically
does
not
achieve
demographic
parity
maximize
utility
for
the
user
alone
their
difference
lies
merely
in
equalized
odds
represents
the
equal
opportunity
principle
for
super
the
utility
measure
that
is
maximized
under
extrinsic
diversity
vised
learning
and
defines
the
constraint
that
the
false
positive
and
24
the
utility
measure
accounts
for
uncertainty
and
diminishing
true
positive
rates
should
be
equal
for
different
protected
groups
returns
from
multiple
relevant
results
25
under
intrinsic
di
15
several
recent
works
have
focused
on
learning
algorithms
versity
24
the
utility
measure
considers
rankings
as
portfolios
compatible
with
these
definitions
of
fair
classification
31
33
35
and
reflects
redundancy
10
and
under
exploration
diversity
24
including
causal
approaches
to
fairness
20
21
23
in
this
paper
the
aim
is
to
maximize
utility
to
the
user
in
the
long
term
through
we
draw
on
many
of
the
concepts
introduced
in
the
context
of
fair
more
effective
learning
the
work
on
fairness
in
this
paper
is
fun
supervised
learning
but
do
not
consider
the
problem
of
learning
damentally
different
in
its
motivation
and
mechanism
as
it
does
instead
we
ask
how
to
fairly
allocate
exposure
in
rankings
based
on
not
modify
the
utility
measure
for
the
user
but
instead
introduces
relevance
independent
of
how
these
relevances
may
be
estimated
rights
of
the
items
that
are
being
ranked
2.2
fairness
in
rankings
several
recent
works
have
raised
the
question
of
group
fairness
framework
for
ranking
under
in
rankings
yang
and
stoyanovich
32
propose
statistical
parity
fairness
constraints
based
measures
that
compute
the
difference
in
the
distribution
acknowledging
the
ubiquity
of
rankings
across
applications
we
of
different
groups
for
different
prefixes
of
the
ranking
top-10
conjecture
that
there
is
no
single
definition
of
what
constitutes
top-20
and
so
on
the
differences
are
then
averaged
for
these
fair
ranking
but
that
fairness
depends
on
context
and
application
prefixes
using
discounted
weighting
like
in
dcg
this
measure
in
particular
we
will
see
below
that
different
notions
of
fairness
is
then
used
as
regularization
term
zehlike
et
al
34
formulate
imply
different
trade-offs
in
utility
which
may
be
acceptable
in
the
problem
of
finding
fair
top-k
ranking
that
optimizes
utility
one
situation
but
not
in
the
other
to
address
this
range
of
pos
while
satisfying
two
sets
of
constraints
first
in-group
monotonicity
sible
fairness
constraints
this
section
develops
framework
for
for
utility
more
relevant
items
above
less
relevant
within
the
formulating
fairness
constraints
on
rankings
and
then
computing
group
second
fairness
constraint
that
the
proportion
of
protected
the
utility-maximizing
ranking
subject
to
these
fairness
constraints
group
items
in
every
prefix
of
the
top-k
ranking
is
above
minimum
with
provable
guarantees
threshold
celis
et
al
propose
constrained
maximum
weight
for
simplicity
consider
single
query
and
assume
that
we
matching
algorithm
for
ranking
set
of
items
efficiently
under
want
to
present
ranking
of
set
of
documents
fairness
constraint
indicating
the
maximum
number
of
items
with
denoting
the
utility
of
ranking
for
query
with
each
sensitive
attribute
allowed
in
the
top
positions
some
recent
the
problem
of
optimal
ranking
under
fairness
constraints
can
be
approaches
like
asudeh
et
al
have
also
looked
at
the
task
2221
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
formulated
as
the
following
optimization
problem
since
utility
is
linear
in
both
and
we
can
combine
the
indi
vidual
utilities
into
an
expectation
argmaxr
is
fair
rank
rel
in
this
way
we
generalize
the
goal
of
the
probabilistic
ranking
rank
principle
which
emerges
as
the
special
case
of
no
fairness
con
straints
to
fully
instantiate
and
solve
this
optimization
problem
we
will
specify
the
following
four
components
first
we
define
where
general
class
of
utility
measures
that
contains
many
com
rel
monly
used
ranking
metrics
second
we
address
the
problem
of
how
to
optimize
over
rankings
which
are
discrete
combinatorial
objects
by
extending
the
class
of
rankings
to
probabilistic
rankings
is
the
expected
utility
of
document
for
query
in
the
case
of
third
we
reformulate
the
optimization
problem
as
an
efficiently
binary
relevances
and
as
the
identity
function
is
equivalent
solvable
linear
program
which
implies
convenient
yet
expressive
to
the
probability
of
relevance
it
is
easy
to
see
that
sorting
the
language
for
formulating
fairness
constraints
and
finally
we
show
documents
by
leads
to
the
ranking
that
maximizes
the
utility
how
probabilistic
ranking
can
be
efficiently
recovered
from
the
solution
of
the
linear
program
argmaxr
argsortd
for
any
function
that
decreases
with
rank
this
is
the
insight
3.1
utility
of
ranking
behind
the
probability
ranking
principle
prp
28
virtually
all
utility
measures
used
for
ranking
evaluation
derive
the
utility
of
the
ranking
from
the
relevance
of
the
individual
items
3.2
probabilistic
rankings
being
ranked
for
each
user
and
query
rel
denotes
the
rankings
are
combinatorial
objects
such
that
naively
searching
the
binary
relevance
of
the
document
whether
the
document
space
of
all
rankings
for
utility-maximizing
ranking
under
fairness
is
relevant
to
user
or
not
note
that
different
users
can
have
constraints
would
take
time
that
is
exponential
in
to
avoid
different
rel
even
if
they
share
the
same
to
account
for
such
combinatorial
optimization
we
consider
probabilistic
rankings
personalization
we
assume
that
the
query
also
contains
any
instead
of
single
deterministic
ranking
probabilistic
ranking
personalization
features
and
that
is
the
set
of
all
users
that
lead
is
distribution
over
rankings
and
we
can
naturally
extend
the
to
identical
beyond
binary
relevance
rel
could
also
represent
definition
of
utility
to
probabilistic
rankings
other
relevance
rating
systems
such
as
likert
scale
in
movie
ratings
or
real-valued
score
rank
rel
generic
way
to
express
many
utility
measures
commonly
used
in
information
retrieval
is
rank
rank
rel
while
distributions
over
rankings
are
still
exponential
in
size
we
where
and
are
two
application-dependent
functions
the
func
can
make
use
of
the
additional
insight
that
utility
can
already
be
tion
rank
models
how
much
attention
document
gets
at
computed
from
the
marginal
rank
distributions
of
the
documents
rank
rank
and
is
function
that
maps
the
relevance
of
the
let
pi
be
the
probability
that
places
document
di
at
rank
then
document
for
user
to
its
utility
in
particular
the
choice
of
could
forms
doubly
stochastic
matrix
of
size
which
means
be
based
on
the
position
bias
the
fraction
of
users
who
examine
that
the
sum
of
each
row
and
each
column
of
the
matrix
is
equal
the
document
shown
at
particular
position
out
of
the
total
number
to
in
other
words
the
sum
of
probabilities
for
each
position
is
and
the
sum
of
probabilities
for
each
document
is
pi
of
users
who
issue
the
query
the
choice
of
mapping
relevance
to
and
pi
with
knowledge
of
the
doubly
stochastic
matrix
utility
is
somewhat
arbitrary
for
example
widely
used
evaluation
measure
discounted
cumulative
gain
dcg
17
can
be
repre
expected
utility
for
probabilistic
ranking
can
be
computed
as
sented
in
our
framework
where
rank
log
rank
d1
and
rel
2rel
or
sometimes
simply
rel
pi
di
2rel
dcg
to
make
notation
more
concise
we
can
rewrite
the
utility
of
the
lo–¥
rank
ranking
as
matrix
product
for
this
we
introduce
two
vectors
is
column
vector
of
size
with
ui
di
and
is
another
for
measure
like
dcg
we
can
choose
rank
column
vector
of
size
with
vj
so
the
expected
utility
log
rank
for
rank
and
rank
for
rank
dcg
can
be
written
as
ut
pv
2222
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
3.3
optimizing
fair
rankings
via
linear
where
and
where
the
ai
are
permutation
programming
matrices
in
our
case
the
permutation
matrices
correspond
to
deterministic
rankings
of
the
document
set
and
the
coefficients
we
will
see
in
section
3.4
that
not
only
does
imply
doubly
correspond
to
the
probability
of
sampling
each
ranking
according
stochastic
matrix
but
that
we
can
also
efficiently
compute
to
the
marcus-ree
theorem
there
exists
decomposition
with
no
probabilistic
ranking
for
every
doubly
stochastic
matrix
we
can
more
than
permutation
matrices
22
such
decompo
therefore
formulate
the
problem
of
finding
the
utility-maximizing
sition
can
be
computed
efficiently
in
polynomial
time
using
several
probabilistic
ranking
under
fairness
constraints
in
terms
of
doubly
algorithms
11
for
the
experiments
in
this
paper
we
use
the
stochastic
matrices
instead
of
distributions
over
rankings
implementation
provided
at
https://github.com/jfinkels/birkhoff.
argmaxp
ut
pv
expected
utility
sum
of
probabilities
for
each
position
3.5
summary
of
algorithm
the
following
summarizes
the
algorithm
for
optimal
ranking
under
p1
sum
of
probabilities
for
each
document
fairness
constraints
note
that
we
have
assumed
knowledge
of
the
pi
valid
probability
true
relevances
throughout
this
paper
whereas
in
practice
is
fair
fairness
constraints
one
would
work
with
estimates
from
some
predictive
model
note
that
the
optimization
objective
is
linear
in
variables
pi
set
up
the
utility
vector
the
position
discount
vector
as
furthermore
the
constraints
ensuring
that
is
doubly
well
as
the
vectors
and
and
the
scalar
for
the
fairness
stochastic
are
linear
as
well
where
is
the
column
vector
of
size
constraints
see
section
containing
all
ones
without
the
fairness
constraint
and
for
any
vj
solve
the
linear
program
from
section
3.3
for
that
decreases
with
the
solution
is
the
permutation
matrix
that
compute
the
birkhoff-von
neumann
decomposition
ranks
the
set
of
documents
in
decreasing
order
of
utility
conform
p1
p2
pn
ing
to
the
prp
sample
permutation
matrix
pi
with
probability
proportional
now
that
we
have
expressed
the
problem
of
finding
the
utility
to
and
display
the
corresponding
ranking
maximizing
probabilistic
ranking
besides
the
fairness
constraint
note
that
the
rankings
sampled
in
the
last
step
of
the
algorithm
as
linear
program
convenient
language
to
express
fairness
fulfill
the
fairness
constraints
in
expectation
while
at
the
same
time
constraints
would
be
linear
constraints
of
the
form
they
maximize
expected
utility
ft
pg
constructing
group
fairness
one
or
more
of
such
constraints
can
be
added
and
the
resulting
constraints
linear
program
can
still
be
solved
efficiently
and
optimally
with
now
that
we
have
established
framework
for
formulating
fairness
standard
algorithms
like
interior
point
methods
as
we
will
show
constraints
and
optimally
solving
the
associated
ranking
problem
in
section
the
vectors
and
the
scalar
can
be
chosen
to
we
still
need
to
understand
the
expressiveness
of
constraints
of
the
implement
range
of
different
fairness
constraints
to
give
some
form
ft
pg
in
this
section
we
explore
how
three
concepts
from
intuition
the
vector
can
be
used
to
encode
group
identity
and
or
algorithmic
fairness
demographic
parity
disparate
treatment
relevance
of
each
document
while
will
typically
reflect
the
im
and
disparate
impact
can
be
implemented
in
our
framework
and
portance
of
each
position
position
bias
thus
be
enforced
efficiently
and
with
provable
guarantees
they
all
aim
to
fairly
allocate
exposure
which
we
now
define
formally
let
3.4
sampling
rankings
vj
represent
the
importance
of
position
or
more
concretely
the
the
solution
of
the
linear
program
is
matrix
containing
probabil
position
bias
at
which
is
the
fraction
of
users
that
examine
the
ities
of
each
document
at
each
position
to
implement
this
solution
item
at
this
position
then
we
define
exposure
for
document
di
in
ranking
system
we
need
to
compute
probabilistic
ranking
under
probabilistic
ranking
as
that
corresponds
to
from
this
probabilistic
ranking
we
can
then
sample
rankings
to
present
to
the
user3
given
the
exposure
di
pi
vj
derivation
of
our
approach
it
is
immediately
apparent
that
the
rankings
sampled
from
fulfill
the
specified
fairness
constraints
in
expectation
the
goal
is
to
allocate
exposure
fairly
between
groups
doc
computing
from
can
be
achieved
via
the
birkhoff-von
neu
uments
and
items
may
belong
to
different
groups
because
of
some
mann
bvn
decomposition
which
provides
transformation
sensitive
attributes
for
example
news
stories
belong
to
different
to
decompose
doubly
stochastic
matrix
into
convex
sum
of
per
sources
products
belong
to
different
manufacturers
applicants
be
mutation
matrices
in
particular
if
is
doubly
stochastic
matrix
long
to
different
genders
the
fairness
constraints
we
will
formulate
there
exists
decomposition
of
the
form
in
the
following
implement
different
goals
for
allocating
exposure
between
groups
a1
a2
an
to
illustrate
the
effect
of
the
fairness
constraints
we
will
provide
empirical
results
on
two
ranking
problems
for
both
we
use
the
forusability
reasons
it
is
preferable
to
make
this
sampling
pseudo-random
based
on
hash
of
the
user
identity
so
that
the
same
user
receives
the
same
ranking
if
the
average
relevance
of
each
document
normalized
between
and
as
the
utility
ui
di
and
set
the
position
bias
to
vj
log
same
query
is
repeated
2223
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
just
like
in
the
standard
definition
of
dcg
more
generally
one
can
experiments
we
solved
the
linear
program
in
3.3
twice
also
plug
in
the
actual
position-bias
value
which
can
be
estimated
once
without
and
once
with
the
demographic
parity
constraint
through
an
intervention
experiment
18
from
above
for
the
job
seeker
example
figure
shows
the
optimal
rankings
in
terms
of
without
and
with
fairness
constraint
in
panels
job-seeker
example
we
come
back
to
the
job-seeker
example
and
respectively
color
indicates
the
probability
value
from
the
introduction
and
as
illustrated
in
figure
the
ranking
note
that
the
fair
ranking
according
to
demographic
parity
in
problem
consists
of
applicants
with
probabilities
of
relevance
to
cludes
substantial
amount
of
stochasticity
however
panels
an
employer
of
0.81
0.80
0.79
0.78
0.77
0.76
groups
and
show
that
the
fair
ranking
can
be
decomposed
into
mix
and
reflect
gender
with
the
first
three
applicants
belonging
to
ture
of
two
deterministic
permutation
matrices
with
the
associated
the
male
group
and
the
last
three
to
the
female
group
weights
compared
to
the
dcg
of
the
unfair
ranking
with
3.8193
the
news
recommendation
dataset
we
use
subset
the
yow
news
optimal
fair
ranking
has
slightly
lower
utility
with
dcg
of
3.8031
recommendation
dataset
36
to
analyze
our
method
on
larger
however
the
drop
in
utility
due
to
the
demographic
parity
con
and
real-world
relevance
distribution
the
dataset
contains
explicit
straint
could
be
substantially
larger
for
example
if
we
lowered
the
and
implicit
feedback
from
set
of
users
for
news
articles
from
relevances
for
the
female
group
to
0.82
0.81
0.80
0.03
0.02
different
rss
feeds
we
randomly
sample
subset
of
news
articles
0.01
we
would
still
get
the
same
fair
ranking
as
the
current
so
in
the
people
topic
coming
from
the
top
two
sources
the
sources
lution
since
this
fairness
constraint
is
ignorant
of
relevance
in
are
identified
using
rss
feed
identifier
and
used
as
groups
and
this
ranking
roughly
every
second
document
has
low
relevance
the
relevant
field
is
used
as
the
measure
of
relevance
for
our
leading
to
large
drop
in
dcg
it
is
interesting
to
point
out
that
task
since
the
relevance
is
given
as
rating
from
to
we
divide
the
effect
of
demographic
parity
in
ranking
is
therefore
analogous
it
by
and
add
small
amount
of
gaussian
noise
0.05
to
break
to
its
effect
in
supervised
learning
where
it
can
also
lead
to
large
ties
the
resulting
ui
are
clipped
to
lie
between
and
drop
in
classification
accuracy
12
in
the
following
we
formulate
fairness
constraints
using
three
we
also
conducted
the
same
experiment
on
the
news
recom
ideas
for
allocation
of
exposure
to
different
groups
in
particular
we
mendation
dataset
figure
shows
the
optimal
ranking
matrix
and
will
define
constraints
of
the
form
ft
pg
for
the
optimization
the
fair
probabilistic
ranking
along
with
dcg
for
each
note
that
problem
in
3.3
for
simplicity
we
will
only
present
the
case
of
even
though
the
optimal
unfair
ranking
places
documents
from
binary
valued
sensitive
attribute
two
groups
and
how
starting
at
position
the
constraint
pushes
the
ranking
of
the
news
ever
these
constraints
may
be
defined
for
each
pair
of
groups
and
items
from
further
up
the
ranking
starting
either
at
rank
or
for
each
sensitive
attribute
and
be
included
in
the
linear
program
rank
in
this
case
the
optimal
fair
ranking
happens
to
be
almost
deterministic
except
at
the
beginning
4.1
demographic
parity
constraints
arguably
the
simplest
way
of
defining
fairness
of
exposure
between
groups
is
to
enforce
that
the
average
exposure
of
the
documents
in
both
the
groups
is
equal
denoting
average
exposure
in
group
4.2
disparate
treatment
constraints
with
unlike
demographic
parity
the
constraints
we
explore
in
this
and
the
following
section
depend
on
the
relevance
of
the
items
being
exposure
exposure
di
ranked
in
this
way
these
constraints
have
the
potential
to
address
the
concerns
for
the
job-seeker
example
from
the
introduction
this
can
be
expressed
as
the
following
constraint
in
our
framework
where
small
difference
in
relevance
was
magnified
into
large
difference
in
exposure
furthermore
we
saw
that
in
the
image
exposure
exposure
search
example
from
the
introduction
that
it
may
be
desirable
to
have
exposure
be
proportional
to
relevance
to
achieve
some
form
pi
vj
pi
vj
of
unbiased
statistical
representation
denoting
the
average
utility
of
group
with
1di
1di
pi
vj
1d
1d
pv
with
fi
gi
gi
ui
in
the
last
step
we
obtain
constraint
in
the
form
ft
pg
which
one
can
plug
it
into
the
linear
program
from
section
3.3
we
call
this
demographic
parity
constraint
similar
to
an
analogous
constraint
in
fair
supervised
learning
37
similar
to
that
setting
in
our
case
also
such
constraint
may
lead
to
big
loss
in
utility
in
this
motivates
the
following
type
of
constraint
which
enforces
that
cases
when
the
two
groups
are
very
different
in
terms
of
relevance
exposure
of
the
two
groups
to
be
proportional
to
their
average
distribution
2224
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.500
0.500
0.4
0.2
0.0
dcg
3.8193
dcg
3.8031
dcg
3.8132
dcg
3.7929
figure
job
seeker
example
with
demographic
parity
constraint
optimal
unfair
ranking
that
maximizes
dcg
optimal
fair
ranking
under
demographic
parity
and
are
the
bvn
decomposition
of
the
fair
ranking
position
experiments
we
again
compute
the
optimal
ranking
without
10
15
20
10
15
20
fairness
constraint
and
with
the
disparate
treatment
constraint
the
results
for
the
job-seeker
example
are
shown
in
figure
the
0.8
figure
also
shows
the
bvn
decomposition
of
the
resultant
proba
document
id
bilistic
ranking
into
three
permutation
matrices
as
expected
the
10
10
0.6
fair
ranking
has
an
optimal
dtr
while
the
unfair
ranking
has
15
15
0.4
dtr
of
1.7483
also
expected
is
that
the
fair
ranking
has
lower
dcg
than
the
optimal
deterministic
ranking
but
that
it
has
higher
20
20
0.2
dcg
than
the
optimal
fair
ranking
under
demographic
parity
we
conducted
the
same
experiment
for
the
news
recommenda
dcg
5.2027
dcg
5.1360
tion
dataset
figure
shows
the
optimal
ranking
matrix
and
the
fair
probabilistic
ranking
along
with
dcg
for
each
here
the
ranking
computed
without
the
fairness
constraint
happened
to
be
almost
figure
news
recommendation
dataset
with
demographic
fair
according
to
disparate
treatment
already
and
the
fairness
con
parity
constraint
document
id
14
15
24
opti
straint
has
very
little
impact
on
dcg
mal
unfair
ranking
that
maximizes
dcg
optimal
fair
ranking
under
demographic
parity
4.3
disparate
impact
constraints
utility
in
the
previous
section
we
constrained
the
exposures
treatments
exposure
exposure
for
the
two
groups
to
be
proportional
to
their
average
utility
how
ever
we
may
want
to
go
step
further
and
define
constraint
√≠n
√≠n
on
the
impact
the
expected
clickthrough
or
purchase
rate
as
pi
this
more
directly
reflects
the
economic
impact
of
the
ranking
in
particular
we
may
want
to
assure
that
the
clickthrough
rates
for
the
groups
as
determined
by
the
exposure
and
relevance
are
pro
1di
1di
pi
vj
portional
to
their
average
utility
to
formally
define
this
let
us
first
model
the
probability
of
document
getting
clicked
according
to
1di
1di
the
following
simple
click
model
27
pv
with
fi
click
on
document
examining
is
relevant
we
name
this
constraint
disparate
treatment
constraint
be
cause
allocating
exposure
to
group
is
analogous
to
treating
the
exposure
di
is
relevant
two
groups
of
documents
this
is
motivated
in
principle
by
the
con
√µn
cept
of
recommendations
as
treatments
29
where
recommending
pi
vj
ui
or
exposing
document
is
considered
as
treatment
and
the
user
click
or
purchase
is
considered
the
effect
of
the
treatment
to
quantify
treatment
disparity
we
also
define
measure
called
we
can
now
compute
the
average
clickthrough
rate
of
documents
disparate
treatment
ratio
dtr
to
evaluate
how
unfair
ranking
in
group
as
is
in
this
respect
how
differently
the
two
groups
are
treated
exposure
dtr
exposure
ctr
pi
ui
vj
note
that
this
ratio
equals
one
if
the
disparate
treatment
constraint
in
equation
is
fulfilled
whether
the
value
is
less
than
or
greater
than
tells
which
group
out
of
or
is
disadvantaged
in
terms
the
following
disparate
impact
constraint
enforces
that
the
ex
of
disparate
treatment
pected
clickthrough
rate
of
each
group
is
proportional
to
its
average
2225
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.497
0.453
0.050
0.4
0.2
0.0
dcg
3.8193
dcg
3.8044
dcg
3.7972
dcg
3.8132
dcg
3.7959
dtr
1.7483
dtr
1.0000
dtr
0.8179
dtr
1.2815
dtr
0.7879
figure
job
seeker
example
with
disparate
treatment
constraint
optimal
unfair
ranking
fair
ranking
under
disparate
treatment
constraint
are
the
bvn
decomposition
of
the
fair
ranking
position
the
results
for
the
news
recommendation
dataset
are
given
in
10
15
20
10
15
20
figure
where
we
also
see
large
improvement
in
dir
the
dcg
is
lower
than
the
unconstrained
dcg
and
the
dcg
with
disparate
0.8
treatment
constraint
but
higher
than
the
dcg
with
demographic
document
id
parity
constraint
10
10
0.6
15
15
0.4
discussion
in
the
last
section
we
implemented
three
fairness
constraints
in
20
20
0.2
our
framework
motivated
by
the
concepts
of
demographic
parity
dcg
5.2027
dcg
5.1983
disparate
treatment
and
disparate
impact
the
main
purpose
was
dtr
1.0859
dtr
1.0000
to
explore
the
expressiveness
of
the
framework
and
we
do
not
argue
that
these
constraints
are
the
only
conceivable
ones
or
the
figure
news
recommendation
dataset
with
disparate
treat
correct
ones
for
given
application
in
particular
it
appears
that
ment
constraint
optimal
unfair
ranking
fair
ranking
fairness
in
rankings
is
inherently
trade-off
between
the
utility
under
disparate
treatment
constraint
of
the
users
and
the
rights
of
the
items
that
are
being
ranked
and
that
different
applications
require
making
this
trade-off
in
different
utility
ways
for
example
we
may
not
want
to
convey
strong
rights
to
the
ctr
ctr
books
in
library
when
user
is
trying
to
locate
book
but
the
situation
is
different
when
candidates
are
being
ranked
for
job
√≠n
opening
we
therefore
focused
on
creating
flexible
framework
pi
ui
that
covers
substantial
range
of
fairness
constraints
10
group
fairness
vs
individual
fairness
in
our
experiments
1di
1di
we
observe
that
even
though
the
constraints
ensure
that
the
rank
ui
pi
vj
11
ings
have
no
disparate
treatment
or
disparate
impact
across
groups
individual
items
within
group
might
still
be
considered
to
suffer
1di
1di
pv
with
fi
ui
from
disparate
treatment
or
impact
for
example
in
the
job-seeker
experiment
for
disparate
treatment
figure
the
allocation
of
similar
to
dtr
we
can
define
the
following
disparate
impact
exposure
to
the
candidates
within
group
still
follows
the
same
ratio
dir
to
measure
the
extent
to
which
the
disparate
impact
exposure
drop-off
going
down
the
ranking
that
we
considered
un
constraint
is
violated
fair
according
to
the
disparate
treatment
constraint
as
remedy
ctr
one
could
include
additional
fairness
constraints
for
other
sensitive
dir
ctr
attributes
like
race
disability
and
national
origin
to
further
refine
note
that
this
ratio
equals
one
if
the
disparate
impact
constraint
in
the
desired
notion
of
fairness
in
the
extreme
our
framework
allows
equation
11
is
fulfilled
similar
to
dtr
whether
dir
is
less
than
protected
groups
of
size
one
such
that
it
can
also
express
notions
of
or
greater
than
tells
which
group
is
disadvantaged
in
terms
of
individual
fairness
for
example
in
the
case
of
disparate
treatment
disparate
impact
we
could
express
individual
fairness
as
set
of
constraints
over
groups
of
size
one
resulting
in
notion
of
fairness
similar
experiments
we
again
compare
the
optimal
rankings
with
and
to
however
for
the
disparate
impact
constraint
where
the
without
the
fairness
constraint
the
results
for
the
job-seeker
ex
expected
clickthrough
rates
are
proportional
to
the
relevances
it
is
ample
are
shown
in
figure
again
the
optimal
fair
ranking
has
not
clear
whether
individual
fairness
makes
sense
unless
we
rank
bvn
decomposition
into
three
deterministic
rankings
and
it
has
items
uniformly
at
random
slightly
reduced
dcg
however
there
is
large
improvement
in
dir
from
the
fairness
constraint
since
the
prp
ranking
has
using
estimated
utilities
in
our
definitions
and
experiments
substantial
disparate
impact
on
the
two
groups
we
assumed
that
we
have
access
to
the
true
expected
utilities
2226
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.536
0.304
0.160
0.4
0.2
0.0
dcg
3.8193
dcg
3.8025
dcg
3.8059
dcg
3.7929
dcg
3.8089
dir
1.8193
dir
1.0000
dir
1.1192
dir
0.7501
dir
1.1801
figure
job
seeker
example
with
disparate
impact
constraint
optimal
unfair
ranking
fair
ranking
under
disparate
impact
constraint
are
the
bvn
decomposition
of
the
fair
ranking
position
of
and
vice
versa
for
the
minimum
10
15
20
10
15
20
exposure
0.8
max
exposure
vj
document
id
10
10
0.6
all
documents
in
top
positions
0.4
15
15
exposure
min
exposure
20
20
0.2
all
documents
in
bottom
positions
dcg
5.2027
dcg
5.1461
dir
1.5211
dir
1.0000
hence
fair
ranking
according
to
disparate
treatment
only
exists
if
the
ratio
of
average
utilities
lies
within
the
range
of
possible
values
figure
news
recommendation
dataset
with
disparate
im
for
the
exposure
pact
constraint
optimal
unfair
ranking
fair
ranking
under
disparate
impact
constraint
relevances
in
practice
these
utilities
are
typically
estimated
via
machine
learning
this
learning
step
is
subject
to
other
biases
however
in
such
scenario
the
constraint
can
still
be
satisfied
if
that
may
in
turn
lead
to
biased
estimates
most
importantly
we
introduce
more
documents
belonging
to
neither
group
or
the
biased
estimates
may
be
the
result
of
selection
biases
in
click
data
group
with
more
relevant
documents
this
increases
the
range
of
but
recent
counterfactual
learning
techniques
18
have
been
shown
the
lhs
and
the
ranking
doesn
have
to
give
undue
exposure
to
to
permit
unbiased
learning-to-rank
despite
biased
click
data
one
of
the
groups
cost
of
fairness
including
the
fairness
constraints
in
the
opti
conclusions
mization
problem
comes
at
the
cost
of
effectiveness
as
measured
in
this
paper
we
considered
fairness
of
rankings
through
the
lens
by
dcg
and
other
conventional
measures
this
loss
in
utility
can
of
exposure
allocation
between
groups
instead
of
defining
single
be
computed
as
cof
ut
where
is
the
deterministic
notion
of
fairness
we
developed
general
framework
that
em
optimal
ranking
and
represents
the
fair
ranking
we
have
already
ploys
probabilistic
rankings
and
linear
programming
to
compute
discussed
for
the
demographic
parity
constraint
that
this
cost
can
the
utility-maximizing
ranking
under
whole
class
of
fairness
con
be
substantial
in
particular
for
demographic
parity
it
is
easy
to
straints
to
verify
the
expressiveness
of
this
class
we
showed
how
see
that
the
utility
of
the
fair
ranking
approaches
zero
if
all
rele
to
express
fairness
constraints
motivated
by
the
concepts
of
de
vant
documents
are
in
one
group
and
the
size
of
the
other
group
mographic
parity
disparate
treatment
and
disparate
impact
we
approaches
infinity
conjecture
that
the
appropriate
definition
of
fair
exposure
depends
feasibility
of
fair
solutions
the
linear
program
from
sec
on
the
application
which
makes
this
expressiveness
desirable
tion
3.3
may
not
have
solution
in
extreme
conditions
corre
sponding
to
cases
where
no
fair
solution
exists
consider
the
dis
acknowledgments
parate
treatment
constraint
this
work
was
supported
by
nsf
awards
iis-1615706
and
iis-1513692
exposure
any
opinions
findings
and
conclusions
or
recommendations
ex
pressed
in
this
material
are
those
of
the
author
and
do
not
neces
exposure
sarily
reflect
the
views
of
the
national
science
foundation
we
can
adversarially
construct
an
infeasible
constraint
by
choosing
the
relevance
so
that
the
ratio
on
the
rhs
lies
outside
the
range
that
references
lhs
can
achieve
by
varying
the
maximum
of
the
rhs
occurs
abolfazl
asudehy
hv
jagadishy
julia
stoyanovichz
and
gautam
das
2017
when
all
the
documents
of
are
placed
above
all
the
documents
designing
fair
ranking
schemes
arxiv
preprint
arxiv
1712.09752
2017
2227
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
solon
barocas
and
andrew
selbst
2016
big
data
disparate
impact
cal
20
niki
kilbertus
mateo
rojas
carulla
giambattista
parascandolo
moritz
hardt
rev
104
2016
671
dominik
janzing
and
bernhard
sch√∂lkopf
2017
avoiding
discrimination
through
asia
biega
krishna
gummadi
and
gerhard
weikum
2018
equity
of
attention
causal
reasoning
in
nips
656
666
amortizing
individual
fairness
in
rankings
arxiv
preprint
arxiv
1805.01788
21
matt
kusner
joshua
loftus
chris
russell
and
ricardo
silva
2017
counterfac
2018
tual
fairness
in
nips
4069
4079
garrett
birkhoff
1940
lattice
theory
vol
25
american
mathematical
soc
22
marvin
marcus
and
rimhak
ree
1959
diagonals
of
doubly
stochastic
matrices
amelia
butterly
2015
google
image
search
for
ceo
has
barbie
as
first
the
quarterly
journal
of
mathematics
10
1959
296
302
female
result
2015
http://www.bbc.co.uk/newsbeat/article/32332603/
23
razieh
nabi
and
ilya
shpitser
2017
fair
inference
on
outcomes
arxiv
preprint
google-image-search-for-ceo-has-barbie-as-first-female-result
arxiv
1705.10378
2017
toon
calders
faisal
kamiran
and
mykola
pechenizkiy
2009
building
classifiers
24
filip
radlinski
paul
bennett
ben
carterette
and
thorsten
joachims
2009
with
independency
constraints
in
data
mining
workshops
icdmw
13
18
redundancy
diversity
and
interdependent
document
relevance
in
acm
sigir
jaime
carbonell
and
jade
goldstein
1998
the
use
of
mmr
diversity-based
forum
vol
43
46
52
reranking
for
reordering
documents
and
producing
summaries
in
sigir
335
25
filip
radlinski
robert
kleinberg
and
thorsten
joachims
2008
learning
diverse
336
https://doi.org/10.1145/290941.291025
rankings
with
multi-armed
bandits
in
icml
acm
784
791
elisa
celis
damian
straszak
and
nisheeth
vishnoi
2017
ranking
with
26
addam
raff
2009
search
but
you
may
not
find
new
york
times
2009
fairness
constraints
arxiv
preprint
arxiv
1704.06840
2017
http://www.nytimes.com/2009/12/28/opinion/28raff.html
cheng-shang
chang
wen-jyh
chen
and
hsiang-yi
huang
1999
on
service
27
matthew
richardson
ewa
dominowska
and
robert
ragno
2007
predicting
guarantees
for
input-buffered
crossbar
switches
capacity
decomposition
ap
clicks
estimating
the
click-through
rate
for
new
ads
in
www
521
530
proach
by
birkhoff
and
von
neumann
in
iwqos
ieee
79
86
https://doi.org/10.1145/1242572.1242643
10
charles
clarke
maheedhar
kolla
gordon
cormack
olga
vechtomova
28
stephen
robertson
1977
the
probability
ranking
principle
in
ir
journal
of
azin
ashkan
stefan
b√ºttcher
and
ian
mackinnon
2008
novelty
and
diversity
documentation
33
1977
294
304
in
information
retrieval
evaluation
in
sigir
659
666
https://doi.org/10.1145/
29
tobias
schnabel
adith
swaminathan
ashudeep
singh
navin
chandak
and
1390334.1390446
thorsten
joachims
2016
recommendations
as
treatments
debiasing
learning
11
fanny
dufoss√©
and
bora
u√ßar
2016
notes
on
birkhoff
von
neumann
decompo
and
evaluation
in
icml
1670
1679
sition
of
doubly
stochastic
matrices
linear
algebra
appl
497
2016
108
115
30
mark
scott
2017
google
fined
record
2.7
billion
in
antitrust
rul
12
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
and
richard
ing
new
york
times
2017
https://www.nytimes.com/2017/06/27/technology/
zemel
2012
fairness
through
awareness
in
itcs
214
226
eu-google-fine
html
13
laura
granka
2010
the
politics
of
search
decade
retrospective
the
31
blake
woodworth
suriya
gunasekar
mesrob
ohannessian
and
nathan
srebro
information
society
26
2010
364
374
2017
learning
non-discriminatory
predictors
arxiv
preprint
arxiv
1702.06081
14
james
grimmelmann
2011
some
skepticism
about
search
neutrality
the
next
2017
digital
decade
essays
on
the
future
of
the
internet
2011
435
https://ssrn.com/
32
ke
yang
and
julia
stoyanovich
2017
measuring
fairness
in
ranked
outputs
abstract
1742444
ssdbm
2017
15
moritz
hardt
eric
price
and
nati
srebro
2016
equality
of
opportunity
in
33
muhammad
bilal
zafar
isabel
valera
manuel
gomez
rodriguez
and
krishna
supervised
learning
in
nips
3315
3323
gummadi
2017
fairness
beyond
disparate
treatment
disparate
impact
learn
16
lucas
introna
and
helen
nissenbaum
2000
shaping
the
web
why
the
politics
ing
classification
without
disparate
mistreatment
in
www
1171
1180
of
search
engines
matters
the
information
society
16
2000
169
185
34
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
mega
17
kalervo
j√§rvelin
and
jaana
kek√§l√§inen
2002
cumulated
gain-based
evaluation
hed
and
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
of
ir
techniques
acm
transactions
on
information
systems
tois
20
2002
cikm
2017
422
446
35
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
and
cynthia
dwork
2013
18
thorsten
joachims
adith
swaminathan
and
tobias
schnabel
2017
unbiased
learning
fair
representations
in
icml
325
333
learning-to-rank
with
biased
feedback
in
wsdm
781
789
36
yi
zhang
2005
bayesian
graphical
model
for
adaptive
information
filtering
phd
19
matthew
kay
cynthia
matuszek
and
sean
munson
2015
unequal
represen
dissertation
carnegie
mellon
university
tation
and
gender
stereotypes
in
image
search
results
for
occupations
in
chi
37
indre
zliobaite
2015
on
the
relation
between
accuracy
and
fairness
in
binary
3819
3828
https://doi.org/10.1145/2702123.2702520
classification
fatml
workshop
at
icml
2015
2228