vldb
journal
2022
31
431
458
https://doi.org/10.1007/s00778-021-00697-y
regular
paper
fairness
rankings
recommendations
overview
evaggelia
pitoura1
kostas
stefanidis2
georgia
koutrika3
received
18
december
2020
revised
july
2021
accepted
28
august
2021
published
online
october
2021
author
2021
abstract
increasingly
depend
variety
data-driven
algorithmic
systems
assist
us
many
aspects
life
search
engines
recommender
systems
among
others
used
sources
information
help
us
making
sort
decisions
selecting
restaurants
books
choosing
friends
careers
given
rise
important
concerns
regarding
fairness
systems
work
aim
presenting
toolkit
definitions
models
methods
used
ensuring
fairness
rankings
recommendations
objectives
threefold
provide
solid
framework
novel
quickly
evolving
impactful
domain
present
related
methods
put
perspective
highlight
open
challenges
research
paths
future
work
keywords
fairness
rankings
recommendations
introduction
algorithmic
systems
driven
large
amounts
data
increasingly
used
aspects
society
assist
people
forming
opinions
taking
decisions
algorithmic
systems
offer
enormous
opportunities
since
accelerate
scientific
discovery
various
domains
including
personalized
medicine
smart
weather
forecasting
many
fields
can
also
automate
tasks
regarding
simple
personal
decisions
help
improving
daily
life
personal
assistants
recommendations
like
eat
news
moving
forward
potential
transforming
society
open
government
many
benefits
often
systems
used
assist
even
replace
human
decision
making
diverse
domains
examples
include
software
systems
used
school
admissions
housing
pricing
goods
services
credit
score
estimation
kostas
stefanidis
konstantinos.stefanidis@tuni.fi
evaggelia
pitoura
pitoura@cs.uoi.gr
georgia
koutrika
georgia@athenarc.gr
university
ioannina
ioannina
greece
tampere
university
tampere
finland
athena
research
center
marousi
greece
job
applicant
selection
sentencing
decisions
courts
surveillance
automation
raises
concerns
much
can
trust
systems
steady
stream
studies
shown
decision
support
systems
can
unintentionally
encode
existing
human
biases
introduce
new
ones
20
example
image
search
query
doctors
nurses
percentage
images
portraying
women
get
result
evidence
shows
stereotype
exaggeration
systematic
underrepresentation
women
compared
actual
percentage
estimated
us
bureau
labor
statistics
50
two
interesting
conclusions
study
people
prefer
rate
search
results
higher
results
consistent
stereotypes
another
interesting
result
shift
representation
gender
image
search
results
people
perception
real-world
distribution
tends
shift
another
well-known
example
compas
system
commercial
tool
uses
risk
assessment
algorithm
predict
categories
future
crime
specifically
tool
used
courts
usa
assist
bail
sentencing
decisions
found
false
positive
rate
people
labeled
tool
high
risk
re-offend
nearly
twice
high
africanamerican
white
defendants
means
many
times
ubiquitous
use
decision
support
systems
may
create
possible
threats
economic
loss
social
stigmatization
even
loss
liberty
many
case
studies
123
432
like
ones
example
names
used
men
women
color
much
likely
generate
ads
related
arrest
records
81
also
using
tool
called
adfisher
found
set
gender
female
will
result
getting
ads
less
high
paid
jobs1
case
word
embeddings
vector
represents
computer
programming
closer
men
women
data-driven
systems
also
employed
search
recommendation
engines
movie
music
platforms
advertisements
social
media
news
outlets
among
others
recent
studies
report
social
media
become
main
source
online
news
2.4
billion
internet
users
nearly
64.5
receive
breaking
news
social
media
instead
traditional
sources
63
thus
great
extent
search
recommendation
engines
systems
play
central
role
shaping
experiences
influencing
perception
world
example
people
come
musical
tastes
kinds
ways
us
listen
music
now
offers
specific
problems
embedded
bias
streaming
service
offers
music
recommendations
studying
music
listened
creates
suggestions
loop
amplifying
existing
bias
reducing
diversity
recent
study
analyzed
publicly
available
listening
records
330
000
users
one
service
showed
female
artists
represented
25
music
listened
users
study
identified
gender
fairness
one
artists
main
concerns
female
artists
given
equal
exposure
music
recommendations
29
another
study
led
university
southern
california
facebook
ad
recommendations
revealed
recommendation
system
disproportionately
showed
certain
types
job
ads
men
women
43
system
likely
present
job
ads
users
gender
identity
reflected
concentration
gender
particular
position
industry
hence
recommendations
amplified
existing
bias
created
fewer
opportunities
people
based
gender
however
ads
may
targeted
based
qualifications
protected
categories
based
us
law
article
pay
special
attention
concept
fairness
rankings
recommendations
fairness
typically
mean
lack
discrimination
bias
bias
may
come
algorithm
reflecting
example
commercial
preferences
designers
even
actual
data
example
survey
contains
biased
questions
specific
population
misrepresented
input
data
fairness
elusive
concept
abundance
definitions
models
fairness
proposed
well
several
algorithmic
approaches
fair
rankings
recommendations
making
landscape
convoluted
order
https://fairlyaccountable.org/adfisher/.
123
pitoura
et
al
make
real
progress
building
fair-aware
systems
need
de-mystify
done
understand
model
approach
can
used
finally
distinguish
research
challenges
ahead
us
therefore
follow
systematic
structured
approach
explain
various
sides
approaches
fairness
survey
present
fairness
models
rankings
recommendations
separately
computational
methods
used
enforce
since
many
computational
methods
originally
introduced
specific
model
applicable
models
well
providing
overview
spectrum
different
models
computational
methods
new
ways
combine
may
evolve
start
presenting
fairness
models
first
provide
birds
eye
view
notions
fairness
rankings
recommendations
formalized
also
present
taxonomy
specifically
distinguish
individual
group
fairness
consumer
producer
fairness
fairness
single
multiple
outputs
present
concrete
models
definitions
rankings
recommendations
highlight
differences
commonalities
present
models
fit
taxonomy
describe
solutions
fair
rankings
recommendations
organize
pre-processing
approaches
aim
transforming
data
remove
underlying
bias
discrimination
in-processing
approaches
aim
modifying
existing
introducing
new
algorithms
result
fair
rankings
recommendations
post-processing
approaches
modify
output
algorithm
within
category
classify
approaches
along
several
dimensions
discuss
cases
system
needs
make
decisions
fairness
also
important
present
open
research
challenges
pertaining
fairness
broader
context
data
management
best
knowledge
first
survey
provides
toolkit
definitions
models
methods
used
ensuring
fairness
rankings
recommendations
recent
survey
focuses
fairness
ranking
91
two
surveys
complementary
terms
perspective
terms
coverage
fairness
evasive
concept
integrating
algorithms
systems
emerging
fastchanging
field
provide
technical
classification
recent
work
whereas
view
91
socio-technical
one
aims
placing
various
approaches
fairness
within
value
framework
content
different
well
since
also
cover
recommendations
rank
aggregation
recent
tutorials
stricter
focus
focusing
concepts
metrics
fairness
challenges
applying
recommendations
information
retrieval
well
scoring
methods
presented
respectively
25
66
hand
article
much
wider
coverage
depth
presenting
structured
survey
comparison
fairness
rankings
recommendations
overview
methods
models
ensuring
fairness
rankings
recommendations
remaining
survey
organized
follows
section
presents
core
definitions
fairness
sect
reviews
definitions
fairness
applicable
specifically
rankings
recommenders
rank
aggregation
section
discusses
distinction
methods
achieving
fairness
sects
organize
present
detail
pre
post-processing
methods
section
offers
comparison
post-processing
methods
section
studies
can
verify
whether
program
fair
finally
sect
10
elaborates
critical
open
issues
challenges
future
work
sect
11
summarizes
status
current
research
fairness
ranking
recommender
systems
fairness
problem
section
start
overview
approaches
modeling
fairness
provide
taxonomy
different
types
fairness
models
ranking
recommendations
433
unjustifiably
influenced
values
protected
attributes
recommendation
systems
retrieve
interesting
items
users
based
profiles
history
depending
application
recommendation
system
history
may
include
explicit
user
ratings
items
selection
items
views
clicks
general
recommenders
estimate
score
rating
user
item
reflects
preference
user
item
words
relevance
item
user
recommendation
list
formed
user
includes
items
highest
estimated
score
scores
can
seen
utility
scores
case
recommenders
abstract
terms
recommendation
fair
values
protected
attributes
users
items
affect
outcome
recommendation
high
level
can
distinguish
two
approaches
formalizing
fairness
24
individual
fairness
definitions
based
premise
similar
entities
treated
similarly
group
fairness
definitions
group
entities
based
value
one
protected
attributes
ask
groups
treated
similarly
2.1
general
view
fairness
approaches
algorithmic
fairness
interpret
fairness
lack
discrimination
31
32
asking
algorithm
discriminate
input
entities
based
attributes
relevant
task
hand
attributes
called
protected
sensitive
often
include
among
others
gender
religion
age
sexual
orientation
race
far
work
defining
detecting
removing
unfairness
focused
classification
algorithms
used
decision
making
classification
algorithms
input
entity
assigned
one
set
predefined
classes
case
characteristics
input
entities
relevant
task
hand
influence
output
classifier
example
values
protected
attributes
hinder
assignment
entity
positive
class
positive
class
may
example
correspond
getting
job
admitted
school
paper
focus
ranking
recommendation
algorithms
given
set
entities
ranking
algorithm
produces
ranking
entities
assignment
mapping
entities
ranking
positions
ranking
based
measure
relative
quality
entities
task
hand
example
entities
output
search
query
ranked
mainly
based
relevance
query
following
will
also
refer
measure
quality
utility
abstractly
fair
ranking
one
assignment
entities
positions
operationalize
approaches
fairness
need
define
similarity
input
output
algorithm
input
similarity
need
means
quantifying
similarity
entities
case
individual
fairness
way
partitioning
entities
groups
case
group
fairness
output
similarity
individual
group
fairness
need
formal
definition
similar
treatment
means
input
similarity
individual
fairness
common
approach
defining
input
similarity
distance-based
one
24
let
set
entities
assume
distance
metric
pair
entities
dissimilar
entities
larger
distance
metric
task
specific
two
entities
may
similar
one
task
dissimilar
another
example
two
individuals
may
considered
similar
similar
qualifications
comes
admitted
college
dissimilar
comes
receiving
loan
metric
may
externally
imposed
regulatory
body
externally
proposed
civil
rights
organization
ideally
metric
express
ground
truth
best
available
approximation
finally
metric
made
public
open
discussion
refinement
group
fairness
challenge
lies
determining
partition
entities
groups
output
similarity
specifying
means
entities
groups
entities
treated
similarly
intricate
problem
social
technical
perspective
123
434
social
perspective
fundamental
distinction
made
equity
equality
simply
put
equality
refers
treating
entities
equally
equity
refers
treating
entities
according
needs
finally
receive
output
even
individuals
disadvantaged
note
often
blindness
hiding
values
protected
attributes
suffice
produce
fair
outputs
since
may
proxy
attributes
correlated
protected
ones
case
also
known
redundant
encoding
take
example
zip
code
attribute
zip
codes
may
reveal
sensitive
information
majority
residents
neighborhood
belong
specific
ethnic
group
45
fact
considerations
led
making
redlining
practice
arbitrary
denying
limiting
financial
services
specific
neighborhoods
illegal
usa
another
social-based
differentiation
can
made
disparate
treatment
disparate
impact
disparate
treatment
often
illegal
practice
treating
entity
differently
based
protected
attributes
disparate
impact
refers
cases
output
depends
protected
attributes
even
entities
treated
way
disparate
impact
doctrine
solidified
usa
griggs
duke
power
co
1971
high
school
diploma
required
unskilled
work
excluding
applicants
color
technical
perspective
output
similarity
translated
quantifiable
measures
depends
clearly
specific
type
algorithm
paper
focus
ranking
recommendation
algorithms
overall
case
ranking
central
issue
manifestation
position
bias
fact
people
tend
consider
items
appear
top
positions
ranking
even
attention
items
receive
visibility
items
highly
skewed
regard
position
list
high
level
output
similarity
case
ranking
refers
offering
similar
visibility
similar
items
group
items
placing
similar
positions
ranking
especially
comes
top
positions
recommendations
one
approach
defining
output
fairness
consider
recommendation
problem
classification
problem
positive
class
recommendation
list
another
approach
consider
recommendation
list
ranked
list
case
position
recommended
item
list
also
taken
account
refine
individual
group
fairness
based
type
output
similarity
next
section
2.2
taxonomy
fairness
definitions
previous
section
distinguished
individual
group
fairness
formulations
will
refer
123
pitoura
et
al
distinction
fairness
models
level
fairness
comes
ranking
recommendation
systems
besides
different
levels
also
one
side
fairness
criteria
applicable
finally
differentiate
fairness
models
based
whether
fairness
requirements
applied
single
multiple
outputs
algorithm
different
dimensions
fairness
summarized
fig
note
various
fairness
definitions
following
sections
can
used
conditions
system
must
satisfy
fair
measures
fairness
instance
can
measure
much
fairness
condition
violated
define
condition
setting
threshold
fairness
measure
2.2
levels
fairness
now
refine
individual
group
fairness
based
output
similarity
specified
seminal
research
formalizing
algorithmic
fairness
focused
classification
algorithms
used
decision
making
research
influential
study
fairness
types
algorithms
refer
research
briefly
relate
ranking
recommendations
types
individual
fairness
one
way
formulating
individual
fairness
distance-based
one
intuitively
given
distance
measure
two
entities
distance
measure
outputs
algorithm
like
distance
output
algorithm
two
entities
small
entities
similar
let
us
see
concrete
example
area
probabilistic
classifiers
24
let
classifier
maps
entities
outcomes
case
probabilistic
classifiers
randomized
mappings
entities
probability
distributions
outcomes
specifically
classify
entity
choose
outcome
according
distribution
say
classifier
individually
fair
mapping
satisfies
lipschitz
property
distance
measure
probability
distributions
distance
metric
entities
words
distance
probability
distributions
assigned
classifier
greater
actual
distance
entities
another
form
individual
fairness
counterfactual
fairness
57
intuition
case
output
fair
toward
entity
actual
world
counterfactual
world
entity
belonged
different
group
causal
inference
used
formalize
notion
fairness
types
group
fairness
simplicity
let
us
assume
two
groups
namely
protected
group
non
fairness
rankings
recommendations
overview
435
fig
fairness
definitions
taxonomy
protected
privileged
group
will
start
presenting
statistical
approaches
commonly
used
classification
assume
actual
predicted
output
binary
classifier
ground
truth
output
algorithm
let
positive
class
leads
favorable
decision
someone
getting
loan
admitted
competitive
school
predicted
probability
certain
classification
statistical
approaches
group
fairness
can
distinguished
33
84
base
rates
approaches
use
output
algorithm
accuracy
approaches
use
output
algorithm
ground
truth
calibration
approaches
use
predicted
probability
ground
truth
classification
base
rate
fairness
compares
probability
entity
receives
favorable
outcome
belongs
protected
group
corresponding
probability
receives
favorable
outcome
belongs
non-protected
group
compare
two
may
take
ratio
92
28
difference
15
now
abstract
terms
base
rate
fairness
definition
ranking
may
compare
probabilities
items
group
appear
similarly
good
ranking
positions
recommendations
probabilities
recommended
probabilities
favorable
outcome
equal
two
groups
special
type
fairness
termed
demographic
statistical
parity
statistical
parity
preserves
input
ratio
demographics
individuals
receiving
favorable
outcome
demographics
underlying
population
statistical
parity
natural
way
model
equity
members
group
chance
receiving
favorable
output
base
rate
fairness
ignores
actual
output
output
may
fair
may
reflect
ground
truth
example
assume
classification
task
getting
job
protected
attribute
gender
statistical
parity
asks
specific
ratio
women
positive
class
even
many
women
input
well
qualified
job
accuracy
calibration
look
traditional
evaluation
measures
require
algorithm
works
equally
well
terms
prediction
errors
groups
classification
accuracy-based
fairness
warrants
various
types
classification
errors
true
positives
false
positives
equal
across
groups
depending
type
classification
errors
considered
achieved
type
fairness
takes
different
names
40
example
case
ask
case
equal
true
positive
rate
two
groups
called
equal
opportunity
comparing
equal
opportunity
statistical
parity
members
two
groups
chance
getting
favorable
outcome
members
qualify
thus
equal
opportunity
close
equality
interpretation
fairness
analogy
accuracy
case
ranking
ground
truth
reflected
utility
items
thus
approaches
take
account
utility
defining
fairness
ranking
can
seen
accuracy-based
ones
recommendations
accuracy-based
definitions
look
differences
actual
predicted
ratings
items
two
groups
calibration-based
fairness
considers
probabilistic
classifiers
predict
probability
class
22
53
general
classification
algorithm
considered
wellcalibrated
algorithm
predicts
set
individuals
probability
belonging
positive
class
approximately
fraction
set
actual
members
positive
class
terms
fairness
intuitively
like
classifier
equally
well
calibrated
groups
example
calibration-based
fairness
asking
predicted
probability
score
probability
actually
getting
favorable
outcome
equal
groups
123
436
group-based
measures
general
tend
ignore
merits
individual
group
individuals
group
may
better
given
task
individuals
group
captured
group-based
fairness
definitions
issue
may
lead
two
problematic
behaviors
namely
self-fulfilling
prophecy
deliberately
choosing
less
qualified
members
protected
group
aim
building
bad
track
record
group
reverse
tokenism
choosing
well
qualified
member
non-protected
group
aim
creating
convincing
refutations
members
protected
group
also
selected
2.2
multi-sided
fairness
ranking
recommendations
least
two
sides
involved
items
ranked
recommended
users
receive
rankings
recommendations
distinguish
producer
item-side
fairness
consumer
user-side
fairness
note
items
ranked
recommended
may
also
people
example
case
ranking
job
applicants
call
items
simplicity
producer
item-side
fairness
focuses
items
ranked
recommended
case
like
similar
items
groups
items
ranked
recommended
similar
way
appear
similar
positions
ranking
main
type
fairness
discussed
far
instance
consider
political
orientation
protected
attribute
article
may
ask
value
attribute
affect
ranking
articles
search
result
news
feed
consumer
user-side
fairness
focuses
users
receive
consume
data
items
ranking
search
result
recommendation
abstract
terms
like
similar
users
groups
users
receive
similar
rankings
recommendations
instance
gender
protected
attribute
user
receiving
job
recommendations
may
ask
gender
user
influence
job
recommendations
user
receives
cases
system
may
require
fairness
consumers
providers
instance
users
items
belong
protected
groups
example
assume
rental
property
business
wishes
treat
minority
applicants
protected
class
ensure
access
properties
similar
renters
time
wishes
treat
minority
landlords
protected
class
ensure
highly
qualified
tenants
referred
rate
landlords
different
types
recommendation
systems
may
call
specializations
consumer
producer
fairness
case
group
recommendation
systems
group
recommendation
systems
recommend
items
groups
users
opposed
123
pitoura
et
al
single
user
example
movie
group
friends
event
online
community
excursion
group
tourists
46
case
different
types
consumer
fairness
since
now
consumer
just
single
user
another
case
bundle
package
recommendation
systems
recommend
complex
items
sets
items
instead
just
single
item
example
set
places
visit
courses
attend
86
case
may
different
types
producer
fairness
since
now
recommended
items
composite
discuss
special
types
fairness
sect
3.3
can
expand
sides
fairness
considering
additional
stakeholders
may
involved
recommendation
system
besides
consumers
producers
example
recommendation
system
items
recommended
may
belong
different
providers
instance
case
movie
recommendations
movies
may
produced
different
studios
case
may
ask
producer
fairness
respect
providers
items
instead
single
items
example
online
craft
marketplace
may
want
ensure
market
diversity
avoid
monopoly
domination
system
wishes
ensure
new
entrants
market
get
reasonable
share
recommendations
even
though
fewer
shoppers
established
vendors
note
one
way
model
provider
fairness
treating
provider
protected
attribute
items
sides
fairness
include
fairness
owners
recommendation
system
especially
owners
different
producers
fairness
system
regulators
auditors
example
data
scientists
machine
learning
researchers
policymakers
governmental
auditors
using
system
decision
making
2.2
output
multiplicity
may
cases
feasible
achieve
fairness
considering
just
single
ranking
single
recommendation
output
thus
recently
exist
approaches
propose
achieving
fairness
via
series
outputs
consider
example
case
items
appear
results
multiple
search
queries
users
receive
one
recommendation
cases
may
ask
item
necessarily
treated
fairly
every
search
result
item
treated
fairly
overall
set
search
results
similarly
user
may
treated
unfairly
single
recommendation
fairly
sequence
recommendations
concretely
distinguish
single
output
multiple
output
fairness
multiple
output
fairness
ask
eventual
amortized
consumer
producer
fairness
ask
consumers
producers
treated
fairly
series
rankings
recommendations
whole
although
fairness
rankings
recommendations
overview
may
treated
unfairly
one
single
ranking
recommendation
series
another
case
sequential
recommenders
suggest
items
interest
modeling
sequential
dependencies
user
item
interactions
sequence
means
recommender
treats
user
item
interactions
dynamic
sequence
takes
sequential
dependencies
account
capture
current
previous
user
preferences
increasing
quality
recommendations
80
system
recommends
different
items
interaction
retaining
knowledge
past
interactions
interestingly
due
multiple
user
item
interactions
sequential
recommender
systems
fairness
correction
can
performed
moving
one
interaction
next
437
fairness
constraint
ed
requires
least
two
items
property
red
top-4
positions
satisfied
rankings
rm
rr
ranking
rl
discounted
cumulative
fairness
another
approach
looks
proportional
representation
items
protected
group
top
prefixes
ranking
various
values
87
proposed
model
builds
discounted
cumulative
gain
dcg
measure
dcg
standard
way
measuring
ranking
quality
top-k
items
dcg
accumulates
utility
util
item
position
top
position
position
logarithmically
discounted
position
item
thus
favoring
higher
utility
scores
first
positions
dcg
models
fairness
previous
section
presented
overview
fairness
various
dimensions
ranking
recommendations
section
present
number
concrete
models
definitions
fairness
proposed
ranking
recommendations
rank
aggregation
3.1
fairness
rankings
approaches
fairness
ranking
handle
producer
fairness
goal
ensure
items
ranked
treated
fairly
general
ranking
fairness
asks
similar
items
group
items
receive
similar
visibility
appear
similar
positions
ranking
main
issue
accounting
position
bias
since
western
cultures
read
top
bottom
left
right
visibility
lower-ranked
items
drops
rapidly
compared
visibility
higher-ranked
ones
will
use
example
rankings
fig
protected
attribute
color
red
protected
group
score
utility
item
ranking
left
rl
corresponds
ranking
based
solely
utility
ranking
middle
rm
achieves
highest
possible
representation
protected
group
top
positions
ranking
right
rr
intermediate
one
fairness
constraints
number
group-based
fairness
models
ranking
focus
representation
number
items
protected
group
top-k
position
ranking
one
type
group
fairness
achieved
constraining
number
items
different
groups
can
appear
top-k
positions
specifically
fairness
constraints
approach
18
given
number
protected
attributes
properties
fairness
requirements
expressed
specifying
upper
bound
ul
lower
bound
number
items
property
allowed
appear
top-k
positions
ranking
example
fig
util
log2
dcg
value
normalized
dcg
perfect
ranking
obtaining
ndcg
example
dcg
values
three
rankings
fig
dcg
10
rl
1.81
dcg
10
rm
1.7
dcg
10
rr
1.77
clearly
rl
ranks
items
solely
utility
largest
dcg
value
discounted
cumulative
fairness
accumulates
number
items
belonging
protected
group
discrete
positions
ranking
positions
10
discounts
numbers
accordingly
favor
representation
protected
group
prefixes
higher
positions
three
different
definitions
based
general
idea
provided
first
one
normalized
discounted
difference
ranking
measures
difference
proportion
items
protected
group
top
prefixes
various
values
overall
population
opt_r
10
log2
total
number
items
number
items
protected
group
top
positions
opt_r
optimal
value
example
optimal
value
fig
one
ranking
rm
ranking
maximum
possible
representation
protected
group
equal
0.93
rm
ranking
opt_r
log1
45
10
based
utility
smallest
rm
log1
25
10
log
10
10
10
0.93
rr
rr
0.93
log1
35
10
log
10
10
10
0.5
variation
termed
normalized
discounted
ratio
measures
difference
proportion
items
123
438
pitoura
et
al
fig
example
rankings
rl
based
solely
utility
rm
optimal
ranking
terms
representation
protected
group
rr
intermediate
ranking
two
red
protected
value
protected
group
top
positions
items
non-protected
group
top
positions
various
values
achieved
modifying
equation
instead
dividing
total
number
items
position
divide
number
items
protected
group
top
positions
instead
dividing
divide
finally
normalized
kl-divergence
definition
fairness
uses
kl-divergence
compute
expectation
difference
membership
probability
distribution
protected
group
top
positions
10
overall
population
fairness
exposure
problem
discounted
cumulative
approach
fact
account
skew
visibility
counting
items
discrete
positions
fully
capture
fact
minimal
differences
relevance
scores
may
translate
large
differences
visibility
different
groups
position
bias
results
large
skew
distribution
exposure
example
fig
average
utility
items
rl
belong
non-protected
group
0.35
whereas
average
utility
items
belong
protected
group
0.33
gives
us
difference
just
0.02
however
compute
exposure
using
dcg
discount
utility
logarithmically
exposure
items
protected
group
25
smaller
exposure
items
non-protected
group
fairness
exposure
approach
77
generalizes
logarithmic
discount
assigning
position
ranking
specific
value
represents
importance
position
fraction
users
examine
item
position
captured
using
position
discount
vector
represents
importance
position
note
can
get
logarithmic
discount
set
log
1j
rankings
seen
probabilistic
particular
ranking
items
positions
modeled
doubly
stochastic
matrix
pi
probability
item
ranked
position
123
given
position
discount
vector
exposure
item
ranking
defined
exposure
pi
exposure
group
defined
average
exposure
items
group
exposure
exposure
analogy
base
rate
statistical
parity
classification
get
demographic
parity
definition
ranking
fairness
asking
two
groups
get
exposure
exposure
exposure
classification
can
also
get
additional
statistical
fairness
definitions
taking
account
actual
output
case
utility
items
relevance
search
query
called
disparate
treatment
constraint
77
expressed
asking
exposure
two
groups
receive
proportional
average
utility
exposure
exposure
utility
utility
yet
another
definition
termed
disparate
impact
considers
instead
just
exposure
impact
ranking
impact
measured
using
click-through
rate
ctr
ctr
estimated
function
exposure
relevance
definition
asks
impact
ranking
two
groups
proportional
average
utility
ctr
ctr
utility
utility
fairness
rankings
recommendations
overview
439
fairness
exposure
approach
also
taken
define
individual
fairness
rankings
specifically
equity
attention
11
asks
item
receives
attention
ai
views
clicks
proportional
utility
utili
relevance
given
query
a2
a1
i1
i2
util1
util2
general
unlikely
equity
attention
can
satisfied
single
ranking
example
multiple
items
may
similarly
relevant
given
query
yet
obviously
occupy
ranking
position
case
example
items
id
x329
x23
fig
address
amortized
fairness
proposed
sequence
rankings
offers
amortized
equity
attention
11
item
receives
cumulative
attention
proportional
cumulative
relevance
a1
ml
2l
util1
util2
case
unfairness
defined
distance
attention
utility
distributions
unfairness
ai
utili
next
present
number
approaches
proposed
specifically
recommenders
discuss
relationship
approaches
presented
ranking
unfairness
predictions
recommendation
systems
widely
applied
several
domains
suggest
data
items
like
movies
jobs
courses
however
since
predictions
based
observed
data
can
inherit
bias
may
already
exist
handle
issue
measures
consumer-side
unfairness
introduced
88
look
discrepancy
prediction
behavior
protected
non-protected
users
specifically
proposed
accuracy-based
fairness
metrics
count
difference
predicted
actual
scores
errors
prediction
data
items
recommended
users
protected
group
items
recommended
users
non-protected
group
let
size
recommendation
list
average
predicted
score
item
receives
protected
users
non-protected
users
respectively
corresponding
average
actual
score
item
alternatives
defining
unfairness
can
summarized
follows
value
unfairness
uval
counts
inconsistencies
estimation
errors
across
groups
one
group
given
higher
lower
predictions
true
preferences
10
normalized
version
unfairness
definition
considers
number
items
ranked
number
rankings
sequence
proposed
13
formally
unfairness
nm
11
3.2
fairness
recommenders
general
recommendation
systems
estimate
score
rating
user
item
reflects
relevance
recommendation
list
formed
user
includes
items
highest
estimated
score
simple
approach
defining
producer-side
item-side
fairness
recommendations
consider
recommendation
problem
classification
problem
positive
class
recommendation
list
fairness
definitions
sect
2.2
readily
applicable
defining
producer-side
fairness
yet
another
approach
defining
producer-side
fairness
consider
recommendation
list
ranked
list
apply
various
definitions
described
sect
3.1
uval
12
value
unfairness
occurs
one
group
users
consistently
given
higher
lower
predictions
actual
preferences
example
considering
course
recommendations
value
unfairness
may
suggest
male
students
engineering
courses
even
interested
engineering
topics
female
students
recommended
engineering
courses
even
interested
topics
absolute
unfairness
uabs
counts
inconsistencies
absolute
estimation
errors
across
user
groups
uval
13
absolute
unfairness
unsigned
captures
single
statistic
representing
quality
prediction
group
underestimation
unfairness
uunder
counts
inconsistencies
much
predictions
underestimate
true
ratings
123
440
pitoura
et
al
max
uunder
max
14
underestimation
unfairness
important
missing
recommendations
critical
extra
recommendations
instance
underestimation
may
lead
top
students
recommended
explore
topics
excel
overestimation
unfairness
uover
counts
inconsistencies
much
predictions
overestimate
true
ratings
important
users
may
overwhelmed
recommendations
max
max
uover
15
finally
non-parity
unfairness
par
counts
absolute
difference
overall
average
ratings
protected
users
non-protected
users
upar
16
calibrated
recommendations
calibration-based
approach
producer-side
fairness
proposed
78
classification
algorithm
considered
well
calibrated
predicted
proportions
groups
various
classes
agree
actual
proportions
input
data
analogy
goal
calibrated
recommendation
algorithm
reflect
interests
user
recommendations
appropriate
proportions
intuitively
proportion
different
groups
items
recommendation
list
similar
corresponding
proportions
history
user
example
consider
movies
items
recommended
genre
protected
attribute
quantifying
degree
calibration
list
recommended
movies
respect
user
history
played
movies
approach
considers
two
distributions
genre
movie
specifically
distribution
genres
set
movies
history
user
wu
wu
17
set
movies
played
user
past
wu
weight
movie
reflecting
recently
played
turn
distribution
genres
list
movies
recommended
123
wr
wr
18
set
recommended
movies
wr
weight
movie
due
rank
recommendation
list
compare
distributions
several
methods
can
used
like
example
kullback
leibler
kl
divergence
employed
calibration
metric
log
19
target
distribution
small
used
handle
fact
kl-divergence
diverge
kl-divergence
ensures
genres
user
rarely
played
will
also
reflected
recommended
list
corresponding
proportions
namely
sensitive
small
discrepancies
distributions
favors
uniform
less
extreme
distributions
case
perfect
calibration
value
pairwise
fairness
instead
looking
scores
items
receive
pairwise
fairness
looks
relative
position
pairs
items
recommendation
list
pairwise
approach
proposed
10
accuracy-based
one
positive
class
includes
items
receive
positive
feedback
user
clicks
high
ratings
increased
user
engagement
dwell
time
simplicity
following
will
assume
click-based
feedback
let
user
clicked
item
otherwise
assume
predicted
probability
clicks
monotonic
ranking
function
let
set
items
group
protected
non-protected
items
respectively
pairwise
accuracy
based
probability
clicked
item
ranked
another
unclicked
item
user
20
succinctness
let
cu
main
idea
ask
two
groups
similar
pairwise
accuracy
specifically
achieve
pairwise
fairness
cu
cu
21
work
also
considers
actual
engagement
conditioning
items
engaged
amount
can
also
distinguish
intra
inter-group
fairness
intra-group
pairwise
fairness
achieved
like
fairness
rankings
recommendations
overview
lihood
clicked
item
ranked
another
relevant
unclicked
item
group
independent
group
eq
21
belong
group
inter-group
pairwise
fairness
achieved
likelihood
clicked
item
ranked
another
relevant
unclicked
item
opposite
group
independent
group
eq
21
belong
opposite
groups
3.3
fairness
rank
aggregation
addition
efforts
focus
fairness
single
rankings
recommendations
problem
fairness
rank
aggregation
also
emerged
problem
arises
number
ranked
outputs
produced
need
aggregate
outputs
order
construct
new
ranked
consensus
output
typically
problem
fair
rank
aggregation
largely
unexplored
recently
works
study
mitigate
bias
introduced
aggregation
phase
done
mainly
umbrella
group
recommendations
instead
individual
user
requesting
recommendations
system
request
made
group
users
example
consider
group
friends
wants
watch
movie
member
group
likes
dislikes
system
needs
properly
balance
users
preferences
offer
group
list
movies
degree
relevance
member
typical
way
apply
ranking
recommendation
method
member
individually
aggregate
separate
lists
one
group
64
72
aggregation
phase
intuitively
movie
can
calculate
average
score
across
users
group
preference
scores
movie
average
approach
alternative
can
use
minimum
function
rather
average
one
least
misery
approach
even
can
focus
ensure
fairness
attempting
minimize
feeling
dissatisfaction
within
group
members
next
present
fairness
model
general
rank
aggregation
problem
additional
models
defined
group
recommendations
top-k
parity
55
formalizes
fair
rank
aggregation
problem
constrained
optimization
problem
specifically
given
set
rankings
fair
rank
aggregation
problem
returns
closest
ranking
given
set
rankings
satisfies
particular
fairness
criterion
given
data
item
protected
attribute
partitions
dataset
disjoint
groups
work
uses
fairness
criterion
general
formulation
statistical
parity
rankings
considers
top-k
prefix
rankings
namely
top-k
parity
formally
given
ranking
data
items
belonging
mutually
exclusive
groups
ranking
441
satisfies
top-k
parity
following
condition
met
22
denotes
position
data
item
ranking
dissatisfaction
fairness
counting
fairness
measure
quantifying
satisfaction
utility
user
group
given
list
recommendations
group
can
used
namely
checking
relevant
recommended
items
user
59
formally
given
user
group
set
items
recommended
individual
utility
util
items
can
defined
average
items
utility
normalized
respect
relevance
el
util
el
elmax
23
elmax
denotes
maximum
value
el
can
take
turn
overall
satisfaction
users
group
recommendation
quality
group
utility
estimated
via
aggregating
individual
utilities
called
social
welfare
sw
defined
sw
util
24
estimating
fairness
need
compare
utilities
users
group
intuitively
example
list
minimizes
dissatisfaction
user
group
can
considered
fair
sense
fairness
enforces
least
misery
principle
among
users
utilities
emphasizing
gap
least
highest
utilities
group
members
following
concept
fairness
can
defined
min
util
25
similarly
fairness
can
encourage
group
members
achieve
close
utilities
using
variance
variance
util
26
pareto
optimal
fairness
instead
computing
users
individual
utility
list
recommendations
summing
relevance
scores
items
list
59
item
positions
recommendation
list
can
considered
73
specifically
solution
making
fair
group
recommendations
based
notion
pareto
optimality
means
item
pareto
optimal
group
exists
item
ranks
higher
according
users
group
item
dominates
item
123
442
pitoura
et
al
n-level
pareto
optimal
turn
direct
extension
contains
items
dominated
items
used
identifying
best
items
recommend
set
items
fair
definition
since
contains
top
choices
user
group
fairness
package-to-group
recommendations
given
group
approach
fair
package-to-group
recommendations
recommend
package
items
requiring
user
least
one
item
high
preferences
included
76
even
resulting
package
best
overall
fair
since
exists
least
one
item
satisfies
user
specifically
two
different
aspects
fairness
examined
76
fairness
proportionality
ensuring
user
finds
sufficient
number
items
package
likes
compared
items
package
fairness
envyfreeness
ensuring
user
sufficient
number
items
package
likes
users
formally
m-proportionality
group
users
package
m-proportionality
defined
fprop
27
set
users
mproportional
turn
m-proportional
user
exist
least
items
one
ranked
top-δ
preferences
items
input
parameter
m-envy-freeness
group
users
package
m-envy-freeness
defined
fe
28
set
users
m-envy-free
turn
m-envy-free
user
envy-free
least
items
item
el
top-δ
preferences
set
el
sequential
hybrid
aggregation
approach
fair
sequential
group
recommenders
targets
two
independent
objectives
80
first
one
considers
group
entity
aims
offering
best
possible
results
maximizing
overall
group
satisfaction
sequence
recommendations
satisfaction
user
group
group
recommendation
gr
received
jth
round
recommendations
computed
comparing
quality
recommendations
receives
member
group
quality
recommendations
received
individual
given
list
au
top-n
items
user
satisfaction
calculated
based
group
recommendation
list
every
item
gr
sum
utility
scores
appear
user
au
123
ideal
case
user
sum
utility
scores
top-n
items
au
formally
sat
gr
dz
gr
util
dz
dz
au
util
dz
29
second
objective
considers
group
members
independently
aims
behave
fairly
possible
toward
members
minimizing
variance
user
satisfaction
scores
intuitively
variance
represents
potential
disagreement
users
group
formally
disagreement
defined
groupdis
gr
max
sat
gr
min
sat
gr
30
sat
gr
overall
satisfaction
sequence
gr
recommendations
defined
average
satisfaction
scores
round
group
disagreement
difference
overall
satisfaction
scores
satisfied
least
satisfied
user
group
measure
takes
low
values
group
members
satisfied
degree
3.4
summary
short
can
categorize
various
definitions
fairness
used
rankings
recommendations
rank
aggregation
methods
based
level
side
fairness
output
multiplicity
specifically
regarding
level
fairness
can
distinguished
individual
group
fairness
side
dimension
considers
consumer
producer
fairness
output
multiplicity
dimension
considers
single
multiple
outputs
table
presents
summary
various
definitions
fairness
make
several
observations
specifically
observe
fairness
definitions
based
one
following
criteria
position
item
ranking
recommendation
list
item
utility
prediction
error
rating
number
items
user
satisfaction
defined
item
utility
rankings
fairness
typically
defined
items
ranked
matching
existing
definitions
producer
fairness
except
equity
attention
definitions
group
based
position
bias
use
constraints
proportion
items
top
positions
based
idea
providing
exposure
proportional
utility
recommenders
definitions
group
based
case
distinction
consumer
fairness
producer
fairness
makes
sense
given
focus
either
individuals
receive
recommendation
individuals
recommended
nevertheless
existing
works
target
consumer
fairness
fairness
rankings
recommendations
overview
443
table
fairness
definitions
taxonomy
rankings
recommenders
rank
aggregation
individual
group
consumer
producer
single
multiple
criterion
rankings
fairness
constraints
18
position
discounted
cumulative
fairness
87
position
fairness
exposure
77
position
utility
equity
attention
11
position
utility
recommenders
calibrated
recommendations
78
number
items
value
absolute
unfairness
88
error
predictions
overestimation
unfairness
88
error
ratings
non-parity
unfairness
88
ratings
rank
aggregation
top-k
parity
55
position
dissatisfaction
fairness
59
user
satisfaction
pareto
optimal
fairness
73
position
proportionality
fairness
76
number
items
envy-freeness
fairness
76
number
items
sequential
hybrid
aggregation
80
user
satisfaction
rank
aggregation
recently
mainly
group
recommenders
fairness
considered
need
aggregate
number
ranked
outputs
order
produce
new
ranked
consensus
output
specifically
group
recommenders
goal
evaluate
system
takes
consideration
individual
preferences
single
user
group
making
approaches
research
literature
target
individual
consumer
fairness
recently
approaches
focus
another
form
fairness
applicable
consider
sequence
rankings
recommendations
instead
just
single
one
follows
will
study
models
definitions
fairness
applied
algorithms
methods
achieving
fairness
taking
cross-type
view
present
section
taxonomy
organize
place
related
works
perspective
specifically
fig
3a
show
traditional
way
results
ranked
outputs
fig
3b
present
various
options
general
distinction
methods
generating
fair
ranked
outputs
recommendations
namely
methods
distinguished
following
categories
pre-processing
methods
aim
transforming
data
remove
underlying
bias
discrimination
typically
methods
application
agnostic
consider
bias
training
data
try
mitigate
bias
data
may
produced
due
data
collection
process
example
based
decisions
pieces
data
collect
assumptions
make
missing
values
even
using
data
different
way
intended
collection
in-processing
methods
aim
modifying
existing
introducing
new
algorithms
result
fair
rankings
recommendations
removing
bias
discrimination
model
training
process
typically
methods
targets
learning
model
bias
considering
fairness
training
model
example
incorporating
changes
objective
function
algorithm
fairness
term
imposing
fairness
constraints
without
offering
guarantees
fairness
ranked
outputs
post-processing
methods
modify
output
algorithm
typically
methods
can
treat
ranking
recommendation
algorithm
black
box
without
ability
modify
improve
fairness
re-rank
data
items
output
naturally
post-processing
methods
fairness
comes
cost
accuracy
since
definition
methods
transform
optimal
output
hand
clear
advantage
post-processing
methods
offer
ranked
outputs
easy
understand
comparing
outputs
outputs
application
post-processing
fairness
method
123
444
pitoura
et
al
fig
general
distinction
methods
ensuring
fair
ranked
outputs
next
will
use
taxonomy
organize
present
related
works
describe
following
sections
pre-processing
methods
bias
underlying
data
systems
trained
can
take
two
forms
bias
rows
data
exists
enough
representative
individuals
minority
groups
example
according
reuters
article
23
amazon
experimental
automated
system
review
job
applicants
resumes
showed
significant
gender
bias
toward
male
candidates
females
due
historical
discrimination
training
data
bias
columns
features
biased
correlated
sensitive
attributes
example
zip
code
tends
predict
race
due
history
segregation
44
direct
discrimination
occurs
protected
attributes
used
explicitly
making
decisions
disparate
treatment
pervasive
nowadays
indirect
discrimination
protected
attributes
used
reliance
variables
correlated
leads
significantly
different
outcomes
different
groups
also
known
disparate
impact
address
bias
avoid
discrimination
several
methods
proposed
pre-processing
data
many
methods
studied
context
classification
proposed
context
recommender
systems
5.1
suppression
tackle
bias
data
naïve
approach
used
practice
simply
omit
protected
attribute
say
race
gender
training
classifier
48
simply
excluding
protected
variable
insufficient
avoid
discriminatory
predictions
included
variables
correlated
protected
variables
still
contain
information
protected
characteristic
classifier
still
learns
discrimination
reflected
training
data
example
answers
personality
tests
identify
people
disabilities
85
word
embeddings
trained
google
news
articles
exhibit
female
male
gender
stereotypes
123
fig
job
application
example
48
12
tackle
dependencies
one
can
find
attributes
correlate
sensitive
attribute
remove
well
5.2
class
relabeling
approach
also
known
massaging
48
changes
labels
objects
dataset
order
remove
discrimination
input
data
good
selection
labels
change
essential
idea
consider
subset
data
minority
group
promotion
candidates
change
class
label
similarly
subset
majority
group
chosen
demotion
candidates
select
best
candidates
relabeling
ranker
used
ranks
objects
based
probability
positive
labels
example
naïve
bayesian
classifier
can
used
ranking
learning
47
48
top-k
minority
promotion
objects
bottom-k
majority
demotion
objects
chosen
number
pairs
needed
modified
make
dataset
discrimination-free
can
calculated
follows
let
us
assume
two
groups
namely
protected
group
non-protected
privileged
group
modify
objects
group
resulting
dis
fairness
rankings
recommendations
overview
445
crimination
will
disc
disc
31
reach
zero
discrimination
number
modifications
needed
disc
32
number
positive
objects
belong
minority
group
majority
group
discrimination
disc
probability
positive
class
objects
minority
group
versus
majority
group
example
consider
dataset
fig
dataset
contains
sex
ethnicity
highest
degree
ten
job
applicants
job
type
applied
outcome
selection
procedure
class
want
learn
classifier
predict
class
objects
predictions
non-discriminatory
toward
females
can
rank
objects
positive
class
probability
given
naïve
bayes
classification
model
figure
shows
extra
column
gives
probability
applicant
belongs
positive
class
second
step
arrange
data
separately
female
applicants
class
descending
order
male
applicants
class
ascending
order
respect
positive
class
probability
ordered
promotion
demotion
candidates
given
fig
reach
zero
discrimination
number
modifications
needed
0.4
disc
10
33
relabel
highest
scoring
female
negative
label
lowest
scoring
male
positive
label
discrimination
becomes
zero
resulting
dataset
will
used
training
classifier
problem
classification
without
discrimination
sensitive
attribute
multi-objective
optimization
problem
lowering
discrimination
will
result
lowering
accuracy
vice
versa
5.3
reweighing
previous
approach
rather
intrusive
changes
labels
objects
instead
weights
can
assigned
objects
compensate
bias
48
idea
assign
lower
weights
objects
deprived
fig
job
applications
positive
class
probability
fig
promotion
demotion
candidates
favored
weights
can
used
directly
method
based
frequency
counts
frequently
used
family
analytical
methods
grouped
propensity
score
matching
41
methods
model
probability
object
group
receiving
treatment
use
predicted
probabilities
propensities
make
confounding
treatment
variables
interest
balance
data
simple
probability-based
reweighing
method
following
48
let
us
consider
sensitive
attribute
every
object
will
assigned
weight
pex
class
class
pobs
class
class
34
weight
object
will
expected
probability
see
instance
sensitive
attribute
value
class
given
independence
divided
observed
probability
example
consider
dataset
fig
dataset
unbiased
sensitive
attribute
sex
example
class
statistically
independent
expected
probability
females
promoted
pexp
sex
class
0.5
0.6
0.3
reality
however
observed
probability
based
dataset
sex
class
0.2
hence
one
can
use
re-weighting
factor
0.3
0.2
1.5
balance
bias
dataset
entropy
balancing
aims
covariate
balance
data
binary
classification
39
relies
maximum
entropy
reweighting
scheme
calibrates
individual
weights
reweighed
groups
satisfy
set
balance
constraints
123
446
pitoura
et
al
imposed
sample
moments
covariate
distributions
balance
constraints
ensure
reweighed
groups
match
exactly
specified
moments
adjusting
way
inequalities
representation
generated
weights
can
passed
standard
classifier
adaptive
sensitive
reweighing
uses
convex
model
estimate
distributions
underlying
labels
adapt
weights
54
assumes
exists
unobservable
underlying
set
class
labels
corresponding
training
samples
predicted
yield
unbiased
classification
respect
fairness
objective
searches
sample
weights
make
weighted
training
original
dataset
also
train
toward
labels
without
explicitly
observing
specifically
consider
binary
probabilistic
classifier
produces
probability
estimates
yi
yi
training
samples
features
xi
class
labels
yi
exist
underlying
unobservable
class
labels
yield
estimated
labels
conform
designated
fairness
accuracy
trade-offs
training
goal
minimize
weighted
error
observed
labels
distance
weighted
observed
labels
unweighed
underlying
labels
min
wi
yi
35
min
wi
yi
36
simultaneously
adjust
training
weights
alongside
classifier
training
classifier-agnostic
iterative
approach
proposed
first
classifier
fully
trained
based
uniform
weights
method
appropriately
readjusts
weights
process
repeated
convergence
5.4
data
transformation
common
theme
importance
balancing
discrimination
control
utility
processed
data
can
formulated
optimization
problem
producing
preprocessing
transformations
trade-off
discrimination
control
data
utility
individual
distortion
67
assuming
one
protected
sensitive
variables
denotes
non-protected
variables
outcome
random
variable
goal
determine
randomized
mapping
px
transforms
training
data
test
data
mapping
satisfy
three
properties
discrimination
control
first
objective
limit
dependence
transformed
outcome
protected
variables
requires
conditional
distribution
py
close
target
distribution
pyt
values
123
distortion
control
mapping
px
satisfy
distortion
constraints
reduce
avoid
certain
large
changes
low
credit
score
mapped
high
credit
score
utility
preservation
distribution
statistically
close
distribution
ensure
model
learned
transformed
data
averaged
protected
variables
different
one
learned
original
data
example
bank
existing
policy
approving
loans
change
much
learnt
transformed
data
5.5
database
repair
handling
bias
data
can
considered
database
repair
problem
one
approach
remove
information
protected
variables
set
covariates
used
predictive
models
28
61
test
disparate
impact
based
well
protected
class
can
predicted
attributes
data
repair
algorithm
numerical
attributes
proposed
28
algorithm
strongly
preserves
rank
means
changes
data
way
predicting
class
still
possible
chain
conditional
models
can
used
protecting
adjusting
variables
arbitrary
type
61
framework
allows
arbitrary
number
variables
adjusted
variables
protected
variables
continuous
discrete
another
data
repair
approach
based
measuring
discriminatory
causal
influence
protected
attribute
outcome
algorithm
approach
removes
discrimination
repairing
training
data
order
remove
effect
discriminatory
causal
relationship
protected
attribute
classifier
predictions
74
work
introduced
notion
interventional
fairness
ensures
protected
attribute
affect
output
algorithm
configuration
system
obtained
fixing
variables
arbitrary
values
system
repairs
input
data
inserting
removing
tuples
changing
empirical
probability
distribution
remove
influence
protected
attribute
outcome
causal
pathway
includes
inadmissible
attributes
attributes
influence
protected
attribute
5.6
data
augmentation
different
approach
augment
training
data
additional
data
71
framework
starts
existing
matrix
factorization
recommender
system
already
trained
input
ratings
data
adds
new
users
provide
ratings
existing
items
new
users
ratings
called
antidote
data
chosen
improve
fairness
rankings
recommendations
overview
socially
relevant
property
recommendations
provided
original
users
proposed
framework
includes
measures
individual
group
unfairness
5.7
summary
pre-processing
methods
table
summarizes
pre-processing
approaches
fairness
based
whether
focus
bias
rows
columns
level
fairness
individual
group
algorithm
will
use
pre-processed
data
many
pre-processing
methods
studied
context
classification
ranking
suppression
simple
brute-force
approach
depend
algorithm
downside
algorithm
may
still
learn
discrimination
correlated
attributes
trying
remove
attributes
well
can
seriously
hurt
value
dataset
class
relabeling
approach
works
different
rankers
naive
bayes
classifier
nearest-neighbor
classifier
however
objective
lower
discrimination
will
result
lowering
accuracy
vice
versa
finding
right
balance
challenging
moreover
intrusive
changes
dataset
reweighing
methods
parameter-free
rely
ranker
hence
can
work
ranking
algorithm
long
leverage
frequencies
adaptive
sensitive
reweighing
additional
classification
overhead
since
simultaneously
adjusts
training
weights
alongside
classifier
training
iterative
approach
repeated
convergence
data
transformation
methods
work
different
classifiers
can
applied
numerical
datasets
modify
data
aforementioned
approaches
modify
training
data
explicitly
suppressing
attributes
changing
class
labels
implicitly
adding
weights
data
augmentation
leaves
training
data
just
augment
additional
data
one
data
augmentation
method
proposed
context
recommender
system
particular
matrix
factorization
approach
studied
individual
group
fairness
71
general
group
fairness
easier
track
handle
abundance
machine
learning
algorithms
used
practice
search
recommendations
general
dictating
clear
need
future
systematic
study
relationship
dataset
features
algorithms
pre-processing
performance
in-processing
methods
in-processing
methods
achieving
fairness
rankings
recommendations
focus
modifying
existing
introducing
new
models
algorithms
section
survey
447
unified
way
methods
distinguishing
learning
approaches
approaches
using
preference
functions
6.1
learning
approaches
rankings
recommenders
learning
approaches
typically
use
machine
learning
construct
ranking
models
often
using
set
labeled
training
data
input
general
ranking
model
ranks
unseen
lists
way
similar
ranking
training
data
overall
goal
learn
model
minimizes
loss
function
captures
distance
learned
input
ranking
various
approaches
exist
varying
form
training
data
type
loss
function
60
point-wise
approach
21
training
data
item
relevancescore
pairs
query
case
learning
can
seen
regression
problem
given
item
query
goal
predict
score
item
pair-wise
approach
training
data
pairs
items
first
item
relevant
second
item
given
query
14
30
case
learning
can
seen
binary
classification
problem
given
two
items
classifier
decides
whether
first
item
better
second
one
finally
list-wise
approach
16
input
consists
query
list
items
ordered
relevance
query
note
loss
function
takes
many
different
forms
depending
approach
example
pair-wise
approach
loss
may
computed
average
number
inversions
ranked
output
following
present
number
approaches
toward
making
ranking
models
fair
note
proposed
approaches
can
adopted
work
different
types
input
data
loss
functions
6.1
adding
regularization
terms
general
in-processing
approach
achieving
fairness
adding
regularization
terms
loss
function
learning
model
regularization
terms
express
measures
unfairness
model
must
minimize
addition
minimization
original
loss
function
depending
form
training
data
loss
function
measure
fairness
different
instantiations
general
approach
possible
deltr
approach
90
extends
listnet
16
learning
rank
framework
listnet
list-wise
framework
training
set
consists
query
list
items
ordered
relevance
listnet
learns
ranking
function
minimizes
loss
function
measures
extent
ordering
items
induced
query
differs
ordering
items
appear
training
set
query
loss
function
deltr
123
448
pitoura
et
al
table
pre-processing
methods
bias
rows
bias
columns
fairness
algorithm
suppression
48
group
class
relabeling
47
48
group
ranker
reweighing
39
48
54
group
individual
ranker
data
transformation
67
group
individual
ranker
data
repair
28
61
74
group
ranker
data
augmentation
71
group
individual
matrix
factorization
deltr
deltr
unfairness
37
deltr
extends
original
loss
function
listnet
term
imposes
fairness
constraint
parameter
controls
trade-off
ranking
utility
distance
input
ranking
captured
original
loss
function
fairness
exposure
used
measure
unfairness
produced
output
specifically
unfairness
max
exposure
exposure
38
using
squared
hinge
loss
makes
loss
function
differentiable
also
model
prefers
rankings
exposure
protected
group
less
exposure
non-protected
group
vice
versa
regularization
approach
also
taken
recommender
systems
49
let
denote
random
variables
users
items
respectively
denote
random
variable
recommendation
output
let
also
sensitive
attribute
information
ignored
recommendation
process
like
gender
user
popularity
item
goal
case
achieve
recommendation
statistical
independence
means
include
information
sensitive
feature
influences
outcome
well
recommendations
satisfy
recommendation
independence
constraint
core
regularization
approach
included
equation
39
adopts
regularizer
imposing
constraint
independence
training
recommendation
model
loss
ri
xi
yi
si
ind
reg
39
independence
parameter
controls
balance
independence
accuracy
ind
123
independence
term
regularizer
constrain
independence
larger
value
indicates
recommendations
sensitive
values
independent
loss
empirical
loss
regularization
parameter
l2
regularizer
several
alternatives
can
used
independence
term
like
example
mutual
information
histogram
models
normal
distributions
exploiting
distance
measures
case
distribution
matching
using
bhattacharyya
distance
6.1
learning
via
variational
autoencoders
variational
autoencoders
vae
proposed
state-ofthe-art
collaborative
filtering
task
recommenders
multinomial
likelihood
generative
model
controlled
regularization
parameter
possible
estimate
normal
distribution
parameters
middle
layer
mlp
enriches
rating
data
representation
outperforms
previous
neural
network-based
approaches
58
situation
requires
drawing
samples
inferred
distributions
order
propagate
values
decoder
trivial
task
take
gradients
sampling
step
re-parameterization
trick
52
re-parameterize
sampled
values
incorporating
normal
distributed
noise
gradient
can
back-propagate
sampled
variable
training
instead
using
re-parameterization
trick
training
phase
noise
variable
can
incorporated
test
phase
vae
well
13
order
enhance
fairness
ranking
order
recommendations
using
definition
fairness
eq
10
motivation
different
noise
distributions
directly
affect
rankings
depending
frequently
latent
values
vary
around
mean
inside
interval
defined
variance
specifically
experimentally
shown
noisy
effect
gaussian
uniform
distributions
varies
output
scores
data
input
unfairness
reduced
despite
small
decrease
quality
ranking
higher
variance
new
component
greater
effect
predicted
scores
consequently
ranking
order
fairness
rankings
recommendations
overview
449
6.1
learning
fair
representations
main
idea
approach
learn
fair
representation
input
data
use
task
hand
previous
work
fair
classification
used
idea
achieve
fairness
introducing
intermediate
level
input
space
represents
individuals
output
space
represents
classification
outcomes
92
fair
representation
best
encodes
obfuscates
information
membership
protected
group
specifically
modeled
multinomial
random
variable
size
values
represents
prototype
cluster
space
goal
learn
minimize
loss
function
λz
40
first
term
refers
quality
encoding
expresses
requirement
distance
points
representation
small
second
term
refers
fairness
last
term
refers
accuracy
prediction
based
representation
accurate
parameters
λx
λz
hyper-parameters
control
trade-off
among
three
objectives
one
can
enforce
different
forms
fairness
appropriately
defining
objective
statistical
parity
used
92
captured
following
objective
pr
pr
41
probability
random
element
belongs
protected
group
maps
particular
prototype
equal
probability
random
element
belongs
non-protected
group
maps
prototype
fair
representation
approach
also
used
fair
ranking
instead
classification
achieved
modifying
last
objective
equation
40
represent
accuracy
case
ranking
opposed
accuracy
classification
87
modified
objective
asks
distance
ground-truth
ranking
estimated
ranking
small
6.2
linear
preference
functions
applications
items
ranked
based
score
weighted
linear
combination
values
attributes
specifically
let
item
scoring
attributes
linear
ranking
function
uses
weight
vector
w1
wd
compute
utility
goodness
score
item
dj
case
fairness
formulated
following
problem
given
function
weight
vector
w1
wd
find
function
weight
vector
w1
wd
produces
fair
ranking
weights
close
weights
original
possible
cosine
minimized
6.3
constraint
optimization
rank
aggregation
fairness-preserving
rank
aggregation
55
presents
solution
balances
aggregation
accuracy
fairness
using
pairwise
rank
representation
case
ranking
represented
set
pairwise
comparisons
data
items
two
rankings
number
pairs
items
order
two
rankings
expresses
kendall
tau
distance
51
overall
given
set
rankings
ranking
minimum
average
kendall
tau
distance
rankings
set
known
kemeny
optimal
rank
aggregation
consider
case
data
items
two
different
groups
pairs
cartesian
product
can
divided
three
subsets
pairs
containing
items
pairs
containing
items
pairs
containing
one
item
one
item
rpar
computes
probability
ranking
item
group
ranked
item
group
rpar
xi
xi
42
following
pairwise
formulation
statistical
parity
two
groups
defined
given
ranking
consisting
data
items
belong
mutually
exclusive
groups
satisfies
pairwise
statistical
parity
following
condition
met
rpar
rpar
43
linear
integer
programming
solution
parity
constraint
offered
aggregating
many
rankings
producing
fair
consensus
achieving
better
efficiency
branch-and-bound
fairness-aware
ranking
algorithm
designed
integrates
rank
parity-preserving
heuristic
sect
summarize
in-processing
methods
along
post-processing
ones
highlighting
advantages
category
post-processing
methods
post-processing
approaches
agnostic
ranking
recommendation
algorithm
typically
take
input
123
450
ranking
specification
required
form
fairness
produce
new
ranking
satisfies
fairness
requirements
respects
initial
ranking
extent
possible
7.1
fairness
generative
process
let
us
consider
simple
case
group
parity
ask
specific
proportion
items
top
positions
ranking
belong
protected
group
given
ranking
generative
process
introduced
87
creates
ranking
initializing
empty
list
incrementally
adding
items
specifically
position
bernoulli
trial
performed
trial
succeeds
select
best
available
highly
ranked
item
protected
group
otherwise
select
best
available
item
nonprotected
group
example
shown
fig
ranking
rl
left
original
ranking
based
utility
score
items
middle
ranking
rm
corresponds
extreme
case
members
protected
group
placed
top
positions
ranking
tr
right
corresponds
0.5
produced
ranking
satisfies
in-group
monotonicity
constraints
means
within
group
items
ordered
decreasing
qualifications
also
shown
assumptions
ranking
also
maximizes
utility
expressed
average
score
items
top-k
positions
89
statistical
test
generative
model
proposed
89
specifically
given
specific
position
seen
specific
number
items
group
onetailed
binomial
test
used
compare
null
hypotheses
ranking
generated
using
model
parameter
mean
protected
group
represented
less
desired
7.2
fair
ranking
constraint
optimization
problem
another
post-processing
approach
formulates
problem
producing
fair
ranking
optimization
problem
let
fairness
measure
rankings
let
measure
utility
ranking
particular
task
example
let
relevance
ranking
given
query
two
general
ways
formulating
optimization
problem
involving
fairness
utility
namely
maxfcou
maximizing
fairness
subject
constraint
utility
maxucof
maximizing
utility
subject
constraint
fairness
123
pitoura
et
al
maxfcou
formulation
underlying
idea
produce
ranking
fair
possible
remaining
relevant
11
example
ask
fair
ranking
among
rankings
also
satisfies
utility
constraint
loss
utility
regards
original
ranking
remains
given
threshold
argmaxr
distance
alternatively
maxucof
formulation
look
ranking
maximum
possible
utility
among
rankings
whose
fairness
sufficient
11
77
example
approach
proposed
77
produces
ranking
argmaxr
fair
characterize
maxucof
approaches
postprocessing
since
assume
utility
item
known
can
estimated
thus
implicitly
original
non-fair
ranking
items
ordered
solely
utility
given
individual
utilities
new
ranking
produced
also
satisfies
fairness
constraint
general
complexity
maxfcou
maxucof
optimization
problems
depends
type
utility
fairness
functions
form
constraints
cases
optimization
problems
can
solved
using
linear
integer
programming
ilp
11
special
cases
using
dynamic
programming
algorithm
18
constraints
can
also
used
producing
fair
recommendation
packages
items
groups
users
76
intuition
method
greedily
construct
package
adding
rounds
item
satisfies
largest
number
non-satisfied
users
specifically
given
satg
denotes
users
satisfied
round
goal
maximize
satg
satg
44
considering
fairness
proportionality
equation
28
satg
contains
users
item
belongs
top-δ
preferable
items
envy-freeness
fairness
eq
27
satg
contains
users
envy-free
item
fair
user
m-proportional
m-envy-free
least
items
satg
method
generalized
include
constraints
restrict
set
candidate
packages
can
recommended
group
users
two
types
constraints
discussed
namely
category
distance
constraints
simple
words
category
constraints
selecting
item
specific
category
remove
items
category
candidate
set
distance
constraints
fairness
rankings
recommendations
overview
451
consider
candidate
items
items
added
existing
solution
satisfy
specific
input
distance
constraints
7.3
fairness
via
calibration
methods
calibration
methods
suggest
re-rank
list
items
post-processing
step
equation
19
sect
3.2
quantifies
degree
calibration
recommender
outputs
based
specific
metrics
78
determine
optimal
set
recommended
items
movies
suggested
scenario
maximum
marginal
relevance
function
17
used
argmax
45
determines
trade-off
prediction
scores
movies
calibration
metric
equation
19
namely
tradeoff
accuracy
calibration
controlled
greedily
method
starts
empty
set
iteratively
adds
one
movie
time
namely
movie
maximizes
eq
45
similar
way
group
recommendations
best
suggestions
group
maximize
social
welfare
sw
eq
24
fairness
eqs
25
26
using
scheme
59
sw
46
greedy
solution
select
item
added
current
recommendation
list
achieves
highest
fairness
time-efficient
alternatives
offered
via
integer
programming
techniques
considering
notion
pareto
optimality
group
recommendations
simple
heuristic
can
used
compile
approximately
identify
list
top-n
recommendations
group
73
specifically
given
largest
number
items
system
can
recommend
individual
user
method
proceeds
follows
requests
top-n
recommendations
user
group
takes
union
identifies
level
pareto
optimal
items
among
items
union
7.4
fairness
multiple
outputs
providing
rankings
recommendations
users
typically
pay
attention
first
positions
attention
wears
items
lower
positions
ranking
situation
greatest
estimated
probabilities
quite
close
equal
algorithm
needs
arrange
proper
order
necessarily
present
high
scores
high
positions
promotes
unfair
result
can
mitigated
long
term
changing
position
items
sequential
rounds
rankings
recommendations
case
rankings
one
approach
11
require
ranked
items
receive
attention
proportional
utility
sequence
rankings
eq
way
unfair
position
one
item
appears
single
ranking
can
compensated
next
rankings
changes
position
whole
session
contemplates
long-term
fairness
however
act
reducing
unfairness
implies
reduction
ranking
quality
due
perturbation
utility-based
rankings
trade-off
ranking
quality
fairness
formulated
constrained
optimization
problem
11
targeting
minimizing
unfairness
subject
constraints
quality
lower-bound
minimum
acceptable
quality
specifically
sequence
rankings
items
ordered
utility
score
inducing
zero
quality
loss
aim
reordering
minimize
distance
attention
utility
distributions
constraints
ndcg-quality
loss
ranking
formally
min
ai
ui
47
subject
ndcg-quality
ai
ui
denote
cumulative
attention
utility
scores
item
gained
across
rankings
consider
different
scenario
group
users
interacts
recommender
multiple
times
80
following
traditional
methods
group
recommendations
like
average
aggregation
method
least
misery
one
degree
satisfaction
user
group
equation
29
good
enough
users
group
leading
cases
almost
none
reported
items
interest
users
group
recommender
system
unfair
users
unfairness
continues
throughout
number
recommendations
rounds
overcome
drawbacks
average
least
misery
aggregation
methods
capitalize
advantages
aggregation
method
called
sequential
hybrid
aggregation
method
offers
weighted
combination
80
specifically
score
dz
avgscore
dz
leastscore
dz
48
group
avgscor
dz
returns
score
item
dz
computed
average
aggregation
method
round
least
scor
dz
returns
least
satisfied
user
score
dz
round
self-regulate
value
effectively
describe
consensus
group
set
dynamically
iteration
subtracting
minimum
satisfaction
score
123
452
pitoura
et
al
table
post-processing
methods
level
fairness
individual
group
side
fairness
consumer
producer
output
multiplicity
single
multiple
adding
regularization
terms
49
90
49
90
49
90
learning
fair
representations
92
92
92
learning
vaes
13
13
linear
preference
functions
constraint
optimization
rank
aggregation
55
in-processing
methods
55
13
55
post-processing
methods
fairness
generative
process
87
89
87
89
fairness
constraint
optimization
problem
11
76
87
89
18
77
76
11
18
77
18
76
77
fairness
calibration
methods
59
73
78
59
73
78
59
73
78
fairness
multiple
rounds
11
80
80
11
group
members
previous
iteration
maximum
score
max
sat
gr
min
sat
gr
49
sat
gr
defines
satisfaction
user
group
recommendations
gr
round
dynamic
calculations
counteract
individual
drawbacks
average
least
misery
method
intuitively
group
members
equally
satisfied
last
round
takes
low
values
aggregation
will
closely
follow
average
everyone
treated
equal
hand
one
group
member
extremely
unsatisfied
specific
round
takes
high
value
promotes
member
preferences
next
round
next
summarize
in-processing
postprocessing
methods
provide
advantages
summary
post-processing
methods
section
summarize
in-processing
postprocessing
approaches
achieving
fairness
overall
exist
methods
proposed
context
rankings
recommender
systems
well
rank
aggregation
problem
table
organizes
methods
based
level
fairness
namely
individual
group
side
fairness
namely
consumer
producer
output
multiplicity
namely
method
focuses
single
output
multiple
outputs
general
existing
learning
linear
preference
functions
in-processing
approaches
target
group
producer
fairness
approaches
consider
single
output
123
11
11
80
recent
exception
13
using
vaes
considers
multiple
outputs
typically
approaches
category
extend
objective
function
use
including
fairness
unfairness
term
target
find
best
balance
accuracy
objective
fairness
objective
optimization
problem
particular
approach
applies
regarding
post-processing
approaches
observe
exist
works
focusing
different
options
fairness
definitions
pre-processing
case
postprocessing
methods
treat
algorithms
producing
rankings
recommendations
black
boxes
without
changing
inner
workings
means
post-processing
approach
receives
input
ranked
output
re-ranks
data
items
output
improve
fairness
respecting
initial
ranked
output
extent
possible
typically
in-processing
approaches
manage
offer
better
trade-offs
fairness
accuracy
compared
post-processing
methods
since
naturally
identify
balance
via
objective
function
use
however
time
offer
guarantees
fairness
output
rankings
recommendations
since
fairness
considered
training
phase
considering
post-processing
approaches
important
note
can
lead
unpredictable
losses
accuracy
since
treat
algorithms
producing
rankings
recommendations
black
boxes
especially
true
pre-processing
methods
well
positive
side
postprocessing
methods
offer
outputs
easy
understand
comparing
outputs
outputs
application
post-processing
fairness
method
realize
offer
fairer
output
recently
92
combines
pre-processing
inprocessing
strategies
jointly
learning
fair
representation
data
classifier
parameters
approach
two
main
limitations
leads
non-convex
optimiza
fairness
rankings
recommendations
overview
tion
problem
guarantee
optimality
accuracy
classifier
depends
dimension
fair
representation
needs
chosen
rather
arbitrarily
verifying
fairness
previous
sections
studied
methods
achieving
fairness
pre-processing
post-processing
methods
treat
recommendation
ranking
algorithm
black
box
try
address
fairness
input
output
algorithm
respectively
particular
post-processing
methods
assume
already
know
algorithm
creates
discrimination
output
try
mitigate
question
naturally
arises
can
verify
whether
program
fair
first
place
program
fairness
verification
aims
analyzing
given
decision-making
program
constructing
proof
fairness
unfairness
just
traditional
static
program
verifier
prove
correctness
program
respect
example
lack
divisions
zero
however
several
challenges
class
decision-making
programs
program
model
will
capture
input
program
describe
means
program
fair
fully
automate
verification
process
one
simple
approach
take
decision-making
program
dataset
input
34
using
concrete
dataset
simplifies
verification
problem
also
raises
questions
whether
dataset
representative
population
trying
prove
fairness
alternative
approach
use
population
model
input
can
probabilistic
model
defines
joint
probability
distribution
inputs
need
define
program
fair
unfair
want
prove
group
fairness
example
algorithm
just
likely
hire
minority
applicant
nonminority
applicants
define
post-condition
like
following
pr
true
vs
pr
true
vs
verifier
will
prove
disprove
fair
given
population
general
proving
group
fairness
easier
verification
process
reduces
computing
probability
number
events
respect
program
population
model
however
proving
individual
fairness
requires
complex
reasoning
involving
multiple
runs
program
notoriously
hard
problem
moreover
case
negative
result
verifier
provide
users
proof
unfairness
depending
fairness
definition
producing
453
human
readable
proof
might
challenging
argument
might
involve
multiple
potentially
infinite
inputs
example
group
fairness
might
challenging
explain
program
outputs
true
40
minority
inputs
70
majority
inputs
overall
program
fairness
verification
difficult
less
investigated
topic
different
approach
make
fairness
first-class
citizen
programming
fairness-aware
programming
developers
can
state
fairness
expectations
natively
code
run-time
system
monitor
decision-making
report
violations
fairness
approach
analogous
notion
assertions
modern
programming
languages
instance
developer
might
assert
indicating
expect
value
positive
certain
point
code
difficulty
however
fairness
definitions
typically
probabilistic
therefore
detecting
violation
done
single
execution
traditional
assertions
instead
monitor
decisions
made
procedure
using
statistical
tools
infer
fairness
property
hold
reasonably
high
confidence
example
consider
movie
recommendation
system
user
data
used
train
recommender
given
user
profile
recommends
single
movie
suppose
recommender
constructed
goal
ensuring
male
users
isolated
movies
strong
female
lead
developer
may
add
following
specification
recommender
code
spec
pr
emalelead
male
0.2
specification
ensures
male
users
procedure
recommends
movie
female
lead
least
20
time
determine
procedure
satisfies
fairness
specification
need
maintain
statistics
inputs
outputs
procedure
applied
specifically
compile
specification
run-time
monitoring
code
executes
every
time
applied
storing
aggregate
results
every
probability
event
appearing
earlier
example
movie
recommendation
monitoring
code
maintain
number
times
procedure
returned
true
movie
female
lead
big
challenge
checking
individual
fairness
case
run-time
system
remember
decisions
made
explicitly
compare
new
decisions
past
ones
10
open
challenges
paper
just
begun
realize
need
fairness
particular
fields
namely
rankings
recommender
systems
next
highlight
critical
open
123
454
issues
challenges
future
work
aim
support
advanced
services
making
accountable
complex
rankings
recommender
systems
codification
definitions
described
sects
universal
definition
expressing
fairness
rankings
recommenders
instead
list
potential
definitions
long
specifically
just
fairness
alone
exist
plethora
different
definitions
even
compatible
sense
method
can
satisfy
simultaneously
except
highly
constrained
special
cases
53
furthermore
also
need
make
explicit
correspondence
definitions
interpretation
bias
diversity
materializes
limitations
definition
compatibility
among
incurred
trade-offs
domain
applicability
assumptions
parameters
well
understood
yet
moreover
interesting
see
general
public
views
fairness
decision
making
testing
people
perception
different
fairness
definitions
can
understand
definitions
fairness
appropriate
particular
contexts
37
69
one
attempt
investigates
definitions
people
perceive
fairest
context
loan
decisions
75
whether
fairness
perceptions
change
addition
sensitive
information
race
loan
applicants
lack
data
major
challenge
available
data
often
limited
way
analysis
done
data
acquired
independently
process
data
scientist
limited
control
collecting
data
analysis
challenging
will
help
discover
types
biases
long
run
one
envision
benchmarks
measuring
societal
impact
algorithm
along
lines
tpc
benchmark2
database
performance
unified
approach
data
pipeline
one
limitation
current
work
fairness
studied
specific
tasks
isolation
current
work
fairness
focusing
classification
task
goal
non-discrimination
however
need
consider
fairness
along
whole
data
pipeline
79
includes
pre-processing
steps
data
selection
acquisition
cleaning
filtering
integration
pre-processing
removing
bias
can
viewed
action
repairing
replacing
modifying
deleting
data
cause
bias
pipeline
also
includes
post-processing
steps
data
representation
data
visualization
user
interfaces
example
results
presented
can
introduce
bias
need
understand
implications
fairness
pitoura
et
al
lack
evaluation
tools
besides
coming
correct
way
defining
fairness
rankings
recommenders
also
need
tools
investigating
bias
evaluating
quality
dataset
algorithm
system
first
attempts
ibm
ai
fairness
3603
tensor
flow
fairness
indicators4
however
focus
mainly
statistical
group
measures
fairness
classification
concepts
like
context
provenance
important
can
directly
considered
designing
tools
direction
also
need
efficient
ways
measuring
fairness
monitoring
evolution
time
previous
research
stream
processing
incrementally
maintaining
statistics
may
relevant
perhaps
pending
question
quantify
long-term
impact
enforcing
methods
target
ensuring
fairness
work
favor
social
good
backfire
ways
predict
lack
real
applications
fairness
lot
work
done
research
setting
still
see
many
actual
applications
results
many
challenges
making
algorithms
systems
fairer
real
world
company
example
needs
consider
business
metrics
click-through
rate
purchases
make
sure
affected
example
design
algorithms
take
account
different
objectives
challenging
example
real
application
fairness
found
linkedin
35
developed
fair
framework
applied
linkedin
talent
search
online
tests
showed
considerable
improvement
fairness
metrics
without
significant
impact
business
metrics
paved
way
deployment
linkedin
users
worldwide
multi-level
architecture
value
systems
algorithms
problem
intrinsic
definition
fairness
definitions
rankings
recommenders
fact
attempt
quantify
philosophical
legal
often
elusive
even
controversial
notions
justice
social
good
complexity
aggregated
notions
reflect
value
systems
beliefs
interact
mechanisms
implementing
least
clear
distinction
constitutes
belief
mechanism
measure
codifying
belief
technical
point
view
able
focus
assessing
whether
proposed
measure
appropriate
codification
given
belief
opposed
assessing
belief
calls
developing
different
levels
abstractions
mappings
somehow
reminiscent
data
independence
supported
database
management
systems
three-level
architecture
physical
conceptual
external
level
mappings
https://aif360.mybluemix.net/.
http://www.tpc.org/information/benchmarks.asp.
123
https://www.tensorflow.org/tfx/guide/fairness_indicators.
fairness
rankings
recommendations
overview
levels
70
lower
level
beliefs
value
systems
higher
level
fairness
definitions
intermediate
levels
used
support
transformations
getting
lower
higher
level
relating
algorithmic
fairness
notions
fairness
systems
focus
survey
fairness
decision-making
processes
particular
rankings
recommendation
systems
used
context
however
several
cases
system
needs
make
decisions
fairness
also
important
particular
several
problems
familiar
data
management
community
resource
allocation
scheduling
fairness
studied
present
representative
examples
instance
approach
presented
56
cache
allocation
fairness
based
pareto
efficiency
sharing
incentives
multiple
resource
allocation
approach
introduced
36
generalizes
max
min
fairness
multiple
resource
types
max
min
fairness
context
refers
maximizing
minimum
allocation
received
user
system
another
notion
fairness
termed
proportionate
progress
proposed
periodic
scheduling
problem
weighted
resources
allocated
tasks
specific
time
units
interval
proportionate
progress
fairness
task
scheduled
resources
according
importance
weight
finally
scheduling
38
fairness
means
workload
scheduler
performs
way
query
starves
resources
another
notion
fairness
different
problem
namely
chairman
selection
problem
proposed
82
chairman
selection
problem
set
states
want
form
union
select
chairman
year
fairness
context
refers
guaranteeing
small
discrepancy
number
chairmen
representing
state
states
satisfied
clearly
fairness
concerns
emerged
various
contexts
years
providing
unified
view
different
notions
fairness
computational
methods
used
enforce
open
problem
opportunity
embed
models
algorithms
fairness
system
involves
type
decision
making
also
open
course
new
models
algorithms
may
needed
different
contexts
principles
fairness
may
different
works
well
recommendation
problem
may
work
well
query
optimizer
resource
sharing
cloud
computing
however
unified
view
will
potentially
provide
new
insights
opportunities
crossfertilization
fairness
domains
paper
focused
ranking
recommendation
algorithms
many
algorithms
revisited
lens
fairness
two
examples
clustering
ranking
networks
455
instance
fair
clustering
adopts
parity
definition
fairness
asks
group
must
approximately
equal
representation
every
cluster
19
link
analysis
problem
networks
fair
algorithms
introduced
83
use
parity-based
definition
fairness
apply
constraints
proportion
pagerank
allocated
members
group
nodes
network
algorithmic
fairness
fast
changing
field
open
challenge
provide
fairness
measures
definitions
kinds
algorithms
use
evaluating
algorithms
analogy
say
response
time
measuring
performance
68
recently
concept
fairness
also
studied
different
domains
covering
different
needs
discuss
two
domains
labor
market
social
matching
instance
previous
research
examines
labor
market
cases
fairness
concerns
shows
relevance
concerns
economic
outcomes
especially
considering
employment
contracts
time
27
recently
domain
work
42
pinpoints
persistence
racial
inequalities
designs
solutions
respect
dynamic
reputational
model
labor
market
highlighting
results
groups
divergent
accesses
resources
recent
work
examines
fairness
online
job
marketplaces
terms
ranking
job
applicants
26
instead
partitioning
individuals
predefined
groups
authors
seek
find
partitioning
individuals
based
protected
attributes
exhibits
highest
unfairness
problem
professional
social
matching
conventional
mechanisms
optimizing
similarity
triadic
closure
studied
65
shown
involve
risks
strengthening
homophily
bias
echo
chambering
problem
team
assembly
work
62
designs
fairness-aware
solutions
multidisciplinary
teams
need
formed
allocated
work
different
projects
requirements
members
skills
application
models
mechanisms
algorithmic
fairness
variety
domains
involve
social
economic
activities
labor
market
social
matching
team
formation
opens
lot
opportunities
fruitful
interdisciplinary
work
11
conclusions
ranking
recommender
systems
several
applications
hiring
lending
college
admissions
notion
fairness
important
since
decision
making
involved
begun
understand
nature
representation
variety
several
definitions
fairness
appropriate
methods
ensuring
article
follow
systematic
structured
approach
explain
various
sides
approaches
123
456
fairness
first
lay
ground
presenting
general
fairness
definitions
zoom
models
definitions
rankings
recommendations
problem
rank
aggregation
organize
taxonomy
highlight
differences
commonalities
analysis
naturally
leads
number
open
questions
fairness
definitions
fare
definition
suitable
context
people
perceive
fairness
different
contexts
mean
fair
unified
way
able
judge
whether
outcome
algorithm
fair
move
describing
solutions
fair
rankings
recommendations
organize
approaches
tackle
unfairness
ensure
fairer
outcome
pre
postprocessing
approaches
within
category
classify
along
several
dimensions
still
early
say
one
works
best
context
evaluation
puts
lens
generally
conclusive
results
fare
better
may
case
combination
methods
applied
combining
pre-processing
in-processing
steps
also
open
question
different
efforts
adapted
single
angle
problem
focus
survey
fairness
rankings
recommendation
systems
discuss
several
cases
system
needs
make
decisions
fairness
also
important
can
verify
whether
program
fair
finally
discuss
open
research
challenges
pertaining
fairness
broader
context
data
management
designing
building
managing
evaluating
fair
data
systems
applications
open
access
article
licensed
creative
commons
attribution
4.0
international
license
permits
use
sharing
adaptation
distribution
reproduction
medium
format
long
give
appropriate
credit
original
author
source
provide
link
creative
commons
licence
indicate
changes
made
images
third
party
material
article
included
article
creative
commons
licence
unless
indicated
otherwise
credit
line
material
material
included
article
creative
commons
licence
intended
use
permitted
statutory
regulation
exceeds
permitted
use
will
need
obtain
permission
directly
copyright
holder
view
copy
licence
visit
http://creativecomm
ons
org
licenses
4.0
references
angwin
et
al
machine
bias
propublica
2016
https
www.propublica.org/article/machine-bias-risk-assessments-incriminal-sentencing
albarghouthi
antoni
drews
nori
fairness
program
property
corr
abs
1610.06067
2016
http://arxiv.
org
abs
1610.06067
123
pitoura
et
al
albarghouthi
vinitsky
fairness-aware
programming
proceedings
conference
fairness
accountability
transparency
fat
2019
pp
211
219
acm
2019
amer-yahia
roy
chawla
das
yu
group
recommendation
semantics
efficiency
proc
vldb
endow
754
765
2009
asudeh
jagadish
fairly
evaluating
scoring
items
data
set
proc
vldb
endow
13
12
3445
3448
2020
asudeh
jagadish
stoyanovich
das
designing
fair
ranking
schemes
proceedings
2019
international
conference
management
data
sigmod
conference
2019
pp
1259
1276
acm
2019
asudeh
jin
jagadish
assessing
remedying
coverage
given
dataset
35th
ieee
international
conference
data
engineering
icde
2019
pp
554
565
ieee
2019
baeza-yates
bias
web
commun
acm
61
54
61
2018
baruah
cohen
plaxton
varvel
proportionate
progress
notion
fairness
resource
allocation
algorithmica
15
600
625
1996
10
beutel
chen
doshi
qian
wei
wu
heldt
zhao
hong
chi
goodrow
fairness
recommendation
ranking
pairwise
comparisons
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
2019
pp
2212
2220
acm
2019
11
biega
gummadi
weikum
equity
attention
amortizing
individual
fairness
rankings
41st
international
acm
sigir
conference
research
development
information
retrieval
sigir
2018
pp
405
414
acm
2018
12
bolukbasi
chang
zou
saligrama
kalai
man
computer
programmer
woman
homemaker
debiasing
word
embeddings
advances
neural
information
processing
systems
29
annual
conference
neural
information
processing
systems
2016
pp
4349
4357
2016
13
borges
stefanidis
enhancing
long
term
fairness
recommendations
variational
autoencoders
11th
international
conference
management
digital
ecosystems
medes
2019
pp
95
102
acm
2019
14
burges
shaked
renshaw
lazier
deeds
hamilton
hullender
learning
rank
using
gradient
descent
machine
learning
proceedings
twenty-second
international
conference
icml
2005
acm
international
conference
proceeding
series
vol
119
pp
89
96
acm
2005
15
calders
verwer
three
naive
bayes
approaches
discrimination-free
classification
data
min
knowl
discov
21
277
292
2010
16
cao
qin
liu
tsai
li
learning
rank
pairwise
approach
listwise
approach
machine
learning
proceedings
twenty-fourth
international
conference
icml
2007
acm
international
conference
proceeding
series
vol
227
pp
129
136
acm
2007
17
carbinell
goldstein
use
mmr
diversity-based
reranking
reordering
documents
producing
summaries
sigir
forum
51
209
210
2017
18
celis
straszak
vishnoi
ranking
fairness
constraints
45th
international
colloquium
automata
languages
programming
icalp
2018
lipics
vol
107
pp
28
28
15
schloss
dagstuhl
leibniz-zentrum
für
informatik
2018
19
chierichetti
kumar
lattanzi
vassilvitskii
fair
clustering
fairlets
advances
neural
information
processing
systems
30
annual
conference
neural
information
processing
systems
2017
december
2017
long
beach
ca
usa
pp
5029
5037
2017
fairness
rankings
recommendations
overview
20
chouldechova
roth
snapshot
frontiers
fairness
machine
learning
commun
acm
63
82
89
2020
https
doi
org
10.1145
3376898
21
crammer
singer
pranking
ranking
advances
neural
information
processing
systems
14
neural
information
processing
systems
natural
synthetic
nips
2001
pp
641
647
mit
press
2001
22
chouldechova
fair
prediction
disparate
impact
study
bias
recidivism
prediction
instruments
big
data
153
163
2017
23
dastin
rpt-insight-amazon
scraps
secret
ai
recruiting
tool
showed
bias
women
reuters
2018
24
dwork
hardt
pitassi
reingold
zemel
fairness
awareness
innovations
theoretical
computer
science
2012
pp
214
226
acm
2012
25
ekstrand
burke
diaz
fairness
discrimination
retrieval
recommendation
proceedings
42nd
international
acm
sigir
conference
research
development
information
retrieval
sigir
2019
pp
1403
1404
acm
2019
26
elbassuoni
amer-yahia
ghizzawi
fairness
scoring
online
job
marketplaces
trans
data
sci
29
29
30
2020
27
fehr
goette
zehnder
behavioral
account
labor
market
role
fairness
concerns
ann
rev
econ
355
384
2009
28
feldman
friedler
moeller
scheidegger
venkatasubramanian
certifying
removing
disparate
impact
proceedings
21th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
pp
259
268
acm
2015
29
ferraro
serra
bauer
break
loop
gender
imbalance
music
recommenders
scholer
thomas
elsweiler
joho
kando
smith
eds
chiir
21
acm
sigir
conference
human
information
interaction
retrieval
canberra
act
australia
march
14
19
2021
pp
249
254
acm
2021
30
freund
iyer
schapire
singer
efficient
boosting
algorithm
combining
preferences
proceedings
fifteenth
international
conference
machine
learning
icml
1998
pp
170
178
morgan
kaufmann
1998
31
friedler
scheidegger
venkatasubramanian
im
possibility
fairness
corr
abs
1609.07236
2016
http
arxiv
org
abs
1609.07236
32
friedler
scheidegger
venkatasubramanian
im
possibility
fairness
different
value
systems
require
different
mechanisms
fair
decision
making
commun
acm
64
136
143
2021
33
friedler
scheidegger
venkatasubramanian
choudhary
hamilton
roth
comparative
study
fairnessenhancing
interventions
machine
learning
proceedings
conference
fairness
accountability
transparency
fat
2019
pp
329
338
acm
2019
34
galhotra
brun
meliou
fairness
testing
testing
software
discrimination
proceedings
2017
11th
joint
meeting
foundations
software
engineering
esec
fse
2017
pp
498
510
acm
2017
35
geyik
ambler
kenthapadi
fairness-aware
ranking
search
recommendation
systems
application
linkedin
talent
search
proceedings
25th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
2019
pp
2221
2231
acm
2019
36
ghodsi
zaharia
hindman
konwinski
shenker
stoica
dominant
resource
fairness
fair
allocation
multiple
resource
types
proceedings
8th
usenix
symposium
networked
systems
design
implementation
nsdi
2011
usenix
association
2011
37
grgic-hlaca
redmiles
gummadi
weller
human
perceptions
fairness
algorithmic
decision
making
457
case
study
criminal
risk
prediction
proceedings
2018
world
wide
web
conference
world
wide
web
www
2018
pp
903
912
acm
2018
38
gupta
mehta
wang
dayal
fair
effective
efficient
differentiated
scheduling
enterprise
data
warehouse
edbt
2009
12th
international
conference
extending
database
technology
acm
international
conference
proceeding
series
vol
360
pp
696
707
acm
2009
39
hainmueller
entropy
balancing
causal
effects
multivariate
reweighting
method
produce
balanced
samples
observational
studies
polit
anal
20
25
46
2012
40
hardt
price
srebro
equality
opportunity
supervised
learning
nips
pp
3315
3323
2016
41
ho
imai
king
stuart
matchit
nonparametric
preprocessing
parametric
causal
inference
stat
softw
42
28
2011
42
hu
chen
short-term
intervention
long-term
fairness
labor
market
proceedings
2018
world
wide
web
conference
world
wide
web
www
2018
lyon
france
april
23
27
2018
pp
1389
1398
acm
2018
43
imana
korolova
heidemann
auditing
discrimination
algorithms
delivering
job
ads
proceedings
web
conference
2021
www
21
2021
44
ingold
soper
amazon
doesn
consider
race
customers
bloomberg
2016
45
ingold
soper
amazon
doesn
consider
race
customers
2016
46
jameson
smyth
recommendation
groups
adaptive
web
methods
strategies
web
personalization
lecture
notes
computer
science
vol
4321
pp
596
627
springer
2007
47
kamiran
calders
classifying
without
discriminating
2009
2nd
international
conference
computer
control
communication
pp
2009
48
kamiran
calders
data
preprocessing
techniques
classification
without
discrimination
knowl
inf
syst
33
33
2011
49
kamishima
akaho
asoh
sakuma
recommendation
independence
conference
fairness
accountability
transparency
fat
2018
proceedings
machine
learning
research
vol
81
pp
187
201
pmlr
2018
50
kay
matuszek
munson
unequal
representation
gender
stereotypes
image
search
results
occupations
proceedings
33rd
annual
acm
conference
human
factors
computing
systems
chi
2015
pp
3819
3828
acm
2015
51
kendall
new
measure
rank
correlation
biometrika
30
81
93
1938
52
kingma
welling
auto-encoding
variational
bayes
2nd
international
conference
learning
representations
iclr
2014
2014
53
kleinberg
mullainathan
raghavan
inherent
tradeoffs
fair
determination
risk
scores
8th
innovations
theoretical
computer
science
conference
itcs
2017
lipics
vol
67
pp
43
43
23
schloss
dagstuhl
leibniz-zentrum
für
informatik
2017
54
krasanakis
spyromitros-xioufis
papadopoulos
kompatsiaris
adaptive
sensitive
reweighting
mitigate
bias
fairness-aware
classification
pp
853
862
2018
55
kuhlman
rundensteiner
rank
aggregation
algorithms
fair
consensus
proc
vldb
endow
13
11
2706
2719
2020
56
kunjir
fain
munagala
babu
robus
fair
cache
allocation
data-parallel
workloads
proceedings
2017
acm
international
conference
management
data
sigmod
conference
2017
pp
219
234
acm
2017
57
kusner
loftus
russell
silva
counterfactual
fairness
advances
neural
information
processing
systems
123
458
30
annual
conference
neural
information
processing
systems
2017
pp
4066
4076
2017
58
liang
krishnan
hoffman
jebara
variational
autoencoders
collaborative
filtering
proceedings
2018
world
wide
web
conference
world
wide
web
www
2018
pp
689
698
acm
2018
59
lin
zhang
zhang
gu
liu
ma
fairnessaware
group
recommendation
pareto-efficiency
proceedings
eleventh
acm
conference
recommender
systems
recsys
2017
pp
107
115
acm
2017
60
liu
learning
rank
information
retrieval
springer
2011
61
lum
johndrow
statistical
framework
fair
predictive
algorithms
corr
abs
1610.08077
2016
http://arxiv.org/
abs
1610.08077
62
machado
stefanidis
fair
team
recommendations
multidisciplinary
projects
2019
ieee
wic
acm
international
conference
web
intelligence
wi
2019
thessaloniki
greece
october
14
17
2019
pp
293
297
acm
2019
63
martin
social
media
changed
consume
news
forbes
2018
https://www.forbes.com/sites/nicolemartin1/
2018
11
30
how-social-media-has-changed-how-we-consumenews
18ae4c093c3c
64
ntoutsi
stefanidis
nørvåg
kriegel
fast
group
recommendations
applying
user
clustering
conceptual
modeling
31st
international
conference
er
2012
proceedings
lecture
notes
computer
science
vol
7532
pp
126
140
springer
2012
65
olsson
huhtamäki
kärkkäinen
directions
professional
social
matching
systems
commun
acm
63
60
69
2020
66
oosterhuis
jagerman
de
rijke
unbiased
learning
rank
counterfactual
online
approaches
seghrouchni
sukthankar
liu
van
steen
eds
companion
2020
web
conference
2020
pp
299
300
acm
iw3c2
2020
67
du
pin
calmon
wei
vinzamuri
ramamurthy
varshney
optimized
pre-processing
discrimination
prevention
advances
neural
information
processing
systems
30
annual
conference
neural
information
processing
systems
2017
pp
3992
4001
2017
68
pitoura
social-minded
measures
data
quality
fairness
diversity
lack
bias
acm
data
inf
qual
12
12
12
2020
69
plane
redmiles
mazurek
tschantz
exploring
user
perceptions
discrimination
online
targeted
advertising
26th
usenix
security
symposium
usenix
security
2017
pp
935
951
usenix
association
2017
70
ramakrishnan
gehrke
database
management
systems
edn
mcgraw-hill
2003
71
rastegarpanah
gummadi
crovella
fighting
fire
fire
using
antidote
data
improve
polarization
fairness
recommender
systems
proceedings
twelfth
acm
international
conference
web
search
data
mining
wsdm
2019
pp
231
239
acm
2019
72
roy
amer-yahia
chawla
das
yu
space
efficiency
group
recommendation
vldb
19
877
900
2010
73
sacharidis
top-n
group
recommendations
fairness
proceedings
34th
acm
sigapp
symposium
applied
computing
sac
2019
pp
1663
1670
acm
2019
74
salimi
rodriguez
howe
suciu
interventional
fairness
causal
database
repair
algorithmic
fairness
proceedings
2019
international
conference
management
data
sigmod
conference
2019
pp
793
810
acm
2019
75
saxena
huang
de
filippis
radanovic
parkes
liu
fairness
definitions
fare
examining
public
123
pitoura
et
al
attitudes
towards
algorithmic
definitions
fairness
proceedings
2019
aaai
acm
conference
ai
ethics
society
aies
2019
pp
99
106
acm
2019
76
serbos
qi
mamoulis
pitoura
tsaparas
fairness
package-to-group
recommendations
proceedings
26th
international
conference
world
wide
web
www
2017
pp
371
379
acm
2017
77
singh
joachims
fairness
exposure
rankings
proceedings
24th
acm
sigkdd
international
conference
knowledge
discovery
data
mining
kdd
2018
pp
2219
2228
acm
2018
78
steck
calibrated
recommendations
proceedings
12th
acm
conference
recommender
systems
recsys
2018
pp
154
162
acm
2018
79
stoyanovich
howe
abiteboul
miklau
sahuguet
weikum
fides
towards
platform
responsible
data
science
proceedings
29th
international
conference
scientific
statistical
database
management
ssdbm
2017
pp
26
26
acm
2017
80
stratigi
nummenmaa
pitoura
stefanidis
fair
sequential
group
recommendations
sac
20
35th
acm
sigapp
symposium
applied
computing
pp
1443
1452
acm
2020
81
sweeney
discrimination
online
ad
delivery
commun
acm
56
44
54
2013
82
tijdeman
chairman
assignment
problem
discret
math
32
323
330
1980
83
tsioutsiouliklis
pitoura
tsaparas
kleftakis
mamoulis
fairness-aware
pagerank
www
21
web
conference
2021
virtual
event
ljubljana
slovenia
april
19
23
2021
pp
3815
3826
acm
iw3c2
2021
84
verma
rubin
fairness
definitions
explained
proceedings
international
workshop
software
fairness
fairware
icse
2018
pp
acm
2018
85
weber
dwoskin
workplace
personality
tests
fair
wall
street
2014
86
xie
lakshmanan
wood
composite
recommendations
items
packages
front
comput
sci
264
277
2012
87
yang
stoyanovich
measuring
fairness
ranked
outputs
proceedings
29th
international
conference
scientific
statistical
database
management
ssdbm
2017
pp
22
22
acm
2017
88
yao
huang
beyond
parity
fairness
objectives
collaborative
filtering
advances
neural
information
processing
systems
30
annual
conference
neural
information
processing
systems
2017
pp
2921
2930
2017
89
zehlike
bonchi
castillo
hajian
megahed
baeza-yates
fa
ir
fair
top-k
ranking
algorithm
proceedings
2017
acm
conference
information
knowledge
management
cikm
2017
pp
1569
1578
acm
2017
90
zehlike
diehn
castillo
reducing
disparate
exposure
ranking
learning
rank
approach
web
conference
www
acm
2020
91
zehlike
yang
stoyanovich
fairness
ranking
survey
corr
abs
2103.14000
2021
92
zemel
wu
swersky
pitassi
dwork
learning
fair
representations
proceedings
30th
international
conference
machine
learning
icml
2013
jmlr
workshop
conference
proceedings
vol
28
pp
325
333
jmlr
org
2013
publisher
note
springer
nature
remains
neutral
regard
jurisdictional
claims
published
maps
institutional
affiliations