session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
control
fair
bia
dynam
learning-to-rank
marco
morik
ashudeep
singh
m.morik@tu-berlin.d
technisch
univerität
berlin
berlin
germani
ashudeep@cs.cornell.edu
cornel
univers
ithaca
ny
jessica
hong
thorsten
joachim
jwh296@cornell.edu
cornel
univers
ithaca
ny
tj@cs.cornell.edu
cornel
univers
ithaca
ny
abstract
rank
primari
interfac
mani
onlin
platform
match
user
item
news
product
music
video
two-sid
market
user
draw
util
rank
rank
also
determin
util
exposur
revenu
item
provid
publish
seller
artist
studio
alreadi
note
myopic
optim
util
user
done
virtual
learning-to-rank
algorithm
can
unfair
item
provid
therefor
present
learning-to-rank
approach
explicitli
enforc
meritbas
fair
guarante
group
item
articl
publish
track
artist
particular
propos
learn
algorithm
ensur
notion
amort
group
fair
simultan
learn
rank
function
implicit
feedback
data
algorithm
take
form
control
integr
unbias
estim
fair
util
dynam
adapt
data
becom
avail
addit
rigor
theoret
foundat
converg
guarante
find
empir
algorithm
highli
practic
robust
consid
problem
dynam
learning-to-rank
ltr
rank
function
dynam
adapt
base
feedback
user
provid
dynam
ltr
problem
ubiquit
onlin
system
news-fe
rank
adapt
number
like
articl
receiv
onlin
store
adapt
number
posit
review
product
movie-recommend
system
adapt
watch
movi
system
learn
predict
dynam
intertwin
past
feedback
influenc
futur
rank
specif
form
onlin
learn
partial-inform
feedback
17
dynam
ltr
system
widespread
use
unquestion
use
least
two
issu
requir
care
design
consider
first
rank
system
induc
bia
rank
present
particular
item
rank
highli
like
collect
addit
feedback
turn
can
influenc
futur
rank
promot
mislead
rich-get-rich
dynam
31
32
39
second
rank
system
arbit
much
exposur
item
receiv
exposur
directli
influenc
opinion
ideolog
orient
present
news
articl
econom
gain
revenu
product
sale
stream
provid
item
rais
fair
consider
exposur
alloc
base
merit
item
14
41
will
show
follow
naiv
dynam
ltr
method
oblivi
issu
can
lead
econom
dispar
unfair
polar
paper
present
first
dynam
ltr
algorithm
call
fairco
overcom
rich-get-rich
dynam
enforc
configur
allocation-of-exposur
scheme
unlik
exist
fair
ltr
algorithm
14
41
42
47
fairco
explicitli
address
dynam
natur
learn
problem
system
unbias
fair
even
though
relev
merit
item
still
learn
core
approach
lie
merit-bas
exposure-alloc
criterion
amort
learn
process
14
41
view
enforc
merit-bas
exposur
criterion
control
problem
deriv
p-control
optim
fair
exposur
well
qualiti
rank
crucial
compon
control
abil
estim
merit
relev
accur
even
though
feedback
reveal
increment
system
oper
feedback
bias
rank
shown
process
31
effect
fairco
includ
new
unbias
cardin
relev
cc
concept
inform
system
learn
rank
keyword
rank
learning-to-rank
fair
bia
select
bia
exposur
acm
refer
format
marco
morik
ashudeep
singh
jessica
hong
thorsten
joachim
2020
control
fair
bia
dynam
learning-to-rank
proceed
43rd
intern
acm
sigir
confer
research
develop
inform
retriev
sigir
20
juli
25
30
2020
virtual
event
china
acm
new
york
ny
usa
10
page
https://doi.org/10.1145/3397271.3401100
equal
contribut
work
conduct
cornel
univers
permiss
make
digit
hard
copi
part
work
person
classroom
use
grant
without
fee
provid
copi
made
distribut
profit
commerci
advantag
copi
bear
notic
full
citat
first
page
copyright
compon
work
own
other
author
must
honor
abstract
credit
permit
copi
otherwis
republish
post
server
redistribut
list
requir
prior
specif
permiss
fee
request
permiss
permissions@acm.org
sigir
20
juli
25
30
2020
virtual
event
china
2020
copyright
held
owner
author
public
right
licens
acm
acm
isbn
978
4503
8016
20
07
15.00
https://doi.org/10.1145/3397271.3401100
429
introduct
session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
estim
oppos
exist
ordin
method
32
can
use
unbias
merit
estim
fair
rank
criterion
addit
theoret
justif
fairco
provid
empir
result
synthet
news-fe
data
real-world
movi
recommend
data
find
fairco
effect
enforc
fair
provid
good
rank
perform
furthermor
fairco
effici
robust
easi
implement
revers
remain
49
user
left-lean
like
articl
left
rank
articl
sole
true
averag
relev
put
item
right
posit
10
item
left
posit
11
20
mean
platform
give
articl
left
vastli
less
exposur
right
argu
can
consid
unfair
sinc
two
group
receiv
disproportion
differ
outcom
despit
similar
merit
relev
differ
averag
relev
lead
much
larger
differ
exposur
group
argu
two
defici
name
bia
unfair
just
undesir
undesir
consequ
exampl
bias
estim
lead
poor
rank
qualiti
unfair
like
alien
leftlean
user
exampl
drive
platform
encourag
polar
furthermor
note
two
defici
specif
news
exampl
naiv
algorithm
lead
analog
problem
mani
domain
exampl
consid
rank
system
job
applic
rich-get-rich
dynam
exposur
alloc
may
perpetu
even
amplifi
exist
unfair
dispar
male
femal
applic
similarli
consid
onlin
marketplac
product
differ
seller
group
rank
rich-get-rich
dynam
unfair
exposur
alloc
can
encourag
monopoli
drive
seller
market
exampl
illustr
follow
two
desiderata
less
naiv
dynam
ltr
algorithm
fulfil
motiv
consid
follow
illustr
exampl
dynam
ltr
problem
onlin
news-aggreg
platform
want
present
rank
top
news
articl
front
page
extern
mechan
identifi
set
20
20
articl
begin
day
left
learn
problem
rank
20
articl
front
page
user
start
come
platform
platform
use
follow
naiv
algorithm
learn
rank
algorithm
naiv
dynam
ltr
algorithm
initi
counter
foreach
user
present
rank
argsort
random
tiebreak
increment
articl
read
user
execut
algorithm
begin
day
platform
start
present
20
articl
random
order
first
user
may
observ
user
read
articl
posit
increment
counter
articl
next
user
articl
now
get
rank
first
counter
updat
base
second
user
read
cycl
continu
subsequ
user
unfortun
naiv
algorithm
least
two
defici
make
suboptim
unsuit
mani
rank
applic
first
defici
lie
choic
estim
averag
relev
articl
name
fraction
user
want
read
articl
unfortun
even
infinit
amount
user
feedback
counter
consist
estim
averag
relev
31
32
39
particular
item
happen
get
read
earli
iter
get
rank
highli
user
find
thu
opportun
provid
posit
feedback
perpetu
rich-get-rich
dynam
feedback
count
record
articl
reflect
mani
user
actual
want
read
articl
second
defici
naiv
algorithm
lie
rank
polici
creat
sourc
unfair
even
true
averag
relev
articl
accur
known
14
41
consid
follow
omnisci
variant
naiv
algorithm
rank
articl
true
averag
relev
true
fraction
user
want
read
articl
can
rank
unfair
let
us
assum
two
group
articl
right
left
10
item
articl
polit
rightand
left-lean
sourc
51
user
right-lean
want
read
articl
group
right
articl
group
left
unbiased
algorithm
bias
subject
rich-get-rich
dynam
fair
algorithm
enforc
fair
alloc
exposur
base
merit
relev
two
desiderata
mind
paper
develop
altern
naiv
algorithm
particular
introduc
dynam
learning-to-rank
set
section
section
formal
amort
notion
merit-bas
fair
account
fact
merit
unknown
begin
learn
process
learn
throughout
section
address
bia
problem
provid
estim
elimin
present
bia
global
person
rank
polici
final
section
propos
control-bas
algorithm
design
optim
rank
qualiti
dynam
enforc
fair
relat
work
rank
algorithm
wide
recogn
potenti
societ
impact
form
core
mani
onlin
system
includ
search
engin
recommend
system
news
feed
onlin
vote
control
rich-get-rich
phenomena
recommend
rank
studi
perspect
optim
util
explor
well
ensur
fair
system
40
48
sever
advers
consequ
naiv
rank
system
19
polit
polar
11
misinform
45
unfair
alloc
exposur
42
bias
judgment
phenomena
matthew
effect
23
view
rank
problem
two-sid
market
user
item
deriv
util
rank
system
bring
novel
perspect
tackl
problem
41
430
session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
inform
want
learn
singl
global
rank
like
introductori
news
exampl
present
rank
σt
system
receiv
feedback
vector
ct
user
non-neg
valu
ct
everi
simplest
case
click
click
will
use
word
click
placehold
throughout
paper
simplic
feedback
may
take
mani
form
binari
exampl
video
stream
servic
feedback
may
percentag
user
watch
video
feedback
ct
receiv
dynam
ltr
algorithm
now
updat
rank
polici
produc
polici
πt
use
next
time
step
work
take
inspir
work
develop
method
mitig
bia
unfair
dynam
set
machin
learn
method
underli
rank
algorithm
grow
concern
around
question
machin
learn
algorithm
can
unfair
especi
given
numer
real-world
applic
10
sever
definit
propos
fair
binari
classif
set
well
recent
domain
rank
recommend
inform
retriev
13
14
16
41
definit
fair
rank
span
one
pure
base
composit
top-k
16
relevance-bas
definit
fair
exposur
41
amort
attent
equiti
14
will
discuss
definit
greater
detail
section
work
also
relat
recent
interest
studi
impact
fair
learn
algorithm
appli
dynam
set
21
35
43
inform
retriev
long-stand
interest
learn
rank
bias
click
data
alreadi
argu
bia
log
click
data
occur
feedback
incomplet
bias
present
numer
approach
base
prefer
25
30
click
model
18
random
intervent
36
exist
recent
new
approach
de-bias
feedback
data
use
techniqu
causal
infer
miss
data
analysi
propos
provabl
elimin
select
bias
32
follow
approach
paper
extend
dynam
rank
set
propos
new
unbias
regress
object
section
learn
dynam
rank
set
relat
convent
learning-to-rank
algorithm
lambdarank
lambdamart
ranknet
softrank
etc
15
44
howev
implement
fair
constraint
base
merit
need
explicitli
estim
relev
user
measur
merit
score
estim
method
don
necessarili
mean
set
also
close
relat
onlin
learn
rank
top-k
rank
feedback
observ
top-k
item
henc
explor
intervent
necessari
ensur
converg
26
34
37
49
algorithm
design
respect
click-model
assumpt
49
learn
presenc
document
featur
34
key
differ
method
consid
explor
explicit
intervent
mere
exploit
user-driven
explor
howev
explicit
explor
also
incorpor
algorithm
improv
converg
rate
method
πt
σ1
c1
σt
ct
instanc
dynam
ltr
algorithm
naiv
algorithm
alreadi
outlin
section
mere
comput
ct
produc
new
rank
polici
global
rank
independ
4.1
partial
bias
feedback
key
challeng
dynam
ltr
lie
fact
feedback
ct
provid
meaning
feedback
item
user
examin
follow
larg
bodi
work
click
model
18
model
censor
process
specif
binari
vector
et
indic
item
examin
user
model
relationship
ct
rt
follow
et
ct
otherwis
begin
formal
defin
dynam
ltr
problem
given
set
item
need
rank
respons
incom
request
time
step
request
come
back
run
exampl
news
rank
rt
contain
full
inform
articl
user
interest
read
ct
reveal
inform
articl
examin
user
et
analog
job
placement
applic
rt
indic
candid
whether
qualifi
receiv
interview
call
ct
reveal
inform
candid
examin
employ
second
challeng
lie
fact
examin
vector
et
observ
impli
feedback
valu
ct
ambigu
may
either
indic
lack
examin
et
neg
feedback
rt
problemat
et
uniformli
random
item
get
examin
strongli
bias
rank
σt
present
user
current
iter
specif
user
like
look
item
high
rank
one
lower
31
model
posit
bia
probabl
distribut
examin
vector
rt
et
σt
rt
dynam
learning-to-rank
arriv
rank
system
request
consist
featur
vector
describ
user
inform
need
queri
user
profil
user
vector
true
relev
rate
rt
item
collect
featur
vector
visibl
system
true
relev
rate
rt
hidden
base
inform
rank
polici
πt
produc
rank
σt
present
user
note
polici
may
ignor
click
model
can
brought
form
18
simplic
paper
mere
use
position-bas
model
pbm
20
assum
margin
probabl
examin
pt
item
depend
rank
rank
present
rank
despit
simplic
found
pbm
can
captur
main
effect
posit
bia
accur
enough
reliabl
practic
32
46
431
session
3a
bia
fair
4.2
sigir
20
juli
25
30
2020
virtual
event
china
evalu
rank
perform
candid
discuss
section
estim
pt
take
group-bas
approach
fair
aggreg
exposur
group
gm
pt
exp
measur
qualiti
rank
polici
util
user
virtual
rank
metric
use
inform
retriev
defin
util
rank
function
relev
individu
item
case
item-bas
relev
repres
articl
user
like
read
candid
qualifi
interview
commonli
use
util
measur
dcg
29
dcg
log2
rank
group
can
legal
protect
group
gender
race
reflect
structur
item
sold
particular
seller
simpli
put
item
group
individu
fair
order
formul
fair
criteria
relat
exposur
merit
defin
merit
item
expect
averag
relev
aggreg
group
merit
ndcg
normal
dcg
optim
rank
distribut
request
rank
polici
evalu
expect
util
4.3
section
will
discuss
get
unbias
estim
merit
use
bias
feedback
data
ct
definit
hand
can
address
type
dispar
identifi
section
specif
extend
dispar
treatment
criterion
41
dynam
rank
problem
use
amort
notion
fair
14
particular
two
group
dispar
íτ
exp
íτ
exp
dτe
10
merit
merit
optim
rank
perform
user-fac
goal
dynam
ltr
converg
polici
argmaxπ
maxim
util
even
solv
problem
estim
despit
lack
knowledg
maxim
problem
comput
challeng
sinc
space
rank
polici
exponenti
even
learn
just
singl
global
rank
fortun
easi
show
38
sorting-bas
polici
argsort
measur
far
amort
exposur
time
step
fulfil
exposure-bas
fair
dispar
express
far
averag
time
step
group
item
got
exposur
proport
relev
dispar
zero
greater
violat
fair
note
alloc
strategi
beyond
proportion
implement
well
use
altern
definit
dispar
41
exposur
can
also
alloc
base
fair
criteria
exampl
dispar
impact
specif
exposur
alloc
impli
41
consid
feedback
ct
click
purchas
vote
measur
impact
ct
11
imp
optim
virtual
commonli
use
ir
dcg
problem
lie
estim
expect
relev
item
condit
learn
singl
global
rank
simplifi
estim
expect
averag
relev
item
global
rank
can
deriv
via
argsort
section
will
use
techniqu
causal
infer
missing-data
analysi
design
unbias
consist
estim
requir
access
observ
feedback
ct
keep
follow
dispar
close
zero
control
exposur
alloc
make
impact
proport
relev
íτ
imp
íτ
imp
dτ
12
merit
merit
fair
dynam
ltr
sort
global
rank
may
provid
optim
util
user
introductori
exampl
alreadi
illustr
rank
can
unfair
grow
bodi
literatur
address
unfair
rank
now
extend
merit-bas
fair
14
41
dynam
ltr
set
key
scarc
resourc
rank
polici
alloc
among
item
exposur
base
model
introduc
previou
section
defin
exposur
item
margin
probabl
examin
pt
et
σt
rt
probabl
user
will
see
thu
opportun
read
articl
buy
product
interview
refer
impact-bas
fair
dispar
section
will
deriv
control
drive
exposur
impact
dispar
zero
unbias
estim
abl
implement
rank
polici
equat
fair
dispar
equat
10
12
need
accur
estim
posit
bia
pt
expect
condit
relev
expect
averag
relev
consid
estim
problem
follow
432
session
3a
bia
fair
6.1
sigir
20
juli
25
30
2020
virtual
event
china
use
unobserv
true
relev
r1
rτ
estim
posit
bia
learn
model
pt
part
dynam
ltr
problem
position-bia
model
mere
input
dynam
ltr
algorithm
fortun
sever
techniqu
estim
positionbia
model
alreadi
exist
literatur
22
32
46
agnost
use
simplest
case
examin
probabl
pt
depend
rank
item
analog
position-bas
click
model
20
fix
probabl
rank
shown
32
46
position-bas
probabl
can
estim
explicit
implicit
swap
intervent
furthermor
shown
22
contextu
featur
user
queri
can
incorpor
neural-network
base
propens
model
allow
captur
certain
user
may
explor
rank
queri
propens
model
learn
can
appli
predict
pt
new
queri
rank
σt
6.2
ee
õõ
ct
ct
2r
et
σt
pt
et
rt
rt
2r
pt
pt
rt
rt
rt
estim
condit
relev
key
challeng
estim
equat
lie
inabl
directli
observ
true
relev
rt
instead
data
partial
bias
feedback
ct
overcom
problem
take
approach
inspir
32
extend
dynam
rank
set
key
idea
correct
select
bia
relev
label
observ
ct
use
techniqu
survey
sampl
causal
infer
27
28
howev
unlik
ordin
estim
propos
32
need
cardin
relev
estim
sinc
fair
dispar
cardin
natur
therefor
propos
follow
cardin
relev
estim
key
idea
behind
estim
lie
train
object
use
ct
expect
equival
least-squar
object
access
rt
start
deriv
let
consid
estim
access
relev
label
r1
rτ
previou
time
step
straightforward
solut
solv
follow
least-squar
object
given
regress
model
neural
network
paramet
model
rt
line
formul
expect
term
margin
exposur
probabl
et
σt
decompos
expect
object
addit
note
et
σt
therefor
equal
pt
exposur
model
line
substitut
ct
et
rt
simplifi
express
sinc
et
rt
whenev
user
expos
item
note
propens
pt
expos
item
now
cancel
long
bound
away
zero
mean
item
probabl
found
user
case
user
natur
explor
low
enough
rank
activ
intervent
can
use
stochast
promot
item
order
ensur
non-zero
examin
propens
26
note
unbiased
hold
sequenc
r1
σ1
xt
rt
σt
matter
complex
depend
rank
σt
beyond
proof
unbiased
possibl
use
standard
concentr
inequ
show
converg
size
train
sequenc
increas
thu
standard
condit
capac
uniform
converg
possibl
show
converg
minim
least-squar
regressor
size
train
sequenc
increas
will
use
regress
object
learn
neuralnetwork
ranker
section
8.2
13
minimum
object
least-squar
regress
estim
sinc
r1
rτ
avail
defin
asymptot
equival
object
mere
use
bias
feedback
c1
cτ
new
object
correct
posit
bia
use
invers
propens
score
ip
weight
27
28
posit
bia
p1
pτ
take
role
missing
model
ct
ct
2r
pt
6.3
estim
averag
relev
condit
relev
use
rank
polici
equat
defin
merit
equat
fair
dispar
averag
relev
need
furthermor
serv
rank
criterion
global
rank
equat
margin
deriv
argu
follow
direct
way
get
unbias
estim
14
denot
regress
estim
defin
minimum
object
reg
regress
object
14
unbias
mean
expect
equal
regress
object
ip
433
ct
pt
15
session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
now
posit
state
fairco
rank
polici
fairco
στ
argsort
errτ
17
follow
show
estim
unbias
long
propens
bound
away
zero
1õ
et
rt
ee
ip
et
σt
pt
exposure-bas
dispar
τe
use
error
term
refer
polici
fairco
exp
impact-bas
dispar
τi
use
refer
fairco
imp
like
polici
section
4.3
fairco
sort-bas
polici
howev
sort
criterion
combin
relev
error
term
repres
fair
violat
idea
behind
fairco
error
term
push
item
underexpos
group
upward
rank
paramet
can
chosen
posit
constant
choic
lead
asymptot
converg
shown
theorem
exposur
fair
suitabl
choic
can
influenc
finite-sampl
behavior
fairco
higher
can
lead
oscil
behavior
smaller
make
converg
smoother
slower
explor
role
experi
find
keep
fix
0.01
work
well
across
experi
anoth
key
qualiti
fairco
agnost
choic
error
metric
conjectur
can
easili
adapt
type
fair
dispar
furthermor
easi
implement
effici
make
well
suit
practic
applic
illustr
theoret
properti
fairco
now
analyz
converg
case
exposure-bas
fair
disentangl
converg
estim
merit
converg
fairco
consid
time
point
τ0
merit
alreadi
close
merit
can
thu
focu
question
whether
fairco
can
drive
zero
start
unfair
may
persist
time
τ0
make
problem
well-pos
need
assum
exposur
avail
overabund
otherwis
may
unavoid
give
group
exposur
deserv
even
put
bottom
rank
suffici
condit
exclud
case
consid
problem
follow
true
pair
group
rank
entir
time
point
et
rt
pt
1õ
rt
follow
experi
will
use
estim
whenev
direct
estim
need
fair
dispar
global
rank
criterion
dynam
control
fair
given
formal
dynam
ltr
problem
definit
fair
deriv
estim
relev
paramet
now
posit
tackl
problem
rank
enforc
fair
condit
view
control
problem
sinc
need
robust
uncertainti
estim
begin
learn
process
specif
propos
control
abl
make
initi
uncertainti
estim
converg
learn
process
follow
pairwis
definit
amort
fair
section
quantifi
much
fair
class
violat
use
follow
overal
dispar
metric
dτ
dτ
16
metric
can
instanti
dispar
dτe
equat
10
exposure-bas
fair
dτi
equat
12
impact-bas
fair
sinc
optim
fair
achiev
seek
minim
end
now
deriv
method
call
fairco
take
form
proport
control
p-control
12
p-control
wide
use
control-loop
mechan
appli
feedback
correct
term
proport
error
applic
error
correspond
violat
amort
fair
dispar
equat
10
12
specif
set
disjoint
group
gm
error
term
control
item
defin
errτ
max
exp
exp
merit
merit
18
intuit
condit
state
rank
ahead
reduc
dispar
underexpos
past
can
now
state
follow
theorem
theorem
7.1
set
disjoint
group
gm
fix
target
merit
merit
fulfil
18
relev
model
exposur
model
pt
pt
pmax
valu
run
fairco
exp
gi
time
τ0
will
alway
ensur
overal
dispar
respect
error
term
errτ
zero
group
alreadi
maximum
exposur
impact
merit
item
group
error
term
grow
increas
dispar
note
dispar
error
term
use
esˆ
timat
merit
equat
15
converg
merit
sampl
size
increas
avoid
divis
zero
merit
can
set
minimum
constant
target
merit
converg
zero
rate
τ1
matter
unfair
exposur
τ10
τt
exp
τ0
proof
theorem
includ
full
version
paper
arxiv
note
theorem
hold
time
point
τ0
even
estim
merit
chang
substanti
τ0
estim
merit
converg
true
merit
434
session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
fairco
exp
will
ensur
amort
dispar
converg
zero
well
empir
evalu
8.1
avg
cumul
ndcg
addit
theoret
justif
approach
also
conduct
empir
evaluation1
first
present
experi
semi-synthet
news
dataset
investig
differ
aspect
propos
method
control
condit
evalu
method
real-world
movi
prefer
data
extern
valid
robust
analysi
news
data
abl
evalu
method
varieti
specif
design
test
set
creat
follow
simul
environ
articl
ad
font
media
bia
dataset2
simul
dynam
rank
problem
set
news
articl
belong
two
group
left
right
left-lean
right-lean
news
articl
trial
sampl
set
30
news
articl
articl
dataset
contain
polar
valu
rescal
interv
user
polar
simul
user
polar
drawn
mixtur
two
normal
distribut
clip
ut
clip
pneд
0.5
0.2
pneд
0.5
0.2
19
0.700
0.675
0.650
1000
2000
3000
0.15
naiv
d-ultr
glob
0.10
fairco
imp
0.05
0.00
1000
2000
3000
user
8.1
can
fairco
reduc
unfair
maintain
good
rank
qualiti
key
question
evalu
fairco
figur
show
ndcg
unfair
converg
naiv
dultr
glob
fairco
imp
plot
show
naiv
achiev
lowest
ndcg
unfair
remain
high
number
user
interact
increas
d-ultr
glob
achiev
best
ndcg
predict
theori
unfair
margin
better
naiv
fairco
manag
substanti
reduc
unfair
come
small
decreas
ndcg
compar
d-ultr
glob
follow
question
will
provid
insight
result
evalu
compon
fairco
explor
robust
0.3
averag
0.2
naiv
ip
0.1
0.0
remaind
simul
follow
dynam
rank
setup
time
step
user
ut
arriv
system
algorithm
present
unperson
rank
σt
user
provid
feedback
ct
accord
pt
rt
algorithm
observ
ct
rt
investig
group-fair
group
item
accord
polar
item
polar
belong
left-lean
group
left
item
polar
belong
right-lean
group
right
measur
rank
qualiti
averag
cumul
ndcg
íτ
dcg
user
time
measur
500
1000
1500
user
2000
2500
3000
figur
error
relev
estim
number
user
increas
30
10
trial
8.1
unbias
estim
converg
true
relev
first
compon
fairco
evalu
unbias
ip
estim
ip
equat
15
figur
show
absolut
differ
estim
global
relev
true
global
relev
ip
estim
use
naiv
error
naiv
stagnat
around
0.25
estim
error
ip
approach
zero
number
user
increas
verifi
ip
elimin
effect
posit
bia
learn
accur
estim
true
expect
relev
news
articl
can
use
fair
rank
criteria
exposur
unfair
via
impact
unfair
via
defin
equat
16
0.725
figur
converg
ndcg
left
unfair
right
number
user
increas
100
trial
model
user
behavior
use
position-bas
click
model
pbm
18
margin
probabl
user
ut
examin
articl
depend
posit
choos
exposur
drop-off
analog
gain
function
dcg
pt
20
log2
rank
σt
0.20
user
pneд
probabl
user
left-lean
mean
0.5
use
pneд
0.5
unless
specifi
addit
user
open
paramet
0.05
0.55
indic
breadth
interest
outsid
polar
base
polar
user
ut
item
true
relev
drawn
bernoulli
distribut
ut
rt
bernoulli
exp
0.750
impact
unfair
news
experi
learn
global
rank
compar
follow
method
naiv
rank
sum
observ
feedback
ct
d-ultr
glob
dynam
ltr
sort
via
unbias
estim
ip
eq
15
fairco
imp
fair
control
eq
17
impact
fair
implement
avail
https://github.com/marcomorik/dynamic-fairness.
https://www.adfontesmedia.com/interactive-media-bias-chart/
435
0.2
sigir
20
juli
25
30
2020
virtual
event
china
see
solut
unfair
increas
method
start
enforc
fair
expens
ndcg
experi
found
evid
linprog
baselin
superior
fairco
howev
linprog
substanti
expens
comput
make
fairco
prefer
practic
naiv
d-ultr
glob
fairco
imp
0.1
0.0
50
100
150
200
250
300
number
right-lean
user
begin
350
400
impact
unfair
impact
unfair
session
3a
bia
fair
0.75
ndcg
figur
effect
block
right-lean
user
unfair
impact
50
trial
3000
user
0.70
0.65
8.1
fairco
overcom
rich-get-rich
dynam
illustr
exampl
section
argu
naiv
rank
item
highli
sensit
initi
condit
item
get
first
click
lead
rich-get-rich
dynam
now
test
whether
fairco
overcom
problem
particular
adversari
modifi
user
distribut
first
user
right-lean
pneд
follow
left-lean
user
pneд
continu
balanc
user
distribut
pneд
0.5
figur
show
unfair
3000
user
interact
expect
naiv
sensit
head-start
rightlean
articl
get
d-ultr
glob
fare
better
unfair
remain
constant
high
independ
initi
user
distribut
sinc
unbias
estim
ip
correct
present
bia
estim
still
converg
true
relev
fairco
inherit
robust
initi
condit
sinc
use
ip
estim
activ
control
unfair
make
method
achiev
low
unfair
across
whole
rang
0.68
10
10
10
10
10
10
10
fairco
imp
0.1
0.2
0.4
proport
left-lean
item
ndcg
8.1
fairco
effect
differ
group
size
experi
vari
asymmetri
polar
within
set
30
news
articl
rang
left
left
15
news
articl
group
size
run
20
trial
3000
user
figur
show
regardless
group
ratio
fairco
reduc
unfair
whole
rang
maintain
ndcg
contrast
naiv
d-ultr
glob
suffer
high
unfair
impact
unfair
impact
unfair
ndcg
0.70
0.2
figur
ndcg
left
unfair
right
vari
proport
left
20
trial
3000
user
0.8
0.7
0.72
naiv
d-ultr
glob
0.0
0.2
0.4
proport
left-lean
item
0.74
0.15
0.3
linprog
fairco
imp
0.2
0.4
0.6
0.8
proport
left-lean
user
0.10
0.05
0.4
0.2
0.0
naiv
d-ultr
glob
fairco
imp
0.2
0.4
0.6
0.8
proport
left-lean
user
figur
ndcg
left
unfair
right
vari
user
distribut
20
trial
3000
user
0.00
10
10
10
10
100
101
102
8.1
fairco
effect
differ
user
distribut
final
examin
robust
vari
user
distribut
control
polar
distribut
user
vari
pneд
equat
19
run
20
trial
3000
user
figur
observ
naiv
d-ultr
glob
suffer
high
unfair
larg
imbal
minor
major
group
fairco
abl
control
unfair
set
figur
compar
lp
baselin
p-control
term
ndcg
left
unfair
right
differ
valu
15
trial
3000
user
8.1
effect
fairco
compar
expens
linear-program
baselin
baselin
adapt
linear
program
method
41
dynam
ltr
set
minim
amort
fair
dispar
consid
work
method
use
current
relev
dispar
estim
solv
linear
program
problem
whose
solut
stochast
rank
polici
satisfi
fair
constraint
expect
detail
method
describ
full
version
paper
arxiv
figur
show
ndcg
impact
unfair
3000
user
averag
15
trial
linprog
fairco
differ
valu
hyperparamet
method
reduc
d-ultr
glob
can
8.2
evalu
real-world
prefer
data
evalu
method
real-world
prefer
data
adopt
ml-20m
dataset
24
select
five
product
compani
movi
dataset
mgm
warner
bro
paramount
20th
centuri
fox
columbia
product
compani
form
group
aim
ensur
fair
exposur
exclud
movi
rate
divers
user
popul
set
300
rate
movi
product
compani
select
100
movi
highest
standard
436
sigir
20
juli
25
30
2020
virtual
event
china
2000
4000
5000
0.0
6000
2000
4000
6000
user
0.20
impact
unfair
avg
cumul
ndcg
0.9
0.8
0.7
2000
4000
6000
naiv
d-ultr
glob
d-ultr
fairco
imp
0.15
0.10
0.05
0.00
2000
4000
6000
user
8.2
can
fairco
reduc
unfair
figur
show
fairco
exp
can
effect
control
exposur
unfair
unlik
method
activ
consid
fair
similarli
figur
show
fairco
imp
effect
control
impact
unfair
expect
improv
fair
come
reduct
ndcg
reduct
small
6000
0.20
0.20
0.15
0.10
0.05
0.00
figur
compar
ndcg
person
nonperson
rank
movi
data
10
trial
2000
4000
user
6000
0.15
d-ultr
fairco
imp
0.10
fairco
exp
0.05
0.00
2000
4000
6000
user
figur
10
unfair
exposur
left
unfair
impact
right
person
control
optim
either
exposur
impact
10
trial
8.2
person
via
unbias
object
improv
ndcg
first
evalu
whether
train
person
model
use
de-bias
reg
regress
estim
improv
rank
perform
non-person
model
figur
show
rank
reg
d-ultr
provid
substanti
higher
ndcg
unbias
global
rank
d-ultr
glob
naiv
rank
get
upper
bound
perform
person
model
also
train
skylin
model
use
practic
unobserv
true
relev
rt
least-squar
object
eq
13
even
though
unbias
regress
estim
reg
0.1
access
partial
feedback
ct
track
perform
skylin
predict
theori
appear
converg
asymptot
d-ultr
skylin
3000
user
4000
0.2
figur
ndcg
left
impact
unfair
right
movi
data
number
user
interact
increas
10
trial
0.7
1000
2000
naiv
d-ultr
glob
d-ultr
fairco
exp
0.3
user
0.8
0.7
impact
unfair
naiv
d-ultr
glob
0.8
0.4
figur
ndcg
left
exposur
unfair
right
movi
data
number
user
interact
increas
10
trial
1.0
0.9
0.9
user
exposur
unfair
avg
cumul
ndcg
deviat
rate
across
user
user
select
104
user
rate
number
chosen
movi
leav
us
partial
fill
rate
matrix
104
user
100
movi
avoid
miss
data
eas
evalu
use
off-the-shelf
matrix
factor
algorithm3
fill
miss
entri
normal
rate
appli
sigmoid
function
center
rate
slope
10
serv
relev
probabl
higher
star
rate
correspond
higher
likelihood
posit
feedback
final
trial
obtain
binari
relev
matrix
draw
bernoulli
sampl
user
movi
pair
probabl
use
user
embed
matrix
factor
model
user
featur
follow
experi
use
fairco
learn
sequenc
rank
polici
πt
person
base
goal
maxim
ndcg
provid
fair
exposur
product
compani
user
interact
simul
analog
previou
experi
time
step
sampl
user
rank
algorithm
present
rank
100
movi
user
follow
position-bas
model
equat
20
reveal
ct
accordingli
condit
relev
model
reg
use
fairco
d-ultr
use
one
hidden-lay
neural
network
consist
50
input
node
fulli
connect
64
node
hidden
layer
relu
activ
connect
100
output
node
sigmoid
output
predict
probabl
relev
movi
sinc
train
network
less
100
observ
unreli
use
global
ranker
d-ultr
glob
first
100
user
train
network
100
user
updat
network
everi
10
user
previous
collect
feedback
c1
cτ
use
unbias
regress
object
eq
14
adam
optim
33
exposur
unfair
avg
cumul
ndcg
session
3a
bia
fair
8.2
differ
exposur
impact
fair
figur
10
evalu
algorithm
optim
exposur
fair
perform
term
impact
fair
vice
versa
plot
show
two
criteria
achiev
differ
goal
substanti
differ
fact
optim
fair
impact
can
even
increas
unfair
exposur
illustr
choic
criterion
need
ground
requir
applic
surpris
librari
http://surpriselib.com/)
svd
bias
fals
50
437
session
3a
bia
fair
sigir
20
juli
25
30
2020
virtual
event
china
conclus
20
nick
craswel
onno
zoeter
michael
taylor
bill
ramsey
2008
experiment
comparison
click
position-bia
model
wsdm
21
daniel
ensign
sorel
friedler
scott
nevil
carlo
scheidegg
suresh
venkatasubramanian
2018
runaway
feedback
loop
predict
polic
confer
fair
account
transpar
22
zhichong
fang
agarw
joachim
2019
intervent
harvest
context-depend
examination-bia
estim
sigir
23
fabrizio
germano
vicenç
gómez
gaël
le
men
2019
few-getrich
surpris
consequ
popularity-bas
rank
arxiv
preprint
arxiv
1902.02580
2019
24
maxwel
harper
joseph
konstan
2015
movielen
dataset
histori
context
acm
tii
2015
25
herbrich
graepel
obermay
2000
larg
margin
rank
boundari
ordin
regress
advanc
larg
margin
classifi
26
katja
hofmann
shimon
whiteson
maarten
de
rijk
2013
balanc
explor
exploit
listwis
pairwis
onlin
learn
rank
inform
retriev
inform
retriev
2013
27
daniel
horvitz
donovan
thompson
1952
gener
sampl
without
replac
finit
univers
journal
american
statist
associ
1952
28
guido
imben
donald
rubin
2015
causal
infer
statist
social
biomed
scienc
cambridg
univers
press
29
kalervo
järvelin
jaana
kekäläinen
2002
cumul
gain-bas
evalu
ir
techniqu
toi
2002
30
joachim
2002
optim
search
engin
use
clickthrough
data
acm
sigkdd
confer
knowledg
discoveri
data
mine
kdd
133
142
31
joachim
granka
bing
pan
hembrook
radlinski
gay
2007
evalu
accuraci
implicit
feedback
click
queri
reformul
web
search
acm
toi
2007
32
joachim
swaminathan
schnabel
2017
unbias
learning-to-rank
bias
feedback
wsdm
33
diederik
kingma
jimmi
ba
2014
adam
method
stochast
optim
arxiv
preprint
arxiv
1412.6980
2014
34
shuai
li
tor
lattimor
csaba
szepesvári
2018
onlin
learn
rank
featur
arxiv
preprint
arxiv
1810.02567
2018
35
lydia
liu
sarah
dean
esther
rolf
max
simchowitz
moritz
hardt
2018
delay
impact
fair
machin
learn
icml
36
radlinski
joachim
2006
minim
invas
random
collect
unbias
prefer
clickthrough
log
aaai
1406
1412
37
radlinski
kleinberg
joachim
2008
learn
divers
rank
multi-arm
bandit
icml
38
stephen
robertson
1977
probabl
rank
principl
ir
journal
document
1977
39
salganik
sheridan
dodd
watt
2006
experiment
studi
inequ
unpredict
artifici
cultur
market
scienc
2006
40
tobia
schnabel
adith
swaminathan
ashudeep
singh
navin
chandak
thorsten
joachim
2016
recommend
treatment
debias
learn
evalu
icml
41
ashudeep
singh
thorsten
joachim
2018
fair
exposur
rank
acm
sigkdd
42
ashudeep
singh
thorsten
joachim
2019
polici
learn
fair
rank
neurip
43
behzad
tabibian
vicenç
gómez
abir
de
bernhard
schölkopf
manuel
gomez
rodriguez
2019
consequenti
rank
algorithm
long-term
welfar
arxiv
preprint
arxiv
1905.05305
2019
44
michael
taylor
john
guiver
stephen
robertson
tom
minka
2008
softrank
optim
non-smooth
rank
metric
wsdm
acm
45
soroush
vosoughi
deb
roy
sinan
aral
2018
spread
true
fals
news
onlin
scienc
2018
46
xuanhui
wang
nadav
golbandi
michael
benderski
donald
metzler
marc
najork
2018
posit
bia
estim
unbias
learn
rank
person
search
wsdm
acm
47
himank
yadav
zhengxiao
du
thorsten
joachim
2019
fair
learning-torank
implicit
feedback
arxiv
cs
lg
1911.08054
48
hongzhi
yin
bin
cui
jing
li
junji
yao
chen
chen
2012
challeng
long
tail
recommend
vldb
2012
49
masrour
zoghi
toma
tuni
mohammad
ghavamzadeh
branislav
kveton
csaba
szepesvari
zheng
wen
2017
onlin
learn
rank
stochast
click
model
icml
identifi
bias
feedback
uncontrol
exposur
alloc
can
lead
unfair
undesir
behavior
dynam
ltr
address
problem
propos
fairco
abl
adapt
enforc
amort
merit-bas
fair
constraint
even
though
underli
relev
still
learn
algorithm
robust
present
bia
thu
exhibit
rich-get-rich
dynam
final
fairco
easi
implement
comput
effici
make
well
suit
practic
applic
acknowledg
research
support
part
nsf
award
iis-1901168
gift
workday
content
repres
opinion
author
necessarili
share
endors
respect
employ
sponsor
refer
himan
abdollahpouri
gedimina
adomaviciu
robin
burk
ido
guy
dietmar
jannach
toshihiro
kamishima
jan
krasnodebski
luiz
pizzato
2019
beyond
person
research
direct
multistakehold
recommend
arxiv
preprint
arxiv
1905.01986
2019
himan
abdollahpouri
robin
burk
bamshad
mobash
2017
control
popular
bia
learning-to-rank
recommend
acm
recsi
lada
adam
bernardo
huberman
2000
power-law
distribut
world
wide
web
scienc
2000
agarw
takatsu
zaitsev
joachim
2019
gener
framework
counterfactu
learning-to-rank
sigir
agarw
zaitsev
xuanhui
wang
cheng
li
najork
joachim
2019
estim
posit
bia
without
intrus
intervent
wsdm
qingyao
ai
kepe
bi
cheng
luo
jiafeng
guo
bruce
croft
2018
unbias
learn
rank
unbias
propens
estim
sigir
michael
ekstrand
sebastian
kohlmeier
asia
biega
fernando
diaz
2019
trec
fair
rank
track
https://fair-trec.github.io/
onlin
access
08
14
2019
ricardo
baeza-y
2018
bia
web
commun
acm
2018
solon
baroca
moritz
hardt
arvind
narayanan
2018
fair
machin
learn
2018
10
solon
baroca
andrew
selbst
2016
big
data
dispar
impact
calif
rev
2016
11
michael
beam
2014
autom
news
person
news
recommend
system
design
choic
impact
news
recept
commun
research
2014
12
wayn
bequett
2003
process
control
model
design
simul
prentic
hall
profession
13
alex
beutel
jilin
chen
tulse
doshi
hai
qian
li
wei
yi
wu
lukasz
heldt
zhe
zhao
lichan
hong
ed
chi
cristo
goodrow
2019
fair
recommend
rank
pairwis
comparison
acm
sigkdd
14
asia
biega
krishna
gummadi
gerhard
weikum
2018
equiti
attent
amort
individu
fair
rank
sigir
15
christoph
jc
burg
2010
ranknet
lambdarank
lambdamart
overview
learn
2010
16
elisa
celi
damian
straszak
nisheeth
vishnoi
2017
rank
fair
constraint
arxiv
preprint
arxiv
1704.06840
2017
17
nicolò
cesa-bianchi
gábor
lugosi
2006
predict
learn
game
cambridg
univers
press
18
aleksandr
chuklin
ilya
markov
maarten
de
rijk
2015
click
model
web
search
synthesi
lectur
inform
concept
retriev
servic
2015
19
giovanni
luca
ciampaglia
azadeh
nematzadeh
filippo
menczer
alessandro
flammini
2018
algorithm
popular
bia
hinder
promot
qualiti
scientif
report
2018
438