research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
fairness
exposure
rankings
ashudeep
singh
thorsten
joachims
cornell
university
cornell
university
ithaca
ny
ithaca
ny
ashudeep@cs.cornell.edu
tj@cs.cornell.edu
abstract
anything
ranked
today
products
jobs
job
seek
rankings
ubiquitous
online
world
today
ers
opinions
potential
romantic
partners
nevertheless
one
transitioned
finding
books
libraries
ranking
products
guiding
technical
principles
behind
optimization
ranking
sys
jobs
job
applicants
opinions
potential
romantic
partners
tems
still
dates
back
four
decades
ago
namely
probability
substantial
precedent
ranking
systems
responsibility
ranking
principle
prp
28
states
ideal
ranking
users
also
items
ranked
address
order
items
decreasing
order
probability
relevance
often
conflicting
responsibilities
propose
conceptual
since
ranking
maximizes
utility
retrieval
sys
computational
framework
allows
formulation
fairness
tem
user
broad
range
common
utility
measures
constraints
rankings
terms
exposure
allocation
part
information
retrieval
uncompromising
focus
utility
framework
develop
efficient
algorithms
finding
rankings
users
still
appropriate
ranking
books
maximize
utility
user
provably
satisfying
library
people
products
opinions
specifiable
notion
fairness
since
fairness
goals
can
application
now
substantial
arguments
precedent
many
specific
show
broad
range
fairness
constraints
can
ranking
systems
use
today
responsibility
implemented
using
framework
including
forms
demographic
users
also
items
ranked
particular
parity
disparate
treatment
disparate
impact
constraints
scarce
resource
ranking
systems
allocate
exposure
illustrate
effect
constraints
providing
empirical
items
users
exposure
largely
determined
position
results
two
ranking
problems
ranking
job
applicant
chances
interviewed
employer
airbnb
host
ability
rent
property
ccs
concepts
writer
read
exposes
companies
operating
sensitive
data
legal
reputation
risks
disagreements
information
systems
probabilistic
retrieval
models
retrieval
fair
allocation
exposure
already
led
high-profile
effectiveness
presentation
retrieval
results
legal
challenges
european
union
antitrust
violation
fine
keywords
google
30
sparked
policy
debate
search
neutrality
14
unlikely
will
universal
definition
fairness
rankings
fairness
algorithmic
bias
position
bias
equal
fairness
appropriate
across
applications
give
opportunity
three
concrete
examples
ranking
system
may
perceived
acm
reference
format
unfair
biased
treatment
items
ashudeep
singh
thorsten
joachims
2018
fairness
exposure
ranked
ranking
system
may
want
impose
fairness
rankings
kdd
18
24th
acm
sigkdd
international
conference
constraints
guarantee
notion
fairness
knowledge
discovery
data
mining
august
19
23
2018
london
united
main
contribution
paper
conceptual
compu
kingdom
acm
new
york
ny
usa
10
pages
https://doi.org/10.1145/
tational
framework
formulating
fairness
constraints
rank
3219819.3220088
ings
associated
efficient
algorithms
computing
utility
maximizing
rankings
subject
fairness
constraints
introduction
framework
provides
flexible
way
balancing
fairness
rankings
become
one
dominant
forms
items
ranked
utility
rankings
provide
online
systems
present
results
user
far
surpassing
con
users
way
limited
single
definition
fair
ception
library
science
tool
finding
books
library
ness
since
different
application
scenarios
probably
require
different
prevalence
rankings
now
ranges
search
engines
online
trade-offs
rights
items
can
consid
stores
recommender
systems
news
feeds
consequently
ered
acceptable
loss
utility
user
show
broad
longer
just
books
ranked
hardly
range
fairness
constraints
can
implemented
framework
permission
make
digital
hard
copies
part
work
personal
using
expressive
power
link
exposure
relevance
impact
classroom
use
granted
without
fee
provided
copies
made
distributed
particular
show
implement
forms
demographic
profit
commercial
advantage
copies
bear
notice
full
citation
first
page
copyrights
components
work
owned
others
parity
disparate
treatment
disparate
impact
constraints
author
must
honored
abstracting
credit
permitted
copy
otherwise
ranking
algorithm
develop
provides
provable
guarantees
republish
post
servers
redistribute
lists
requires
prior
specific
permission
optimizing
expected
utility
obeying
specified
notion
fee
request
permissions
permissions@acm.org
kdd
18
august
19
23
2018
london
united
kingdom
fairness
expectation
2018
copyright
held
owner
author
publication
rights
licensed
associa
motivate
need
range
situations
one
may
tion
computing
machinery
want
trade-off
utility
notion
fairness
start
acm
isbn
978
4503
5552
18
08
15.00
https://doi.org/10.1145/3219819.3220088
presenting
following
three
application
scenarios
make
2219
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
ùëé1
ùëé2
ùëé6
ùëé5
0.82
0.81
0.77
0.78
ùëé4
ùëé3
0.79
0.80
ùëé1
ùëé2
0.71
0.81
ùëé3
ranking
0.03
difference
avg
relevance
0.32
difference
avg
exposure
ùëé4
0.78
ùëé5
0.39
figure
image
search
result
page
query
ceo
ùëé6
showing
disproportionate
number
male
ceos
relevance
prob
interview
exposure
figure
job
seeker
example
illustrate
small
differ
example
fairly
representing
distribution
results
ence
relevance
can
lead
large
difference
exposure
times
results
query
used
statistical
sample
either
opportunity
group
females
explicitly
implicitly
example
user
may
expect
im
age
search
query
ceo
search
engine
returns
roughly
use
concept
protected
groups1
fairness
related
right
number
male
female
executives
reflecting
true
differences
groups
treated
however
later
discuss
distribution
male
vs
female
ceos
world
search
engine
extends
individual
fairness
considering
groups
size
returns
highly
disproportionate
number
males
compared
one
three
examples
illustrate
fairness
can
related
females
like
hypothetical
results
figure
search
biased
allocation
opportunity
misrepresentation
real-world
engine
may
perceived
biased
fact
study
detected
distributions
fairness
freedom
speech
principle
presence
gender
bias
image
search
results
variety
occupations
19
biased
information
environment
may
affect
example
fairly
allocating
economic
opportunity
consider
users
perceptions
behaviors
shown
biases
web-service
connects
employers
users
potential
employees
indeed
affect
people
belief
various
occupations
19
note
items
following
example
demonstrates
small
differences
probability
ranking
principle
necessarily
produce
item
relevance
can
cause
large
difference
exposure
results
represent
relevance
distribution
unbiased
way
therefore
economic
opportunity
across
groups
case
web
means
even
users
relevance
distribution
agrees
service
uses
ranking-based
system
present
set
applicants
true
distribution
female
ceos
optimal
ranking
accord
software
engineering
position
relevant
employers
figure
ing
probability
ranking
principle
may
still
look
like
set
contains
males
females
male
applicants
figure
instead
solely
relying
prp
seems
reasonable
relevance
0.80
0.79
0.78
respectively
employers
distribute
exposure
proportional
relevance
even
may
female
applicants
relevance
0.77
0.76
0.75
respectively
mean
drop
utility
users
follow
standard
probabilistic
definition
relevance
0.77
means
77
employers
issuing
query
find
example
giving
speakers
fair
access
willing
listeners
rank
applicant
relevant
probability
ranking
principle
suggests
ing
systems
play
increasing
role
medium
speech
creating
ranking
applicants
decreasing
order
relevance
connection
bias
fairness
rankings
principles
males
top
positions
followed
females
behind
freedom
speech
14
ability
produce
speech
mean
exposure
two
groups
consider
make
speech
available
internet
certainly
created
standard
exposure
drop-off
position
bias
log
new
opportunities
exercise
freedom
speech
speaker
position
ranking
commonly
used
remains
question
whether
free
speech
makes
way
discounted
cumulative
gain
dcg
measure
female
applicants
interested
listeners
hence
study
medium
becomes
will
get
30
less
exposure
even
though
average
difference
necessary
search
engines
popular
mediums
relevance
male
female
applicants
just
0.03
see
kind
therefore
immense
capability
influencing
user
figure
winner-take-all
allocation
exposure
fair
attention
editorial
policies
sparked
pol
context
even
winner
just
tiny
advantage
relevance
icy
debate
around
search
neutrality
13
14
16
unified
seems
reasonable
distribute
exposure
evenly
even
definition
search
neutrality
exists
many
argue
search
en
may
mean
small
drop
utility
employers
gines
editorial
policies
results
comprehensive
impartial
solely
ranked
relevance
26
groups
protected
discrimination
law
based
sex
race
age
disability
ranking
solely
relevance
necessarily
imply
proba
color
creed
national
origin
religion
use
broader
meaning
protected
groups
bility
ranking
principle
relevance-based
ranking
suits
domain
note
tiny
advantage
may
come
employers
gender
principles
lead
medium
equitable
distribution
biased
problem
addressing
exposure
access
willing
listeners
2220
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
related
work
designing
fair
scoring
functions
satisfy
desirable
fairness
introducing
algorithmic
framework
formulating
constraints
broad
range
fairness
constraints
rankings
first
survey
fairness
constraints
defined
previous
work
three
related
strands
prior
work
first
paper
draws
con
reflect
parity
constraints
restricting
fraction
items
cepts
algorithmic
fairness
supervised
learning
presence
attribute
ranking
framework
propose
goes
beyond
sensitive
attributes
second
relate
prior
work
algorith
parity
constraints
propose
general
algorithmic
frame
mic
fairness
rankings
finally
contrast
fairness
work
efficiently
computing
optimal
probabilistic
rankings
well-studied
area
diversified
ranking
information
retrieval
large
class
possible
fairness
constraints
concurrent
independent
work
biega
et
al
formulates
2.1
algorithmic
fairness
fairness
rankings
similar
special
case
framework
discussed
section
4.2
aiming
achieve
amortized
fairness
algorithmic
techniques
especially
machine
learning
find
wide
attention
making
exposure
proportional
relevance
spread
applications
much
interest
understanding
focus
individual
fairness
framework
amounts
societal
impacts
algorithmic
decisions
can
counteract
ex
special
case
protected
groups
size
one
two
approaches
isting
biases
algorithmic
data-driven
decision
making
affords
differ
expressive
power
algorithmically
solve
new
mechanisms
introducing
unintended
bias
integer
linear
program
generate
series
rankings
numerous
attempts
define
notions
fairness
super
approach
provides
provably
efficient
solution
via
standard
vised
learning
setting
individual
fairness
perspective
states
linear
program
birkhoff-von
neumann
decomposition
two
individuals
similar
respect
task
clas
sified
similarly
12
individual
fairness
hard
define
precisely
lack
agreement
task-specific
similarity
metrics
2.3
information
diversity
retrieval
individuals
also
group
fairness
perspective
super
first
glance
fairness
diversity
rankings
may
appear
re
vised
learning
implies
constraints
like
demographic
parity
lated
since
lead
diverse
rankings
however
equalized
odds
demographic
parity
posits
decisions
motivation
mechanisms
fundamentally
different
like
balanced
around
sensitive
attribute
like
gender
race
37
prp
diversified
ranking
entirely
beholden
maximizing
utility
however
shown
demographic
parity
causes
loss
user
approach
fairness
balances
needs
utility
infringes
individual
fairness
12
since
even
users
items
particular
prp
diversified
ranking
perfect
predictor
typically
achieve
demographic
parity
maximize
utility
user
alone
difference
lies
merely
equalized
odds
represents
equal
opportunity
principle
super
utility
measure
maximized
extrinsic
diversity
vised
learning
defines
constraint
false
positive
24
utility
measure
accounts
uncertainty
diminishing
true
positive
rates
equal
different
protected
groups
returns
multiple
relevant
results
25
intrinsic
di
15
several
recent
works
focused
learning
algorithms
versity
24
utility
measure
considers
rankings
portfolios
compatible
definitions
fair
classification
31
33
35
reflects
redundancy
10
exploration
diversity
24
including
causal
approaches
fairness
20
21
23
paper
aim
maximize
utility
user
long
term
draw
many
concepts
introduced
context
fair
effective
learning
work
fairness
paper
fun
supervised
learning
consider
problem
learning
damentally
different
motivation
mechanism
instead
ask
fairly
allocate
exposure
rankings
based
modify
utility
measure
user
instead
introduces
relevance
independent
relevances
may
estimated
rights
items
ranked
2.2
fairness
rankings
several
recent
works
raised
question
group
fairness
framework
ranking
rankings
yang
stoyanovich
32
propose
statistical
parity
fairness
constraints
based
measures
compute
difference
distribution
acknowledging
ubiquity
rankings
across
applications
different
groups
different
prefixes
ranking
top-10
conjecture
single
definition
constitutes
top-20
differences
averaged
fair
ranking
fairness
depends
context
application
prefixes
using
discounted
weighting
like
dcg
measure
particular
will
see
different
notions
fairness
used
regularization
term
zehlike
et
al
34
formulate
imply
different
trade-offs
utility
may
acceptable
problem
finding
fair
top-k
ranking
optimizes
utility
one
situation
address
range
pos
satisfying
two
sets
constraints
first
in-group
monotonicity
sible
fairness
constraints
section
develops
framework
utility
relevant
items
less
relevant
within
formulating
fairness
constraints
rankings
computing
group
second
fairness
constraint
proportion
protected
utility-maximizing
ranking
subject
fairness
constraints
group
items
every
prefix
top-k
ranking
minimum
provable
guarantees
threshold
celis
et
al
propose
constrained
maximum
weight
simplicity
consider
single
query
assume
matching
algorithm
ranking
set
items
efficiently
want
present
ranking
set
documents
fairness
constraint
indicating
maximum
number
items
denoting
utility
ranking
query
sensitive
attribute
allowed
top
positions
recent
problem
optimal
ranking
fairness
constraints
can
approaches
like
asudeh
et
al
also
looked
task
2221
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
formulated
following
optimization
problem
since
utility
linear
can
combine
indi
vidual
utilities
expectation
argmaxr
fair
rank
rel
way
generalize
goal
probabilistic
ranking
rank
principle
emerges
special
case
fairness
con
straints
fully
instantiate
solve
optimization
problem
will
specify
following
four
components
first
define
general
class
utility
measures
contains
many
com
rel
monly
used
ranking
metrics
second
address
problem
optimize
rankings
discrete
combinatorial
objects
extending
class
rankings
probabilistic
rankings
expected
utility
document
query
case
third
reformulate
optimization
problem
efficiently
binary
relevances
identity
function
equivalent
solvable
linear
program
implies
convenient
yet
expressive
probability
relevance
easy
see
sorting
language
formulating
fairness
constraints
finally
show
documents
leads
ranking
maximizes
utility
probabilistic
ranking
can
efficiently
recovered
solution
linear
program
argmaxr
argsortd
function
decreases
rank
insight
3.1
utility
ranking
behind
probability
ranking
principle
prp
28
virtually
utility
measures
used
ranking
evaluation
derive
utility
ranking
relevance
individual
items
3.2
probabilistic
rankings
ranked
user
query
rel
denotes
rankings
combinatorial
objects
naively
searching
binary
relevance
document
whether
document
space
rankings
utility-maximizing
ranking
fairness
relevant
user
note
different
users
can
constraints
take
time
exponential
avoid
different
rel
even
share
account
combinatorial
optimization
consider
probabilistic
rankings
personalization
assume
query
also
contains
instead
single
deterministic
ranking
probabilistic
ranking
personalization
features
set
users
lead
distribution
rankings
can
naturally
extend
identical
beyond
binary
relevance
rel
also
represent
definition
utility
probabilistic
rankings
relevance
rating
systems
likert
scale
movie
ratings
real-valued
score
rank
rel
generic
way
express
many
utility
measures
commonly
used
information
retrieval
rank
rank
rel
distributions
rankings
still
exponential
size
two
application-dependent
functions
func
can
make
use
additional
insight
utility
can
already
tion
rank
models
much
attention
document
gets
computed
marginal
rank
distributions
documents
rank
rank
function
maps
relevance
let
pi
probability
places
document
di
rank
document
user
utility
particular
choice
forms
doubly
stochastic
matrix
size
means
based
position
bias
fraction
users
examine
sum
row
column
matrix
equal
document
shown
particular
position
total
number
words
sum
probabilities
position
sum
probabilities
document
pi
users
issue
query
choice
mapping
relevance
pi
knowledge
doubly
stochastic
matrix
utility
somewhat
arbitrary
example
widely
used
evaluation
measure
discounted
cumulative
gain
dcg
17
can
repre
expected
utility
probabilistic
ranking
can
computed
sented
framework
rank
log
rank
d1
rel
2rel
sometimes
simply
rel
pi
di
2rel
dcg
make
notation
concise
can
rewrite
utility
lo–¥
rank
ranking
matrix
product
introduce
two
vectors
column
vector
size
ui
di
another
measure
like
dcg
can
choose
rank
column
vector
size
vj
expected
utility
log
rank
rank
rank
rank
dcg
can
written
ut
pv
2222
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
3.3
optimizing
fair
rankings
via
linear
ai
permutation
programming
matrices
case
permutation
matrices
correspond
deterministic
rankings
document
set
coefficients
will
see
section
3.4
imply
doubly
correspond
probability
sampling
ranking
according
stochastic
matrix
can
also
efficiently
compute
marcus-ree
theorem
exists
decomposition
probabilistic
ranking
every
doubly
stochastic
matrix
can
permutation
matrices
22
decompo
therefore
formulate
problem
finding
utility-maximizing
sition
can
computed
efficiently
polynomial
time
using
several
probabilistic
ranking
fairness
constraints
terms
doubly
algorithms
11
experiments
paper
use
stochastic
matrices
instead
distributions
rankings
implementation
provided
https://github.com/jfinkels/birkhoff.
argmaxp
ut
pv
expected
utility
sum
probabilities
position
3.5
summary
algorithm
following
summarizes
algorithm
optimal
ranking
p1
sum
probabilities
document
fairness
constraints
note
assumed
knowledge
pi
valid
probability
true
relevances
throughout
paper
whereas
practice
fair
fairness
constraints
one
work
estimates
predictive
model
note
optimization
objective
linear
variables
pi
set
utility
vector
position
discount
vector
furthermore
constraints
ensuring
doubly
well
vectors
scalar
fairness
stochastic
linear
well
column
vector
size
constraints
see
section
containing
ones
without
fairness
constraint
vj
solve
linear
program
section
3.3
decreases
solution
permutation
matrix
compute
birkhoff-von
neumann
decomposition
ranks
set
documents
decreasing
order
utility
conform
p1
p2
pn
ing
prp
sample
permutation
matrix
pi
probability
proportional
now
expressed
problem
finding
utility
display
corresponding
ranking
maximizing
probabilistic
ranking
besides
fairness
constraint
note
rankings
sampled
last
step
algorithm
linear
program
convenient
language
express
fairness
fulfill
fairness
constraints
expectation
time
constraints
linear
constraints
form
maximize
expected
utility
ft
pg
constructing
group
fairness
one
constraints
can
added
resulting
constraints
linear
program
can
still
solved
efficiently
optimally
now
established
framework
formulating
fairness
standard
algorithms
like
interior
point
methods
will
show
constraints
optimally
solving
associated
ranking
problem
section
vectors
scalar
can
chosen
still
need
understand
expressiveness
constraints
implement
range
different
fairness
constraints
give
form
ft
pg
section
explore
three
concepts
intuition
vector
can
used
encode
group
identity
algorithmic
fairness
demographic
parity
disparate
treatment
relevance
document
will
typically
reflect
im
disparate
impact
can
implemented
framework
portance
position
position
bias
thus
enforced
efficiently
provable
guarantees
aim
fairly
allocate
exposure
now
define
formally
let
3.4
sampling
rankings
vj
represent
importance
position
concretely
solution
linear
program
matrix
containing
probabil
position
bias
fraction
users
examine
ities
document
position
implement
solution
item
position
define
exposure
document
di
ranking
system
need
compute
probabilistic
ranking
probabilistic
ranking
corresponds
probabilistic
ranking
can
sample
rankings
present
user3
given
exposure
di
pi
vj
derivation
approach
immediately
apparent
rankings
sampled
fulfill
specified
fairness
constraints
expectation
goal
allocate
exposure
fairly
groups
doc
computing
can
achieved
via
birkhoff-von
neu
uments
items
may
belong
different
groups
mann
bvn
decomposition
provides
transformation
sensitive
attributes
example
news
stories
belong
different
decompose
doubly
stochastic
matrix
convex
sum
per
sources
products
belong
different
manufacturers
applicants
mutation
matrices
particular
doubly
stochastic
matrix
long
different
genders
fairness
constraints
will
formulate
exists
decomposition
form
following
implement
different
goals
allocating
exposure
groups
a1
a2
illustrate
effect
fairness
constraints
will
provide
empirical
results
two
ranking
problems
use
forusability
reasons
preferable
make
sampling
pseudo-random
based
hash
user
identity
user
receives
ranking
average
relevance
document
normalized
utility
ui
di
set
position
bias
vj
log
query
repeated
2223
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
just
like
standard
definition
dcg
generally
one
can
experiments
solved
linear
program
3.3
twice
also
plug
actual
position-bias
value
can
estimated
without
demographic
parity
constraint
intervention
experiment
18
job
seeker
example
figure
shows
optimal
rankings
terms
without
fairness
constraint
panels
job-seeker
example
come
back
job-seeker
example
respectively
color
indicates
probability
value
introduction
illustrated
figure
ranking
note
fair
ranking
according
demographic
parity
problem
consists
applicants
probabilities
relevance
cludes
substantial
amount
stochasticity
however
panels
employer
0.81
0.80
0.79
0.78
0.77
0.76
groups
show
fair
ranking
can
decomposed
mix
reflect
gender
first
three
applicants
belonging
ture
two
deterministic
permutation
matrices
associated
male
group
last
three
female
group
weights
compared
dcg
unfair
ranking
3.8193
news
recommendation
dataset
use
subset
yow
news
optimal
fair
ranking
slightly
lower
utility
dcg
3.8031
recommendation
dataset
36
analyze
method
larger
however
drop
utility
due
demographic
parity
con
real-world
relevance
distribution
dataset
contains
explicit
straint
substantially
larger
example
lowered
implicit
feedback
set
users
news
articles
relevances
female
group
0.82
0.81
0.80
0.03
0.02
different
rss
feeds
randomly
sample
subset
news
articles
0.01
still
get
fair
ranking
current
people
topic
coming
top
two
sources
sources
lution
since
fairness
constraint
ignorant
relevance
identified
using
rss
feed
identifier
used
groups
ranking
roughly
every
second
document
low
relevance
relevant
field
used
measure
relevance
leading
large
drop
dcg
interesting
point
task
since
relevance
given
rating
divide
effect
demographic
parity
ranking
therefore
analogous
add
small
amount
gaussian
noise
0.05
break
effect
supervised
learning
can
also
lead
large
ties
resulting
ui
clipped
lie
drop
classification
accuracy
12
following
formulate
fairness
constraints
using
three
also
conducted
experiment
news
recom
ideas
allocation
exposure
different
groups
particular
mendation
dataset
figure
shows
optimal
ranking
matrix
will
define
constraints
form
ft
pg
optimization
fair
probabilistic
ranking
along
dcg
note
problem
3.3
simplicity
will
present
case
even
though
optimal
unfair
ranking
places
documents
binary
valued
sensitive
attribute
two
groups
starting
position
constraint
pushes
ranking
news
ever
constraints
may
defined
pair
groups
items
ranking
starting
either
rank
sensitive
attribute
included
linear
program
rank
case
optimal
fair
ranking
happens
almost
deterministic
except
beginning
4.1
demographic
parity
constraints
arguably
simplest
way
defining
fairness
exposure
groups
enforce
average
exposure
documents
groups
equal
denoting
average
exposure
group
4.2
disparate
treatment
constraints
unlike
demographic
parity
constraints
explore
following
section
depend
relevance
items
exposure
exposure
di
ranked
way
constraints
potential
address
concerns
job-seeker
example
introduction
can
expressed
following
constraint
framework
small
difference
relevance
magnified
large
difference
exposure
furthermore
saw
image
exposure
exposure
search
example
introduction
may
desirable
exposure
proportional
relevance
achieve
form
pi
vj
pi
vj
unbiased
statistical
representation
denoting
average
utility
group
1di
1di
pi
vj
1d
1d
pv
fi
gi
gi
ui
last
step
obtain
constraint
form
ft
pg
one
can
plug
linear
program
section
3.3
call
demographic
parity
constraint
similar
analogous
constraint
fair
supervised
learning
37
similar
setting
case
also
constraint
may
lead
big
loss
utility
motivates
following
type
constraint
enforces
cases
two
groups
different
terms
relevance
exposure
two
groups
proportional
average
distribution
2224
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.500
0.500
0.4
0.2
0.0
dcg
3.8193
dcg
3.8031
dcg
3.8132
dcg
3.7929
figure
job
seeker
example
demographic
parity
constraint
optimal
unfair
ranking
maximizes
dcg
optimal
fair
ranking
demographic
parity
bvn
decomposition
fair
ranking
position
experiments
compute
optimal
ranking
without
10
15
20
10
15
20
fairness
constraint
disparate
treatment
constraint
results
job-seeker
example
shown
figure
0.8
figure
also
shows
bvn
decomposition
resultant
proba
document
id
bilistic
ranking
three
permutation
matrices
expected
10
10
0.6
fair
ranking
optimal
dtr
unfair
ranking
15
15
0.4
dtr
1.7483
also
expected
fair
ranking
lower
dcg
optimal
deterministic
ranking
higher
20
20
0.2
dcg
optimal
fair
ranking
demographic
parity
conducted
experiment
news
recommenda
dcg
5.2027
dcg
5.1360
tion
dataset
figure
shows
optimal
ranking
matrix
fair
probabilistic
ranking
along
dcg
ranking
computed
without
fairness
constraint
happened
almost
figure
news
recommendation
dataset
demographic
fair
according
disparate
treatment
already
fairness
con
parity
constraint
document
id
14
15
24
opti
straint
little
impact
dcg
mal
unfair
ranking
maximizes
dcg
optimal
fair
ranking
demographic
parity
4.3
disparate
impact
constraints
utility
previous
section
constrained
exposures
treatments
exposure
exposure
two
groups
proportional
average
utility
ever
may
want
go
step
define
constraint
√≠n
√≠n
impact
expected
clickthrough
purchase
rate
pi
directly
reflects
economic
impact
ranking
particular
may
want
assure
clickthrough
rates
groups
determined
exposure
relevance
pro
1di
1di
pi
vj
portional
average
utility
formally
define
let
us
first
model
probability
document
getting
clicked
according
1di
1di
following
simple
click
model
27
pv
fi
click
document
examining
relevant
name
constraint
disparate
treatment
constraint
cause
allocating
exposure
group
analogous
treating
exposure
di
relevant
two
groups
documents
motivated
principle
con
√µn
cept
recommendations
treatments
29
recommending
pi
vj
ui
exposing
document
considered
treatment
user
click
purchase
considered
effect
treatment
quantify
treatment
disparity
also
define
measure
called
can
now
compute
average
clickthrough
rate
documents
disparate
treatment
ratio
dtr
evaluate
unfair
ranking
group
respect
differently
two
groups
treated
exposure
dtr
exposure
ctr
pi
ui
vj
note
ratio
equals
one
disparate
treatment
constraint
equation
fulfilled
whether
value
less
greater
tells
group
disadvantaged
terms
following
disparate
impact
constraint
enforces
ex
disparate
treatment
pected
clickthrough
rate
group
proportional
average
2225
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.497
0.453
0.050
0.4
0.2
0.0
dcg
3.8193
dcg
3.8044
dcg
3.7972
dcg
3.8132
dcg
3.7959
dtr
1.7483
dtr
1.0000
dtr
0.8179
dtr
1.2815
dtr
0.7879
figure
job
seeker
example
disparate
treatment
constraint
optimal
unfair
ranking
fair
ranking
disparate
treatment
constraint
bvn
decomposition
fair
ranking
position
results
news
recommendation
dataset
given
10
15
20
10
15
20
figure
also
see
large
improvement
dir
dcg
lower
unconstrained
dcg
dcg
disparate
0.8
treatment
constraint
higher
dcg
demographic
document
id
parity
constraint
10
10
0.6
15
15
0.4
discussion
last
section
implemented
three
fairness
constraints
20
20
0.2
framework
motivated
concepts
demographic
parity
dcg
5.2027
dcg
5.1983
disparate
treatment
disparate
impact
main
purpose
dtr
1.0859
dtr
1.0000
explore
expressiveness
framework
argue
constraints
conceivable
ones
figure
news
recommendation
dataset
disparate
treat
correct
ones
given
application
particular
appears
ment
constraint
optimal
unfair
ranking
fair
ranking
fairness
rankings
inherently
trade-off
utility
disparate
treatment
constraint
users
rights
items
ranked
different
applications
require
making
trade-off
different
utility
ways
example
may
want
convey
strong
rights
ctr
ctr
books
library
user
trying
locate
book
situation
different
candidates
ranked
job
√≠n
opening
therefore
focused
creating
flexible
framework
pi
ui
covers
substantial
range
fairness
constraints
10
group
fairness
vs
individual
fairness
experiments
1di
1di
observe
even
though
constraints
ensure
rank
ui
pi
vj
11
ings
disparate
treatment
disparate
impact
across
groups
individual
items
within
group
might
still
considered
suffer
1di
1di
pv
fi
ui
disparate
treatment
impact
example
job-seeker
experiment
disparate
treatment
figure
allocation
similar
dtr
can
define
following
disparate
impact
exposure
candidates
within
group
still
follows
ratio
dir
measure
extent
disparate
impact
exposure
drop-off
going
ranking
considered
un
constraint
violated
fair
according
disparate
treatment
constraint
remedy
ctr
one
include
additional
fairness
constraints
sensitive
dir
ctr
attributes
like
race
disability
national
origin
refine
note
ratio
equals
one
disparate
impact
constraint
desired
notion
fairness
extreme
framework
allows
equation
11
fulfilled
similar
dtr
whether
dir
less
protected
groups
size
one
can
also
express
notions
greater
tells
group
disadvantaged
terms
individual
fairness
example
case
disparate
treatment
disparate
impact
express
individual
fairness
set
constraints
groups
size
one
resulting
notion
fairness
similar
experiments
compare
optimal
rankings
however
disparate
impact
constraint
without
fairness
constraint
results
job-seeker
ex
expected
clickthrough
rates
proportional
relevances
ample
shown
figure
optimal
fair
ranking
clear
whether
individual
fairness
makes
sense
unless
rank
bvn
decomposition
three
deterministic
rankings
items
uniformly
random
slightly
reduced
dcg
however
large
improvement
dir
fairness
constraint
since
prp
ranking
using
estimated
utilities
definitions
experiments
substantial
disparate
impact
two
groups
assumed
access
true
expected
utilities
2226
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
position
1.0
document
id
0.8
0.6
0.536
0.304
0.160
0.4
0.2
0.0
dcg
3.8193
dcg
3.8025
dcg
3.8059
dcg
3.7929
dcg
3.8089
dir
1.8193
dir
1.0000
dir
1.1192
dir
0.7501
dir
1.1801
figure
job
seeker
example
disparate
impact
constraint
optimal
unfair
ranking
fair
ranking
disparate
impact
constraint
bvn
decomposition
fair
ranking
position
vice
versa
minimum
10
15
20
10
15
20
exposure
0.8
max
exposure
vj
document
id
10
10
0.6
documents
top
positions
0.4
15
15
exposure
min
exposure
20
20
0.2
documents
bottom
positions
dcg
5.2027
dcg
5.1461
dir
1.5211
dir
1.0000
hence
fair
ranking
according
disparate
treatment
exists
ratio
average
utilities
lies
within
range
possible
values
figure
news
recommendation
dataset
disparate
im
exposure
pact
constraint
optimal
unfair
ranking
fair
ranking
disparate
impact
constraint
relevances
practice
utilities
typically
estimated
via
machine
learning
learning
step
subject
biases
however
scenario
constraint
can
still
satisfied
may
turn
lead
biased
estimates
importantly
introduce
documents
belonging
neither
group
biased
estimates
may
result
selection
biases
click
data
group
relevant
documents
increases
range
recent
counterfactual
learning
techniques
18
shown
lhs
ranking
doesn
give
undue
exposure
permit
unbiased
learning-to-rank
despite
biased
click
data
one
groups
cost
fairness
including
fairness
constraints
opti
conclusions
mization
problem
comes
cost
effectiveness
measured
paper
considered
fairness
rankings
lens
dcg
conventional
measures
loss
utility
can
exposure
allocation
groups
instead
defining
single
computed
cof
ut
deterministic
notion
fairness
developed
general
framework
em
optimal
ranking
represents
fair
ranking
already
ploys
probabilistic
rankings
linear
programming
compute
discussed
demographic
parity
constraint
cost
can
utility-maximizing
ranking
whole
class
fairness
con
substantial
particular
demographic
parity
easy
straints
verify
expressiveness
class
showed
see
utility
fair
ranking
approaches
zero
rele
express
fairness
constraints
motivated
concepts
de
vant
documents
one
group
size
group
mographic
parity
disparate
treatment
disparate
impact
approaches
infinity
conjecture
appropriate
definition
fair
exposure
depends
feasibility
fair
solutions
linear
program
sec
application
makes
expressiveness
desirable
tion
3.3
may
solution
extreme
conditions
corre
sponding
cases
fair
solution
exists
consider
dis
acknowledgments
parate
treatment
constraint
work
supported
nsf
awards
iis-1615706
iis-1513692
exposure
opinions
findings
conclusions
recommendations
ex
pressed
material
author
neces
exposure
sarily
reflect
views
national
science
foundation
can
adversarially
construct
infeasible
constraint
choosing
relevance
ratio
rhs
lies
outside
range
references
lhs
can
achieve
varying
maximum
rhs
occurs
abolfazl
asudehy
hv
jagadishy
julia
stoyanovichz
gautam
das
2017
documents
placed
documents
designing
fair
ranking
schemes
arxiv
preprint
arxiv
1712.09752
2017
2227
research
track
paper
kdd
2018
august
19
23
2018
london
united
kingdom
solon
barocas
andrew
selbst
2016
big
data
disparate
impact
cal
20
niki
kilbertus
mateo
rojas
carulla
giambattista
parascandolo
moritz
hardt
rev
104
2016
671
dominik
janzing
bernhard
sch√∂lkopf
2017
avoiding
discrimination
asia
biega
krishna
gummadi
gerhard
weikum
2018
equity
attention
causal
reasoning
nips
656
666
amortizing
individual
fairness
rankings
arxiv
preprint
arxiv
1805.01788
21
matt
kusner
joshua
loftus
chris
russell
ricardo
silva
2017
counterfac
2018
tual
fairness
nips
4069
4079
garrett
birkhoff
1940
lattice
theory
vol
25
american
mathematical
soc
22
marvin
marcus
rimhak
ree
1959
diagonals
doubly
stochastic
matrices
amelia
butterly
2015
google
image
search
ceo
barbie
first
quarterly
journal
mathematics
10
1959
296
302
female
result
2015
http://www.bbc.co.uk/newsbeat/article/32332603/
23
razieh
nabi
ilya
shpitser
2017
fair
inference
outcomes
arxiv
preprint
google-image-search-for-ceo-has-barbie-as-first-female-result
arxiv
1705.10378
2017
toon
calders
faisal
kamiran
mykola
pechenizkiy
2009
building
classifiers
24
filip
radlinski
paul
bennett
ben
carterette
thorsten
joachims
2009
independency
constraints
data
mining
workshops
icdmw
13
18
redundancy
diversity
interdependent
document
relevance
acm
sigir
jaime
carbonell
jade
goldstein
1998
use
mmr
diversity-based
forum
vol
43
46
52
reranking
reordering
documents
producing
summaries
sigir
335
25
filip
radlinski
robert
kleinberg
thorsten
joachims
2008
learning
diverse
336
https://doi.org/10.1145/290941.291025
rankings
multi-armed
bandits
icml
acm
784
791
elisa
celis
damian
straszak
nisheeth
vishnoi
2017
ranking
26
addam
raff
2009
search
may
find
new
york
times
2009
fairness
constraints
arxiv
preprint
arxiv
1704.06840
2017
http://www.nytimes.com/2009/12/28/opinion/28raff.html
cheng-shang
chang
wen-jyh
chen
hsiang-yi
huang
1999
service
27
matthew
richardson
ewa
dominowska
robert
ragno
2007
predicting
guarantees
input-buffered
crossbar
switches
capacity
decomposition
ap
clicks
estimating
click-through
rate
new
ads
www
521
530
proach
birkhoff
von
neumann
iwqos
ieee
79
86
https://doi.org/10.1145/1242572.1242643
10
charles
clarke
maheedhar
kolla
gordon
cormack
olga
vechtomova
28
stephen
robertson
1977
probability
ranking
principle
ir
journal
azin
ashkan
stefan
b√ºttcher
ian
mackinnon
2008
novelty
diversity
documentation
33
1977
294
304
information
retrieval
evaluation
sigir
659
666
https://doi.org/10.1145/
29
tobias
schnabel
adith
swaminathan
ashudeep
singh
navin
chandak
1390334.1390446
thorsten
joachims
2016
recommendations
treatments
debiasing
learning
11
fanny
dufoss√©
bora
u√ßar
2016
notes
birkhoff
von
neumann
decompo
evaluation
icml
1670
1679
sition
doubly
stochastic
matrices
linear
algebra
appl
497
2016
108
115
30
mark
scott
2017
google
fined
record
2.7
billion
antitrust
rul
12
cynthia
dwork
moritz
hardt
toniann
pitassi
omer
reingold
richard
ing
new
york
times
2017
https://www.nytimes.com/2017/06/27/technology/
zemel
2012
fairness
awareness
itcs
214
226
eu-google-fine
html
13
laura
granka
2010
politics
search
decade
retrospective
31
blake
woodworth
suriya
gunasekar
mesrob
ohannessian
nathan
srebro
information
society
26
2010
364
374
2017
learning
non-discriminatory
predictors
arxiv
preprint
arxiv
1702.06081
14
james
grimmelmann
2011
skepticism
search
neutrality
next
2017
digital
decade
essays
future
internet
2011
435
https://ssrn.com/
32
ke
yang
julia
stoyanovich
2017
measuring
fairness
ranked
outputs
abstract
1742444
ssdbm
2017
15
moritz
hardt
eric
price
nati
srebro
2016
equality
opportunity
33
muhammad
bilal
zafar
isabel
valera
manuel
gomez
rodriguez
krishna
supervised
learning
nips
3315
3323
gummadi
2017
fairness
beyond
disparate
treatment
disparate
impact
learn
16
lucas
introna
helen
nissenbaum
2000
shaping
web
politics
ing
classification
without
disparate
mistreatment
www
1171
1180
search
engines
matters
information
society
16
2000
169
185
34
meike
zehlike
francesco
bonchi
carlos
castillo
sara
hajian
mohamed
mega
17
kalervo
j√§rvelin
jaana
kek√§l√§inen
2002
cumulated
gain-based
evaluation
hed
ricardo
baeza-yates
2017
fa
ir
fair
top-k
ranking
algorithm
ir
techniques
acm
transactions
information
systems
tois
20
2002
cikm
2017
422
446
35
rich
zemel
yu
wu
kevin
swersky
toni
pitassi
cynthia
dwork
2013
18
thorsten
joachims
adith
swaminathan
tobias
schnabel
2017
unbiased
learning
fair
representations
icml
325
333
learning-to-rank
biased
feedback
wsdm
781
789
36
yi
zhang
2005
bayesian
graphical
model
adaptive
information
filtering
phd
19
matthew
kay
cynthia
matuszek
sean
munson
2015
unequal
represen
dissertation
carnegie
mellon
university
tation
gender
stereotypes
image
search
results
occupations
chi
37
indre
zliobaite
2015
relation
accuracy
fairness
binary
3819
3828
https://doi.org/10.1145/2702123.2702520
classification
fatml
workshop
icml
2015
2228