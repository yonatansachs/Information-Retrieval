Fairness in Online Jobs: A Case Study on TaskRabbit
and Google
Sihem Amer-Yahia, Shady Elbassuoni, Ahmad Ghizzawi, Ria Mae Borromeo,
Emilie Hoareau, Philippe Mulhem

To cite this version:
Sihem Amer-Yahia, Shady Elbassuoni, Ahmad Ghizzawi, Ria Mae Borromeo, Emilie Hoareau, et
al.. Fairness in Online Jobs: A Case Study on TaskRabbit and Google. International Conference on
Extending Database Technologies (EDBT), 2020, Copenhagen, Denmark. �10.5441/002/edbt.2020.62�.
�hal-02972559�

HAL Id: hal-02972559
https://hal.science/hal-02972559v1
Submitted on 20 Oct 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Industry and Applications Paper

Fairness in Online Jobs: A Case Study on TaskRabbit and
Google
“Applications” paper
Sihem Amer-Yahia

Shady Elbassuoni

Ahmad Ghizzawi

CNRS, Univ. Grenoble Alpes, France
sihem.amer-yahia@
univ-grenoble-alpes.fr

American University of Beirut,
Lebanon
se58@aub.edu.lb

American University of Beirut,
Lebanon
ahg05@mail.aub.edu

Ria Mae Borromeo

Emilie Hoareau

Philippe Mulhem

UP Open University, Philippines
rhborromeo@up.edu.ph

IAE, Univ. Grenoble Alpes, France
emilie.hoareau@
univ-grenoble-alpes.fr

CNRS, Univ. Grenoble Alpes, France
philippe.mulhem@
univ-grenoble-alpes.fr

ABSTRACT

In online job search, either jobs are ranked for people or people are ranked for jobs. For instance, on Google and Facebook job
search, a potential employee sees a ranked list of jobs while on
TaskRabbit, an employer sees a ranked list of potential employees.
This ranking of jobs or individuals naturally poses the question
of fairness. For instance, consider two different users searching
for a software development job in San Francisco using Google
job search. If the users are shown different jobs based on their
search and browsing history, which could correlate with their
demographics such as race or gender, this may be considered
unfair. Similarly, a ranking of job seekers in NYC might be unfair
if it is biased towards certain groups of people, say where White
Males are consistently ranked above Black Males or White Females. This can commonly happen since such rankings might
depend on the ratings of individuals and the number of jobs they
completed, both of which can perpetuate bias against certain
groups of individuals.
In this paper, we propose to quantify unfairness in ranking
when looking for jobs online. We develop a unified framework
to address group unfairness, which is defined as the unequal
treatment of individuals based on their protected attributes such
as gender, race, ethnicity, neighborhood, income, etc. [11]. To
quantify unfairness for a group, we measure the difference in
rankings between that group and its comparable groups, i.e., those
groups which share at least one protected attribute value with the
given group. For instance, consider the group “Black Females”,
comparable groups would be “Black Males”, “White Females” and
“Asian Females”.
The difference in ranking naturally depends on what is being
ranked, jobs or people, and we formalize various measures of
unfairness on different types of sites (job search sites and online job marketplaces). Figures 1 and 2 illustrate examples of job
ranking on Google job search and people ranking on TaskRabbit,
respectively. For a given query on Google job search, “Home
Cleaning" in location "San Francisco” in Figure 1, we quantify
unfairness in ranking for a given demographic group, “Black
Females”, using Kendall Tau (we also use Jaccard Coefficient in
our data model), between the search results of black females and
all other users in comparable groups, as is done in [12]. To quantify unfairness for “Black Females” on TaskRabbit for the query
"Cleaning Services" in location "New York City", we compute the
average Earth Mover’s Distance [20] between the distribution of
rankings of Black Females and all comparable groups, as in [11].
In our framework, we also compute the difference of exposure
of workers from this demographic group and their relevance in

Online job marketplaces are becoming very popular. Either jobs
or people are ranked by algorithms. For example, Google and
Facebook job search return a ranked list of jobs given a search
query. TaskRabbit and Fiverr, on the other hand, produce rankings of workers for a given query. Qapa, an online marketplace,
can be used to rank both workers and jobs. In this paper, we develop a unified framework for fairness to study ranking workers
and jobs. We case study two particular sites: Google job search
and TaskRabbit. Our framework addresses group fairness where
groups are obtained with any combination of protected attributes.
We define a measure for unfairness for a given group, query and
location. We also define two generic fairness problems that we
address in our framework: quantification, such as finding the k
groups (resp., queries, locations) for which the site is most or least
unfair, and comparison, such as finding the locations at which
fairness between two groups differs from all locations, or finding
the queries for which fairness at two locations differ from all
queries. Since the number of groups, queries and locations can
be arbitrarily large, we adapt Fagin top-k algorithms to address
our fairness problems. To evaluate our framework, we run extensive experiments on two datasets crawled from TaskRabbit and
Google job search.

1

INTRODUCTION

Online job search is gaining popularity as it allows to find people
to hire for jobs or to find jobs to apply for. Many online job
search sites exist nowadays such as Facebook job search1 and
Google job search2 . On those sites, users can find jobs that match
their skills in nearby businesses. On the other hand, freelancing
platforms such as TaskRabbit3 and Fiverr4 are examples of online
job marketplaces that provide access to a pool of temporary
employees in the physical world (e.g., looking for a plumber), or
employees to complete virtual "micro-gigs" such as designing a
logo.
1 https://www.facebook.com/jobs/
2 https://jobs.google.com/about/
3 https://www.taskrabbit.com/
4 https://www.fiverr.com/

© 2020 Copyright held by the owner/author(s). Published in Proceedings of the
23rd International Conference on Extending Database Technology (EDBT), March
30-April 2, 2020, ISBN 978-3-89318-083-7 on OpenProceedings.org.
Distribution of this paper is permitted under the terms of the Creative Commons
license CC-by-nc-nd 4.0.

Series ISSN: 2367-2005

510

10.5441/002/edbt.2020.62

a house cleaner than as a gardener? and which jobs are the most
likely to accept hiring asian females over black females?.
We develop efficient Fagin top-k algorithms to solve our problems. Our algorithms make use of three types of indices: groupbased, query-based, and location-based, that pre-compute unfairness values for combinations of groups, queries and locations,
for faster processing.
To evaluate our framework, we run extensive experiments on
two datasets crawled from Google job search and TaskRabbit.
The choice of these two platforms is justified by our goal to show
the applicability of our framework to two different treatments of
online employment, namely ranking jobs and ranking workers.
We ran 5,361 queries on TaskRabbit and extracted for each query,
the rank of each tasker, their profile pictures, and demographics,
where the number of taskers returned per query was limited
to 50. We processed the results and recorded unfairness values.
We then derived user groups of interest and equivalent Google
search terms from data crawled from TaskRabbit. This resulted
in 20 queries (the top 10 and bottom 10 frequently searched
queries) and their corresponding locations from data crawled in
TaskRabbit. We setup 60 user studies on Prolific Academic5 and
recruited participants, who belong to chosen groups. To control
for noise in search, we asked those participants to use a Google
Chrome extension we developed that automatically executes
on Google the search queries in 10 locations. We processed the
results and recorded unfairness values.
Our results are organized into the two problems we solve: fairness quantification and fairness comparison. On TaskRabbit, we
found that Asian Females and Asian Males are the ones most discriminated against. We also found that Handyman and Yard Work
are the most unfair jobs and that Furniture Assembly and Delivery,
are the fairest and that Birmingham, UK and Oklahoma City, OK
are the least fair while Chicago and San Francisco are the fairest
locations across all jobs. We also quantified the fairest/unfairest
locations for some jobs and the fairest/unfairest jobs for some
locations. Our TaskRabbit results demonstratethe flexibility and
expressiveness of fairness quantification, and provided the ability
to generate hypotheses to be tested on Google job search.
On Google job search, we found that Washington, DC is deemed
the fairest. On the other hand, London, UK is deemed the unfairest
location. For queries, we found that Yard Work jobs are deemed
the most unfair whereas Furniture Assembly jobs are deemed the
most fair.
While fairness quantification resulted in largely known results,
our fairness comparison experiment on both platforms revealed
new results. For instance, on TaskRabbit, in Chicago, Nashville and
San Francisco, Females are treated more fairly than Males, which
differs from the overall comparison. Most results are consistent
between EMD and Exposure. Similarly for Google job search,
most results are consistent between Jaccard and Kendall Tau.
This is quite encouraging and and merits further investigation in
future work.
The paper is organized as follows. We review related work in
Section 2. In Section 3, we present our data model. In Section 4
we describe our unfairness problems and the algorithms we use
to solve these problems. Section 5 describes our case study on
two sites, Google job search and TaskRabbit. Finally, we conclude
and present future work in Section 6.

contrast to comparable groups and then use this as a measure of
unfairness for this group, as in [2, 22].

Figure 1: The unfairness for “Black Females” for the
Google job search query “Home Cleaning" in location "San
Francisco” using Kendall Tau between the search results
of Black Females and all other users in comparable groups
is 0.70+0.50+0.30
= 0.50.
3

Figure 2: The unfairness for “Black Females” for the
query "Cleaning Services" in location "New York City" on
TaskRabbit using Earth Mover’s Distance between ranking distributions of Black Females and its comparable
groups is 0.45+0.25+0.65
= 0.45.
3
Various fairness questions can be formulated either to quantify
how well a site treats groups for different jobs and at different
locations, or to compare groups, queries or locations. Our framework allows us to define two generic fairness problems: quantification, such as finding the k groups (resp., queries, locations)
for which the site is most or least unfair, and comparison, such
as finding the locations at which fairness between two groups
differs from all locations, or finding the queries for which fairness
at two locations differ from all queries. Examples of quantification
questions are: what are the five groups for which Google job search
is most unfair? what are the five fairest queries for women? and at
which locations do Asians have the highest chance to be hired for a
given job?. Examples of comparison questions are: how differently
does TaskRabbit treat men and women and for which queries is the
treatment different? at which locations is it easiest to be hired as

5 https://prolific.co

511

2

RELATED WORK

There is a wealth of work that empirically assessed fairness
in online markets such as crowdsourcing or freelancing platforms [8, 13, 17, 17, 21]. For instance, the authors in [17] analyze
ten categories of design and policy choices through which platforms may make themselves more or less conducive to discrimination by users. In [13], the authors found evidence of bias in two
prominent online freelance marketplace, TaskRabbit and Fiverr.
Precisely, in both marketplaces, they found that gender and race
are significantly correlated with worker evaluations, which could
harm the employment opportunities afforded to the workers on
these platforms. The work in [21] studies the Uber platform to
explore how bias may creep into evaluations of drivers through
consumer-sourced rating systems. They concluded that while
companies like Uber are legally prohibited from making employment decisions based on protected characteristics of workers,
their reliance on potentially biased consumer ratings to make
material determinations may nonetheless lead to a disparate impact in employment outcomes. Finally, discrimination in Airbnb
was studied in [8] and high evidence of discrimination against
African American guests was reported.
In [7], the authors study ethics in crowd work in general. They
analyze recent crowdsourcing literature and extract ethical issues
by following the PAPA (privacy, accuracy, property, accessibility
of information) concept, a well-established approach in information systems. The review focuses on the individual perspective
of crowd workers, which addresses their working conditions and
benefits.
Several discrimination scenarios in task qualification and algorithmic task assignment were defined in [3]. That includes
only accounting for requester preferences without quantifying
how that affects workers, and vice versa. Another discriminatory
scenario in [3] is related to worker’s compensation since a requester can reject work and not pay the worker or a worker can
be under-payed. Discrimination in crowdsourcing can be defined
for different processes.
In [18], the authors study how to reduce unfairness in virtual marketplaces. Two principles must be adapted: 1) platforms
should track the composition of their population to shed light
on groups being discriminated against; and 2) platforms should
experiment on their algorithms and data-sets in a timely manner to check for discrimination. In this same paper, the authors
define four design strategies to help reduce discrimination, a
platform manager should first answer these questions: 1) are
we providing too much information? 2) can we automate the
transaction process further? 3) can we remind the user of discriminatory consequences when they are making a decision? 4)
should the algorithm be discrimination-aware? In question 1),
they address the issue of transparency. Discrimination and transparency might be highly correlated but their correlation has yet to
be studied profoundly. In [3], transparency plug-ins are reviewed.
Those plug-ins disclose computed information, from worker’s
performance to requester’s ratings such as TurkBench [14], and
Crowd-Workers [5]. Such plug-ins might be helpful in a more
detailed study of the effect of transparency on fairness.

To the best of our knowledge, our work is the first to formalize group-fairness, query-fairness, location-fairness, and fairness
comparisons, and conduct an extensive evaluation of job search
on a virtual marketplace and a job search site. Further statistical and manual investigations are necessary for causality and
explainability. Our goal is to reduce initial manual effort by providing necessary tools to assess fairness.
Fairness has been trending in research for the last few years
as we increasingly rely on algorithms for decision making. Bias
has been identified as a major risk in algorithmic decision making [4, 11, 16, 23, 27]. One algorithmic solution is based on the
formalization in [16] to quantify unfairness. To detect unfairness
in algorithms, a framework [24] for "unwarranted associations"
was designed to identify associations between a protected attribute, such as a person’s race, and the algorithmic output using
the FairTest tool. In [11], the notion of unfairness was defined as
a disparity in treatment between different groups of people based
on their protected attributes (i.e., what is commonly referred to
as group unfairness). In this context, to assess unfairness mathematically, one needs to compare distributions of decisions across
different groups of people. In our work, we adapt the definition of
unfairness in [11]. However, rather than trying to fix it, the goal
of our work is to just reveal any unfairness by the ranking process, which in some cases might be positive discrimination [19]
where certain disadvantaged individuals are favored based on
their protected attributes.
There is a wealth of work on addressing fairness of ranking in
general (for example [6, 16, 22, 24–26]). Unlike our work, the majority of these works that focus on group fairness either assume
the presence of pre-defined groups based on protected attributes
of users, or the presence of ranking constraints that bound the
number of users per protected attribute value in the top-k ranking. On the other hand, the work in [2] focuses on addressing
amortized individual fairness in a series of rankings. In [15], the
authors introduce subgroup fairness and formalize the problem
of auditing and learning classifiers for a rich class of subgroups.
Our work differs in many ways: we are interested in ranking
individuals and not classifying them, as well as ranking jobs and
we seek to quantify the fairness of jobs, locations and groups and
compare fairness across different dimensions.
In [1], the authors develop a system that helps users inspect
how assigning different weights to ranking criteria affects ranking. Each ranking function can be expressed as a point in a multidimensional space. For a broad range of fairness criteria, including proportionality, they show how to efficiently identify groups
(defined as a combination of multiple protected attributes). Their
system tells users whether their proposed ranking function satisfies the desired fairness criteria and, if it does not, suggests the
smallest modification that does.
In [9], the authors studied fairness of ranking in online job
marketplaces. To do this, they defined an optimization problem
to find a partitioning of the individuals being ranked based on
their protected attributes that exhibits the highest unfairness by
a given scoring function. They used the Earth Mover’s Distance
between score distributions as a measure of unfairness. Unlike
other related work, we did not assume a pre-defined partitioning of individuals and instead developed two different fairness
problems, one aiming at quantifying fairness and the other at
comparing it.

3 FRAMEWORK
3.1 Unfairness Model
On any given site, we consider a set of groups G, a set of jobrelated queries Q, and a set of locations L. We associate to each
group д a label label(д) in the form of a conjunction of predicates
a = val. We use A(д) to refer to all attributes used in label(д). For

512

Table 1: Top-3 results for 10 users for the query "Home
Cleaning" in location "San Francisco" on a search engine.

example, if label(д) is (gender = male) ∧ (ethnicity = black), we
have: A(д) is {gender, ethnicity}. We define variants(д, a) where
a ∈ A(д) as all groups whose label differs from д on the value of a.
For instance, variants(д, gender) contains a single group whose label is (gender = female) ∧ (ethnicity = black), variants(д, ethnicity)
contains two groups whose labels are (gender = male) ∧ (ethnicity
= asian) and (gender = male) ∧ (ethnicity = white), respectively.
We define the set of comparable groups for a group д as {д ′ ∈
∪a ∈A(д)variants(д, a)}. In our example, it is variants(д, gender)∪
variants(д, ethnicity). This notion of comparable groups can be
more easily leveraged for explanations. To consider other notions,
we believe we would need to extend only our fairness model, and
not the full framework.
Each query q ∈ Q contains a set of keywords such as “Home
Cleaning” or “Logo Design”. The same query can be asked at
different geographic locations l ∈ L. In some applications such
as TaskRabbit, a query will be used to refer to a set of jobs in
the same category such as Handyman, Furniture Assembly and
Delivery services.
We denote by d <д,q,l > the unfairness value of the triple <
д, q, l >. We discuss next how this unfairness value is computed
for different types of sites.

3.2

Worker

Top-3

w1
w2
w3
w4
w5
w6
w7
w8
w9
w10

b, d, e
d, b, e
a, b, c
b, a, c
a, b, c
d, a, b
a, b, d
d, a, b
a, b, c
a, b, c

Unfairness Measure for Search Engines

In a search engine such as Google Search, each user u ∈ д is
associated with a ranked list of search results Eql (u). We compute
unfairness of д as:
d <д,q,l > = avgд′ DIST (д, д ′ ) ∀д ′ ∈ ∪a ∈A(д)variants(д, a) (1)
A common way to compare search results is to use measures
such as Jaccard Index or Kendall Tau [12]. Hence, we define
DIST (д, д ′ ) as one of the following two:
• avg τ (Eql (u), Eql (u ′ )), ∀u ∈ д, ∀u ′ ∈ д ′ , where τ (Eql (д), Eql (д ′ ))
u,u ′

is the Kendall Tau between the ranked lists Eql (u) and
Eql (u ′ ).
• avg JACCARD(Eql (u), Eql (u ′ )),
u,u ′

∀u ∈ д, ∀u ′ ∈ д ′ , where JACCARD(Eql (u), Eql (u ′ )) is the
Jaccard Index between the ranked lists Eql (u) and Eql (u ′ ).

In Table 1, we display a toy example of the top-3 results for
10 users on a search engine for the query "Home Cleaning" in
location "San Francisco". Figure 3 shows how the unfairness value
for the group "Black Females" is computed using Jaccard index.
In the figure, the Jaccard index between every Black Female user
and Asian Female user is computed and then average of the
Jaccard index is used to measure unfairness value between the
two groups "Black Females" and "Asian Females". To compute the
overall unfairness value for the group "Black Females", the same
computation must be done between Black Females and all other
comparable groups, namely "Black Males" and "White Females"
and then the average of the individual unfairness values between
groups is taken.

3.3

Figure 3: The partial unfairness in a search engine for
“Black Females” in Table 2 with respect to one of its comparable groups, "Asian Females", using Jaccard Index is
0.8+0.5 = 0.65.
2
d <д,q,l > , we can use one of two methods: Earth Mover’s Distance
(EMD) [20] and Exposure [2, 22].
3.3.1 EMD Unfairness. In the EMD notion of unfairness, the
unfairness for a group д for query q at location l is computed as
the distance between the score distributions of workers in group
д and all its comparable groups д ′ ∈ ∪a ∈A(д)variants(д, a) as
follows:
d <д,q,l > = avgд′ DIST (д, д ′ ) ∀д ′ ∈ ∪a ∈A(д)variants(д, a) (2)

Unfairness Measure for Online Job
Marketplaces

where
DIST (д, д ′ ) = EMD(h(д, fql ), h(д ′, fql ))

In online marketplaces such as TaskRabbit, we are given a set
of workers W, and a scoring function fql : W → [0, 1]. Each
worker w ∈ W is ranked based on her score fql (w). To measure

where h(д, fql ) is a histogram of the scores of workers in д using
fql .

513

score available), we rely on the rank of workers rank(w, q, l) to
compute their relevance for a query and location. The rank of
workers for a pair (q, l) is available since it can be observed in
the results of running q at l. We can hence compute relql (w), the
relevance score of a worker as follows:

In Table 2, we show a toy example consisting of 10 workers
looking for a "Home Cleaning" job in San Francisco and their
protected attributes. The ranking of these workers is shown in
Table 3. Figure 4 illustrates how the EMD unfairness of Black
Females, д, is calculated. Since A(д) is Gender and Ethnicity, the
comparable groups in the toy example are Black Males, Asian
Females and White Females.

rank(w, q, l)
N
where rank(w, q, l) denotes the rank of worker w for query q at
location l as shown in Table 3, and N is the number of workers
in the resultset, here set to 10. The relevance scores generated
for all workers in our example are reported in Table 3.
To compute the EMD unfairness of Black Females for this
query at this location, we generate a histogram for Black Females
and each of the comparable groups based on the relevance scores
relql (w) computed for workers. We then compute the average
EMD between the histogram of Black Females and each of the
comparable groups’ histograms.
relql (w) = 1 −

Table 2: Example of 10 workers looking for a "Home Cleaning" job in San Francisco and their protected attributes
Worker

Gender

Nationality

Ethnicity

w1*
w2
w3*
w4
w5
w6*
w7
w8*
w9
w10*

Female
Male
Female
Male
Female
Male
Female
Male
Male
Female

America
America
America
Other
Other
America
America
Other
Other
America

Asian
White
White
Asian
Black
Black
Black
Black
White
White

3.3.2 Exposure Unfairness. In the exposure notion of fairness,
the intuition is that higher ranked workers receive more exposure
as people tend to only examine top-ranked results. Thus, each
worker receives an exposure inversely proportional to her rank
d <д,q,l > as follows. First, for every w ∈ д, we compute her
exposure as:

Table 3: Ranking of the 10 workers for the query "Home
Cleaning" in San Francisco on an online job marketplace
Ranking

Worker

fql (w)

1
2
3
4
5
6
7
8
9
10

w3
w8
w6
w2
w1
w4
w7
w5
w9
w10

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

expql (w) =

1
loд(1 + rank(w, q, l))

We also compute the relevance of worker w ∈ д as relql (w) as
defined above. Now, the exposure of a group of workers д is set
to:
expql (д) = Í

l
w ∈д expq (w)

Í

д ′ ∈д∪a∈A(д)var iant s(д,a)

l
w ∈д ′ expq (w)

Í

Similarly, we define the relevance of a group д as:
Í
l
w ∈д relq (w)
relql (д) = Í
Í
l
д ′ ∈д∪a∈A(д)var iant s(д,a) w ∈д ′ relq (w)
Next, we assume that each group д should receive exposure
proportional to its relevance. We thus measure deviation from
the ideal exposure using the L1-norm as the unfairness of a group
д: d <д,q,l > = |expql (д) − relql (д)|.
Figure 5 illustrates how the exposure unfairness of Black Females, д, is calculated. To compute the exposure unfairness of
Black Females for this query in the given location, we compute
the exposure and relevance of all Black Female workers (bold in
Table 2) and the workers belonging to their comparable groups (*
in Table 2) using fql (w) and ranking shown in 3. We then sum up
the exposure and relevance values for all Black Females workers
and the comparable groups separately.

3.4

Notation Generalization

We have used d <д,q,l > to refer to the unfairness for group д for
the job-related query q at location l. This value is obtained by
contrasting the ranking for group д with the ranking of all its
comparable groups. Unfairness can also be computed for several job-related queries and at multiple locations. For a set of
queries Q ⊆ Q and a set of locations L ⊆ L, we can compute the
unfairness for group д as follows:

Figure 4: The unfairness of “Black Females” based on the
ranking in Table 3 using EMD is 0.70+0.50+0.30
= 0.50.
3
Since the actual scores of each worker for a query and location,
fql (w) is not always available (no job marketplace makes that

d <д,Q, L> = avgq ∈Q,l ∈L d <д,q,l >

514

site is least unfair with respect to all queries at all locations or
to answer the question: Out of Black Males, Asian Males, Asian
Females, and White Females, what are the 2 groups for which the
site, say Google job search, is the most unfair?
When R is a set of queries, the problem, referred to as queryfairness returns k queries which are the most/least unfair. This
instance of the problem can address questions such as what are
the 5 least unfair queries at all locations? or which 2 queries are
black males most likely to get in the West Coast?
Finally, when R refers to locations, the problem, referred to as
location-fairness addresses questions such as Which 3 locations
are the easiest to find a job at? or out of NYC, Boston and Washington DC, what is the least unfair location for women looking for an
event staffing job on a given site, say TaskRabbit?
Our second problem formulation aims to capture comparisons
between two dimensions. It admits two dimensions to compare,
e.g. males and females, or NYC and San Francisco, or cleaning
services and event staffing, and it returns a breakdown of comparison dimensions into sub-dimensions whose fairness comparison
differs from the comparison of the input dimensions.
Figure 5: Computing the unfairness for “Black Females”
based on the ranking in Table 3. The exposure of Black
0.94 = 0.19. Its relevance is 0.5 = 0.15. Its
Females is 0.94+4.0
0.5+2.9
unfairness is 0.19 − 0.15 = 0.04.

Problem 2 (Fairness Comparison). Given two comparison
dimensions r 1 and r 2 , and a breakdown dimension B, return all
b ∈ B s.t. d <r 1,b > >= d <r 2,b > ∧ d <r 1, B > <= d <r 2, B >
∨d <r 1,b > <= d <r 2,b > ∧ d <r 1, B > >= d <r 2, B >
The first instance of our comparison problem is referred to
as group-comparison in which r 1 and r 2 are demographic groups.
For example, when r 1 refers to Males, r 2 to Females, and B to
locations, fairness comparison returns all locations where the
comparison between males and females differs from that of all
males and females. Table 4 shows an example. In this case, our
problem returns the unfairness values of males and females at
those two locations that compare differently from all locations.

Similarly, we could compute the unfairness for a set of groups
G ⊆ G at a location l ∈ L for all queries in Q ⊆ Q as follows:
d <G,Q,l > = avgд ∈G,q ∈Q d <д,q,l >
Finally, we could also compute the unfairness for a set of groups
G ⊆ G for a given query q ∈ Q at all locations L ⊆ L as follows:
d <G,q, L> = avgд ∈G,l ∈L d <д,q,l >

4

Table 4: Comparison between Male and Female workers
in Oklahoma City and Salt Lake City differ from the overall

PROBLEMS AND ALGORITHMS

In this section, we first provide two generic problem formulations
that capture the variety of group fairness questions we may ask
(Section 4.1). We then describe the algorithms we designed to
solve those problems (Section 4.2).

4.1

Problem Variants

To formulate a generic problem, we will use the term dimension
to refer to one of group, query or location. Our first problem aims
to quantify how well a site treats groups for different queries and
at different locations. The problem returns instances of a chosen
dimension, e.g., groups, and aggregates their unfairness values
along the two others, e.g., queries and locations.

Group-comparison

Males

Females

All
Oklahoma City, OK
Salt Lake City, UT

0.48
0.853
0.933

0.74
0.732
0.553

The second instance of our comparison problem is referred
to as query-comparison. For example, if r 1 is lawn mowing and
r 2 furniture mounting and B is ethnicity, fairness comparison
returns all ethnicities for which the comparison between lawn
mowing and furniture mounting differs from the whole population. For instance, our problem finds that ethnicity Black must be
returned because the unfairness values between lawn mowing
and furniture mounting for blacks compare differently from all
ethnicities.
The third instance of our comparison problem is referred to
as location-comparison. For example when r 1 is California, and r 2
is Arizona, and B is outdoor home services, fairness comparison
returns all queries related to outdoor home services (e.g., lawn
mowing, garage cleaning, patio painting, etc), for which the comparison in California and Arizona differs from all outdoor home
services. Our problem returns the jobs garage cleaning and patio
painting because the unfairness values between California and

Problem 1 (Fairness Quantification). Given R a dimension
to be returned and two other dimensions AGG1 and AGG2 to be
aggregated, return the k results in R for which the site is most/least
unfair, where the unfairness for each result r ∈ R, d <AGG1,AGG2,r > ,
is computed as: avgagg1∈AGG1,agg2 ∈AGG2 d <aдд1,aдд2,r >
There are 3 instances of this problem: one where R is a set of
groups, one where it is a set of queries, and the third one where
it is a set of locations.
When R is a set of groups, the problem, referred to as groupfairness, returns k groups for which the site is most/least unfair.
For instance, it could be used to find the 5 groups for which the

515

Algorithm 1 findTopKGroups(G: a set of groups, Q: a set of
queries, L: a set of locations, k: an integer)

Arizona for those two jobs are different from all outdoor home
services.

4.2

1: topk ← createMinHeap()

Algorithms

2: Initialize |Q | ∗ |L| cursors to 0
3: τ ← +∞

The computational complexity of our problems calls for designing
scalable solutions. In this section, we propose adaptations of
Fagin’s algorithms to solve our problems. We first describe the
indices we generate: group-based, query-based, and location-based.
The group-based indices associate to every (q, l) pair an inverted index where groups are sorted in descending order based
on d <д,q,l > .
The query-based indices associate to every (д, l) pair an inverted index where queries q are sorted in descending order based
on d <д,q,l > .
The location-based indices associate to every (д, q) pair an
inverted index where locations l are sorted in descending order
based on d <д,q,l > . Table 5 shows an illustration of the three
types of indices.

4: while topk.minV alue() < τ or topk.size() < k do

τ ←0
for q ∈ Q do
7:
for l ∈ L do
8:
(д, d <д,q,l > ) ← I (q,l ) .f ind(cur (q,l ) )
▷ Read
entry in I (q,l ) pointed to by cursor cur (q,l )
9:
d <д,Q, L> ← d <д,q,l >
10:
τ ← τ + d <д,q,l >
11:
for q ′ ∈ Q do
12:
for l ′ ∈ L do
13:
if q ′ , q or l ′ , l then
14:
d <д,q ′,l ′ > ← I (q ′,l ′ ) .f ind(д)
▷
Perform a random access on I (q ′,l ′ ) to retrieve the unfairness
value of д for the pair (q ′, l ′ )
15:
d <д,Q, L> ← d <д,Q, L> + d <д,q ′,l ′ >
16:
end if
17:
end for
18:
end for
19:
d <д,Q, L> ← d <д,Q, L> /(|Q | ∗ |L|)
20:
if topk.size() < k then
21:
topk.insert(д, d <д,Q, L> )
22:
else
23:
if topk.minV alue() < d <д,Q, L> then
24:
topk.pop()
25:
topk.insert(д, d <д,Q, L> )
26:
end if
27:
end if
28:
cur (q,l ) ← cur (q,l ) + 1
29:
end for
30:
end for
31:
τ ← τ /(|Q | ∗ |L|)
32: end while
33: return topk
5:
6:

Table 5: Group-based, query-based, location-based indices

.
дj
.

I (q,l )
.
d(дj , q, l)
.

.
qj
.

I (д,l )
.
d(q j , д, l)
.

.
lj
.

I (д,q)
.
d(д, q, l j )
.

Algorithm 1 is an adaption of Fagin’s Threshold Algorithm
[10] for the group-fairness instance of our problem. It finds the k
groups for which the site is most unfair. The algorithm takes as
input a set of groups G, a set of queries Q and a set of locations
L, and returns k groups. It makes use of the group-based indices
(Table 5).
All other instances of Problem 1 including query-fairness,
location-fairness and their bottom k versions, are adaptations of
Algorithm 1.
Algorithm 2 solves our second problem (Problem 2) for the
group-comparison instance of our problem. It takes as input 2
groups д1 and д2 and a breakdown dimension L. It first calls Algorithm 3 to compute the fairness values of д1 and д2 for all values
of L and all queries Q. It then calls the query-based index to sum
up all the values for all the queries by scanning the index for each
location and for each of the two groups. Finally, it returns only
those locations for which the order on unfairness values for the
two groups is reversed. All other instances of Problem 2 including
query-comparison and location-comparison are adaptations of
Algorithm 2.
Algorithm 3 computes the fairness for a group д for all queries
in Q and all locations in L. It takes as input a group д, a set
of queries Q and a set of locations L, and returns the average
unfairness value for д over all queries and locations.

5

EXPERIMENTS

Our experiments use real data collected from TaskRabbit and
Google Search and were conducted from June to August 2019.
We first describe the overall setup for each platform and then
report the results.

5.1

Figure 6: Flow of TaskRabbit Experiments

TaskRabbit is supported in 56 different cities mostly in the US.
For each location, we retrieved all jobs offered in that location.
We thus generated a total of 5,361 job-related queries, where
each query is a combination of a job and a location, e.g., Home
Cleaning in New York.

Experimental setup

5.1.1 TaskRabbit setup. TaskRabbit is an online marketplace
that matches freelance labor with local demand, allowing consumers to find immediate help with everyday tasks.

516

Figure 7: Gender breakdown

Figure 8: Ethnic breakdown

Algorithm 2 CompareGroups(Groups: д1 , д2 , L: a set of locations
as breakdown, Q: a set of queries)

(AMT)6 to indicate the gender and ethnicity of the TaskRabbit
taskers based on their profile pictures. The taskers were given
pre-defined categories for gender = {Male, Female} and ethnicity
= {Asian, Black, White}. Each profile picture was labeled by three
different contributors on AMT and a majority vote determined
the final label.
The gender and ethnic breakdowns of the taskers in our dataset
are shown in Figures 7 and 8. Overall, we had a total of 3,311
unique taskers in our crawled dataset, the majority of which were
male (≈ 72%) and white (≈ 66%).

1: loc ← ∅
2: d <д1,Q, L> ← ComputeGroupUnfairness(д1 , Q, L)

3: d (<д2,Q, L> ← ComputeGroupUnfairness(д2 , Q, L)
4: for l ∈ L do

sum 1 ← 0
sum 2 ← 0
7:
cur 1 ← 0
8:
cur 2 ← 0
9:
for q ∈ Q do
10:
sum 1 + = I (д1,l ) .f ind(cur 1 )
11:
sum 2 + = I (д2,l ) .f ind(cur 2 )
12:
cur 1 ← cur 1 + 1
13:
cur 2 ← cur 2 + 1
14:
end for
15:
if reversed(sum1 , sum2 , d <g1,Q,L> , d <g2 ,Q,L> ) then
16:
loc+ = l
17:
end if
18: end for
19: return loc
5:

6:

5.1.2 Google Search setup. Google Search personalizes queries
based on a user’s profile which includes user data, activity, and
saved preferences. While personalization can be beneficial to
users, it may introduce the possibility of unfairness, which we
aim to observe.

Algorithm 3 ComputeGroupUnfairness(д: a group, Q: a set of
queries, L: a set of locations)
1: sum ← 0
2: for q ∈ |Q | do

for l ∈ |L| do
sum ← sum + I (q,l ) .f ind(д)
▷ Perform a random
access on I (q,l ) to retrieve the unfairness value of д for the
pair (q, l)
5:
end for
6: end for
7: return sum/(|Q | ∗ |L|)

3:
4:

Figure 9: Flow of Google job search Experiments
We designed the experiments to ensure that variations in the
search results are largely based on differences in profiles rather
than other known noise sources identified in related work such
as carry-over-effect, geolocation, distributed infrastructure, and
A/B testing [12].
The flow of the Google Search experiment is summarized in
Figure 9. We first derived user groups of interest and equivalent
Google search terms from data crawled from TaskRabbit. We then
setup user studies on Prolific Academic7 and recruited participants, who belong to those groups. We asked those participants

Figure 6 summarizes the flow of the TaskRabbit experiment.
Our algorithms are encapsulated in the F-Box. For each one of
the 5,361 queries, we extracted the rank of each tasker, their
badges, reviews, profile pictures, and hourly rates, where the
number of taskers returned per query was limited to 50. Since
the demographics of the taskers were not readily available on
the platform, we asked workers on Amazon Mechanical Turk

6 https://mturk.com
7 https://prolific.co

517

Table 6: Sample TaskRabbit queries and equivalent Google search terms
TaskRabbit Query

Location

Equivalent Google Search Terms

run errand

London, UK

yard work

New York City, NY

run errand jobs near London UK, errand service jobs near London UK, errand runner jobs near
London, UK, errands and odd jobs near London, UK, jobs running errands for seniors near
London, UK
yard work jobs near New York City, NY, yard worker near New York City, NY, lawn work
needed near New York City, NY, yard help needed near New York City, NY, yard work help
wanted near New York City, NY
Table 7: Number of locations per job

to use our Google Chrome extension that automatically executes
on Google the search queries derived. Finally, we processed the
results and provided them as input to the F-Box and recorded
unfairness values.
Search Queries. For our Google Search experiments, we selected 20 queries (the top 10 and bottom 10 frequently searched
queries) and their corresponding locations from data crawled in
TaskRabbit. From this list, we chose those from 10 unique locations. We then generated equivalent search terms using Google
Keyword Planner, a tool that outputs a list of search terms similar
or related to a given search string and a location. We shortlisted
50 formulations for each query, manually examined them, then
chose 5 search terms whose results are similar to the original
term. Table 6 shows sample queries from TaskRabbit and their
equivalent Google search terms.

Job

Location

yard work
general cleaning
event staffing
moving job
run errand

4
3
1
1
1

distributed infrastructure and different geolocations. The search
results are then inserted to a Google Sheets document. We emphasized to the participants that we store no identifying information
about them.

Groups. The combination of pre-defined categories for gender
= {Male, Female} and ethnicity = {Asian, Black, White} results in
six groups: Asian Male, Asian Female, Black Male, Black Female,
White Male, and White Female.
We recruited an average of 3 participants per study through
Prolific Academic, a crowdsourcing platform that allows researchers
to recruit participants who have been categorized through the
platform’s screening mechanism.
User Study. Given the search terms and the groups, we have a
total of 60 studies. Each study is composed of two tasks. In the
first task, a participant is asked to set her browsing language to
English and install our Google Chrome extension that runs the
search terms. Participants who are able to successfully complete
the first task are invited to do a second task where they are asked
whether they think the instructions of the first task were clear
and whether the reward is fair. The reward for each task is 0.50
GBP.
Given the distribution of workers on Prolific Academic, we
ended up with 10 locations, namely London, UK, New York City,
NY, Los Angeles, CA, Boston, MA, Bristol, UK, Charlotte, NC,
Pittsburg, PA, Birmingham, UK, Manchester, UK and Detroit, MI.
For those 10 locations, we have five categories of jobs: yard work,
general cleaning, event staffing, moving job and run errand. Table
7 shows the number of locations per job that we collected search
results for.
Google Chrome Extension and noise handling. We developed a
Google Chrome extension that automatically executes the Google
search terms. The extension runs the five search terms every 12
minutes to minimize noise due to the carry-over effect. Meanwhile, every search term is executed at least twice to account for
noise caused by A/B testing. The extension also sets the browser’s
location to a fixed location and uses a proxy so that all queries
originate from the same location thus minimizing noise caused by

518

5.2

Fairness quantification

5.2.1 TaskRabbit fairness quantification. We report the results
of solving our fairness quantification problem (Problem 1 in
Section 4.1) for groups, queries and locations using both EMD
and exposure to measure unfairness (see Sections 3.3.1 and 3.3.2
for their formal definitions).
Table 8 reports all groups in TaskRabbit ranked by their decreasing unfairness values (both EMD and exposure). We can
see that the two measures agree on the top 7 groups for whom
TaskRabbit is the most unfair: Asian Females and Asian Males are
the ones most discriminated against.
Table 9 reports all job types in TaskRabbit ranked by their
decreasing unfairness values (both EMD and exposure). The two
measures largely agree on the ranking showing that Handyman
and Yard Work are the most unfair jobs and that Furniture Assembly
and Delivery, are the fairest.
Since the number of locations is large, we report the top and
bottom 10 locations in Tables 10 and 11 respectively. The results
show that Birmingham, UK and Oklahoma City, OK are the least
fair while Chicago and San Francisco are the fairest locations across
all jobs.
We also report the fairest/unfairest locations for some jobs and
the fairest/unfairest jobs for some locations. For Handyman and
Run Errands, the fairest location is San Francisco Bay Area, CA for
both when using EMD and, when using exposure, it is Boston, MA
for Handyman, and San Francisco Bay Area, CA for Run Errands.
The unfairest location for both jobs is Birmingham, UK when using
EMD.
For Birmingham, Detroit, and Nashville, the fairest jobs are
Delivery and Furniture Assembly for all, and the unfairest are Yard
Work, General Cleaning, and General Cleaning, respectively. For
Philadelphia, San Diego and Chicago, the fairest jobs are Delivery,
Furniture Assembly, and Delivery, respectively, and the unfairest is
Yard Work for Birmingham, Detroit, and Run Errands for Nashville.

Table 8: EMD and Exposure of all groups in TaskRabbit,
ranked from the unfairest to the fairest.
Group

EMD

Group

Exposure

Asian Female
Asian Male
Black Female
Asian
Black Male
White Female
Black
Male
Female
White
White Male

0.876
0.755
0.726
0.694
0.578
0.542
0.498
0.468
0.468
0.448
0.421

Asian Female
Asian Male
Black Female
Asian
Black Male
White Female
Black
Female
White Male
Male
White

0.821
0.662
0.615
0.594
0.413
0.359
0.341
0.299
0.154
0.117
0.104

In summary, our results demonstrate the flexibility and expressiveness provided by solving the fairness quantification problem
for groups, queries and locations. They also provide the ability to
generate hypotheses to be tested across platforms, in our case from
TaskRabbit to Google job search.
5.2.2 Google fairness quantification. We ran our unfairness
quantification algorithm (Algorithm 1) on the data crawled from
Google Search. Our algorithm found that regardless of the metrics
we use, Kendall Tau or Jaccard Index, the most discriminated
against group is White Females and the least is Black Males. This
indicates that search results between White Females were the
most different, whereas those for Black Males were the most
similar.
When quantifying unfairness for locations, we found that
Washington, DC is deemed the fairest indicating no difference in
search results between users at this location using both Jaccard
Index and Kendall Tau. On the other hand, London, UK is deemed
the unfairest location.
Finally, for queries, we found that using both metrics, Yard
Work jobs are deemed the most unfair whereas Furniture Assembly
jobs are deemed the most fair.

Table 9: EMD and Exposure for all jobs in TaskRabbit,
ranked from the unfairest to the fairest.
Job

EMD

Job

Exposure

Handyman
Event Staffing
General Cleaning
Yard Work
Moving
Delivery
Furniture Assembly
Run Errands

0.692
0.639
0.611
0.672
0.604
0.499
0.541
0.519

Handyman
Event Staffing
General Cleaning
Yard Work
Moving
Furniture Assembly
Delivery
Run Errands

0.515
0.504
0.456
0.5
0.418
0.383
0.331
0.352

5.3

Table 10: 10 unfairest locations using EMD and Exposure,
ranked from the unfairest to the fairest.
City

EMD

City

Exposure

Birmingham, UK
Oklahoma City, OK
Bristol, UK
Manchester, UK
New Haven, CT
Milwaukee, WI
Indianapolis, IN
Nashville, TN
Detroit, MI

1
0.998
0.91
0.851
0.838
0.824
0.815
0.808
0.806

Birmingham, UK
Oklahoma City, OK
Bristol, UK
Manchester, UK
New Haven, CT
Memphis, TN
Milwaukee, WI
Charlotte, NC
Nashville, TN

0.926
0.819
0.761
0.739
0.67
0.668
0.668
0.643
0.637

Table 12: Comparison between Male and Female workers
after including locations using Exposure. The listed locations are the ones for which Females are treated more fairly
than Males, which differs from the overall comparison.

Table 11: 10 fairest locations using EMD and Exposure,
ranked from the fairest to the unfairest.
City

EMD

City

Exposure

Chicago, IL
San Francisco, CA
Washington, DC
Los Angeles, CA
Boston, MA
Atlanta, GA
Houston, TX
Orlando, FL
Philadelphia, PA
San Diego, CA

0.274
0.286
0.329
0.33
0.353
0.4
0.417
0.431
0.45
0.454

Chicago, IL
San Francisco, CA
Boston, MA
Washington, DC
Los Angeles, CA
Houston, TX
Atlanta, GA
San Diego, CA
Orlando, FL
Philadelphia, PA

0.107
0.12
0.169
0.174
0.189
0.217
0.234
0.241
0.242
0.273

Fairness comparison

5.3.1 TaskRabbit fairness comparison. We report the results
of solving our fairness comparison problem (Problem 2 in Section 4.1) in Tables 12, 13, 14 and 15. The tables only report the
locations, demographics, and jobs that differ from the overall comparison.

Group-comparison

Males

Females

All
Charlotte, NC
Chicago, IL
Nashville, TN
Norfolk, VA
San Francisco Bay Area, CA
St. Louis, MO

0.117
0.399
0.062
0.330
0.331
0.084
0.255

0.299
0.345
0.062
0.309
0.168
0.084
0.190

Table 13: Comparison between Lawn Mowing and Event
Decorating workers after including Ethnicity using EMD.
Caucasians are the ones for which the comparison between
Lawn Mowing jobs and Event Decorating jobs is different
from the whole population, showing that Lawn Mowing
jobs are fairer than Event Decorating for Caucasians.
Job-comparison

Lawn Mowing

Event Decorating

All
White

0.674
0.552

0.613
0.569

In summary, we can conclude that overall, EMD and Exposure
yield the same observations when solving the fairness comparison
problem on TaskRabbit.

519

Table 18: Comparison between Running Errands jobs
and General Cleaning jobs after including Ethnicity using
Kendall Tau.

Table 14: Comparison between Lawn Mowing and Event
Decorating jobs after including Ethnicity using Exposure.
Unlike Table 13, in this case blacks are the ones for whom
Lawn Mowing jobs are fairer than Event Decorating. This
warrants further investigation in the future.
Job-comparison

Lawn Mowing

Event Decorating

All
Black

0.500
0.445

0.442
0.453

Job-comparison

Running Errands

General Cleaning

All
Black
Asian

0.927
0.927
0.925

0.926
0.950
0.938

Table 19: Comparison between Running Errands jobs and
General cleaning jobs after including Ethnicity using Jaccard. The results differ from those reported in Table 18. This
warrants further investigation in the future.

Table 15: Comparison between San Francisco Bay Area
and Chicago after including General Cleaning jobs using
EMD. San Francisco is shown to be fairer for all jobs but the
trend is inverted for the listed jobs.
Location-comparison

San Francisco Bay Area, CA

Chicago, IL

All
Back To Organized
Organize & Declutter
Organize Closet

0.213
0.198
0.224
0.174

0.233
0.135
0.191
0.153

Job-comparison

Running Errands

General Cleaning

All
Black

0.902
0.903

0.887
0.94

Table 20: Comparison between Boston, MA and Bristol,
UK after including General Cleaning jobs using Kendall
Tau. This result is similar to the one reported in Table 21.

5.3.2 Google fairness comparison. Similarly to TaskRabbit,
we report the results of solving our fairness comparison problem
(Problem 2 in Section 4.1) in Tables 16, 17, 18, 19, 20, and 21. The
tables show the cases that differ from the overall comparison.
Table 16: Comparison between Male and Female workers after including locations using Kendall Tau. The listed
locations are the ones for which Females are treated more
fairly than Males, which differs from the overall comparison.

Group Comparison

Boston, MA

Bristol, UK

All
office cleaning jobs
private cleaning jobs

0.641
0.735
0.572

0.689
0.627
0.398

Table 21: Comparison between Boston, MA and Bristol,
UK after including General Cleaning jobs using Jaccard.
This result is similar to the one reported in Table 20.

Group-comparison

Males

Females

Group Comparison

Boston, MA

Bristol, UK

All
Birmingham, UK
Bristol, UK
Detroit, MI
New York City, NY

0.537
0.906
0.921
0.928
0.913

0.552
0.901
0.918
0.901
0.906

All
private cleaning jobs

0.447
0.403

0.603
0.364

6

Table 17: Comparison between Male and Female workers
after including locations using Jaccard. The results differ
from the ones in Table 16 because the overall results differ.
This warrants further investigation in the future.
Group-comparison

Males

Females

All
Boston, MA
Charlotte, NC
London, UK
Los Angeles, CA
Manchester, UK
Pittsburgh, PA

0.395
0.894
0.893
0.776
0.875
0.869
0.877

0.393
0.896
0.901
0.785
0.878
0.875
0.88

CONCLUSION

We develop a framework to study fairness in job search and a
detailed empirical evaluation of two sites: Google job search and
TaskRabbit. We formulate two generic problems. Our first problem returns the k least/most unfair dimensions, i.e., the k groups
for which a site is most/least unfair, the k least/most unfair jobs
(queries), or the k least/most unfair locations. Our second problem captures comparisons between two dimensions. It admits two
dimensions to compare, e.g. males and females, or NYC and San
Francisco, or cleaning services and event staffing, and it returns a
breakdown of those dimensions that exhibits different unfairness
values (for instance, on TaskRabbit, while females are discriminated against when compared to males, this trend is inverted
in California). We apply threshold-based algorithms to solve our
problems. We report the results of extensive experiments on real
datasets from TaskRabbit and Google job search.
Our framework can be used to generate hypotheses and verify
them across sites. That is what we did from TaskRabbit to Google
job search. It can also be used to verify hypotheses by solving the
comparison problem. As a result, one could use it in iterative scenarios where the purpose is to explore and compare fairness. We
are currently designing such exploratory scenarios.

In summary, we observed that Kendall Tau and Jaccard report
mostly similar results when solving the fairness comparison problem
on Google job search. This is quite encouraging and merits further
investigation in future work.

520

ACKNOWLEDGMENTS

[24] Florian Tramèr, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, JeanPierre Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. 2015. Discovering
Unwarranted Associations in Data-Driven Applications with the FairTest
Testing Toolkit. CoRR abs/1510.02377 (2015). http://arxiv.org/abs/1510.02377
[25] Ke Yang and Julia Stoyanovich. 2017. Measuring fairness in ranked outputs.
In SSDM. 22.
[26] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed
Megahed, and Ricardo Baeza-Yates. 2017. Fa* ir: A fair top-k ranking algorithm.
In CIKM. 1569–1578.
[27] Indre Zliobaite. 2015. A survey on measuring indirect discrimination in
machine learning. CoRR abs/1511.00148 (2015). http://arxiv.org/abs/1511.
00148

This work is partially supported by the American University of
Beirut Research Board (URB)

REFERENCES
[1] Abolfazl Asudeh, H. V. Jagadish, Julia Stoyanovich, and Gautam Das. 2019.
Designing Fair Ranking Schemes. In Proceedings of the 2019 International
Conference on Management of Data, SIGMOD Conference 2019, Amsterdam, The
Netherlands, June 30 - July 5, 2019. 1259–1276.
[2] Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. 2018. Equity
of Attention: Amortizing Individual Fairness in Rankings. arXiv preprint
arXiv:1805.01788 (2018).
[3] Ria Mae Borromeo, Thomas Laurent, Motomichi Toyama, and Sihem AmerYahia. 2017. Fairness and Transparency in Crowdsourcing. In Proceedings
of the 20th International Conference on Extending Database Technology, EDBT
2017, Venice, Italy, March 21-24, 2017. 466–469. https://doi.org/10.5441/002/
edbt.2017.46
[4] Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for
discrimination-free classification. Data Mining and Knowledge Discovery 21, 2
(01 Sep 2010), 277–292. https://doi.org/10.1007/s10618-010-0190-x
[5] Chris Callison-Burch. 2014. Crowd-Workers: Aggregating Information Across
Turkers To Help Them Find Higher Paying Work. In The Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP-2014). http:
//cis.upenn.edu/~ccb/publications/crowd-workers.pdf
[6] L Elisa Celis, Damian Straszak, and Nisheeth K Vishnoi. 2017. Ranking with
fairness constraints. arXiv preprint arXiv:1704.06840 (2017).
[7] David Durward, Ivo Blohm, and Jan Marco Leimeister. 2016. Is There
PAPA in Crowd Work?: A Literature Review on Ethical Dimensions
in Crowdsourcing. In Ubiquitous Intelligence & Computing, Advanced
and Trusted Computing, Scalable Computing and Communications, Cloud
and Big Data Computing, Internet of People, and Smart World Congress
(UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld), 2016 Intl IEEE Conferences. IEEE,
823–832.
[8] Benjamin Edelman, Michael Luca, and Dan Svirsky. 2017. Racial discrimination
in the sharing economy: Evidence from a field experiment. American Economic
Journal: Applied Economics 9, 2 (2017), 1–22.
[9] Shady Elbassuoni, Sihem Amer-Yahia, Christine El Atie, Ahmad Ghizzawi,
and Bilel Oualha. 2019. Exploring Fairness of Ranking in Online Job Marketplaces. In Advances in Database Technology - 22nd International Conference
on Extending Database Technology, EDBT 2019, Lisbon, Portugal, March 26-29,
2019. 646–649.
[10] Ronald Fagin, Amnon Lotem, and Moni Naor. 2003. Optimal aggregation
algorithms for middleware. Journal of computer and system sciences 66, 4
(2003), 614–656.
[11] Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2016.
On the (im)possibility of fairness. CoRR abs/1609.07236 (2016). http://arxiv.
org/abs/1609.07236
[12] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and Christo Wilson. 2013. Measuring
personalization of web search. In Proceedings of the 22nd international conference on World Wide Web. ACM, 527–538.
[13] Aniko Hannak, Claudia Wagner, David Garcia, Alan Mislove, Markus
Strohmaier, and Christo Wilson. 2017. Bias in Online Freelance Marketplaces: Evidence from TaskRabbit and Fiverr. In Proceedings of the 2017 ACM
Conference on Computer Supported Cooperative Work and Social Computing,
CSCW 2017, Portland, OR, USA, February 25 - March 1, 2017. 1914–1933.
[14] Benjamin V. Hanrahan, Jutta K. Willamowski, Saiganesh Swaminathan, and
David B. Martin. 2015. TurkBench: Rendering the Market for Turkers.. In CHI,
Bo Begole, Jinwoo Kim, Kori Inkpen, and Woontack Woo (Eds.). ACM, 1613–
1616. http://dblp.uni-trier.de/db/conf/chi/chi2015.html#HanrahanWSM15
[15] Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness.
In Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018. 2569–2577.
[16] Keith Kirkpatrick. 2016. Battling algorithmic bias: how do we ensure algorithms treat us fairly? Commun. ACM 59 (2016), 16–17.
[17] Karen Levy and Solon Barocas. 2017. Designing against discrimination in
online markets. Berkeley Tech. LJ 32 (2017), 1183.
[18] Michael Luca and Rayl Fisman. 2016. Fixing Discrimination in Online Marketplaces. Harvard Business Review (Dec 2016). https://hbr.org/product/
fixing-discrimination-in-online-marketplaces/R1612G-PDF-ENG
[19] Mike Noon. 2010. The shackled runner: time to rethink positive discrimination? Work, Employment and Society 24, 4 (2010), 728–739.
[20] Ofir Pele and Michael Werman. 2009. Fast and robust earth mover’s distances.
In 2009 IEEE 12th International Conference on Computer Vision. IEEE, 460–467.
[21] Alex Rosenblat, Karen EC Levy, Solon Barocas, and Tim Hwang. 2017. Discriminating Tastes: Uber’s Customer Ratings as Vehicles for Workplace Discrimination. Policy & Internet 9, 3 (2017), 256–279.
[22] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rankings. arXiv preprint arXiv:1802.07281 (2018).
[23] Latanya Sweeney. 2013. Discrimination in Online Ad Delivery. CoRR
abs/1301.6822 (2013). http://arxiv.org/abs/1301.6822

521

