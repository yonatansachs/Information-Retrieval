which
stereotypes
are
moderated
and
under-moderated
in
search
engine
autocompletion
alina
leidinger
richard
rogers
university
of
amsterdam
institute
for
logic
language
and
computation
amsterdam
the
netherlands
a.j.leidinger@uva.nl
university
of
amsterdam
department
of
media
studies
amsterdam
the
netherlands
r.a.rogers@uva.nl
abstract
warning
this
paper
contains
content
that
may
be
offensive
or
upsetting
language
technologies
that
perpetuate
stereotypes
actively
cement
social
hierarchies
this
study
enquires
into
the
moderation
of
stereotypes
in
autocompletion
results
by
google
duckduckgo
and
yahoo
we
investigate
the
moderation
of
derogatory
stereotypes
for
social
groups
examining
the
content
and
sentiment
of
the
autocompletions
we
thereby
demonstrate
which
categories
are
highly
moderated
sexual
orientation
religious
affiliation
political
groups
and
communities
or
peoples
and
which
less
so
age
and
gender
both
overall
and
per
engine
we
found
that
under-moderated
categories
contain
results
with
negative
sentiment
and
derogatory
stereotypes
we
also
identify
distinctive
moderation
strategies
per
engine
with
google
and
duckduckgo
moderating
greatly
and
yahoo
being
more
permissive
the
research
has
implications
for
both
moderation
of
stereotypes
in
commercial
autocompletion
tools
as
well
as
large
language
models
in
nlp
particularly
the
question
of
the
content
deserving
of
moderation
specific
traits
while
exaggerating
them
is
google
however
inadvertently
an
engine
for
perpetuating
stereotypes
and
neglecting
moderation
how
does
it
compare
to
other
search
engines
such
as
yahoo
and
duckduckgo
stereotypes
that
are
reproduced
in
language
generation
or
an
autocompletion
task
constitute
representational
harm
14
cadwalladr
10
who
made
some
of
the
earliest
influential
findings
concerning
google
autocompletion
pointed
out
that
autocompletion
stereotypes
frame
and
also
distort
how
we
see
the
world
noble
48
went
further
arguing
that
stereotypical
and
racist
results
perpetuate
oppressive
social
relationships
indeed
vlasceanu
and
amodio
67
demonstrate
that
exposure
to
biased
google
image
search
results
reinforces
gender
stereotyping
in
professional
contexts
roy
et
al
56
link
exposure
to
autocompletions
to
the
psychological
process
of
incidental
learning
62
by
which
information
is
picked
up
unintentionally
and
subconsciously
often
in
the
course
of
another
information-seeking
activity
relatedly
miller
and
record
45
argue
that
autocompletions
induce
changes
in
epistemic
actions
some
of
which
can
be
harmful
45
especially
when
stereotypes
provide
ideological
ccs
concepts
information
systems
web
search
engines
applied
computing
arts
and
humanities
keywords
stereotypes
search
engine
autocompletion
google
autocompletion
content
moderation
debiasing
natural
language
generation
acm
reference
format
alina
leidinger
and
richard
rogers
2023
which
stereotypes
are
moderated
and
under-moderated
in
search
engine
autocompletion
in
2023
acm
conference
on
fairness
accountability
and
transparency
facct
23
june
12
15
2023
chicago
il
usa
acm
new
york
ny
usa
13
pages
https://doi.org/10.1145/3593013.3594062
introduction
stereotypes
have
been
defined
by
lippmann
40
as
pictures
in
our
heads
and
in
the
context
of
research
on
google
autocompletion
as
especially
reductionist
narrowing
person
or
thing
to
this
work
is
licensed
under
creative
commons
attribution
international
4.0
license
facct
23
june
12
15
2023
chicago
il
usa
2023
copyright
held
by
the
owner
author
acm
isbn
979
4007
0192
23
06
https://doi.org/10.1145/3593013.3594062
figure
autocompletions
by
google
top
and
yahoo
bottom
on
feb
1st
2023
screenshots
by
authors
1049
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
justification
to
maintain
social
hierarchies
and
further
marginalisation
google
is
somewhat
vague
about
how
autocompletion
content
moderation
works
stating
that
our
systems
aim
to
prevent
policyviolating
predictions
from
appearing
but
if
any
such
predictions
do
get
past
our
systems
and
we
re
made
aware
such
as
through
public
reporting
options
our
enforcement
teams
work
to
review
and
remove
them
as
appropriate
in
these
cases
we
remove
both
the
specific
prediction
in
question
and
often
use
pattern-matching
and
other
methods
to
catch
closely-related
variations
65
yahoo
is
less
expansive
than
google
in
its
autocompletion
content
moderation
policy
when
concerning
marginalised
groups
removing
suggestions
when
there
is
hate
rather
than
merely
offensive
speech
73
duckduckgo
does
not
specify
an
autocompletion
policy
apart
from
in
press
reports
that
the
company
blocks
offensive
returns
duckduckgo
licenses
its
autosuggestions
from
yahoo
31
in
the
following
we
analyse
moderation
practices
in
search
query
autocompletion
task
common
to
search
engines
proprietary
autocompletion
algorithms
and
publicly
available
state-of-the-art
language
models
58
59
75
given
the
active
but
rather
opaque
moderation
of
google
autocompletion
and
that
of
the
other
engines
in
this
study
we
pose
the
following
research
questions
for
which
social
groups
are
autocompletions
suppressed
or
otherwise
moderated
in
google
yahoo
and
duckduckgo
autocompletions
for
full
list
of
the
150
social
groups
queried
see
table
when
there
are
autocompletions
for
the
groups
under
study
how
is
their
sentiment
characterised
are
they
particularly
negative
how
may
one
portray
the
moderation
of
stereotypes
in
autocompletion
by
each
engine
are
certain
engines
stricter
or
more
permissive
than
others
contributions
we
make
prompting
or
stereotype-eliciting
queries
concerning
approximately
150
terms
for
social
groups
see
table
in
the
three
engines
based
broadly
on
age
gender
lifestyle
political
orientation
peoples
religion
and
sexual
orientation
examining
the
extent
of
the
suppression
or
other
forms
of
moderation
we
undertook
the
queries
in
order
to
gain
sense
of
which
autocompletions
do
not
complete
or
otherwise
show
signs
of
moderation
we
discuss
which
stereotypes
and
categories
of
social
groups
receive
which
types
of
suppression
or
other
moderation
thereby
charting
the
work
engines
are
doing
to
thwart
such
outputs
through
scoring
the
groups
by
moderation
of
stereotypes
we
also
shed
light
on
which
group
stereotypes
are
considered
rather
sensitive
by
search
engine
given
their
removal
or
editing
we
thereby
are
able
to
characterise
result
moderation
overall
and
per
search
engine
under
study
our
findings
could
inform
work
on
content
moderation
policy
whether
in
autocompletion
or
nlp
more
generally
particularly
by
drawing
attention
to
under-moderated
categories
that
have
negative
suggestions
in
light
of
the
harmful
impact
of
stereotype
perpetuation
we
believe
public
discourse
on
moderation
priorities
as
well
as
transparent
documentation
on
the
parts
of
commercial
language
technology
providers
to
be
crucial
1050
related
work
2.1
content
moderation
of
search
engine
autocompletion
most
studies
focus
on
google
results
moderation
rather
than
other
engines
given
its
market
dominance
33
55
content
moderation
has
been
defined
by
grimmelmann
30
as
the
governance
mechanisms
that
structure
participation
in
community
to
facilitate
cooperation
and
prevent
abuse
previous
work
on
the
moderation
of
especially
google
autocompletion
has
concerned
itself
with
how
it
was
once
prone
to
outputting
derogatory
content
such
as
are
jews
evil
where
the
autocompletion
part
is
in
brackets
10
indeed
journalists
and
scholars
alike
have
reported
particularly
shocking
outputs
for
queries
of
women
72
old
men
and
old
women
55
religions
10
sexual
orientation
gender
identity
and
others
generally
speaking
up
until
2016
google
product
outputs
from
web
search
to
autocompletion
were
described
as
organic
by
the
company
or
reflections
however
unpleasant
of
what
was
happening
on
the
web
10
after
press
reports
there
were
noticeable
emergency
take-down
and
patches
25
in
autocompletion
generally
however
results
came
with
disclaimers
in
banner
ads
and
further
explanation
in
blog
posts
or
in
response
to
the
press
concerning
how
they
reflected
what
was
happening
across
the
web
23
that
state
of
affairs
changed
with
the
introduction
of
the
autocompletion
feedback
tool
in
2017
where
users
could
report
on
content
that
they
considered
hateful
racist
offensive
vulgar
sexually
explicit
harmful
dangerous
violent
misleading
or
inaccurate
27
in
2018
danny
sullivan
the
company
public
search
liaison
explained
in
long
blog
post
the
company
autocompletion
removal
policies
63
pointing
to
google
definition
of
inappropriate
content
particularly
derogatory
output
relating
how
the
engine
removes
autocompletions
that
are
hateful
or
prejudicial
with
respect
to
race
ethnic
origin
religion
disability
age
nationality
veteran
status
sexual
orientation
gender
gender
identity
or
any
other
characteristic
that
associated
with
systemic
discrimination
or
marginalisation
behind
the
need
for
the
moderation
of
autocompletion
as
well
as
other
suggestions
or
predictions
that
appear
in
other
search
engine
products
are
the
liabilities
that
arise
from
outputting
words
connected
to
the
incipient
search
query
do
they
defame
individuals
12
could
they
induce
illegal
acts
such
as
downloading
of
copyrighted
material
36
do
they
lead
to
sources
of
child
pornography
or
other
illicit
material
18
do
they
contain
hateful
language
towards
groups
20
what
google
defines
as
inappropriate
content
to
be
moderated
relates
directly
to
these
and
other
legal
liabilities
29
group
stereotyping
however
is
more
of
grey
area
but
would
fall
under
the
moderation
of
what
sullivan
describes
as
offensive
content
64
design
choices
with
respect
to
moderation
of
stereotypes
are
not
detailed
by
the
company
and
constitute
the
object
of
study
for
this
work
we
place
our
work
alongside
algorithmic
auditing
51
57
platform
observability
53
ethical
hacking
for
vulnerabilities
as
well
as
commercial
content
moderation
critique
26
54
though
each
of
these
approaches
has
somewhat
different
emphases
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
table
list
of
groups
targeted
by
stereotypes
english
category
group
age
boomers
children
kids
millennials
old
men
old
people
old
women
teenagers
teens
gender
gender
identity
gendered
girls
women
men
females
males
boys
boyfriends
girlfriends
stepmothers
stepfathers
ladies
gentlemen
brothers
sisters
mothers
fathers
grandfathers
grandmothers
wives
husbands
schoolgirls
schoolboys
transgenders
23
lifestyle
way
of
liv
feminists
frat
boys
geeks
goths
hippies
hipsters
nerds
punks
sorority
girls
celebrities
criminals
ing
homeless
people
poor
people
rich
people
14
political
politicised
capitalists
communists
conservatives
immigrants
liberals
populists
socialists
trump
supporters
peoples
ethnicities
africans
asians
asian
kids
asian
men
asian
parents
asian
women
african
americans
black
americans
blacks
black
fathers
black
kids
black
men
black
parents
black
people
black
women
europeans
hispanics
hispanic
men
hispanic
women
latinas
latinos
latin
people
native
americans
whites
white
americans
white
kids
white
men
white
parents
white
people
white
women
redheads
gingers
blondes
32
nationalities
americans
afghans
albanians
arabs
australians
austrians
bengalis
british
people
chileans
colombians
dutch
people
egyptians
ecuadorians
ethiopians
finns
french
people
germans
ghanaians
greeks
indians
indonesians
iranians
iraqis
irish
people
italians
koreans
lebanese
people
mexicans
moroccans
nepalis
nigerians
norwegians
pakistanis
polish
people
romanians
russians
scots
somalis
south
africans
sudanese
people
swedes
syrians
taiwanese
people
turkish
people
ukrainians
venezuelans
vietnamese
people
47
religion
atheists
buddhists
catholics
christians
hindus
jews
mormons
muslims
protestants
religious
people
sikhs
11
sexual
orientation
total
asexual
people
bisexual
people
gay
people
homosexuals
lesbians
pansexual
people
queer
people
151
2.2
total
content
moderation
in
language
models
search
engine
autocompletion
is
one
real-world
application
of
language
modelling
lm
or
natural
language
generation
nlg
which
has
been
demonstrated
to
suffer
from
undesirable
biases
60
70
methodologically
bias
has
been
quantified
using
intrinsic
measures
11
66
which
operate
on
word
embeddings
or
extrinsic
measures
that
examine
how
bias
manifests
itself
in
downstream
tasks
such
as
sentiment
analysis
34
37
or
hate
speech
detection
19
52
for
measuring
stereotypes
in
particular
bias
benchmarks
consisting
of
contrasting
sentence
pairs
stereoset
46
and
crows-pairs
47
have
been
proposed
in
open-ended
language
generation
prompts
are
often
used
to
assess
to
what
extent
lms
yield
undesirable
output
various
benchmarks
such
as
bold
17
honest
49
holisticbias
61
and
realtoxicityprompts
24
exist
for
this
purpose
choenni
et
al
13
prompt
language
models
to
assess
to
what
extent
they
have
learnt
stereotypes
in
contrast
to
our
work
they
use
search
engine
autocompletions
as
proxy
for
stereotypes
existing
in
the
real
world
and
compare
them
to
lm
output
early
methods
for
measuring
bias
and
stereotypes
have
mainly
focused
on
gender
11
recently
the
field
has
turned
its
attention
1051
also
towards
harms
against
groups
based
on
their
disability
status
35
gender
identity
16
race
22
43
or
religion
42
mitigation
efforts
in
nlp
include
debiasing
methods
which
intervene
to
produce
less
biased
or
stereotyping
output
21
28
69
74
lm
output
can
also
be
flagged
as
harmful
using
manual
inspection
60
lexicons
49
or
another
pretrained
model
44
60
commercial
tools
that
fall
into
the
latter
category
mainly
focus
on
hate
speech
and
toxicity
less
on
stereotypes
perspective
api
provides
scores
for
toxicity
insult
profanity
identity
attack
threat
and
sexually
explicit
openai
reports
on
the
content
moderation
filter
for
their
language
models
which
scores
lm
output
based
on
the
following
criteria
hate
self-harm
sexual
content
and
violence
44
50
methods
3.1
data
collection
we
collected
autocompletions
by
prompting
three
leading
search
engines
google
duckduckgo
and
yahoo
with
the
query
why
are
group
so
for
large
number
of
social
groups
for
the
choice
of
social
groups
we
drew
on
lists
of
groups
from
choenni
et
al
13
and
stereoset
46
benchmark
commonly
used
for
measuring
stereotypes
in
lms
it
features
stereotypes
pertaining
to
321
target
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
terms
falling
into
the
categories
gender
profession
race
and
religion
categories
were
originally
sourced
from
wikidata
relation
triplets
68
we
follow
choenni
et
al
13
in
extending
this
list
of
social
groups
but
excluded
colloquialisms
and
slurs
we
further
reorganised
the
categorisation
using
google
list
of
groups
of
potentially
marginalised
mentioned
in
the
introduction
resulting
in
the
categories
age
gender
gender
identity
gendered
lifestyle
ways
of
living
nationalities
peoples
ethnicities
political
politicised
religion
and
sexual
orientation
while
most
of
our
categories
match
up
with
google
the
groups
listed
under
lifestyle
ways
of
living
as
well
as
political
politicised
fit
google
catch-all
category
of
any
other
characteristic
associated
with
discrimination
or
marginalisation
we
removed
social
groups
belonging
to
the
professions
category
since
the
great
majority
of
those
are
not
commonly
considered
as
marginalised1
see
table
for
the
full
list
of
social
groups
we
followed
choenni
et
al
13
approach
in
querying
the
engines
autocompletion
services
in
january
and
again
in
august
2022
using
the
python
library
requests
and
thus
simulated
an
anonymous
user
querying
autocompletions2
hence
autosuggestions
were
not
influenced
by
personal
search
history
language
and
country
parameters
were
set
to
english
and
the
region
and
the
browser
setting
to
chrome
since
autocompletions
can
also
deviate
from
the
exact
wording
of
the
prompt
we
discarded
those
autocompletions
that
did
not
conform
to
the
phrasing
why
are
group
so
whereas
baker
and
potts
employed
prompts
as
why
do
group
how
do
group
what
do
group
and
where
do
group
finding
them
fruitful
in
triggering
stereotypes
our
prompt
directly
elicits
stereotypes
as
it
asks
for
the
reason
behind
group
characteristics
thereby
assuming
those
inquiring
are
not
questioning
the
stereotype
or
if
questioning
it
through
sarcasm
are
familiar
with
the
stereotype
we
release
our
data
as
part
of
the
supplementary
material
of
this
work
39
3.2
analysis
of
moderation
practices
to
uncover
which
search
engine
moderates
which
target
category
we
considered
the
following
as
strong
indicators
the
target
category
contains
large
percentage
of
groups
yielding
autosuggestions
on
average
the
number
of
autosuggestions
from
this
engine
is
substantially
lower
than
it
is
for
the
other
search
engines
for
this
category
common
negative
stereotypes
are
absent
among
autosuggestions
that
do
appear
in
the
autosuggestions
from
other
engines
additionally
we
have
observed
on
occasion
number
of
autocompletions
or
single
autocompletion
charged
with
positive
sentiment
in
comparison
to
other
engines
that
return
many
mainly
negative
autosuggestions
for
this
category
we
recorded
summary
statistics
and
sentiment
scores
to
operationalise
our
reasoning
to
corroborate
our
findings
we
drew
1we
would
like
to
add
that
some
intersectional
groups
old
women
fall
into
more
than
one
category
gender
and
age
we
decided
to
follow
the
categorisation
of
choenni
et
al
13
in
this
case
we
could
have
also
created
more
intersectional
categories
queer
indian
men
but
left
the
broader
terms
with
up
to
one
qualifier
so
that
it
would
allow
for
comparison
of
moderation
attention
across
engines
in
the
categories
demarcated
by
google
the
source
code
developed
by
13
is
available
here
https://github.com/
rochellechoenni
stereotypes_in_lms
3we
found
this
prompt
to
be
particularly
effective
in
returning
the
maximal
number
of
results
for
non-marginalised
or
non-politicised
groups
during
an
initial
data
exploration
compared
to
four
others
in
the
original
research
by
choenni
et
al
13
1052
comparison
of
summary
statistics
and
scores
between
our
two
timestamps
of
january
and
august
2022
to
quantify
sentiment
we
scored
the
sentiment
of
each
full
autocompletion
using
large
language
model
fine-tuned
for
sentiment
classification
specifically
we
used
roberta
41
optimised
by
hartmann
et
al
32
for
this
purpose4
we
chose
this
model
in
particular
since
it
is
fine-tuned
on
large
set
of
english
language
datasets
stemming
from
various
domains
tweets
reviews
etc
binary
sentiment
scores
are
in
the
range
between
and
with
higher
scores
indicating
more
negative
sentiment
findings
in
the
following
we
discuss
our
findings
with
respect
to
the
moderation
or
under-moderation
of
autocompletion
by
the
search
engines
which
categories
and
terms
for
groups
appear
to
be
the
source
of
moderation
when
one
engine
returns
plentiful
results
including
negative
stereotypes
while
another
returns
none
or
single
result
our
findings
of
moderation
are
supported
while
there
are
exceptions
we
are
able
to
characterise
the
individual
engines
generally
as
greatly
moderating
google
duckduckgo
and
permissive
yahoo
4.1
the
moderation
of
autocompletion
none
of
the
engines
returned
autocompletions
for
sexual
orientation
as
whole
across
the
engines
the
categories
nationalities
peoples
ethnicities
political
politicised
and
religion
had
relatively
few
autocompletions
for
google
age
and
gender
autocompletions
are
overall
the
most
negative
though
there
are
outliers
where
there
are
very
few
returns
and
those
returns
are
negative
as
is
the
case
with
why
are
protestants
so
in
google
yahoo
has
overall
the
most
negative
autosuggestions
while
duckduckgo
has
the
least
the
overall
average
sentiment
scores
for
autocompletions
are
0.78
for
yahoo
0.59
for
google
and
0.49
for
duckduckgo
where
the
higher
the
score
the
more
negative
the
sentiment
for
more
details
see
table
4.1
sexual
orientation
none
of
the
search
engines
in
either
time
period
served
autocompletions
for
groups
in
the
sexual
orientation
category
sexual
orientation
is
alone
in
this
regard
indicating
particularly
well
moderated
set
of
terms
4.1
religion
autcompletions
for
social
groups
in
the
religion
category
seem
to
be
heavily
moderated
by
google
and
duckduckgo
see
table
yahoo
contrariwise
furnishes
substantial
number
of
autocompletions
in
particular
for
jews
including
anti-semitic
slurs
such
as
cheap
and
rich
google
returns
no
autocompletions
for
religious
groups
with
the
exception
of
mormons
where
we
see
in
both
periods
the
potentially
actively
curated
single
suggestion
of
nice
the
other
religious
groups
that
are
under-moderated
at
least
for
one
time
period
are
protestants
bitter
boring
socalled
judgemental
as
well
as
christians
judgemental
which
are
mainly
negative
qualifiers
and
result
in
negative
sentiment
score
duckduckgo
appears
to
block
all
autocompletions
for
religions
there
are
no
autocompletions
for
hindus
and
buddhists
from
any
of
the
search
engines
4we
used
the
huggingface
library
71
and
the
following
checkpoint
https
huggingface
co
siebert
sentiment-roberta-large-english
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
table
proportion
of
queries
that
yield
or
autosuggestions
us
january
2022
left
august
2022
right
category
groups
google
yahoo
duck
google
yahoo
duck
age
50
25
25
50
25
25
gender
gender
identity
gendered
23
39.1
43.5
60.9
30.4
47.8
60.9
lifestyle
way
of
living
14
50
78.6
78.6
57.1
78.6
78.6
political
politicised
100
62.5
87.5
100
62.5
87.5
peoples
ethnicities
32
75.8
42.4
87.9
84.8
48.4
87.9
nationalities
47
78.8
74.5
66
78.8
85.1
66
religion
11
100
36.4
100
90.9
36.4
100
sexual
orientation
100
100
100
100
100
100
table
autocompletions
for
religious
groups
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
yahoo
atheists
catholics
christians
jews
judgemental
mormons
nice
muslims
protestants
bitter
boring
so-called
judgemental
afraid
of
god
angry
against
abortion
liberal
negative
unlike
christ
into
politics
devoted
to
mary
angry
controlling
divided
easily
offended
fearful
happy
hated
judgemental
to
gays
liberal
persecuted
powerful
rich
cheap
smart
successful
hated
wealthy
funny
disliked
happy
interested
in
genealogy
successful
prepared
rich
strict
wealthy
patriotic
controversial
interested
in
ancestry
into
genealogy
misunderstood
religious
spoken
word
conservative
divided
racist
brainwashed
miserable
negative
religious
people
table
autocompletions
for
political
politicised
groups
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
immigrants
trump
supporters
conservatives
successful
liberals
yahoo
duck
angry
brainwashed
delusional
gullible
hateful
ignorant
loyal
mad
stupid
violent
dumb
fat
afraid
of
higher
hateful
angry
miserable
racist
brainwashed
stubborn
intolerant
anti
afraid
of
change
pro
life
education
abortion
paranoid
mean
cold
hearted
fearful
popular
in
angry
condescending
dumb
hateful
ignorant
intolerant
racist
stupid
canada
unhappy
violent
miserable
4.1
political
politicised
duckduckgo
and
google
seem
to
be
moderating
autocompletions
for
groups
in
this
category
quite
rigorously
while
for
yahoo
the
content
management
is
less
prevalent
table
duckduckgo
blocks
nearly
all
autocompletions
for
google
autocompletions
are
mainly
suppressed
with
one
exception
1053
conservatives
who
are
afraid
of
higher
education
in
yahoo
conservatives
are
hateful
angry
miserable
brainwashed
etc
as
are
liberals
resulting
in
high
negative
sentiment
score
no
search
engine
provided
any
autocompletions
for
communists
socialists
capitalists
or
populists
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
table
autocompletions
for
select
peoples
ethnicities
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
yahoo
black
people
good
at
running
tall
aggressive
angry
athletic
big
cool
fast
hated
racist
religious
tall
good
at
sports
much
bigger
than
africans
today
poor
racist
violent
loud
religious
good
at
sports
angry
sensitive
about
everything
big
tall
racist
against
whites
religious
hated
in
america
different
physically
tall
aggressive
tall
cheap
angry
attracted
to
white
women
violent
homophobic
popular
hot
promiscuous
hated
angry
big
difficult
loud
religious
undesirable
violent
heavy
hard
to
date
confident
angry
bad
disrespectful
disrespectful
in
school
hyper
violent
wild
mean
strict
abusive
angry
attractive
bad
beautiful
entitled
racist
rich
scared
of
black
people
smart
violent
cold
racist
angry
afraid
of
blacks
obsessed
with
blacks
racist
racist
against
blacks
smart
angry
attracted
to
asian
women
entitled
nice
tall
threatened
by
black
men
threatened
by
black
women
violent
angry
attracted
to
black
men
beautiful
easy
hot
sensitive
thin
bad
smart
black
americans
african
americans
blacks
black
men
black
women
black
kids
black
parents
white
people
white
americans
whites
white
men
white
women
white
kids
white
parents
laid
back
asians
asian
men
asian
women
asian
kids
asian
parents
smart
good
at
math
strict
on
grades
critical
negative
judgemental
angry
critical
reddit
loud
peaceful
feminine
popular
depressed
smart
short
strict
about
grades
latin
people
latinos
latinas
hispanics
hard
to
date
loyal
desired
stupid
involved
in
kobe
bryant
passionate
easy
loud
4.1
peoples
ethnicities
overall
moderation
practices
in
this
category
fit
the
overall
pattern
of
google
and
duckduckgo
as
moderating
engines
and
yahoo
as
more
permissive
see
table
yahoo
returned
the
largest
number
of
autocompletions
which
also
resulted
in
the
highest
score
for
negative
sentiment
both
google
as
well
as
duckduckgo
returned
lower
sentiment
score
in
particular
for
the
terms
black
people
and
white
people
we
found
few
autocompletions
in
google
complete
lack
of
duckduckgo
autocompletions
and
some
strong
stereotyping
in
yahoo
autocompletions
which
accounts
for
the
negative
sentiment
yahoo
autocompletions
touch
on
racism
why
are
white
people
racist
and
negative
images
aggressive
angry
bad
mean
etc
number
of
yahoo
autocompletions
we
found
for
latinos
loud
stupid
involved
in
1054
kobe
bryant
and
asians
feminine
men
popular
women
smart
and
depressed
kids
strict
parents
contain
negative
valences
4.1
nationalities
google
moderation
extends
to
nationalities
theirs
and
duckduckgo
autocompletions
were
among
the
most
positive
of
all
autocompletions
we
collected
google
outputs
positive
autocompletions
for
many
nationalities
including
americans
friendly
germans
smart
tall
indians
smart
moroccans
strong
beautiful
australians
tall
russians
tall
good
at
chess
pretty
somalis
rich
tall
successful
and
syrians
beautiful
it
returns
no
autocompletions
for
most
groups
including
austrians
british
people
ethiopians
french
people
greeks
irish
people
italians
mexicans
nigerians
pakistanis
polish
people
and
romanians
when
duckduckgo
returns
autocompletions
for
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
nationalities
there
is
smattering
of
stereotypes
in
evidence
but
not
enough
to
result
in
negative
sentiment
score
yahoo
again
scored
highest
for
negative
sentiment
for
example
egyptians
are
loud
french
people
are
mean
and
germans
are
cold
4.1
gender
gender
identity
gendered
none
of
the
search
engines
returned
results
for
transgenders
but
otherwise
the
results
in
this
category
are
perhaps
the
most
surprising
overall
for
the
remaining
social
groups
we
see
substantial
number
of
autocompletions
many
of
which
are
stereotypical
as
well
as
insulting
see
table
all
three
engines
scored
on
the
negative
end
of
the
sentiment
spectrum
with
yahoo
being
overall
the
most
negative
followed
by
google
and
then
duckduckgo
google
gives
unflattering
suggestions
for
most
of
the
terms
in
the
gender
category
for
women
females
and
girls
we
found
such
autocompletions
as
controlling
clingy
and
dramatic
and
for
men
males
and
boys
boring
mean
insensitive
lonely
and
immature
yahoo
negative
autocompletions
are
more
plentiful
for
men
males
and
boys
we
found
such
suggestions
as
difficult
complicated
angry
at
women
needy
insensitive
and
confusing
duckduckgo
furnishes
fewer
autocompletions
than
google
and
yahoo
but
the
autocompletions
have
similar
terms
4.1
age
google
is
the
only
engine
that
curates
most
autocompletions
for
the
age
category
as
the
overall
number
of
autocompletions
is
comparatively
small
see
table
it
stands
alone
in
suppressing
most
ageist
autocompletions
for
queries
concerning
older
people
with
the
exception
of
old
people
as
entitled
yahoo
and
duckduckgo
return
stereotypes
as
angry
grumpy
cold
negative
stubborn
difficult
entitled
and
slow
while
google
is
marginally
lower
sentiment
scores
for
all
three
engines
combined
were
among
the
highest
overall
all
engines
return
stereotyping
autocompletions
for
boomers
children
and
teenagers
4.1
lifestyle
ways
of
living
this
category
follows
the
overall
pattern
of
yahoo
furnishing
large
amount
of
negative
autocompletions
google
some
though
not
for
the
same
ones
and
duckduckgo
returning
few
autocompletions
for
feminists
the
homeless
the
rich
the
poor
and
criminals
are
suppressed
by
nearly
all
search
engines
and
rather
exceptionally
there
is
possibly
evidence
of
positive
moderation
on
the
part
duckduckgo
as
well
as
yahoo
see
table
yahoo
autocompletions
for
rich
people
are
in
evidence
while
poor
people
happy
poor
and
homeless
people
happy
have
positive
inflections
google
is
the
only
search
engine
to
return
completions
for
punks
frats
goths
hippies
hipsters
and
nerds
duckduckgo
and
yahoo
have
no
results
with
the
exception
of
duckduckgo
why
are
nerds
so
attractive
successful
which
could
be
an
easter
egg
4.1
autocompletion
engine
moderation
and
sentiment
all
in
all
google
appears
to
moderate
results
in
much
greater
quantities
than
duckduckgo
and
yahoo
google
often
returns
no
autocompletions
in
both
january
and
august
2022
for
not
only
sexual
orientation
but
also
others
as
seen
in
table
or
single
results
some
charged
with
positivity
why
are
immigrants
so
successful
see
table
for
an
overview
given
the
universe
of
stereotypes
potentially
associated
with
the
groups
when
only
one
autocompletion
appears
and
has
positive
valence
it
could
be
an
indication
of
curation
point
we
return
to
in
the
discussion
1055
as
rule
through
such
moderation
the
sentiment
scores
become
less
negative
compared
to
yahoo
duckduckgo
mainly
suppresses
potentially
stereotypical
or
inappropriate
autocompletions
completely
and
overall
has
the
lowest
negative
sentiment
scores
yahoo
characterised
as
by
far
the
most
permissive
engine
was
found
to
moderate
the
least
and
have
the
highest
negative
sentiment
in
those
cases
when
it
does
not
permit
stereotypes
to
appear
it
removes
all
autosuggestions
discussion
we
would
like
to
discuss
four
implications
of
the
research
the
first
concerns
the
distribution
of
moderation
overall
the
second
the
permissiveness
of
particular
search
engines
for
certain
queries
the
third
the
continuing
stakes
of
perpetuating
certain
negativity
or
insults
in
services
that
the
user
cannot
turn
off
and
finally
the
question
of
the
transparency
of
the
moderation
we
also
would
like
to
ask
whether
engines
can
do
better
in
the
research
we
found
hierarchy
of
concern
which
we
suggest
could
be
flattened
further
we
also
found
differentiation
in
moderation
across
engines
which
could
be
evened
with
respect
to
the
hierarchy
of
concern
sexual
orientation
is
moderated
as
are
most
ethnicities
and
religions
though
with
some
exceptions
such
as
protestants
in
google
gender
is
under-moderated
given
the
stereotypes
and
insults
returned
for
especially
women
older
people
as
category
is
also
under-moderated
at
least
compared
to
the
other
categories
with
respect
to
individual
engines
yahoo
moderation
stands
out
for
the
amount
of
stereotypes
and
insults
allowed
to
pass
through
across
most
categories
given
that
there
is
the
exception
of
sexual
orientation
yahoo
is
not
an
un-moderated
engine
but
certainly
one
where
attention
is
called
for
the
under-moderation
has
resulted
in
negative
autocompletions
as
evidenced
by
the
sentiment
scoring
these
are
groups
of
people
who
historically
have
faced
discrimination
and
marginalisation
and
the
autocompletions
could
be
considered
what
noble
48
called
reinforcement
users
thereby
can
come
across
the
stereotypes
and
insults
picking
them
up
while
searching
for
other
information
learning
abusive
remarks
for
groups
or
witnessing
their
reinforcement
is
the
presence
of
these
stereotypes
and
insults
reason
enough
to
make
the
service
optional
or
disabled
by
default
when
certain
groups
see
stereotypes
and
insults
and
others
are
conspicuously
absent
the
question
arises
about
search
engine
policy
and
its
implementation
while
there
appears
to
have
been
an
expansion
in
moderation
activities
over
the
past
few
years
its
documentation
has
been
supplied
only
in
rather
general
terms
while
we
have
read
company
blog
posts
concerning
the
moderation
of
this
content
as
far
as
we
can
tell
the
scope
as
well
as
the
types
of
moderated
stereotypes
have
not
as
yet
become
part
of
transparency
reports
or
other
official
company
documentation
moreover
harmful
stereotypes
are
also
not
among
the
kinds
of
inappropriate
autocompletion
content
that
users
can
report
through
the
interface
tool
of
search
engines
at
least
explicitly
documentation
on
content
filters
built
into
commercial
language
models
often
does
not
mention
stereotypes
explicitly
either
50
the
implication
is
that
search
engines
could
provide
not
only
content
moderation
policy
but
also
evidence
beyond
the
blog
posts
of
its
effective
implementation
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
table
autocompletions
for
select
lifestyle
way
of
living
groups
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
yahoo
feminists
homeless
people
poor
people
rich
people
angry
all
the
time
happy
angry
mad
happy
loud
cheap
mean
stingy
miserable
liberal
wasteful
entitled
healthy
rich
duckduckgo
table
autocompletions
for
select
gender
gender
identity
gendered
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
women
attractive
beautiful
controlling
hot
sensitive
short
females
girls
men
males
boys
yahoo
duckduckgo
difficult
unhappy
mean
to
each
other
emo
tional
angry
jealous
of
other
women
important
dramatic
picky
about
men
hot
clingy
emotional
during
period
difficult
emotional
jealous
of
me
protec
moody
emotional
during
period
attraccompetitive
messy
stubborn
cute
de
tive
against
other
female
tive
sensitive
emotional
entitled
cute
fensive
attractive
sensitive
bippoaggressive
lar
weak
in
naruto
dramatic
emotional
insecure
sensi
complicated
dramatic
emotional
difficult
cute
confusing
pretty
sensitive
attractive
attractive
soft
pretty
cute
sensitive
short
mean
to
each
other
confus
tive
wierd
mean
dramatic
hot
ing
expensive
competitive
mean
to
boys
complicated
boring
sensitive
hot
in
insensitive
visual
angry
needy
attracted
shallow
aggressive
self-absorbed
simsensitive
lonely
warm
hairy
to
women
loud
hot
and
cold
complicated
ple
jealous
sensitive
moody
hot
cute
angry
at
women
sensitive
difficult
attractive
attracted
to
females
angry
difficult
emotional
protective
against
other
attractive
aggressive
rare
mean
loud
female
cute
immature
complicated
confusing
mean
to
girls
ugh
confusing
aggressive
cute
funny
stupid
quotes
wearing
nail
polish
dramatic
complicated
difficult
hot
strong
when
sick
loud
competitive
tall
cute
best
friends
sensitive
destructive
these
four
points
aim
to
orient
the
discussion
around
autocompletion
moderation
particularly
the
decisions
on
what
to
moderate
as
well
as
disclose
about
moderation
and
the
stakes
of
undermoderating
5.1
additional
categories
of
moderation
strategy
cursory
look
at
the
press
reports
in
the
2010s
concerning
particularly
shocking
autocompletions
such
as
are
jews
evil
yields
follow-up
articles
detailing
how
those
suggestions
have
been
fixed
or
removed
25
such
suppressions
or
patches
are
implemented
in
direct
response
to
journalistic
discoveries
and
related
queries
are
presumably
also
fixed
such
as
are
christians
evil
but
we
also
have
observed
autocompletion
result
lists
that
leave
single
suggestion
occasionally
charged
with
positivity
such
as
why
are
homeless
people
so
happy
apart
from
the
blocking
this
example
could
point
to
third
way
which
could
be
dubbed
curation
which
entails
retaining
fewer
sometimes
positively
charged
results
curation
is
more
complex
however
when
considering
autocompletion
results
that
contain
synthetic
content
such
as
the
query
1056
completed
with
near
me
or
meaning
or
knowledge
base
content
such
as
the
query
completed
names
of
pop
songs
famous
people
or
official
organisations
for
an
overview
of
examples
see
table
12
in
the
appendix
prior
to
sullivan
blog
posts
at
google
there
was
not
much
written
about
specific
moderation
practices
especially
the
interplay
in
autocompletion
between
organic
results
the
output
of
real
searches
and
synthetic
ones
the
output
of
word
patterns
in
our
dataset
there
are
synthetic
additions
see
table
12
that
did
not
result
in
more
substantive
or
useful
autocompletions
raising
the
question
of
the
current
effectiveness
of
the
strategy
of
using
such
word
patterns
as
part
of
content
moderation
compared
to
pruning
autocompletion
outputs
as
danny
sullivan
indicated
in
the
2020
post
certain
pruning
however
could
be
construed
as
form
of
editing
that
overly
minimises
offence
in
turn
it
could
result
in
politicised
public
outcry
conclusion
overall
autocompletion
is
an
actively
moderated
space
we
found
distribution
of
moderation
of
categories
with
some
intuitive
as
well
as
counter-intuitive
results
sexual
orientation
has
been
moderated
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
table
autocompletions
for
age
us
jan
and
aug
2022
where
autocompletions
in
normal
font
are
from
both
jan
and
aug
bold
autocompletions
are
from
jan
only
and
italicised
from
aug
only
group
google
yahoo
duckduckgo
aggressive
controlling
rich
out
of
toxic
conservative
angry
annoying
selfish
fat
conservative
greedy
entitouch
entitled
bad
with
technology
entled
liberal
salty
titled
reddit
out
of
touch
reddit
loud
bad
with
technology
reddit
angry
reddit
clueless
children
loud
annoying
energetic
important
disrespectful
curious
creative
loud
important
recruel
honest
expensive
annoying
silient
noisy
loving
honest
vulneraimpressionable
special
competitive
ble
cute
stubborn
easily
influenced
kids
loud
energetic
cute
happy
cruel
an
annoying
cruel
loud
selfish
stupid
weird
energetic
fat
noisy
loud
cringe
noying
mean
disrespectful
sensitive
these
entitled
now
mean
in
middle
school
days
dumb
today
fat
happy
lazy
cute
mean
to
other
kids
happy
these
days
toxic
millennials
old
people
entitled
cold
tired
angry
negative
dependent
difficult
grumpy
entitled
stubborn
naive
loud
grouchy
negative
nice
stiff
when
they
get
up
cold
slow
cute
old
men
old
women
teenagers
angry
sad
depressed
difficult
mean
to
their
par
angry
moody
lazy
tired
emotional
ents
angry
emotional
hormonal
tired
skinny
awkward
stressed
dramatic
all
the
time
irritable
alienating
to
grumpy
ward
each
other
forgetful
stressed
unhappy
teens
stressed
depressed
difficult
attached
to
their
phones
tired
moody
depressed
emotional
sad
rebeladdicted
to
their
phones
depressed
lious
stressed
edgy
impulsive
lazy
these
days
sad
derek
thompson
sad
today
easily
influenced
boomers
out
of
autocompletion
peoples
and
ethnicity
are
highly
moderated
followed
by
religion
the
category
gender
gender
identity
gendered
is
rather
under-moderated
however
and
is
populated
with
stereotypes
with
negative
attributions
stereotypes
are
attached
to
both
men
and
women
age
is
also
rather
under-moderated
and
autosuggestions
are
more
negative
though
google
moderates
more
than
the
other
engines
there
are
sporadic
stereotypes
in
all
engines
even
those
as
google
the
example
of
protestants
and
duckduckgo
nationalities
one
rather
counter-intuitive
finding
is
the
lack
of
moderation
in
yahoo
while
sexual
orientation
and
few
other
sensitive
categories
have
been
addressed
compared
to
google
and
duckduckgo
the
situation
with
yahoo
is
not
that
far
removed
from
what
baker
and
potts
described
for
google
in
2013
the
auto-completion
questions
offer
window
into
the
collective
internet
consciousness
and
what
this
window
reveals
is
not
an
attractive
scene
indeed
the
sentiment
associated
with
yahoo
autocompletions
under
study
is
considerably
more
negative
compared
to
google
and
duckduckgo
our
work
more
generally
has
pointed
to
moderation
in
google
in
particular
across
range
of
terms
that
were
not
moderated
decade
ago
according
to
the
journalistic
pieces
where
offensive
1057
results
were
reported
age
is
under-moderated
with
the
exception
of
google
overall
gender
however
remains
under-moderated
limitations
and
future
work
there
are
several
limitations
to
be
discussed
including
the
work
centric
orientation
our
search
engine
choice
the
formulation
of
prompts
and
the
use
of
pretrained
language
models
for
sentiment
classification
the
research
undertaken
is
largely
centric
and
certain
of
the
stereotypical
sensitivities
could
be
interpreted
as
such
future
work
would
benefit
from
developing
culturally
specific
sets
of
queries
across
variety
of
languages
in
order
to
study
moderation
practices
across
regions
and
compare
regions
given
the
centric
orientation
we
also
could
have
included
bing
ecosia
and
other
smaller
engines
studying
baidu
yandex
and
naver
could
broaden
the
scope
of
comparison
we
re-used
the
prompts
from
previous
work
remapping
them
onto
google
categories
of
derogatory
remarks
but
could
have
added
ones
from
other
journalistic
or
scholarly
work
on
autocompletion
while
our
lists
of
social
groups
does
cover
some
intersections
15
they
do
so
with
only
one
qualifier
finally
there
were
certain
groups
for
which
no
results
were
returned
which
one
could
interpret
as
moderation
or
as
the
result
of
relevance
threshold
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
table
sentiment
score
higher
score
is
more
negative
us
jan
2022
left
and
aug
2022
right
number
of
completions
per
category
average
sentiment
score
standard
deviation
of
sentiment
scores
category
age
gender
gender
identity
gendered
lifestyle
way
of
living
political
politicised
peoples
ethnicities
nationalities
religion
sexual
orientation
google
yahoo
duck
google
yahoo
duck
18
70.0
0.4
98
68.3
0.4
31
74.9
0.4
33.5
0.6
30
63.1
0.5
60
28.1
0.4
96.5
0.08
42
83.2
0.4
64
77.8
0.4
20
81.0
0.4
30
96.5
0.2
94
73.3
0.4
36
68.4
0.4
38
75.5
0.4
45
75.3
0.4
52
59.6
0.5
12
43.9
0.5
99.8
14
35.8
0.5
88
31.9
0.5
19
76.6
0.4
90
72.9
0.4
30
63.4
0.5
33.5
0.6
33
54.4
0.5
54
32.9
0.5
91.1
0.1
48
85.2
0.4
79
76.7
0.4
20
86
0.3
30
96.5
0.2
106
71.5
0.4
51
66.2
0.5
42
73.2
0.4
45
75.5
0.4
58
65.5
0.5
11
47.7
0.5
99.1
0.01
16
43.6
0.5
89
29.3
0.4
of
the
candidate
autocompletion
when
one
peruses
the
groups
to
which
this
observation
applies
the
likelihood
that
they
have
been
moderated
rather
than
underpopulated
with
associations
remains
high
pretrained
language
models
have
been
shown
to
suffer
from
what
is
termed
lexical
bias
meaning
that
they
associate
the
mere
mention
of
marginalised
identity
with
negative
sentiment
kiritchenko
and
mohammad
38
this
might
drive
up
scores
for
negative
sentiment
for
certain
categories
acknowledgments
we
thank
our
anonymous
reviewers
for
their
insightful
comments
the
work
for
this
publication
is
financially
supported
by
the
project
from
learning
to
meaning
new
approach
to
generic
sentences
and
implicit
biases
project
number
406.18
tw
007
of
the
research
programme
sgw
open
competition
which
is
partly
financed
by
the
dutch
research
council
nwo
references
abubakar
abid
maheen
farooqi
and
james
zou
2021
large
language
models
associate
muslims
with
violence
nature
machine
intelligence
2021
461
463
ls
al-abbas
ahmad
haider
and
riyad
hussein
2020
google
autocomplete
search
algorithms
and
the
arabs
perspectives
on
gender
case
study
of
google
egypt
gema
online
journal
of
language
studies
20
2020
95
112
perspective
api
2023
about
the
api
attributes
and
languages
https://developers.perspectiveapi.com/s/about-the-api-attributes-andlanguages?language=en_us
paul
baker
and
amanda
potts
2013
why
do
white
people
have
thin
lips
google
and
the
perpetuation
of
stereotypes
via
auto-complete
search
forms
critical
discourse
studies
10
may
2013
187
204
https://doi.org/10.1080/
17405904.2012
744320
1058
judit
bar-ilan
2006
web
links
and
search
engine
ranking
the
case
of
google
and
the
query
jew
journal
of
the
american
society
for
information
science
and
technology
57
12
2006
1581
1589
solon
barocas
kate
crawford
aaron
shapiro
and
hanna
wallach
2017
the
problem
with
bias
allocative
versus
representational
harms
in
machine
learning
in
9th
annual
conference
of
the
special
interest
group
for
computing
information
and
society
emily
bender
timnit
gebru
angelina
mcmillan-major
and
shmargaret
shmitchell
2021
on
the
dangers
of
stochastic
parrots
can
language
models
be
too
big
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
610
623
su
lin
blodgett
solon
barocas
hal
daumé
iii
and
hanna
wallach
2020
language
technology
is
power
critical
survey
of
bias
in
nlp
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
5454
5476
tolga
bolukbasi
kai-wei
chang
james
zou
venkatesh
saligrama
and
adam
tauman
kalai
2016
man
is
to
computer
programmer
as
woman
is
to
homemaker
debiasing
word
embeddings
advances
in
neural
information
processing
systems
29
2016
4349
4357
http://papers.nips.cc/book/advancesin-neural-information-processing-systems-29-2016
10
carole
cadwalladr
2016
google
democracy
and
the
truth
about
internet
search
the
guardian
12
2016
2016
11
aylin
caliskan
joanna
bryson
and
arvind
narayanan
2017
semantics
derived
automatically
from
language
corpora
contain
human-like
biases
science
356
6334
2017
183
186
https://doi.org/10.1126/science.aal4230
12
anne
sy
cheung
2015
defaming
by
suggestion
searching
for
search
engine
liability
in
the
autocomplete
era
comparative
perspectives
on
the
fundamentals
of
freedom
of
expression
andras
koltay
ed
forthcoming
university
of
hong
kong
faculty
of
law
research
paper
2015
018
2015
13
rochelle
choenni
ekaterina
shutova
and
robert
van
rooij
2021
stepmothers
are
mean
and
academics
are
pretentious
what
do
pretrained
language
models
learn
about
you
in
proceedings
of
the
2021
conference
on
empirical
methods
in
natural
language
processing
1477
1491
14
kate
crawford
2017
the
trouble
with
bias
keynote
at
neurips
15
kimberlé
crenshaw
2017
on
intersectionality
essential
writings
the
new
press
16
sunipa
dev
masoud
monajatipoor
anaelia
ovalle
arjun
subramonian
jeff
phillips
and
kai-wei
chang
2021
harms
of
gender
exclusivity
and
challenges
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
in
non-binary
representation
in
language
technologies
in
proceedings
of
the
2021
conference
on
empirical
methods
in
natural
language
processing
1968
1994
17
jwala
dhamala
tony
sun
varun
kumar
satyapriya
krishna
yada
pruksachatkun
kai-wei
chang
and
rahul
gupta
2021
bold
dataset
and
metrics
for
measuring
biases
in
open-ended
language
generation
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
862
872
18
nicholas
diakopoulos
2015
algorithmic
accountability
journalistic
investigation
of
computational
power
structures
digital
journalism
2015
398
415
19
lucas
dixon
john
li
jeffrey
sorensen
nithum
thain
and
lucy
vasserman
2018
measuring
and
mitigating
unintended
bias
in
text
classification
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
new
york
ny
usa
2018
12
27
aies
18
association
for
computing
machinery
67
73
https
doi
org
10.1145
3278721.3278729
20
steve
elers
2014
maori
are
scum
stupid
lazy
maori
according
to
google
te
kaharoa
2014
21
kawin
ethayarajh
david
duvenaud
and
graeme
hirst
2019
understanding
undesirable
word
embedding
associations
in
proceedings
of
the
57th
annual
meeting
of
the
association
for
computational
linguistics
florence
italy
2019
association
for
computational
linguistics
1696
1705
https://doi.org/10.18653/
v1
p19-1166
22
anjalie
field
su
lin
blodgett
zeerak
waseem
and
yulia
tsvetkov
2021
survey
of
race
racism
and
anti-racism
in
nlp
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
1905
1925
23
lj
flynn
2004
google
says
it
doesn
plan
to
change
search
results
the
new
york
times
2004
24
samuel
gehman
suchin
gururangan
maarten
sap
yejin
choi
and
noah
smith
2020
realtoxicityprompts
evaluating
neural
toxic
degeneration
in
language
models
in
findings
of
the
association
for
computational
linguistics
emnlp
2020
3356
3369
25
samuel
gibbs
2016
google
alters
search
autocomplete
to
remove
are
jews
evil
suggestion
the
guardian
2016
26
tarleton
gillespie
2018
custodians
of
the
internet
yale
university
press
27
ben
gomes
2017
our
latest
quality
improvements
for
search
google
blog
2017
28
hila
gonen
and
yoav
goldberg
2019
lipstick
on
pig
debiasing
methods
cover
up
systematic
gender
biases
in
word
embeddings
but
do
not
remove
them
in
proceedings
of
the
2019
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
long
and
short
papers
minneapolis
minnesota
2019
06
association
for
computational
linguistics
609
614
https://doi.org/10.18653/v1/n19-1061
29
google
2022
removing
content
from
google
https://support.google.com/legal/
troubleshooter
1114905
ts
9814647
2c9815053
2c3337372
30
james
grimmelmann
2015
the
virtues
of
moderation
yale
jl
tech
17
2015
42
31
kirsten
grind
sam
schechner
robert
mcmillan
and
john
west
2019
how
google
interferes
with
its
search
algorithms
and
changes
your
results
the
wall
street
journal
15
2019
32
jochen
hartmann
mark
heitmann
christian
siebert
and
christina
schamp
2022
more
than
feeling
accuracy
and
application
of
sentiment
analysis
international
journal
of
research
in
marketing
2022
33
timothy
hazen
alexandra
olteanu
gabriella
kazai
fernando
diaz
and
michael
golebiewski
2022
on
the
social
and
technical
challenges
of
web
search
autosuggestion
moderation
first
monday
2022
34
po-sen
huang
huan
zhang
ray
jiang
robert
stanforth
johannes
welbl
jack
rae
vishal
maini
dani
yogatama
and
pushmeet
kohli
2020
reducing
sentiment
bias
in
language
models
via
counterfactual
evaluation
in
findings
of
the
association
for
computational
linguistics
emnlp
2020
online
2020
11
association
for
computational
linguistics
65
83
https://doi.org/10.18653/v1/2020.findingsemnlp.7
35
ben
hutchinson
vinodkumar
prabhakaran
emily
denton
kellie
webster
yu
zhong
and
stephen
denuyl
2020
social
biases
in
nlp
models
as
barriers
for
persons
with
disabilities
in
proceedings
of
the
58th
annual
meeting
of
the
association
for
computational
linguistics
5491
5501
36
stavroula
karapapa
and
maurizio
borghi
2015
search
engine
liability
for
autocomplete
suggestions
personality
privacy
and
the
power
of
the
algorithm
international
journal
of
law
and
information
technology
23
2015
261
289
37
svetlana
kiritchenko
and
saif
mohammad
2018
examining
gender
and
race
bias
in
two
hundred
sentiment
analysis
systems
in
proceedings
of
the
seventh
joint
conference
on
lexical
and
computational
semantics
new
orleans
louisiana
2018
association
for
computational
linguistics
43
53
https://doi.org/10.
18653
v1
s18-2005
38
svetlana
kiritchenko
and
saif
mohammad
2018
examining
gender
and
race
bias
in
two
hundred
sentiment
analysis
systems
arxiv
preprint
arxiv
1805.04508
2018
39
alina
leidinger
and
richard
rogers
2023
stereotype
elicitation
in
google
duckduckgo
and
yahoo
autcompletion
https://doi.org/10.5281/zenodo.7906930
1059
40
walter
lippmann
1922
public
opinion
new
york
mcmillan
41
yinhan
liu
myle
ott
naman
goyal
jingfei
du
mandar
joshi
danqi
chen
omer
levy
mike
lewis
luke
zettlemoyer
and
veselin
stoyanov
2019
roberta
robustly
optimized
bert
pretraining
approach
arxiv
preprint
arxiv
1907.11692
2019
42
vijit
malik
sunipa
dev
akihiro
nishi
nanyun
peng
and
kai-wei
chang
2022
socially
aware
bias
measurements
for
hindi
language
representations
in
proceedings
of
the
2022
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
1041
1052
43
thomas
manzini
lim
yao
chong
alan
black
and
yulia
tsvetkov
2019
black
is
to
criminal
as
caucasian
is
to
police
detecting
and
removing
multiclass
bias
in
word
embeddings
in
proceedings
of
the
2019
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
volume
long
and
short
papers
615
621
44
todor
markov
chong
zhang
sandhini
agarwal
tyna
eloundou
teddy
lee
steven
adler
angela
jiang
and
lilian
weng
2022
holistic
approach
to
undesired
content
detection
in
the
real
world
arxiv
preprint
arxiv
2208.03274
2022
45
boaz
miller
and
isaac
record
2017
responsible
epistemic
technologies
socialepistemological
analysis
of
autocompleted
web
search
new
media
society
19
12
2017
1945
1963
46
moin
nadeem
anna
bethke
and
siva
reddy
2021
stereoset
measuring
stereotypical
bias
in
pretrained
language
models
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
5356
5371
47
nikita
nangia
clara
vania
rasika
bhalerao
and
samuel
bowman
2020
crowspairs
challenge
dataset
for
measuring
social
biases
in
masked
language
models
in
proceedings
of
the
2020
conference
on
empirical
methods
in
natural
language
processing
emnlp
1953
1967
48
safiya
umoja
noble
2018
algorithms
of
oppression
new
york
university
press
49
debora
nozza
federico
bianchi
dirk
hovy
et
al
2021
honest
measuring
hurtful
sentence
completion
in
language
models
in
proceedings
of
the
2021
conference
of
the
north
american
chapter
of
the
association
for
computational
linguistics
human
language
technologies
association
for
computational
linguistics
50
openai
2023
moderation
https://platform.openai.com/docs/guides/moderation/
overview
51
devah
pager
2007
the
use
of
field
experiments
for
studies
of
employment
discrimination
contributions
critiques
and
directions
for
the
future
the
annals
of
the
american
academy
of
political
and
social
science
609
2007
104
133
52
ji
ho
park
jamin
shin
and
pascale
fung
2018
reducing
gender
bias
in
abusive
language
detection
in
proceedings
of
the
2018
conference
on
empirical
methods
in
natural
language
processing
brussels
belgium
2018
10
association
for
computational
linguistics
2799
2804
https://doi.org/10.18653/v1/d18-1302
53
bernhard
rieder
and
jeanette
hofmann
2020
towards
platform
observability
internet
policy
review
2020
28
54
sarah
roberts
2019
behind
the
screen
yale
university
press
55
senjooti
roy
and
liat
ayalon
2020
age
and
gender
stereotypes
reflected
in
google
autocomplete
function
the
portrayal
and
possible
spread
of
societal
stereotypes
the
gerontologist
60
2020
1020
1028
56
senjooti
roy
liat
ayalon
gabi
weisfeld
and
barbara
bowers
2020
age
and
gender
stereotypes
reflected
in
google
autocomplete
function
the
portrayal
and
possible
spread
of
societal
stereotypes
the
gerontologist
60
aug
2020
1020
1028
https://doi.org/10.1093/geront/gnz172
57
christian
sandvig
kevin
hamilton
karrie
karahalios
and
cedric
langbort
2014
auditing
algorithms
research
methods
for
detecting
discrimination
on
internet
platforms
data
and
discrimination
converting
critical
concerns
into
productive
inquiry
22
2014
4349
4357
58
victor
sanh
albert
webson
colin
raffel
stephen
bach
lintang
sutawika
zaid
alyafeai
antoine
chaffin
arnaud
stiegler
teven
scao
arun
raja
et
al
2022
multitask
prompted
training
enables
zero-shot
task
generalization
in
international
conference
on
learning
representations
59
teven
le
scao
angela
fan
christopher
akiki
ellie
pavlick
suzana
ilić
daniel
hesslow
roman
castagné
alexandra
sasha
luccioni
françois
yvon
matthias
gallé
et
al
2022
bloom
176b-parameter
open-access
multilingual
language
model
arxiv
preprint
arxiv
2211.05100
2022
60
emily
sheng
kai-wei
chang
prem
natarajan
and
nanyun
peng
2021
societal
biases
in
language
generation
progress
and
challenges
in
proceedings
of
the
59th
annual
meeting
of
the
association
for
computational
linguistics
and
the
11th
international
joint
conference
on
natural
language
processing
volume
long
papers
4275
4293
61
eric
michael
smith
melissa
hall
melanie
kambadur
eleonora
presani
and
adina
williams
2022
sorry
to
hear
that
finding
bias
in
language
models
with
holistic
descriptor
dataset
arxiv
preprint
arxiv
2205.09209
2022
62
he
stanton
1971
incidental
and
intentional
learning
one
process
or
two
australian
psychologist
1971
26
30
63
danny
sullivan
2018
how
google
autocomplete
works
in
search
retrieved
november
22
2018
2018
facct
23
june
12
15
2023
chicago
il
usa
leidinger
and
rogers
64
danny
sullivan
2019
how
we
keep
search
relevant
and
useful
google
the
keyword
blog
july
15
2019
65
danny
sullivan
2020
how
google
autocomplete
predictions
are
generated
retrieved
october
2020
2020
66
yi
chern
tan
and
elisa
celis
2019
assessing
social
and
intersectional
biases
in
contextualized
word
representations
in
advances
in
neural
information
processing
systems
32
annual
conference
on
neural
information
processing
systems
2019
neurips
2019
december
14
2019
vancouver
bc
canada
2019
11
04
hanna
wallach
hugo
larochelle
alina
beygelzimer
florence
alché
buc
emily
fox
and
roman
garnett
eds
13209
13220
http://papers.nips.cc/
book
advances-in-neural-information-processing-systems-32-2019
67
madalina
vlasceanu
and
david
amodio
2022
propagation
of
societal
gender
inequality
by
internet
search
algorithms
proceedings
of
the
national
academy
of
sciences
119
29
2022
e2204529119
68
denny
vrandečić
and
markus
krötzsch
2014
wikidata
free
collaborative
knowledgebase
commun
acm
57
10
sep
2014
78
85
https://doi.org/10.
1145
2629489
69
kellie
webster
xuezhi
wang
ian
tenney
alex
beutel
emily
pitler
ellie
pavlick
jilin
chen
and
slav
petrov
2020
measuring
and
reducing
gendered
correlations
in
pre-trained
models
http://arxiv.org/abs/2010.06032
70
laura
weidinger
jonathan
uesato
maribeth
rauh
conor
griffin
po-sen
huang
john
mellor
amelia
glaese
myra
cheng
borja
balle
atoosa
kasirzadeh
et
al
1060
2022
taxonomy
of
risks
posed
by
language
models
in
2022
acm
conference
on
fairness
accountability
and
transparency
214
229
71
thomas
wolf
lysandre
debut
victor
sanh
julien
chaumond
clement
delangue
anthony
moi
pierric
cistac
tim
rault
rémi
louf
morgan
funtowicz
et
al
2019
huggingface
transformers
state-of-the-art
natural
language
processing
arxiv
preprint
arxiv
1910.03771
2019
72
un
women
2013
un
women
ad
series
reveals
widespread
sexism
un
women
21
2013
73
yahoo
2023
yahoo
search
autocomplete
policy
hhttps
help
yahoo
com
kb
sln36183
html
74
brian
hu
zhang
blake
lemoine
and
margaret
mitchell
2018
mitigating
unwanted
biases
with
adversarial
learning
in
proceedings
of
the
2018
aaai
acm
conference
on
ai
ethics
and
society
new
york
ny
usa
2018
12
27
aies
18
association
for
computing
machinery
335
340
https://doi.org/10.1145/
3278721.3278779
75
susan
zhang
stephen
roller
naman
goyal
mikel
artetxe
moya
chen
shuohui
chen
christopher
dewan
mona
diab
xian
li
xi
victoria
lin
et
al
2022
opt
open
pre-trained
transformer
language
models
arxiv
preprint
arxiv
2205.01068
2022
additional
tables
stereotype
moderation
in
search
engine
autocompletion
facct
23
june
12
15
2023
chicago
il
usa
table
10
average
number
of
autosuggestions
per
group
us
january
2022
category
groups
google
yahoo
duck
23
14
32
47
11
2.4
3.9
2.1
0.4
1.1
0.2
3.4
1.4
3.6
3.2
1.1
3.8
5.6
2.5
0.8
0.3
0.5
1.9
age
gender
gender
identity
gendered
lifestyle
way
of
living
political
politicised
peoples
ethnicities
nationalities
religion
sexual
orientation
table
11
average
number
of
autosuggestions
per
group
us
august
2022
category
groups
google
yahoo
duck
23
14
32
47
11
2.3
4.3
2.2
0.4
0.9
1.3
0.5
5.3
2.8
1.4
3.8
2.8
0.8
3.5
5.6
2.3
0.0
0.1
0.4
1.9
age
gender
gender
identity
gendered
lifestyle
way
of
living
political
politicised
peoples
ethnicities
nationalities
religion
sexual
orientation
table
12
examples
of
synthetic
patterns
appended
to
organic
search
logs
duckduckgo
why
are
boys
so
ugh
why
are
brothers
so
sweaty
step
brothers
annoying
quiz
why
are
kids
so
entitled
now
yahoo
why
are
americans
so
stupid
2020
angry
ielts
reading
why
are
boys
so
stupid
quotes
why
are
black
americans
so
much
bigger
than
africans
today
why
are
muslims
so
spoken
word
how
come
homeless
people
are
so
sensitive
quotes
why
are
daughters
so
mean
to
their
mothers
now
why
are
russians
so
cruel
people
images
why
are
kids
so
sensitive
these
days
dumb
today
why
are
teens
so
sad
derek
thomson
sad
today
1061