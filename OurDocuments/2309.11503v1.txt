Fairness Vs. Personalization: Towards Equity in Epistemic Utility
JENNIFER CHIEN, UC San Diego, USA
DAVID DANKS, UC San Diego, USA
The applications of personalized recommender systems are rapidly expanding: encompassing social media, online shopping, search
engine results, and more. These systems oï¬€er a more eï¬ƒcient way to navigate the vast array of items available. However, alongside
this growth, there has been increased recognition of the potential for algorithmic systems to exhibit and perpetuate biases, risking

arXiv:2309.11503v1 [cs.IR] 5 Sep 2023

unfairness in personalized domains. In this work, we explicate the inherent tension between personalization and conventional implementations of fairness. As an alternative, we propose equity to achieve fairness in the context of epistemic utility. We provide a
mapping between goals and practical implementations and detail policy recommendations across key stakeholders to forge a path
towards achieving fairness in personalized systems.
Additional Key Words and Phrases: fairness, personalization, recommender systems, equity, epistemic utility, epistemic harms, policy
ACM Reference Format:
Jennifer Chien and David Danks. 2023. Fairness Vs. Personalization: Towards Equity in Epistemic Utility. 1, 1 (September 2023),
11 pages.

1 INTRODUCTION
Personalized algorithmic systems are being increasingly deployed in a broad range of sectors. These algorithms hold
the potential to provide more appropriate outputs on an individual basis by personalizing based on preferences, values,
needs, or environmental conditions. Most prominently, recommender systems are now widely used to provide the most
useful recommendations to each individual in the given domain[1].
At the same time, we have seen a signiï¬cant increase in the recognition that algorithms can exhibit biases and
produce unfair or unjust outcomes. There are many diï¬€erent potential sources of bias in algorithms and models, and
hence many diï¬€erent responses may be appropriate or required. Algorithm development and deployment eï¬€orts now
typically recognize the possibility of algorithmic bias, and the need to (often) do something to mitigate it[2].
We contend that these two desiderata for algorithms â€“ personalization and fairness â€“ stand in signiï¬cant tension.
At a high level, personalization is fundamentally about treating each individual in distinct ways; the goal is to not give
the same output for each individual, but rather to tailor the outputs to their speciï¬c situation. In contrast, fairness is
fundamentally about treating individuals similarly; the algorithm ought to be â€œthe sameâ€, in some sense, for everyone.
Of course, this high-level gloss on the tension is far too quick: for example, fairness allows for diï¬€erential treatment or
outcomes, as long as it is based on morally or legally defensible grounds. Nonetheless, these high-level observations
point towards a tension that, we contend, continues to hold when we look deeper. More speciï¬cally, we analyze fairness
in the context of personalized systems using the notion of epistemic utility â€“ essentially, the beneï¬t that an agent
receives by an epistemic improvement (e.g., reduction in uncertainty) â€“ and provide both practical and policy guidance
about how to achieve fair, personalized systems.
Authorsâ€™ addresses: Jennifer Chien, jjchien@ucsd.edu, UC San Diego, USA; David Danks, ddanks@ucsd.edu, UC San Diego, USA.

2023. Manuscript submitted to ACM
Manuscript submitted to ACM

1

2

Jennifer Chien and David Danks

2 PRELIMINARIES
Recommender System. We consider a recommender system as that which uses an algorithmic approach to provide
an ordering or prioritization for items based on relevance to a user. This form of personalization is often based on their
preferences, past behavior, or similarities with other users with the overarching goal of improving user experience and
engagement.
Group Fairness. Algorithmic fairness is typically framed as metric parity1 across two or more relevantly comparable entities such that the diï¬€erence between them is deemed socially, legally, and/or culturally irrelevant. For instance,
group fairness may be deï¬ned as demographic parity, meaning equality in the likelihood of a given outcome conditioned on the group (e.g. gender, age, disability)[4]. Formally:
ğ‘ƒ (ğ‘¦Ë† | ğ‘” = ğ‘”ğ‘– ) = ğ‘ƒ (ğ‘¦Ë† | ğ‘” = ğ‘” ğ‘— )âˆ€ğ‘”ğ‘– , ğ‘” ğ‘— âˆˆ {1, ..., ğº }

(1)

where ğº is the total number of groups and ğ‘¦Ë† is the likelihood of a given outcome.
Epistemic Utility. We deï¬ne epistemic utility as the value or usefulness of information in terms of improving knowledge, understanding, or beliefs. It encompasses the epistemic beneï¬ts gained from acquiring accurate, reliable, and
relevant information. These beneï¬ts could, of course, enhance decision-making, prediction, problem-solving, or overall intellectual development, but epistemic utility refers only to the beneï¬ts in terms of the agentâ€™s epistemic state
(even if those improvements do not immediately lead to improved outcomes). It thereby captures the intrinsic value of
acquiring knowledge and understanding. It underscores the role of information as a valuable resource for intellectual
growth, social mobility and production, and captures the motivational value of information for peopleâ€™s behavior.
3 RELATED WORK
This work is most closely related to eï¬€orts to understand fairness in recommender systems, many of which deï¬ne
fairness as parity in some performance measure across some sensitive attribute [5, 6, 7, 8, 9, 10]. Methods for individual
notions of fairness similarly require a domain-speciï¬c similarity function and focus on trade-oï¬€s between individual
and group fairness[11, 12]. In contrast, we start from the position that fairness requires equity across all individuals,
rather than being deï¬ned solely through comparisons. This approach enables us to account for cases of unfairness
where everyone is worse oï¬€ (including situations with non-comparative injustices), not only those in which one group
is.
More speciï¬cally, prior work has aimed to ensure proportional representation and exposure, often in e-commerce
settings. Measures such as disparate exposure and visibility still require access to sensitive or group attributes [8, 9, 10].
Calibration, in which the expected proportions of predicted classes match those observed in available data, implements
group fairness without explicit knowledge of sensitive attribute status[13, 14, 15, 16, 17]. However, its application in
domains such as news can contribute to hegemonic regimes of representation, making it limited to applications where
less diversity is valued by users[17]. Additionally, fairness deï¬nitions that target disparities in utility compare average
sensitive attribute group performance rather than providing assurances for individuals or the entire population as a
whole [11, 18, 19, 20].
Concerns about unfairness in personalized algorithms is connected to research on epistemic injustice[21]. The core
concern in this latter work is that epistemic limitations may impair an agentâ€™s ability to recognize or respond to
1 within some tolerance as is legally permissible, e.g., the four-ï¬fths rule[3]

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

3

injustice. For example, if an individual does not have the concept of â€˜racial discriminationâ€™, then they might fail to
detect its occurrence despite suï¬€ering harms from it. That is, the injustice is not only about what happens to the
individual, but also about their (epistemic) inability to recognize, understand, and respond to it. Personalized systems
can readily lead to diï¬€erent individuals having diï¬€erent information â€“ in fact, that is one goal of such systems â€“ and so
we expect that individuals would develop diï¬€erent concepts as a result. Hence, personalized systems can signiï¬cantly
raise the risks of epistemic injustice.
Research on over-personalization and information access is similarly closely related. While the inï¬‚uence of personalization in ï¬lter bubbles and echo chambers has been denied for purely technical systems [22, 23, 24], when considering
human interactions, the inï¬‚uence becomes somewhat complex. This is, in part, due to signiï¬cant variability in how
users engage with a site, accept or reject information, and perceive the impartiality of the content they receive [25, 26,
27, 28, 29]. Our research adopts a sociotechnical approach to deï¬ne and scope the problem, taking into account the
concept of epistemic utility and making reasonable assessments of human eï¬€ort.
4 CORE TENSION: PERSONALIZATION AND STANDARD APPROACHES TO FAIRNESS
Fairness is often deï¬ned as metric parity across similar groups or individuals, where determining similarity across units
remains an open question. In search settings, one might start by grouping together individuals with similar values and
interests, as, on fairness grounds, they may be expected to have a similar disambiguation or content recommendation
experience. However, determining values can be diï¬ƒcult to operationalize because explicit and implicit preferences
may be conï¬‚icting or diï¬ƒcult to collect [30]. In addition, the level of granularity by which to measure such values
remains unknown. In this section, we construct arguments for why user similarity of values is diï¬ƒcult, impractical,
or perhaps even impossible to determine, thereby making similarity-based fairness deï¬nitions diï¬ƒcult to deploy or
conceptually impossible.
Sensitive attributes are problematic and coarse proxies for values. The simplest approach to constructing â€œsimilarâ€
groups for fairness considerations could use sensitive attributes (i.e. race, ethnicity, gender, age, religious aï¬ƒliation,
disability, etc). However, this approach assumes that individuals with the same sensitive attributes not only face similar
struggles, lived experiences, discrimination, and prejudice, but also have relevantly similar values. In practice, this
approach would likely result in the treatment of minority groups as monoliths â€“ perhaps even as a homogeneous
â€˜Otherâ€™. This is problematic not only in its instrumental failures of insuï¬ƒcient speciï¬city and complexity, but also in
its intrinsic treatment of a single sensitive attribute as representative of all values. Additionally, conditional groupings
only ensure parity guarantees for the average member of a group, not for each individual nor necessarily an individual
that exists, therefore enabling unfairness for anyone that doesnâ€™t satisfy such assumptions.
Taking an intersectional approach, i.e. considering subgroups of sensitive attributes may better approximate similarity of values. Consider, for example, race and gender, meaning groups such as â€œBlack Womenâ€ and â€œWhite Menâ€.
This still considers an entire group of people homogeneously, thereby rendering the plight of, for example, all Black
Women into a singular experience. In the extreme, considering all possible sub-groups across all sensitive attributes
faces practical challenges, as each subgroup becomes impractically small (for data availability purposes), thereby reifying sensitive attributes in a constant struggle for relative popularity and public consciousness [31].
User search history is noisy, underdetermines values, and can vary temporally. An alternative approach would be to
use the individualâ€™s behavior to determine the relevant group for fairness considerations. For instance, in the context
Manuscript submitted to ACM

4

Jennifer Chien and David Danks

of a search engine, we could condition on exact search history. Unfortunately, this approach faces three open challenges which we highlight in the context of search, though the problems naturally generalize to other systems that
use observed behavior to personalize on the basis of unobserved (inferred) features.
First, not all search behaviors are equally informative. For example, a search query may be made due to diï¬€erent
situational demands (e.g., homework, third party question) or may arise from very diï¬€erent motivations (e.g., devilâ€™s
advocate vs. searching for validative or supporting arguments). Hence, a speciï¬c history of search queries may or may
not be informative about an individualâ€™s values; the history alone is not suï¬ƒcient.
Second, even if search behaviors are â€œgenuine,â€ they are noisy indicators for the totality of an individualâ€™s interests
or values. For example, the individual agent might not search for a topic they already have suï¬ƒcient information
on, but it may still be a core value. More generally, consider two users with identical search histories at any given
point in time. Conditioning on search history would imply that they should receive similar experiences. However, it is
highly unlikely that these individuals have the exact same sets of values and interests: that is, multiple sets of values
and interests could be consistent with the observed behavior (i.e., the space of values are underdetermined by the
evidence). Therefore, since queries subject to the limitations of observational data and missing contextual information,
conditioning on search history alone has the potential to greatly under-specify an individualâ€™s values.
Finally, search history provides no guarantees of stability over time. Consider, again, two individuals with the same
user history. Adding a diï¬€erent element to each userâ€™s search history could lead them to be placed in fundamentally
diï¬€erent groups. More generally, at any point in time, there are inï¬nitely many ways that a search history could
progress, thereby making it impossible to guarantee fairness over time for these two individuals, let alone all. Fairness
groupings not sensitive to the time and search history, therefore, may not ever converge to fair groupings in the limit.
This echoes a similar problem in fairness research, where fairness constraints at each round of deployment do not
ensure long-term aggregate parity in the same metric [32]. While this has not rendered particular fairness metrics
obsolete, this instability points to ambiguity in the extent to which fairness must provide guarantees temporally.
5 A DIFFERENT APPROACH
5.1

Personalization

Delving further into the notion of a system tailored to each personâ€™s needs, let us deï¬ne personalization. Given a search
engine, companies may strive to deliver a service that maximizes epistemic utility and convenience. In order to avoid
wasting time, resources, or eroding consumer patience, search engine providers may optimize to disambiguate queries
as eï¬ƒciently as possible. Increases in eï¬ƒciency can be made by leveraging all accessible information about a user.
This may include search history, publicly available data, social media proï¬les, and inferred or disclosed demographic
information to accurately disambiguate a userâ€™s query. This motivates the following deï¬nition:
Deï¬nition 5.1. Inï¬nite Personalization. Consider a platform that recommends relevant content to a user ğ‘¢ ğ‘— tailored to their previous history â„ ğ‘—,ğ‘¡ at time ğ‘¡. Since the space of all possible information is very large, for each piece of
content ğ‘ğ‘– âˆˆ ğ¶, the system calculates probabilities ğ‘ƒ (ğ‘ğ‘– | â„ ğ‘—,ğ‘¡ ) estimating the likelihood of relevance. We deï¬ne inï¬nite
personalization as the phenomena in which a system is allowed to produce extremely ï¬ne-grained personalization.
Formally, there exists some ğ‘ğ‘– such that ğ‘ƒ (ğ‘ğ‘– | â„ ğ‘—,ğ‘¡ ) < ğœƒ and ğ‘ƒ (ğ‘ğ‘– | â„ ğ‘—,ğ‘¡+ğœ– ) < ğœƒ, meaning that there exists some content
such that it is arbitrarily improbable < ğœƒ to recommend a particular piece of content ğ‘ğ‘– to a user, even if a user were
to add ğœ– queries to their search history.
Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

5

For instance, user ğ‘¢ ğ‘— with a search history exclusively composed of cats inputs â€œbengal toysâ€ into a search engine
and receives recommendations about Bengal cat toys. We consider the system to be exhibiting inï¬nite personalization
if the user can take ğœ– queries about dogs (or anything else) and still not receive a recommendation for Bengal dog toys
when searching â€œbengal toysâ€.
Having deï¬ned personalization and itâ€™s eï¬€ects in the extreme case, we now deï¬ne fairness as equity and provide
motivation for the outcome we target.

5.2

Equity

In ensuring parity across similar groups, fairness guarantees are only made across those deemed suï¬ƒciently similar. An
equity-based approach, however, centers around every individual achieving their desired outcome. Such approaches
promote social cohesion, collective liberation, and address long-standing inequities, such as poverty, public health,
social mobility, and overall well-being. Moreover, they tackle concerns regarding temporal stability and robustness
by establishing a consistent criterion for each deployment phase. Equity-based approaches lend themselves to audits,
legislation, and transparency, as they deï¬ne a singular outcome requirement for all users. An open challenge, however,
is what metrics to deï¬ne equity with respect to, as each metric targets a diï¬€erent kind of unfairness.
Equity across quality of information targets unfairness when one person gets more utility from their query compared to a diï¬€erent query of another person. For example, this type of equity requires that we equalize the number
of recommendations and quality of information provided for any given query. However, this is infeasible as we cannot control the quantity of information known for a given topic. Thus, given that there are inï¬nitely many distinct
queries any user can make, ensuring that each query has the same number of relevant articles would require perfect
knowledge.
Equity in speed of access to information targets unfairness when one person receives the relevant information faster
than another. This equalizes the utility for every user by either: slowing down or acting intentionally uncooperatively
for queries that are more easily disambiguated, or speeding up the service for â€œslowerâ€ queries. Both are infeasible. The
former fails to maximize overall utility and sacriï¬ces function in the name of fairness. This in turn renders adoption
by stakeholders subject to capitalistic pressures even more unlikely. The latter is also infeasible, as we cannot directly
control disambiguation rate and therefore speed up some queries over others. More generally, information is an inï¬nitely shareable resource, so providing information to one person does not deprive someone else the beneï¬t of that
same information. Therefore speed of access to information is a poor target for an equity-based intervention.
In contrast, equity in epistemic utility targets unfairness in personalized access to the information resources. This is
motivated by the intuition that although we cannot directly control the availability of information or disambiguation
rate, everyone should have some baseline access to all information. Thus, we propose an upper bound on the number
of queries to access to any piece of information. This means that if the information does exist, all users should be able to
access it by exerting a â€œreasonableâ€ amount of eï¬€ort. This does not require that all queries have the same accessibility
or disambiguation, nor that all users be provided the exact same service. It does provide a soft guarantee that everyone
is able to achieve some base-level epistemic utility without requiring the grouping or comparison of individuals on the
basis of their values, needs, or histories. Epistemic utility, particularly through the internet, is recognized as a universal
human right by Article 19 of the Universal Declaration of Human Rights [33], stating that, â€œEveryone has the right to
freedom...to seek, receive and impart information and ideas through any media and regardless of frontiers.â€
Manuscript submitted to ACM

6

Jennifer Chien and David Danks
Deï¬nition 5.2. ğœ–-Equity Fairness. Given a user ğ‘¢ ğ‘— with history â„ ğ‘—,ğ‘¡ , we deï¬ne ğœ–-Equity fairness as the upper bound

(ğœ–) of the number of additions to the search history such that for any given content ğ‘ğ‘– âˆˆ ğ¶, adding ğœ– queries to the
history will result in ğ‘ƒ (ğ‘ğ‘– | â„ ğ‘—,ğ‘¡+ğœ– ) â‰¥ ğœƒ, enabling the user to obtain the relevant content. Here, we consider the case in
which the relevant content does not exist as out of scope and focus purely on access.
Embracing equity as a fundamental principle of fairness presents a range of complex challenges. These challenges
encompass deï¬ning a baseline level of equity for all users, determining the pertinent factors that contribute to establishing such a baseline, and deliberating whether these considerations should be domain-speciï¬c or universally applicable.
Rather than proposing a singular standard, we propose adopting a heuristic operationalization that prioritizes those
who are most disadvantaged in terms of knowledge and understanding. Our intention is not to argue for a strictly prioritarian perspective, but that the incremental advancements can achieve the ultimate goal of equity in the long-term.
Thus, we advocate for equity as a guiding concept to achieve equality by inductively reaching fairness in the limit.
Having established deï¬nitions inï¬nite personalization and ğœ–-equity fairness, next we ï¬‚esh out the tension between
the two.
6 INHERENT CONFLICT AND TRADE-OFFS
Inï¬nite personalization, we posit, comes at a cost to ğœ–-equity fairness. More explicitly, in inï¬nite personalization, the
relevance probability of some information ğ‘ğ‘– to a user can be less than ğœƒ. This is due to the eï¬€ects of personalization:
indexing and re-ranking for eï¬ƒcient navigation across the vast space of information. However, this violates ğœ–-equity
fairness, as some information will be eï¬€ectively rendered unreachable within a ï¬xed window of queries. We provide
some examples of trade-oï¬€s below:
â€¢ Search engine personalization prioritizes results relative to a userâ€™s input query and prior history. For any given
topic orthogonal to a userâ€™s past history, personalization may rank desired items far beyond the average, expected, or typical number of results any given user explores, rendering it practically infeasible to reach.
â€¢ Social media serves many functions: dissemination of news and information, entertainment, and social connection. Personalization in the extreme may contribute to propagation of mis- and disinformation, echo chambers,
ï¬lter bubbles, radicalization, and social disconnection.
â€¢ Github Copilot (developed by OpenAI) focuses on code completion, providing suggestions for code lines or
entire functions directly integrated into interactive development environments. Here, personalization has the
potential to provide codebase-speciï¬c suggestions and completion, but may also infringe on a coderâ€™s ability
construct novel functionality.
â€¢ Google Bard is a language model for synthesizing insights for settings with a wide range of opinions, perspectives, or no right answer. As an augmentation to search-engine information retrieval, this may facilitate more
eï¬ƒcient ordering and disambiguation of search results, but may also infringe on access to seemingly irrelevant
information.
â€¢ ChatGPT is a general-purpose large language model (LLM) designed to engage in human-like conversations and
answer a wide range of questions. Personalization in an extreme case, may lead to conversational loops â€“ or
conversations that remain within the scope of a single topic, rather than having the ï¬‚exibility to move between
topics despite a userâ€™s prompt or request. 2
2 Here we list several examples that use LLMs due to their relevance to personalization, information synthesis and curation, and appeals to human-

language usability. We acknowledge that there are many open relevant problems related to personalization, such as contribution or exacerbation of
representational harms. For the scope of this work, we focus on how such systems can impact access to information.
Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

7

The key takeaway of this list is not that personalization necessitates unfairness or inequity, nor that fairness is not
achievable in personalized systems. Rather, to emphasize that personalization without consideration of fairness can
readily lead to systems that deny some individuals a base-level of expected epistemic utility.
7 POLICY GOALS AND EXAMPLES OF IMPLEMENTATIONS
While this conceptual analysis provides guidance about how best to think about fairness in the context of personalized
algorithms, it does not provide guidance about how to achieve fairer, less biased systems. As we turn to more practical
recommendations, we start by considering key stakeholders and their associated roles (Table 1). This creates a lexicon
and clariï¬es the expected function of each stakeholder.
Each of these stakeholders has a wide range of actions available that can lead to fairer personalized systems (see
Table 2). These actions include a wide range of governance mechanisms, ranging from hard (e.g., regulation) to soft (e.g.,
social norms). We emphasize that there is no single response to â€œï¬xâ€ unfairness in personalized systems: inevitably,
many interdependent actions will likely need to be taken in order to make progress on this problem. Moreover, these
are by no means intended to be exhaustive, but rather provide a grounded starting point that emphasizes the necessity
of cross-stakeholder collaborations.

Manuscript submitted to ACM

8

Jennifer Chien and David Danks

Stakeholder

Role

Government

Develop guidelines, principles, procedures, regulations, and standards for
safe, sustainable deployment. May hold the power to create structural supporting systems directly within the government or ï¬nancially support external systems of enforcement of such regulations. May also enact other means
of operationalizing policy, such as hosting interdisciplinary research summits
or sending representatives to build collaborations with other stakeholders to
better inform outputs. This stakeholder may also set roles and norms for other
stakeholder responsibilities and consequences.

Civil Society

Conduct evaluations and investigations of algorithms and software developed by others. This stakeholder may serve as an external, third-party entity
for conducting unbiased evaluations of performance and compliance with
policy. May be responsible for developing novel methods of measurement.

Industry

Implement technical methods, tools, and technological innovations, usually
manifesting as a consumer-facing service or product. May be subject to practical implementation constraints (i.e. ï¬nancial, competition, or legislative).
While a particular product may be theoretically feasible, it may not be practically viable or sustainable for a company to produce. Responsibilities may
also include foreseeing and adequately mitigating harms of deployed products on those directly and indirectly aï¬€ected.

Academia (broad)

Develop and implement technical, theoretical, and conceptual knowledge
without direct consideration of proï¬ts. Results can include work that builds
upon understanding, implications, or evaluation methods (i.e. downstream
impacts, sociotechnical approaches). This stakeholder has the potential to act
as an unbiased actor in developing best-practices for sustainable, long-term
practices, potentially at the direct cost of proï¬ts (i.e. considering environmental, social, and cultural norms and inï¬‚uences).

General Public

Provide input, feedback, criticism, and opinions on direction and alignment
with values for research and innovation via implicit and explicit signals on
the order of communities, groups, and/or individuals. Public support (or lack
thereof) can be collected across a multitude of avenues and granularities, including ï¬nancial support, interest, engagement, collective action, protests, organizations, and projects.

Table 1. Stakeholder Roles. We define the expected roles of stakeholders for clarity of responsibilities enumerated in Table 2. We
note that these responsibilities are not restrictive. There are many ways in which one stakeholder may overlap or even take over the
responsibilities of another. For instance, if a company develops a policy for user privacy while itâ€™s competitors do not, self-imposed
constraints on data collection may impact their product performance. However, as this design value receives public support, this can
become a standard competitive service without the intervention of government or civil society.

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility

Goal

Example Implementation/Operationalization

Mitigation

Industry, Academia: Develop algorithms that facilitate connections between disjoint parts of the data manifold. This might be analogous to connecting diï¬€erent
parts of the internet together through links, but done by creating arbitrary injections that fabricate similarities, covariances, or links between disjoint parts of the
training data.
Government: Construct structural incentives (ï¬nancial, prestige/recognition,
standards) for achieving baseline levels of interconnectivity to ensure equitable
access.

Audit

Industry, Academia, Civil Society: Develop measures that quantify epistemic
utility for users using a range of behaviors and data (e.g. tool use, experienced frustration, time to complete a task, etc.). Produce formal, domain-speciï¬c representations of distributions of utility, such as non-uniform distributions being indicators
of threats to fairness. Measure utility and study barriers to utility for non-users and
those who have left the platform.
Civil Society: Report the utility distributions for non-users to compare against
those that are already using the service.
Academia: Deï¬ning and measuring downstream impact may be essential to thorough quantiï¬cation of disparities and unfairness. Downstream impacts such as social inï¬‚uences on platform usage can be inï¬‚uenced by an individualâ€™s experience.
Government: Regulation for timing and structure of fairness audits for personalized systems, including guidance on composition of internal and external stakeholders.

Transparency

Industry, Civil Society: Disclosure of aforementioned distributions of utility
across various groups of users. Reports should include comments about whether
these distributions are problematic, what are the proposed solutions, and on what
timeline.
Government: Clear consequences and/or reparations for users aï¬€ected throughout improvement and for continued negligence.
General Public: Active engagement and feedback on reports to inform appropriate
consequences and informed public (dis)favor.

Individual Control

Industry, Academia: Design control mechanisms over the degree of exploration
to a query when serving recommendations. This may include prompting users for
additional input before producing a prediction. Alternatively, allowing users to reset their history or set manual preferences over personalization ï¬lters.
Government: Regulation and enforcement of satisfaction and compliance.
General Public: Participation and feedback on norm setting of what features
should be available, how accessible services should be, determine embedded values such as degree of â€œreasonablenessâ€ that are feasible and realistic for all users.

9

Education & Awareness Industry, Academia, Government, General Public: Workshops and training
sessions can facilitate comprehension of the dangers, limitations, and better calibrate users to appropriate expectations of functionality. These workshops can include education on rights and how to advocate for them. For instance, if companies
fail to comply, where can people ï¬le their grievances so they can be aggregated and
collectively analyzed?
Industry, Academia, Civil Society: Development of interactive tools that enable
visualization and comparison of an individualâ€™s utility compared to the general
population or those relevantly comparable.
Manuscript submitted
to ACM
Civil society, Government: Regulation and continued measurement
to ensure
such tools ï¬‚ag anomalous data and ensure improvement within some time-frame.
Create and publicly release of reports of corrections, adjustments, and investigations made to remedy grievances.
Table 2. Example Policy Interventions and Goals by Stakeholder. Although we separate actions by stakeholders, we emphasize that
these goals require collaborations and contributions across multiple stakeholders to achieve such solutions.

10

Jennifer Chien and David Danks

REFERENCES
[1]

Zeynep Tufekci. How recommendation algorithms run the world. Apr. 2019. url: https://www.wired.com/story/how-recommendation-

[2]

Gurkan Solmaz, Jesmin Jahan Tithi, and Juan Miguel de Joya. Why algorithmic fairness? Oct. 2020. url: https://selects.acm.org/selectio

[3]

Elizabeth Anne Watkins, Michael McKenna, and Jiahao Chen. â€œThe four-ï¬fths rule is not disparate impact: a
woeful tale of epistemic trespassing in algorithmic fairnessâ€. In: arXiv preprint arXiv:2202.09519 (2022).

[4]

Cynthia Dwork et al. â€œFairness through awarenessâ€. In: Proceedings of the 3rd innovations in theoretical computer
science conference. 2012, pp. 214â€“226.

[5]

Rishabh Mehrotra et al. â€œTowards a fair marketplace: Counterfactual evaluation of the trade-oï¬€ between relevance, fairness & satisfaction in recommendation systemsâ€. In: Proceedings of the 27th acm international conference on information and knowledge management. 2018, pp. 2243â€“2251.

[6]

Nasim Sonboli et al. â€œOpportunistic multi-aspect fairness through personalized re-rankingâ€. In: Proceedings of
the 28th ACM Conference on User Modeling, Adaptation and Personalization. 2020, pp. 239â€“247.

[7]

Weiwen Liu et al. â€œPersonalized fairness-aware re-ranking for microlendingâ€. In: Proceedings of the 13th ACM
Conference on Recommender Systems. 2019, pp. 467â€“471.

[8]

Ludovico Boratto, Gianni Fenu, and Mirko Marras. â€œInterplay between upsampling and regularization for provider

[9]

Elizabeth GÃ³mez et al. â€œThe winner takes it all: geographic imbalance and provider (un) fairness in educational

fairness in recommender systemsâ€. In: User Modeling and User-Adapted Interaction 31.3 (2021), pp. 421â€“455.
recommender systemsâ€. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021, pp. 1808â€“1812.
[10]

Abhisek Dash et al. â€œWhen the umpire is also a player: Bias in private label product recommendations on ecommerce marketplacesâ€. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021, pp. 873â€“884.

[11]

Ziwei Zhu, Xia Hu, and James Caverlee. â€œFairness-aware tensor-based recommendationâ€. In: Proceedings of the
27th ACM international conference on information and knowledge management. 2018, pp. 1153â€“1162.

[12]

Bora Edizel et al. â€œFaiRecSys: mitigating algorithmic bias in recommender systemsâ€. In: International Journal of
Data Science and Analytics 9 (2020), pp. 197â€“213.

[13]

Himan Abdollahpouri et al. â€œCalibrated recommendations as a minimum-cost ï¬‚ow problemâ€. In: Proceedings of
the Sixteenth ACM International Conference on Web Search and Data Mining. 2023, pp. 571â€“579.

[14]

Himan Abdollahpouri et al. â€œUser-centered evaluation of popularity bias in recommender systemsâ€. In: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization. 2021, pp. 119â€“129.

[15]

Bruna Wundervald. â€œCluster-based quotas for fairness improvements in music recommendation systemsâ€. In:

[16]

Diego CorrÃªa da Silva, Marcelo Garcia Manzato, and Frederico AraÃºjo DurÃ£o. â€œExploiting personalized calibra-

[17]

Yashar Deldjoo et al. â€œFairness in recommender systems: research landscape and future directionsâ€. In: User

International Journal of Multimedia Information Retrieval 10.1 (2021), pp. 25â€“32.
tion and metrics for fairness recommendationâ€. In: Expert Systems with Applications 181 (2021), p. 115112.
Modeling and User-Adapted Interaction (2023), pp. 1â€“50.
[18]

Yashar Deldjoo et al. â€œRecommender systems fairness evaluation via generalized cross entropyâ€. In: arXiv preprint
arXiv:1908.06708 (2019).

Manuscript submitted to ACM

Fairness Vs. Personalization: Towards Equity in Epistemic Utility
[19]

11

Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. â€œAdversarial machine learning in recommender
systems (aml-recsys)â€. In: Proceedings of the 13th International Conference on Web Search and Data Mining. 2020,
pp. 869â€“872.

[20]

Yunqi Li et al. â€œUser-oriented fairness in recommendationâ€. In: Proceedings of the Web Conference 2021. 2021,
pp. 624â€“632.

[21]

Miranda Fricker. Epistemic injustice: Power and the ethics of knowing. Oxford University Press, 2007.

[22]

CÃ©dric Courtois, Laura Slechten, and Lennert Coenen. â€œChallenging Google Search ï¬lter bubbles in social and
political information: Disconforming evidence from a digital methods case studyâ€. In: Telematics and Informatics
35.7 (2018), pp. 2006â€“2015.

[23]

William H Dutton et al. â€œSearching through ï¬lter bubbles, echo chambersâ€. In: Society and the internet: How

[24]

Efrat Nechushtai and Seth C Lewis. â€œWhat kind of news gatekeepers do we want machines to be? Filter bub-

networks of information and communication are changing our lives (2019), p. 228.
bles, fragmentation, and the normative dimensions of algorithmic recommendationsâ€. In: Computers in human
behavior 90 (2019), pp. 298â€“307.
[25]

Tawanna R Dillahunt, Christopher A Brooks, and Samarth Gulati. â€œDetecting and visualizing ï¬lter bubbles in
Google and Bingâ€. In: Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in
Computing Systems. 2015, pp. 1851â€“1856.

[26]

Axel G EkstrÃ¶m, Diederick C Niehorster, and Erik J Olsson. â€œSelf-imposed ï¬lter bubbles: Selective attention and
exposure in online searchâ€. In: Computers in Human Behavior Reports 7 (2022), p. 100226.

[27]

Mykola Makhortykh and MariÃ«lle Wijermars. â€œCan ï¬lter bubbles protect information freedom? Discussions of
algorithmic news recommenders in Eastern Europeâ€. In: Digital Journalism (2021), pp. 1â€“25.

[28]

Jaime Teevan. â€œHow people recall, recognize, and reuse search resultsâ€. In: ACM Transactions on Information
Systems (TOIS) 26.4 (2008), pp. 1â€“27.

[29]

Kelly Shelton. Council post: The value of search results rankings. Nov. 2017. url: https://www.forbes.com/sites/forbesagencycouncil/2017/10/30/th

[30]

Tessa ES Charlesworth and Mahzarin R Banaji. â€œPatterns of implicit and explicit attitudes: IV. change and stability from 2007 to 2020â€. In: Psychological Science 33.9 (2022), pp. 1347â€“1371.

[31]

Youjin Kong. â€œAre â€œIntersectionally Fairâ€ AI Algorithms Really Fair to Women of Color? A Philosophical Analysisâ€. In: 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 485â€“494.

[32]

Lily Hu and Yiling Chen. â€œA short-term intervention for long-term fairness in the labor marketâ€. In: Proceedings
of the 2018 World Wide Web Conference. 2018, pp. 1389â€“1398.

[33]

url: https://www.unesco.org/en/right-information.

Manuscript submitted to ACM

This figure "acm-jdslogo.png" is available in "png" format from:
http://arxiv.org/ps/2309.11503v1

