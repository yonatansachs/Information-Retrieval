A Framework for Defining Algorithmic Fairness in the
Context of Information Access
Udoh, Emmanuel Sebastian
Yuan, Xiaojun
Rorissa, Abebe

University at Albany - SUNY, New York, USA | eudoh@albany.edu
University at Albany - SUNY, New York, USA | xyuan@albany.edu
University of Tennessee, Knoxville, USA | arorissa@utk.edu

ABSTRACT
As technologies powered by Artificial Intelligence (AI) and Machine Learning (ML) algorithms increasingly take
over personal computing online and public sector domains, they simultaneously raise the promise of an extensively
productive and sustainable future, as well as fears of widening inequalities, information and content divide, and a
more complex information-seeking landscape. Thus, the hopes of improved accuracy, efficiency, productivity,
reduced human bias in decision-making, and access to information are fast giving way to a trove of ethical and
human rights issues with far-reaching consequences for accountability, privacy, social justice, equity, inclusion, and
informed consent, and public participation in decision-making. Since no technology is entirely free of bias, this
paper identifies algorithmic fairness as a more realistic threshold and goal. Building on findings from a previous
PRISMA review of relevant literature, the paper proposes a comprehensive framework for defining algorithmic
fairness in the context of information access.
KEYWORDS
Algorithmic fairness, Information access, Artificial intelligence, Machine learning, Algorithmic bias, fair ranking,
fair recommendation system
INTRODUCTION
With the pervasive Internet, the Internet of Things, the smart city, and the increasing ubiquity of technologies
powered by AI and ML algorithms, the potential and urgency for information access to be at the forefront of
information science research has never been higher. Yet, digital inequalities and the content divide continue to
widen rather than shrink. Even more, with the proliferation of algorithmic decision systems online and in public
sector domains, humanity is faced with an unfortunate dilemma: The algorithmic systems promise greater
effectiveness, accuracy, and productivity, as well as the elimination of human bias in decision-making and resource
access on the one hand, while potentially becoming the new arbiters of access and equity, raising genuine fears of
coded bias, digital redlining, and the new gatekeepers, tools and arbiters of eligibility for benefits, or liability for
penalties.
Expectedly, there is a growing scholarship on responsible and trustworthy AI and ML, all aimed at attaining more
ethics in the use of technologies in general. In a unique way, algorithmic fairness (AF) has been thrown into the
front burner of public discourse. The discourse aims to determine an acceptable ethical threshold when deploying
technologies powered by AI and ML, primarily algorithmic decision systems. Accordingly, scholars have framed
acceptable use of these technologies in various ways, including the equalized odds (Kearns & Roth, 2020; Heidari &
Krause, 2018), equal opportunity (Hardt et al., 2016), equity, disparate impact (Barocas & Selbst, 2016; Feldman et
al, 2015), the attainment of demographic or statistical parity which ensures some form of affirmative action (Kearns
& Roth, 2020), and individual fairness and group fairness (Dwork et al, 2012; Feldman et al., 2015), to name a few.
Unfortunately, while some of these concepts are not comprehensive enough, others appear to be only achievable at
the expense or exclusion of the other ideals. While no technology is entirely free of bias, and the attainment of
equity seems a long-shot goal in the circumstances, fairness has increasingly become more achievable and feasible.
However, a bludgeoning problem is that the concept of algorithmic fairness is not a one-size-fits-all, owing to often
unique contextual realities surrounding the implementation of the algorithmic systems in each domain. One such
area not covered deep enough in the algorithmic fairness discourse is information access. The challenge is
comprehensively conceptualizing algorithmic fairness in the context of information access.
This paper builds on findings from our previous work (Udoh, Yuan, & Rorissa, 2022) triangulated with emerging
and current issues in the area to propose a novel framework comprising the key concepts that should constitute any
conceptualization of algorithmic fairness in the context of information access. In the next section, we highlight key
findings from our previous review of the literature, then present and discuss our framework. Thereafter, we draw
conclusions and state our future research direction.
PREVIOUS WORK
In previous work (Udoh et al., 2022) our Preferred Reporting Items for Systematic reviews and Meta-Analyses
(PRISMA) review of literature on the intersection of algorithmic fairness and information access yielded several
themes including the dimensions of algorithmic fairness, specific issues, concepts, and contexts of algorithmic
87th Annual Meeting of the Association for Information Science & Technology | Oct. 25 – 29, 2024 | Calgary, AB, Canada
ASIS&T Annual Meeting 2024

667

Short Papers

Dimensions of fairness are numerous and include those related to individuals or groups, such as internet/network
fairness or net neutrality (Venkatasubramanian et al., 2021), fair ranking metrics (Lewandowski & Spree, 2011),
reasonable quality of results (Li et al., 2014), and legal/policy fairness (Sonnenberg, 2020; Yang & Chen, 2015).
Inclusion, digital divide, content divide, disability, net neutrality, network fairness, algorithmic oppression, bias and
stereotyping, and [training] data quality appear prominently among examples given as the main fairness issues
(Conn, 2010; Yang & Chen, 2015), whereas search engines, the use of algorithms in online resource ranking, search,
and recommendation systems are the contexts in which algorithmic fairness in information access is discussed in the
literature (Joachims, 2021; Gao & Shah, 2021; Kirnap et al., 2021; Gao & Shah, 2020; Guiver & Snelson, 2008).
The algorithmic fairness study employed the stakeholder (Costa et al., 2020) and multi-stakeholder (Webb et al.,
2018) approaches. Costa et al. (2020) focused on the quality of information in health websites, specifically content
relevant to individuals suffering from lower back pain (LBP). They concluded that the websites most commonly
found in searches did not align with LBP sufferers' information and presentation needs. McKearney and colleagues
(2018) studied the quality and readability of online tinnitus-related information, focusing on what patients typically
encounter. The findings showed that, overall, the quality of the websites was moderate, but their readability was
poor, with notable differences in quality among the sites.
Several authors offer solutions to algorithmic fairness issues discussed above. Because some of those issues revolve
around inclusion and equity based on all the protected characteristics of individuals and groups, the solutions
proffered in the context of information access come in the form of improving information search and access
algorithms and tools to minimize bias and undue exclusion. One of the tools is an agnostic ranking model (Rekabsaz
et al., 2021). Other ways to preserve fairness in the face of misinformation and disinformation include critical
librarianship (Bains, 2020), methods to preserve fairness and trust in news consumption (Mukherjee & Weikum,
2015), and ensuring the fairness of algorithms in search engines (Lamprier et al., 2010). Search
engines/recommender systems and their ranking algorithms and mechanisms, as well as websites, are among the
widely cited examples of information access technologies relevant to algorithmic fairness. As such, any solutions to
algorithmic fairness issues in information access need to deal directly with factors relevant to these technologies,
such as developing fairness metrics concerning web usability, readability, and quality (Hamwela, Ahmed, & Bath,
2018; McKearney et al., 2018) and evaluation of the fairness of search system ranking decisions (Kırnap et al.,
2021; Rekabsaz et al., 2021). Information access is a universal human right. Algorithmic fairness in this context is a
matter of inclusion and equity, both from the representation and access or retrieval perspectives and any strategies to
address information access fairness must recognize these factors and build on these solutions.
A FRAMEWORK: ALGORITHMIC FAIRNESS IN THE CONTEXT OF INFORMATION ACCESS
Several overlapping and intersecting themes emerged from the previous section, and we triangulated those findings
with current themes not captured in the review. Those overlapping and emerging themes include (algorithmic) bias,
the user (the information seeker), the information access technology involved, fairness, barriers to access, the
context, and the fairness metrics, to name a few. Our exploratory review forms the foundation with which we
propose a framework for conceptualizing algorithmic fairness in the context of information access. Figure 1 presents
this framework before discussing some of its essential constitutive concepts.
Bias. A recurrent theme from the literature is bias, often variously presented in the forms of stereotyping, redlining,
disenfranchisement, 'dirty,' or biased data (Richardson et al., 2019; Ferguson, 2017). Bias in this context tends to
take the form of imbalance in user data, bias in user activity, user modeling, content production, item modeling,
retrieval and ranking models, and evaluation models. There are significant assumptions underlying most algorithmic
decision systems and the many forms of bias possible with algorithmic decision systems (Moses & Chan, 2016).
Generally, machine learning algorithms of the classificatory type are particularly susceptible to sociological
concerns and assume that algorithms will classify more ‘objectively.’ This assumption, however, flies in the face of
the amount of human judgment still involved in designing the algorithms, which ultimately get coded into the
algorithms themselves through the human work of defining features, pre-classifying training data, and adjusting
thresholds and parameters. In addition, data does not always reflect reality; the future does not always reflect the
past; omitted variables are a recipe for endogeneity; and algorithms are not always neutral or devoid of values but
come laden with assumptions and projecting specific world views.
In its various manifestations including the context of information access, bias can be grouped into exogenous and
endogenous (Joachims (2021) and into three major categories: preexisting, technical, and emergent (Friedman &
Nissenbaum, 1996). Endogenous biases often manifest themselves in the training data, which then get reflected in

ASIS&T Annual Meeting 2024

668

Short Papers

23739231, 2024, 1, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1077 by Cochrane Israel, Wiley Online Library on [13/11/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

fairness, stakeholders involved in the discourse, fairness metrics and frameworks used, algorithmic fairness
solutions presented, the fairness and access technologies involved, and the theme of information access fairness.
While the authors' expanded piece will provide a detailed discussion, we summarize the above themes here.

Figure 1. A framework for conceptualizing algorithmic fairness in the context of information access
The User. We identify the user here as a critical stakeholder in evaluating algorithmic fairness. In the context of
information access, the question is “fair to whom?” to whom should the algorithmic decision system be fair?
Moreover, how much is fair enough? With algorithmic systems the user faces different fairness challenges. For
example, in job searches, there is the challenge of whether the user receives a fair set of job opportunities in the
recommendations or ads in their feed, whether the match or fit score is a fair score, does not redline or overvalue
some candidates, whether the candidates have fair visibility when recruiters filter for top candidates (Geyik &
Kenthapadi, 2018), and what fairness concerns come from regulatory requirements or procedures. Similarly, fairness
as discoverability also directly impacts the user in news and music, where the ecosystem tends to give disparate
promotion and exposure to songwriters and place users in filter bubbles, thereby exposing them primarily to news
items that reinforce their beliefs and deepen ideological polarization (Pariser, 2011; Alstyne & Brynjolfsson, 2005).
The information access technology involved. Fairness discussions in information access also need to
consider the different information access technologies involved, such as search, sorting, filtering, ranking, and
recommender systems. The sources of unfairness in these technologies may sometimes vary due to their peculiar
nature and algorithms' operations. Machine learning models can also introduce unfairness, primarily through the
direct use of sensitive or protected attributes (e.g., gender and race) in the model. Over time, the models can also
learn to discriminate indirectly from proxy variables such as location being a proxy for race in some instances.
Fairness. In the context of information access, the algorithmic fairness discourse cannot avoid the twin questions
of what type of fairness, and how much is fair enough. The overwhelming conception of fairness in the discourse
markedly follows the Western Euro-American understanding of bias and discrimination (Sambasivan et al., 2020). It
concerns the idea of responsibility in computing systems (Ekstrand et al., 2022, p.10). Therefore, algorithmic
responsibility in information access concerns accountability, transparency, explainability, safety, security, privacy,
and ethics, which promise a robust discussion space, and viable research paths.

ASIS&T Annual Meeting 2024

669

Short Papers

23739231, 2024, 1, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1077 by Cochrane Israel, Wiley Online Library on [13/11/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

the learned ranking policy. In contrast, exogenous biases arise from factors extraneous to the system, even when the
systems might have been trained with unbiased data. In terms of categorization, while preexisting bias is rooted in
social institutions, practices, and attitudes, technical bias emanates from technical constraints or considerations, and
emergent bias arises in the context of the use of technology. As technologies in their own right, algorithmic selection
systems are susceptible to all three forms of bias: The same way they enable personalization and social sorting, they
also potentially enable discrimination, inclusion or exclusion (Nahon, 2016; Crawford, 2016; Pariser, 2012;
Gillespie, 2019; Noble, 2012; Noble, 2018; Latzer et al., 2014; Pasquale, 2015; Tufekci, 2015; Hargittai, 2007;
Nissenbaum, 2001). Moreover, algorithms fit in with and help advance particular ideological worldviews (Beer,
2009, 2013; Fuller & Goffey, 2012; Danaher, 2016; Mackenzie, 2015).

Fairness metrics. The metrics used to determine what and how much is fair in the context of information access
also need to be considered. For example, there needs to be more transparency about sampling strategies used to
decide who is considered for inclusion in the dataset. Such strategies also need to be accompanied by detailed
codebooks that not only define the variables but also state how each variable is recorded, especially where the
variable is categorical in nature. The selection and definition of variables are crucial as observations or proxies need
valid and unbiased measurements of the target construct to avoid measurement bias. Considering group fairness,
group size or amount of representation can also contribute to some types of unfairnesses, as often happens with
representational or distributional harms online (Rolf et al., 2021).
The context of implementation. Technologies do not exist in a vacuum. Despite the pervasiveness of the
Internet, net neutrality is not a common feature in every geopolitical zone, just as the policies, laws, and guidelines
govern Internet use, digital information search, and information access. With censorship, for instance, whole groups
and countries are denied unfettered access to resources on the Internet; just as with the current setup of global
Internet governance, some geopolitical regions have no say in how digital resources are distributed and must
contend with both the digital and content divide. Similarly, when data is involved, input data and the algorithmic
decision results must be collected and interpreted considering the data’s social and cultural context to avoid unfairly
disregarding local knowledge, local perspectives, norms, mores, ethos, and bias. A vital direction would be a focus
on the entire data pipeline as each point is potentially vulnerable to unfairness and unfair access practices.
CONCLUSION
We propose a framework for comprehensively rethinking algorithmic fairness in the context of information access.
Our framework highlights several key concepts and issues that should form the core of every discourse in that
domain of application, including the complex notion of bias, the place of stakeholders especially the user, the nature
and affordances of the specific technologies involved in the information access process, the nuanced concept of
fairness that is not a one-size-fits-all, and but instead needs to be defined relative to the application domain and
context, the barriers to fair information access, the fairness metrics used, as well as the context of implementation
itself. These ideas should challenge researchers to rethink several concepts currently used in the discourse without
adequate attention to the nuances and context. Our work also provides practitioners with various factors to consider
as the call for more responsibility in using algorithmic decision systems increases.
FUTURE WORK
The next phase will be a more extended version, a more elaborate focus on these, and more emerging themes that
should further broaden the scope of the discourse on algorithmic fairness in the context of information access. We
are confident that getting the core constitutive concepts right will enrich and balance the discourse and provide a
better foundation for other research efforts, especially in the information access domain.
GENERATIVE AI USE
The authors did not utilize generative AI in the course of this work.
AUTHOR ATTRIBUTION
First Author: conceptualization, methodology, data curation, formal analysis, writing – original draft; Second
Author: data curation, writing – review and editing; Third Author: data curation, writing – review and editing.
REFERENCES
Alstyne, M. v., & Brynjolfsson, E. (2005). “Global Village or Cyber-Balkans? Modeling and Measuring the Integration
ofElectronic Communities”. In: ManagementScience 51.6 (June 2005), pp. 851–868.
Bains, J. (2020, Jan-Feb.). "Search Engines and Algorithms are Biased. Here's Why That Matters: LIBRARIANS CAN TAKE
STEPS TO ADDRESS INEQUALITY AND BIAS THAT RESULT FROM USING DATA HISTORICALLY WEIGHTED
IN FAVOR OF WHITE MEN." Information Outlook, vol. 24, no. 1, Jan.-Feb. 2020, pp. 21+. Gale Academic OneFile
Select, link.gale.com/apps/doc/A684967319/EAIM?u=albanyu&sid=sitemap&xid=beaa11ca. Accessed 8 Sept. 2023.
Barocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. Calif. L. Rev. 104 (2016), 671.
Beer, D. (2009). Power through the algorithm? Participatory Web cultures and the technological unconscious. New Media and
Society, 11(6), 985–1002.

ASIS&T Annual Meeting 2024

670

Short Papers

23739231, 2024, 1, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1077 by Cochrane Israel, Wiley Online Library on [13/11/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Barriers to fair access. In the context of information access, there are several barriers to algorithmic fairness
including historical and persistent discrimination, censorship, content divide, disproportionate access to
infrastructure, deliberate digital misrepresentation, disparate content distribution, insufficient legislation and
regulation, and lack of net neutrality. Discussions on algorithmic fairness in information access must pay special
attention to the fact that some groups are more susceptible to the impact of these barriers than others. For example,
in the United States, there is a long history of discrimination against black and minority groups dating back to the
county poorhouses of the 19th century (Eubanks, 2018). These surreptitious redlining activities persist online
through the opaque algorithmic ranking, sorting, and recommendation systems, thus disenfranchising some groups.

ASIS&T Annual Meeting 2024

671

Short Papers

23739231, 2024, 1, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1077 by Cochrane Israel, Wiley Online Library on [13/11/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Beer, D. (2013). Algorithms: Shaping Tastes and Manipulating the Circulations of Popular Culture. In Popular Culture and New
Media: The Politics of Circulation. Palgrave McMillan: pp.63-100.
Conn, J. (2010). Still stuck on neutrality. No legislative remedy likely over Internet fair play. Modern healthcare, 40(41), 30-31.
Costa, N., Nielsen, M., Jull, G., Claus, A. P., & Hodges, P. W. (2020). Low back pain websites do not meet the needs of
consumers: a study of online resources at three time points. Health Information Management Journal, 49(2-3), 137-149.
Crawford, K. (2016). Can an Algorithm be Agonistic? Ten Scenes from Life in Calculated Publics. Science, Technology &
Human Values, 41(1), pp.77-92.
Danaher, J. (2016). The Threat of Algocracy: Reality, Resistance and Accommodation. Philosophy and Technology Vol 29 issue
3, pp. 245-268.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. In Proc. of Innovations in
Theoretical Computer Science, 2012, pp.214-226. https://doi.org/10.1145/2090236.2090255
Ekstrand, M. D., Das, A., Burke, R., & Diaz, F. (2022). Fairness in information access systems. Foundations and Trends® in
Information Retrieval, 16(1-2), 1-177.
Epps-Darling, A., Bouyer, R. T., & Cramer, H. (2020). “Artist Gender Representation in Music Streaming”. In: Proceedings of
the 21st International Society for Music Information Retrieval Conference. ISMIR, Oct. 2020, pp. 248–254. url:
Eubanks, V. (2016). Automating Inequality: How High-Tech Tools profile, Police, and Punish the Poor. New York: St. Martin’s
Press.
Eubanks, V. (2018, Jan). The Digital Poorhouse, Harper's Magazine, January 2018 Edition, Available:
https://harpers.org/archive/2018/01/the-digital-poorhouse/
Feldman, M., Friedler, S. A., Moeller, J., Scheidegger,C., & Venkatasubramanian, S. (2015). Certifying and removing disparate
impact. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
259–268, 2015.
Ferguson, A. G. (2017). The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement. New York: New
York University Press.
Friedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems Vol. 14 Issue 3,
pp. 330-347.
Fuller, M., & Goffey, A. (2012). Algorithms. In Evil Media. Cambridge, MA: MIT Press: pp.69-82.
Gao, R., Shah, C. (2020). Toward creating a fairer ranking in search engine results. Information Processing & Management Vol.
57 Issue 1, 102138.
Gao, R., & Shah, C. (2021, July). Addressing bias and fairness in search systems. In Proceedings of the 44th international ACM
SIGIR conference on research and development in information retrieval (pp. 2643-2646).
Geyik, S. C., & Kenthapadi, K. (2018). Building Representative Talent Search at LinkedIn.
https://engineering.linkedin.com/blog/2018/10/building-representative- talent-search-at-linkedin. Accessed: 2018-10-25.
Oct. 2018.
Gillespie, T. (2019). Algorithmically recognizable: Santorum's Google problem, and Google's Santorum problem. In The Social
Power of Algorithms (pp. 63-80). Routledge.
Guiver, J., & Snelson, E. (2008, July). Learning to rank with softrank and gaussian processes. In Proceedings of the 31st annual
international ACM SIGIR conference on Research and development in information retrieval (pp. 259-266).
Hamwela, V., Ahmed, W., Bath, P. A. (2018). Evaluation of websites that contain information relating to malaria in pregnancy.
Public Health (Elsevier) Vol. 157, pp. 50-52.
Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. In Advances in neural information
processing systems. 3315–3323.
Hargittai, E. (2007). The social, political, economic, and cultural dimensions of search engines: An introduction. Journal of
Computer-Mediated Communication, 12(3), pp.769-777.
Heidari, H., & Krause, A. (2018). Preventing Disparate Treatment in Sequential Decision Making. In IJCAI. Pp.2248–2254.
Joachims, T. (2021, July). Fairness and Control of Exposure in Two-Sided Markets. Proceedings of the 2021 ACM SIGIR
International Conference on Theory of Information Retrieval, p.1.
Kearns, M., Roth, A. (2020). The Ethical Algorithm: The Science of Socially Aware Algorithm Design. Oxford: Oxford
University Press. Pp.91-93.
Kırnap, Ö., Diaz, F., Biega, A., Ekstrand, M., Carterette, B., & Yilmaz, E. (2021, April). Estimation of fair ranking metrics with
incomplete judgments. In Proceedings of the Web Conference 2021 (pp. 1065-1075).
Lamprier, S., Amghar, T., Saubion, F., Levrat, B. (2010). Traveling among Clusters: A Way to Reconsider the Benefits of the
Cluster Hypothesis. Proceedings of the 2010 ACM Symposium on Applied Computing, pp. 1774–1780.
Latzer, M., Hollnbuchner, K., Just, N., & Saurwein, F. (2014). The economics of algorithmic selection on the internet. Working
Paper – Media Change & Innovation Division. University of Zurich, Zurich.
http://www.mediachange.ch/media/pdf/publications/economics_of_algorithmic_selection.pd

ASIS&T Annual Meeting 2024

672

Short Papers

23739231, 2024, 1, Downloaded from https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.1077 by Cochrane Israel, Wiley Online Library on [13/11/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Lewandowski, D., & Spree, U. (2011). Ranking of Wikipedia articles in search engines revisited: Fair ranking for reasonable
quality?. Journal of the American Society for Information Science and Technology, 62(1), 117-132.
Li, N., Anderson, A. A., Brossard, D., & Scheufele, D. A. (2014). Channeling Science Information Seekers' Attention? A Content
Analysis of Top-Ranked vs. Lower-Ranked Sites in Google. Journal of Computer-Mediated Communication, 19(3), 562575.
Mackenzie, A. (2015). The Production of Prediction: What Does Machine Learning Want? European Journal of Cultural Studies
18(4/5): 429–45.
McKearney, R. M., MacKinnon, R. C., Smith, M., & Baker, R. (2018). Tinnitus information online–does it ring true?. The
Journal of Laryngology & Otology, 132(11), 984-989.
Moses, L. B., & Chan, J. (2018). Algorithmic prediction in policing: assumptions, evaluation, and accountability. Policing and
Society 28(7): pp.806-822.
Mukherjee, S., Weikum, G. (2015). Leveraging Joint Interactions for Credibility Analysis in News Communities. Proceedings of
the 24th ACM International Conference on Information and Knowledge Management, pp. 353–362.
Nahon, K. (2016). Where there is Social Media, there is Politics, in Bruns A., Skogerbo E., Christensen C., Larsson O.A. and
Enli G.S., eds. Routledge Companion to Social Media and Politics. New York: Routledge: 39-55.
Nissenbaum, H. (2001). How Computer Systems Embody Values, IEEE Computer, 120, pp.118-119.
https://www.nyu.edu/projects/nissenbaum/papers/embodyvalues.pdf
Noble, S. (2012). Missed Connections: What Search Engines Say about Women. Bitch magazine, 12(4): 37-41.
https://safiyaunoble.files.wordpress.com/2012/03/54_search_engines.pdf
Noble, S. (2018). Algorithms of Oppression: Race, Gender and Power in the Digital Age. New York: NYU Press.
Pariser, E. (2012). The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think. Reprint
edition. New York, N.Y.: Penguin Books.
Pasquale, F. (2015). The Black Box Society: The Secret Algorithms That Control Money and Information. Cambridge: Harvard
University Press.
Rekabsaz, N., Kopeinik, S., Schedl, M. (2021). Societal Biases in Retrieved Contents: Measurement Framework and Adversarial
Mitigation of BERT Rankers. Proceedings of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 306–316.
Richardson, R., Schultz, J., & Crawford, K. (2019). Dirty data, bad predictions: How civil rights violations impact police data,
predictive policing systems, and justice. New York University Law Review, 2019.
Rolf, Esther et al. (2021). “Representation Matters: Assessing the Importance of Subgroup Al- locations in Training Data”. In:
Proceedings ofthe 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139.
Proceedings of Machine Learning Research. PMLR, 2021, pp. 9040–9051.
Sambasivan, Nithya et al. (2020). “Non-portability of Algorithmic Fairness in India”. In: (Dec. 2020). arXiv: 2012.03659
[cs.CY]. url: http://arxiv.org/abs/2012.03659.
Sonnenberg, C. (2020). E-government and social media: The impact on accessibility. Journal of Disability Policy Studies, 31(3),
181-191.
Tufekci, Z. (2015). Algorithmic Harms beyond Facebook and Google: Emergent Challenges of Computational Agency. Journal
on Telecommunications & High Tech Law Vol13(23), pp. 203-216.
Udoh, E. S., Yuan, X., & Rorissa, A. (2022). Rethinking Algorithmic Fairness in the Context of Information Access. Proceedings
of the Association for Information Science and Technology, 59(1), 815-817.
Venkatasubramanian, S., Scheidegger, C., Friedler, S., & Clauset, A. (2021). Fairness in networks: social capital, information
access, and interventions. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining
(pp. 4078-4079).
Webb, H., Koene, A., Patel, M., & Vallejos, E. P. (2018, July). Multi-stakeholder dialogue for policy recommendations on
algorithmic fairness. In Proceedings of the 9th international conference on social media and society (pp. 395-399).
Yang, Y. T., Chen, B. (2015). Web Accessibility for Older Adults: A Comparative Analysis of Disability Laws. Gerontologist
Vol. 55 Issue 5, pp. 854-865.
Zambonelli, F., Salim, F., Loke, S. W., De Meuter, W., & Kanhere, S. (2018). Algorithmic Governance in Smart Cities: The
Conundrum and the Potential of Pervasive Computing Solutions. IEEE Technology and Society Magazine, vol 37, issue 2,
pp. 80-87.

