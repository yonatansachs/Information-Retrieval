chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
qualitative
exploration
of
perceptions
of
algorithmic
fairness
allison
woodruff1
sarah
fox2
steven
rousso-schindler3
and
jeff
warshaw4
google
woodruff@acm.org
google
and
human
centered
design
engineering
university
of
washington
sefox@uw.edu
department
of
anthropology
csu
long
beach
steven.rousso-schindler@csulb.edu
google
jeffwarshaw@google.com
abstract
raise
awareness
and
illustrate
the
potential
for
wide-ranging
algorithmic
systems
increasingly
shape
information
people
consequences
researchers
and
the
press
have
pointed
out
are
exposed
to
as
well
as
influence
decisions
about
number
of
specific
instances
of
algorithmic
unfairness
employment
finances
and
other
opportunities
in
some
19
58
for
example
in
predictive
policing
19
43
the
cases
algorithmic
systems
may
be
more
or
less
favorable
to
online
housing
marketplace
27
28
online
ads
certain
groups
or
individuals
sparking
substantial
13
17
20
29
82
and
image
search
results
49
64
discussion
of
algorithmic
fairness
in
public
policy
circles
academia
and
the
press
we
broaden
this
discussion
by
such
cases
demonstrate
that
algorithmic
un
fairness
is
exploring
how
members
of
potentially
affected
complex
industry-wide
issue
bias
can
result
from
many
communities
feel
about
algorithmic
fairness
we
conducted
causes
for
example
data
sets
that
reflect
structural
bias
in
workshops
and
interviews
with
44
participants
from
several
society
human
prejudice
product
decisions
that
populations
traditionally
marginalized
by
categories
of
race
disadvantage
certain
populations
or
unintended
or
class
in
the
united
states
while
the
concept
of
consequences
of
complicated
interactions
among
multiple
algorithmic
fairness
was
largely
unfamiliar
learning
about
technical
systems
accordingly
many
players
in
the
algorithmic
un
fairness
elicited
negative
feelings
that
ecosystem
including
but
not
limited
to
policy
makers
connect
to
current
national
discussions
about
racial
injustice
companies
advocates
and
researchers
have
shared
and
economic
inequality
in
addition
to
their
concerns
about
responsibility
and
opportunity
to
pursue
fairness
potential
harms
to
themselves
and
society
participants
also
algorithmic
fairness
therefore
appears
to
be
wicked
indicated
that
algorithmic
fairness
or
lack
thereof
could
problem
72
with
diverse
stakeholders
but
as
yet
no
substantially
affect
their
trust
in
company
or
product
clear
agreement
on
problem
statement
or
solution
the
human
computer
interaction
hci
community
and
related
author
keywords
disciplines
are
of
course
highly
interested
in
influencing
algorithmic
fairness
algorithmic
discrimination
positive
action
on
such
issues
25
having
for
example
an
acm
classification
keywords
established
tradition
of
conducting
research
to
inform
computers
and
society
miscellaneous
public
policy
for
societal-scale
challenges
50
84
as
well
as
providing
companies
information
about
how
they
can
best
introduction
serve
their
users
indeed
recent
work
by
plane
et
al
on
scholars
and
thought
leaders
have
observed
the
increasing
discrimination
in
online
advertising
is
positioned
as
role
and
influence
of
algorithms
in
society
pointing
out
that
informing
public
policy
as
well
as
company
initiatives
67
they
mediate
our
perception
and
knowledge
of
the
world
as
well
as
affect
our
chances
and
opportunities
in
life
building
on
this
tradition
our
goal
in
this
research
was
to
17
38
54
55
63
76
79
further
academics
and
explore
ethical
and
pragmatic
aspects
of
public
perception
regulators
have
long
refuted
the
presumption
that
of
algorithmic
fairness
to
this
end
we
conducted
algorithms
are
wholly
objective
observing
that
algorithms
qualitative
study
with
several
populations
that
have
can
reflect
or
amplify
human
or
structural
bias
or
introduce
traditionally
been
marginalized
and
are
likely
to
be
affected
complex
biases
of
their
own
10
18
33
35
38
46
64
to
by
algorithmic
un
fairness
specifically
black
or
african
american
hispanic
or
latinx
and
low
socioeconomic
status
participants
in
the
united
states
our
research
questions
centered
around
participants
interpretations
and
this
work
is
licensed
under
creative
commons
experiences
of
algorithmic
un
fairness
as
well
as
their
attribution-noncommercial-noderivs
international
4.0
license
ascription
of
accountability
and
their
ethical
and
pragmatic
expectations
of
stakeholders
in
order
to
draw
more
robust
chi
2018
april
21
26
2018
montreal
qc
canada
conclusions
about
how
participants
interpret
these
highly
2018
copyright
is
held
by
the
owner
author
contextual
issues
we
explored
broad
spectrum
of
acm
isbn
978
4503
5620
18
04
https://doi.org/10.1145/3173574.3174230
different
types
of
algorithmic
unfairness
using
scenarios
to
make
the
discussion
concrete
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
our
findings
indicate
that
while
the
concept
of
algorithmic
image
search
or
predictive
search
results
may
reinforce
or
un
fairness
was
initially
mostly
unfamiliar
and
participants
exaggerate
societal
bias
or
negative
stereotypes
related
to
often
perceived
algorithmic
systems
as
having
limited
race
gender
or
sexual
orientation
49
62
64
others
impact
they
were
still
deeply
concerned
about
algorithmic
raised
concerns
about
potential
use
of
facebook
activity
to
unfairness
they
often
expected
companies
to
address
it
compute
non-regulated
credit
scores
especially
as
this
may
regardless
of
its
source
and
company
response
to
disproportionately
disadvantage
less
privileged
populations
algorithmic
unfairness
could
substantially
impact
user
trust
17
82
edelman
et
al
ran
experiments
on
airbnb
and
these
findings
can
inform
variety
of
stakeholders
from
reported
that
applications
from
guests
with
distinctively
policy
makers
to
corporations
and
they
bolster
the
widely
african
american
names
were
16
less
likely
to
be
espoused
notion
that
algorithmic
fairness
is
societally
accepted
relative
to
identical
guests
with
distinctively
important
goal
for
stakeholders
across
the
ecosystem
from
white
names
28
edelman
and
luca
also
found
non-black
regulator
to
industry
practitioner
to
pursue
with
full
hosts
were
able
to
charge
approximately
12
more
than
recognition
of
the
importance
of
ethical
motivations
these
black
hosts
holding
location
rental
characteristics
and
findings
also
suggest
that
algorithmic
fairness
can
be
good
quality
constant
27
colley
et
al
found
pokémon
go
business
practice
some
readers
may
be
in
search
of
advantaged
urban
white
non-hispanic
populations
for
arguments
to
motivate
or
persuade
companies
to
take
steps
example
potentially
attracting
more
tourist
commerce
to
to
improve
algorithmic
fairness
there
are
many
good
their
neighborhoods
15
and
johnson
et
al
found
that
reasons
for
companies
to
care
about
fairness
including
but
geolocation
inference
algorithms
exhibited
substantially
not
limited
to
ethical
and
moral
imperatives
legal
worse
performance
for
underrepresented
populations
requirements
regulatory
risk
and
public
relations
and
rural
users
47
brand
risk
in
this
paper
we
provide
additional
motivation
this
public
awareness
has
been
accompanied
by
increased
by
illustrating
that
user
trust
is
an
important
but
legal
and
regulatory
attention
for
example
the
upcoming
understudied
pragmatic
incentive
for
companies
across
the
european
union
general
data
protection
regulation
technology
sector
to
pursue
algorithmic
fairness
based
on
contains
an
article
on
automated
individual
decision
our
findings
we
outline
three
best
practices
for
pursuing
making
39
yet
algorithmic
fairness
poses
many
legal
algorithmic
fairness
complexities
and
challenges
and
law
and
regulation
are
background
still
in
nascent
stages
in
this
rapidly
changing
field
algorithmic
fairness
to
investigate
systems
adherence
to
emerging
legal
in
taking
up
algorithmic
fairness
we
draw
on
and
seek
to
regulatory
and
ethical
standards
of
algorithmic
fairness
extend
emerging
strands
of
thought
within
the
fields
of
both
testing
and
transparency
have
been
called
for
science
and
technology
studies
sts
hci
mathematics
14
77
wide
range
of
techniques
have
been
proposed
and
related
disciplines
research
on
algorithmic
fairness
to
scrutinize
algorithms
such
as
model
interpretability
encompasses
wide
range
of
issues
for
example
in
some
audits
expert
analysis
and
reverse
engineering
cases
considering
discrete
decisions
and
their
impact
on
22
42
76
77
investigation
is
complicated
however
by
the
individuals
fair
division
algorithms
explored
in
myriad
potential
causes
of
unfairness
prejudice
structural
51
52
and
in
other
cases
exploring
broader
patterns
bias
choice
of
training
data
complex
interactions
of
human
related
to
groups
that
have
traditionally
been
marginalized
behavior
with
machine
learning
models
unforeseen
supply
in
society
our
focus
tends
towards
the
latter
and
of
and
demand
effects
of
online
bidding
processes
etc
and
particular
relevance
to
our
investigation
is
the
perspective
the
sometimes
impenetrable
and
opaque
nature
of
machine
taken
in
critical
algorithm
studies
which
articulates
the
learning
systems
12
38
in
fact
existing
offline
increasing
influence
of
algorithms
in
society
and
largely
discrimination
problems
may
in
some
cases
be
exacerbated
focuses
on
understanding
algorithms
as
an
object
of
social
and
harder
to
investigate
once
they
manifest
in
online
concern
17
38
54
55
63
76
79
countering
popular
systems
77
and
new
bigotries
based
not
just
on
claims
that
algorithmic
authority
or
data-driven
decisions
immutable
characteristics
but
more
subtle
features
may
may
lead
to
increased
objectivity
many
scholars
have
arise
which
are
more
difficult
to
detect
than
traditional
observed
that
algorithms
can
reflect
amplify
or
introduce
discriminatory
processes
bias
10
18
33
35
38
46
64
not
only
do
opacity
and
complexity
complicate
expert
articles
in
academic
venues
as
well
as
the
popular
press
analysis
but
they
may
also
make
it
difficult
for
have
chronicled
specific
instances
of
unjust
or
prejudicial
stakeholders
to
understand
the
consequences
of
algorithmic
treatment
of
people
based
on
categories
like
race
sexual
systems
many
of
the
proposed
mechanisms
for
scrutinizing
orientation
or
gender
through
algorithmic
systems
or
algorithms
make
certain
assumptions
about
the
public
algorithmically
aided
decision-making
for
example
perez
regulators
and
other
stakeholders
however
research
has
reported
that
microsoft
tay
an
artificial
intelligence
found
that
perception
of
algorithmic
systems
can
vary
chatbot
suffered
coordinated
attack
that
led
it
to
exhibit
substantially
by
individual
factors
as
well
as
platform
21
racist
behavior
65
researchers
have
also
reported
that
and
that
end
users
often
have
fundamental
questions
or
misconceptions
about
technical
details
of
their
operation
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
11
31
69
85
86
an
effect
that
may
be
exacerbated
for
less
be
different
from
those
of
technological
experts
87
privileged
populations
86
for
example
studies
have
taking
this
perspective
we
orient
to
our
workshop
found
that
some
participants
are
not
aware
of
algorithmic
attendees
as
experts
in
how
technology
is
experienced
in
curation
in
the
facebook
news
feed
31
69
or
the
their
daily
lives
framing
that
speaks
to
their
own
sets
of
gathering
of
online
behavioral
data
and
its
use
for
knowledges
that
are
different
but
not
any
less
than
those
of
inferencing
86
or
underestimate
the
prevalence
and
scale
technological
experts
of
data
gathering
and
its
use
in
practical
applications
in
the
1980s
hci
scholars
jungk
and
müllert
first
85
86
further
participants
often
emphasize
the
role
of
described
the
future
workshop
as
format
for
social
human
decision-making
in
algorithmic
systems
for
engagement
which
involved
the
organization
of
events
with
example
misattributing
algorithmic
curation
in
the
members
of
the
public
meant
to
better
address
issues
of
facebook
news
feed
to
actions
taken
by
their
friends
and
democratic
concern
48
similar
in
its
political
roots
family
31
or
framing
algorithms
as
calculator-like
tools
participatory
design
is
method
focused
on
more
actively
that
support
human
decision-making
86
including
members
of
the
public
or
other
under-represented
despite
this
existing
research
on
algorithmic
literacy
very
stakeholders
in
the
processes
of
design
early
examples
of
little
research
has
explored
understandings
of
algorithmic
this
work
from
the
1980s
aimed
to
support
worker
un
fairness
and
there
is
currently
little
insight
into
how
the
autonomy
and
appreciation
of
traditional
expertise
in
light
general
public
and
in
particular
people
affected
by
of
the
introduction
of
digitized
work
practices
and
in
some
algorithmic
unfairness
might
perceive
it
in
rare
cases
automation
of
labor
for
example
pelle
ehn
exception
plane
et
al
surveyed
broad
population
in
the
design
scholar
and
longtime
proponent
of
participatory
us
including
near-census
representative
panel
regarding
design
collaborated
with
scandinavian
graphic
designers
their
responses
to
online
behavioral
advertising
oba
union
to
produce
software
system
meant
to
better
scenarios
that
used
race
as
targeting
variable
for
job
ad
incorporate
their
skilled
practices
compared
with
67
overall
almost
half
of
the
respondents
viewed
the
management-initiated
programs
30
scenarios
as
moderate
or
severe
problem
with
black
more
contemporary
participatory
initiatives
have
taken
up
respondents
finding
them
to
be
of
higher
severity
we
offer
concerns
outside
of
work
or
governmental
contexts
from
complementary
and
novel
exploration
of
algorithmic
exploring
alternative
food
systems
23
24
to
understanding
un
fairness
in
that
we
explore
much
wider
range
of
how
to
promote
play
among
neurodiverse
children
80
potential
types
of
algorithmic
unfairness
we
take
still
others
have
developed
the
design
workshop
as
means
qualitative
approach
that
allows
us
to
deeply
explore
issues
of
examining
critical
theory
through
material
practice
like
with
smaller
population
which
is
complementary
to
plane
making
and
tinkering
70
or
used
craft
to
imagine
et
al
more
narrow
quantitative
exploration
with
larger
alternative
near
futures
that
might
yield
more
equitable
and
more
representative
sample
67
and
we
focus
on
social
arrangements
75
populations
that
are
more
likely
to
be
affected
by
algorithmic
unfairness
rather
than
the
general
public
here
we
build
on
this
legacy
of
participatory
programs
by
workshop
as
method
reporting
on
our
use
of
the
workshop
format
as
research
in
taking
up
workshop
format
we
draw
on
traditions
instrument
toward
understanding
not
only
how
participants
within
and
just
beyond
hci
this
includes
programs
of
perceive
algorithmic
un
fairness
but
also
how
they
might
participatory
action
research
participatory
design
and
elect
to
construct
platforms
differently
due
to
the
living
labs
within
the
context
of
hci
and
design
research
potentially
sensitive
nature
of
the
subject
matter
we
looked
workshop
approaches
often
seek
to
invite
members
of
the
to
dialogical
approaches
like
participatory
design
as
public
to
engage
with
practices
of
design
while
exploring
helpful
technique
for
collaboratively
working
through
values
and
beliefs
around
technology
with
each
other
complex
ideas
machine
learning
and
developing
an
positing
alternative
techniques
and
outcomes
noting
the
open
environment
for
sharing
feelings
and
opinions
we
see
collaborative
and
situated
nature
of
the
approach
rosner
et
these
discussions
and
subsequent
ideas
as
informing
the
al
describe
the
design
workshop
as
inviting
treatment
of
development
of
technology
and
policy
as
well
as
collaboration
and
interdisciplinary
as
localized
and
communication
with
diverse
users
in
the
future
imaginative
practice
74
these
engagements
rely
on
methodology
careful
collaboration
between
researcher
and
in
order
to
better
understand
how
members
of
marginalized
subject
partner
across
sites
like
academic
or
industrial
communities
perceive
algorithmic
un
fairness
we
research
centers
and
community
groups
each
with
their
own
conducted
participatory
design
workshops
with
members
of
goals
for
the
work
relatedly
research
on
the
public
various
communities
throughout
the
san
francisco
bay
understanding
of
science
argues
against
assuming
single
area
we
then
conducted
individual
follow-up
interviews
correct
understanding
of
science
and
technology
with
select
participants
the
workshops
and
interviews
took
emphasizing
that
members
of
the
public
should
not
be
place
july
through
september
of
2016
excluded
from
democratic
decision-making
about
technology
because
their
interpretations
of
technology
may
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
participants
slightly
emphasizing
those
involved
in
care
or
service
we
recruited
44
adults
all
of
whom
responded
to
screener
professions
skills
and
expertise
often
underrecognized
in
survey
administered
by
national
research
recruitment
firm
technology
cultures
57
71
with
respondent
database
including
san
francisco
bay
area
residents
participants
were
compensated
for
their
most
of
the
participants
were
from
the
east
bay
and
san
time
at
or
above
the
living
wage
for
their
area
our
francisco
with
wide
range
of
ages
18
65
and
recruiting
focused
on
inviting
individuals
who
were
occupations
public
transportation
driver
retail
traditionally
marginalized
either
by
categories
of
manager
special
education
instructor
community
activities
socioeconomic
status
or
race
and
we
organized
our
coordinator
tasker
line
cook
laborer
correctional
peace
participants
into
five
workshops
as
follows
two
workshops
officer
office
assistant
theater
assistant
based
on
socioeconomic
status
as
described
below
one
workshop
workshop
with
participants
who
identified
as
black
or
each
group
participated
in
hour
workshop
with
the
african
american
women
one
workshop
with
black
or
following
agenda
an
icebreaker
activity
group
discussion
african
american
or
mixed
race
men
and
women
and
one
of
algorithmic
un
fairness
meal
design
activity
workshop
with
hispanic
or
latinx
men
and
women
while
centered
around
three
cases
and
concluding
group
our
work
was
qualitative
and
non-representative
we
expect
discussion
in
attendance
at
each
workshop
were
between
the
constituencies
on
which
we
focused
comprise
roughly
and
11
participants
researchers
who
acted
as
facilitators
between
40
and
50
of
the
us
population
and
visual
anthropologist
who
focused
on
documentation
participants
were
aware
of
google
involvement
in
the
primary
factors
in
considering
socioeconomic
status
were
study
and
the
workshops
took
place
at
google
location
current
household
income
and
education
level
selected
during
the
workshops
we
took
care
to
encourage
participants
had
an
annual
household
income
of
less
than
collaborative
interpretation
problem-solving
and
the
living
wage
for
their
home
county
an
amount
discussion
among
participants
and
to
make
space
for
all
determined
from
coarse
approximation
of
glasmeier
participants
to
share
their
ideas
and
opinions
additionally
living
wage
model
livingwage
mit
edu
accessed
july
recognizing
the
emotional
complexity
of
the
topic
we
august
2016
in
factoring
this
amount
we
considered
the
explained
that
there
might
be
sensitive
material
and
that
total
number
of
adults
in
the
household
the
number
of
participants
should
feel
free
to
stop
participating
sit
out
on
adults
contributing
to
the
income
the
number
of
dependent
an
activity
or
step
out
of
the
room
children
in
the
household
and
the
number
of
children
outside
the
household
cared
for
financially
by
the
to
start
the
day
we
asked
participants
to
take
part
in
an
respondent
participants
had
also
earned
no
more
than
icebreaker
activity
inspired
by
anti-racism
scholar
peggy
some
college
defined
here
as
up
to
years
of
course
mcintosh
invisible
knapsack
exercise
56
78
meant
to
taking
without
receiving
an
associate
or
bachelor
begin
to
discuss
issues
of
discrimination
power
and
degree
as
secondary
factors
contributing
to
socioeconomic
privilege
in
non-confrontational
manner
after
this
initial
status
determination
we
also
considered
the
respondent
activity
the
researchers
gave
brief
description
of
current
occupation
and
location
of
residence
with
this
the
algorithms
and
algorithmic
un
fairness
broad
discussion
focus
was
on
understanding
the
respondent
current
revolved
around
participant
questions
and
interpretation
of
economic
situation
as
well
as
near
term
opportunity
for
algorithmic
un
fairness
whether
participants
knew
about
it
advancement
based
on
proximate
resources
prior
to
the
workshop
or
had
ever
experienced
it
and
sharing
of
general
feelings
about
it
note
that
during
the
for
the
remainder
of
the
workshops
our
recruitment
workshop
we
used
the
term
algorithmic
discrimination
focused
on
inviting
people
of
color
based
on
their
rather
than
algorithmic
un
fairness
while
algorithmic
responses
in
the
recruitment
screener
as
secondary
fairness
is
often
used
as
term
in
the
academic
literature
consideration
we
also
looked
to
respondent
occupation
our
experience
in
this
study
as
well
as
other
work
at
our
institution
suggests
that
in
user
research
context
fairness
the
us
census
bureau
estimates
that
as
of
july
2016
the
black
or
may
be
construed
overly
narrowly
for
example
as
african
american
population
constitutes
13.3
43
million
people
of
the
emphasizing
equality
rather
than
justice
and
therefore
we
total
us
population
323.1
million
people
the
hispanic
or
latino
preferred
to
use
algorithmic
discrimination
in
our
population
is
17.8
57.5
million
people
and
the
population
with
two
or
more
races
is
2.6
https://www.census.gov/quickfacts,
accessed
august
conversations
with
participants
2017
while
we
were
not
able
to
find
an
estimated
percentage
of
the
us
population
that
meets
the
living
wage
standard
the
poverty
rate
in
2015
for
the
bulk
of
the
day
we
focused
on
series
of
three
was
13.5
43.1
million
people
approximately
51
of
whom
were
black
scenario-based
design
activities
we
began
each
scenario
by
or
hispanic
68
since
the
living
wage
exceeds
the
poverty
threshold
we
describing
case
that
could
be
understood
as
an
instance
of
expect
that
substantially
more
than
13.5
would
not
meet
the
living
wage
algorithmic
unfairness
and
then
invited
participants
to
standard
61
and
in
fact
the
number
seems
likely
to
be
closer
to
the
29
of
americans
that
pew
identified
as
living
in
lower-class
household
37
share
their
initial
reactions
in
brief
group
discussion
overall
this
suggests
that
the
populations
we
focused
on
although
with
during
this
discussion
we
also
occasionally
introduced
only
small
qualitative
sample
conservatively
comprise
nearly
40
of
various
complexities
for
example
suggesting
different
the
us
population
and
more
likely
slightly
over
50
potential
causes
of
unfairness
then
we
asked
participants
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
to
spend
10
minutes
working
individually
to
come
up
with
the
data
from
both
the
workshops
and
interviews
by
closely
ideas
about
what
they
might
do
if
they
were
decision
reviewing
the
text
and
videos
performing
affinity
maker
at
technology
company
in
charge
of
responding
to
clusterings
of
textual
quotations
and
video
clips
to
identify
the
scenario
we
told
participants
they
were
free
to
express
emergent
themes
producing
short
films
synthesizing
their
ideas
using
any
means
of
communication
they
found
key
themes
using
visual
ethnographic
approach
66
and
most
comfortable
drawing
story
writing
performing
iteratively
revising
and
refining
categories
in
keeping
with
were
all
examples
given
after
they
worked
and
recorded
the
general
inductive
approach
our
analytic
process
yielded
their
ideas
we
came
back
together
as
group
and
went
small
number
of
summary
categories
which
we
describe
around
the
table
to
share
and
discuss
everyone
ideas
in
the
findings
section
below
the
scenarios
we
discussed
represented
wide
range
of
limitations
issues
while
the
scenarios
were
based
on
internet-related
we
note
several
limitations
of
our
study
methodology
that
products
and
services
we
also
encouraged
discussion
of
should
be
considered
when
interpreting
this
work
first
due
other
domains
and
the
discussion
often
branched
out
to
to
our
focus
on
traditionally
marginalized
populations
we
other
areas
in
which
algorithmic
unfairness
might
occur
did
not
gather
data
about
how
more
privileged
populations
the
first
scenario
described
man
visiting
newspaper
think
about
or
experience
algorithmic
fairness
second
our
website
and
seeing
ads
for
high-paying
jobs
while
sample
was
not
statistically
representative
of
the
woman
visiting
the
same
website
saw
ads
for
low-wage
populations
we
explored
the
findings
we
report
should
be
work
the
second
scenario
was
about
results
of
predictive
viewed
as
deep
exploration
of
our
sample
beliefs
and
search
feature
which
suggests
possible
search
terms
as
attitudes
but
not
as
generalizing
to
those
populations
as
the
user
types
into
search
box
that
could
be
interpreted
as
whole
third
our
choice
of
scenarios
as
well
as
our
choice
stereotyping
black
men
and
children
as
criminals
with
the
to
use
the
term
algorithmic
discrimination
while
third
and
final
scenario
we
asked
participants
to
consider
appropriate
given
our
focus
may
have
influenced
practice
of
excluding
businesses
in
neighborhoods
with
participants
and
other
framings
of
fairness
may
have
high
crime
rates
from
an
online
restaurant
reviewing
and
yielded
different
results
finally
because
we
touch
on
map
application
after
we
completed
all
three
scenarios
socioeconomic
status
and
ethnicity
in
this
work
we
include
we
concluded
the
workshop
with
broad
group
discussion
the
detail
that
the
research
team
consisted
only
of
college
reflecting
back
on
ideas
that
had
emerged
throughout
the
educated
european-american
researchers
we
describe
day
and
the
experience
of
the
workshop
as
whole
participants
experiences
in
their
own
words
but
our
interpretations
may
lack
context
or
nuance
that
may
have
interviews
been
more
readily
available
to
more
diverse
research
after
the
workshops
were
completed
we
conducted
follow
team
up
interviews
approximately
one
hour
in
length
with
11
participants
who
appeared
particularly
engaged
during
the
findings
workshop
discussions
interviews
were
semi-structured
in
this
section
we
describe
the
main
findings
that
emerged
with
questions
focused
on
gaining
further
understanding
of
from
our
analysis
the
participant
concerns
opinions
and
policy
ideas
unfamiliar
but
not
unfathomable
analysis
most
participants
were
not
aware
of
the
concept
of
all
interviews
were
video-recorded
and
transcribed
in
our
algorithmic
un
fairness
before
participating
in
the
study
analysis
we
used
general
inductive
approach
83
which
although
once
it
was
described
few
reported
that
they
had
relies
on
detailed
readings
of
raw
data
to
derive
themes
had
personal
experiences
with
it
or
had
heard
about
it
in
the
relevant
to
evaluation
objectives
in
our
case
the
primary
media
however
most
participants
reported
extensive
evaluation
objective
was
to
inform
technical
and
policy
experience
with
discrimination
in
their
daily
lives
and
they
approaches
to
algorithmic
fairness
by
learning
about
connected
their
personal
stories
to
the
concept
of
participants
interpretation
of
algorithmic
fairness
and
algorithmic
un
fairness
participants
ascription
of
accountability
and
their
ethical
personal
experiences
with
discrimination
and
pragmatic
expectations
of
stakeholders
especially
most
participants
reported
extensive
negative
experience
companies
accordingly
we
focused
on
these
issues
during
with
discrimination
and
stereotyping
unfair
treatment
or
our
time
with
the
participants
and
then
we
jointly
analyzed
racial
profiling
by
law
enforcement
was
commonly
raised
for
example
some
participants
described
experiences
with
driving
while
black
being
pulled
over
by
police
because
inspired
by
20
which
reported
an
experiment
in
which
simulated
men
of
their
race
particularly
when
driving
in
affluent
visiting
the
times
of
india
website
were
more
likely
than
simulated
women
to
see
an
ad
for
career
coaching
service
for
200k
executive
neighborhoods
with
few
black
residents
53
participants
positions
also
raised
number
of
issues
related
to
social
and
environmental
justice
such
as
white
privilege
societal
inspired
by
62
advantages
conferred
on
caucasians
gentrification
forcing
inspired
by
73
people
with
low
incomes
out
of
their
homes
food
deserts
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
lack
of
access
to
grocery
stores
and
healthy
food
in
one
or
discovery
or
american
express
no
they
ve
already
labeled
me
as
the
low
income
person
p43
impoverished
areas
and
the
proximity
of
low
income
neighborhoods
to
pollution
and
environmental
hazards
p28
they
had
to
hire
eric
holder
to
tamp
down
all
the
racism
of
participants
also
shared
number
of
other
experiences
airbnb
such
as
shopping
while
black
receiving
poor
service
in
facilitator
so
what
do
you
think
airbnb
should
do
retail
establishments
or
being
followed
or
monitored
by
p28
laughs
staff
who
suspect
they
may
steal
36
being
targeted
by
p29
well
something
was
already
done
an
african
american
man
direct
mail
unsolicited
advertisements
sent
by
physical
creating
p28
the
attorney
general
of
the
united
states
they
had
to
hire
the
mail
for
predatory
lending
and
other
disadvantageous
former
attorney
general
the
biggest
lawyer
in
the
united
states
to
opportunities
being
stereotyped
as
angry
because
they
handle
the
racism
of
airbnb
are
black
or
employment-related
discrimination
many
reactions
to
algorithmic
unfairness
viewed
these
as
pervasive
issues
that
framed
their
even
though
most
participants
had
not
been
aware
of
opportunities
and
daily
experiences
often
from
young
algorithmic
unfairness
prior
to
the
study
learning
about
it
age
elicited
strong
negative
feelings
evoking
experiences
with
my
mother
was
taking
us
to
daycare
and
remember
her
getting
discrimination
in
other
settings
for
example
participants
pulled
over
in
city
and
the
police
officer
arresting
her
taking
her
to
jail
me
and
my
sister
had
to
go
to
place
where
there
were
other
drew
connections
between
algorithmic
unfairness
and
children
our
age
at
the
time
we
were
scared
we
didn
know
why
national
dialogues
about
racial
injustice
and
economic
she
was
actually
in
handcuffs
we
stayed
there
all
day
and
it
was
inequality
as
well
as
lost
opportunities
for
personal
because
the
car
was
behind
in
registration
wasn
even
in
advancement
elementary
school
yet
we
were
going
to
preschool
and
it
was
quite
traumatizing
and
do
believe
that
it
was
because
she
was
an
african
if
would
have
searched
and
those
things
popped
up
would
have
american
in
city
so
you
learn
the
roles
that
you
have
or
what
could
been
very
angry
in
fact
it
makes
me
angry
right
now
just
looking
at
it
possibly
happen
at
very
young
age
so
some
things
now
are
just
because
what
should
be
is
that
if
somebody
wants
to
know
if
he
was
anticipated
they
re
not
even
shocking
anymore
p435
thug
they
have
to
type
in
was
he
thug
not
have
it
be
suggested
to
them
because
for
people
like
me
who
feel
like
the
police
are
taking
tell
my
daughter
that
when
you
were
eight
months
in
your
mom
advantage
of
getting
away
with
killing
brown
and
black
people
all
womb
you
were
already
racially
profiled
in
traffic
stop
p20
over
the
country
it
infuriating
so
what
they
should
do
is
no
matter
they
re
following
me
around
the
grocery
store
like
going
to
steal
what
other
people
have
typed
in
before
when
someone
types
it
in
it
something
p11
should
show
up
as
certain
facts
no
adjectives
no
judgments
no
positive
or
negative
connotations
just
whatever
happened
that
has
there
was
lot
of
environmental
racism
in
the
neighborhood
that
been
factually
reported
p23
grew
up
in
it
was
very
impoverished
lots
of
police
brutality
it
just
set
up
that
way
for
us
to
fail
p11
to
have
your
destiny
or
your
destination
in
life
based
on
mathematics
or
something
that
you
don
put
in
for
yourself
to
have
prior
awareness
of
algorithmic
unfairness
everything
that
you
worked
and
planned
for
based
on
something
that
once
algorithmic
unfairness
was
described
to
them
few
totally
out
of
your
control
it
seems
little
harsh
because
it
like
this
is
what
you
re
sent
to
do
and
because
of
algorithm
it
sets
you
back
participants
reported
that
they
were
aware
of
times
they
had
from
doing
just
that
it
not
fair
p04
experienced
it
naturally
participants
may
also
have
experienced
it
and
not
been
aware
of
it
and
few
other
participants
also
drew
connections
with
personal
stories
and
participants
said
they
were
familiar
with
the
concept
from
life
experiences
for
example
they
objected
heavily
to
the
media
for
example
small
number
of
participants
stereotyping
such
as
negative
online
characterizations
of
raised
concerns
about
having
been
targeted
for
low
income
marginalized
groups
or
online
ads
or
information
being
ads
and
few
discussed
turning
off
location
history
to
personalized
based
on
demographic
characteristics
similar
avoid
racial
profiling
and
racially
motivated
advertising
to
concerns
raised
in
67
86
similarly
they
also
felt
it
couple
of
participants
also
discussed
experiences
with
was
very
unfair
to
personalize
ads
or
information
based
on
computer
systems
making
unfair
job
and
scholarship
the
online
behavior
of
other
people
with
similar
decisions
several
participants
also
described
stories
they
characteristics
while
at
first
glance
this
may
appear
to
had
heard
about
in
the
press
regarding
companies
such
as
contrast
with
plane
et
al
finding
that
online
behavioral
airbnb
facebook
google
nextdoor
and
others
advertising
was
seen
as
significantly
less
problematic
than
explicit
demographic
targeting
67
it
seems
likely
that
constantly
bombarded
with
you
can
get
this
low
income
credit
card
you
can
get
this
low
finance
loan
didn
ask
for
no
loan
participants
underlying
concern
in
both
cases
relates
to
the
didn
ask
for
no
credit
card
plus
it
low
income
loan
it
not
use
of
demographic
characteristics
or
other
sensitive
traits
like
would
you
like
to
buy
house
would
you
like
to
buy
boat
to
personalize
information
would
you
like
to
finance
car
no
why
can
have
like
capital
p34
it
totally
unfair
p33
because
not
every
woman
the
same
it
not
accurate
if
you
re
just
basing
it
on
group
p22
for
ease
of
reading
we
have
followed
editing
conventions
consistent
with
applied
social
science
research
practices
as
described
in
16
they
didn
even
base
it
what
was
shown
to
me
on
what
ve
done
specifically
we
edited
quotes
to
remove
content
such
as
filler
words
and
in
the
past
they
re
just
basing
it
on
what
they
think
am
p23
false
starts
and
in
some
cases
we
re-punctuated
we
use
ellipses
to
indicate
substantial
omissions
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
some
participants
oriented
to
algorithmic
unfairness
as
to
be
the
final
decision
maker
it
all
good
so
that
it
can
help
categorize
it
suggest
but
to
be
the
main
decision
maker
that
would
modern
incarnation
of
familiar
forms
of
discrimination
an
be
scary
p05
unwelcome
extension
of
offline
discrimination
into
the
online
arena
further
for
the
most
part
participants
interpreted
small
percentage
biases
of
algorithmic
decisions
as
low-impact
it
setup
for
not
everyone
to
win
since
the
beginning
of
civilization
there
always
been
hierarchy
technology
is
just
and
indicating
natural
imperfection
rather
than
subtle
bias
another
wheel
in
that
p37
while
researchers
have
argued
that
small
statistical
it
seems
like
in
technology
it
fascinating
but
at
the
same
time
it
differences
can
have
significant
cumulative
effects
on
alarming
because
it
seems
like
in
every
phase
people
have
taken
it
individuals
and
or
groups
thereby
perpetuating
or
and
have
always
done
something
wicked
with
it
p30
increasing
inequality
41
participants
appeared
to
interpret
because
it
algorithmic
there
is
some
type
of
system
to
it
which
small
statistical
disparities
as
benign
largely
considering
means
that
there
is
some
type
of
work
being
put
into
this
certain
type
them
to
be
natural
inevitable
and
impossible
to
fix
of
discrimination
that
it
actually
people
in
the
world
that
want
it
to
be
that
way
and
it
like
why
just
don
understand
why
we
it
sounds
fine
to
me
don
expect
perfection
of
course
p43
have
to
live
under
these
type
of
circumstances
p04
high
salience
of
representational
consequences
p12
we
deal
with
this
just
walking
down
the
street
while
participants
may
not
have
always
come
in
with
p14
on
daily
basis
previous
notion
of
the
wide-reaching
implications
of
the
p12
on
daily
basis
we
don
need
this
on
our
internet
on
our
underlying
algorithmic
systems
they
did
care
deeply
about
sites
that
we
trust
the
most
we
don
need
to
see
the
negative
connotation
come
up
every
time
we
have
to
walk
out
of
our
house
the
visible
results
of
these
systems
and
how
marginalized
and
wonder
if
we
re
going
to
make
it
back
in
and
when
we
re
safe
in
groups
were
portrayed
online
participants
were
aware
of
our
homes
we
need
to
feel
safe
especially
if
it
comes
from
google
and
concerned
about
skewed
representations
and
negative
or
site
that
we
trust
stereotypes
for
example
online
sexualization
of
women
or
p11
um-hm
you
have
to
draw
the
line
somewhere
when
we
get
home
we
ve
already
dealt
with
it
all
day
at
work
at
school
and
it
offensive
language
about
particular
ethnic
groups
such
like
want
to
come
home
and
don
want
to
have
to
deal
with
this
offenses
connected
to
broader
system
of
microaggressions
too
when
get
on
the
computer
shouldn
have
to
be
subjected
81
and
personal
stories
from
their
own
lives
to
racial
stereotypes
p29
if
you
type
in
two
black
teenagers
you
will
see
all
mugshots
of
although
parallels
to
other
life
experiences
may
have
black
boys
but
with
white
teenagers
you
will
see
them
playing
driven
initial
negative
responses
participants
shared
basketball
boy
scout
nuanced
and
pragmatic
perspectives
as
the
workshops
p28
you
have
negative
connotations
for
the
word
black
and
positive
unfolded
showing
an
appreciation
for
the
complexity
of
connotations
for
the
word
white
that
just
the
way
it
is
this
topic
as
they
discussed
it
just
really
not
happy
with
the
way
that
these
words
are
put
out
scale
and
impact
of
algorithmic
systems
there
these
ideas
p24
though
small
number
of
participants
expressed
belief
to
see
the
things
that
they
said
criminalizing
that
little
boy
that
that
large-scale
algorithmic
systems
underlie
many
aspects
just
broke
my
heart
he
didn
do
nothing
to
deserve
that
and
the
of
modern
society
many
participants
viewed
algorithmic
fact
that
that
what
society
thinks
of
him
that
not
just
something
that
the
computer
put
out
there
got
sisters
got
little
cousins
little
systems
as
small
in
scope
and
low
in
both
complexity
and
nieces
and
nephews
they
could
look
that
up
and
see
that
that
not
impact
this
was
especially
apparent
in
the
solutions
that
right
that
is
not
right
at
all
that
just
sickening
because
that
many
participants
proposed
to
scenarios
of
algorithmic
whole
bunch
of
human
beings
that
really
typed
that
in
if
had
any
unfairness
which
often
emphasized
manual
work
by
the
type
of
way
to
filter
stuff
like
that
would
because
that
not
cool
would
just
erase
it
all
p04
end
user
or
employees
of
technology
companies
echoing
the
types
of
manual
work
envisioned
by
participants
in
86
participants
were
especially
concerned
with
how
children
for
example
some
participants
proposed
that
filtering
or
might
be
affected
by
negative
representations
recommendation
processes
could
be
made
more
fair
by
there
lots
of
images
that
society
already
tells
young
black
boys
removing
algorithmic
processing
and
allowing
the
end
user
or
boys
of
color
that
they
re
thugs
that
they
re
gangster
and
this
and
to
go
through
the
content
themselves
most
participants
that
wouldn
want
my
son
to
look
up
this
teenage
boy
name
and
tended
to
favor
and
trust
human
decision-making
over
those
type
of
images
or
associations
comes
up
behind
his
name
algorithmic
decision-making
this
appears
to
contrast
with
because
my
son
is
young
black
boy
don
think
people
should
be
stereotyped
and
don
want
my
son
to
think
that
society
even
plane
et
al
results
67
which
could
be
due
to
variety
of
though
it
the
truth
society
does
label
you
because
you
re
young
factors
such
as
the
different
populations
studied
and
bears
black
boy
p11
further
investigation
participants
also
felt
that
popularity
algorithms
are
not
the
algorithm
is
not
person
it
just
mathematical
equation
it
benign
mirrors
of
the
world
pointing
out
that
social
media
just
has
information
then
somebody
chooses
that
information
in
certain
way
and
does
with
it
whatever
that
could
mean
choosing
can
amplify
societal
biases
and
increase
the
reach
of
whether
to
use
you
in
job
or
where
to
put
the
next
k-mart
it
not
stereotyping
messages
making
human
decisions
p39
was
just
talking
to
my
girlfriend
about
this
last
night
it
ridiculous
think
it
should
stick
with
suggestions
mean
what
happens
if
the
how
every
time
you
click
on
facebook
or
turn
on
news
radio
station
computer
makes
bad
decision
does
it
just
suggest
or
is
it
going
or
just
the
internet
in
general
there
some
type
of
discrimination
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
going
on
and
the
main
reason
why
it
gotten
this
big
is
because
wrong
people
are
entitled
to
their
opinions
guess
that
their
way
social
media
is
in
the
middle
of
it
all
p04
of
going
online
and
free
speeching
too
whether
it
right
or
wrong
the
search
engine
not
at
fault
it
humanity
wouldn
blame
feeding
into
that
stuff
to
me
is
going
backwards
even
encouraging
company
for
that
people
to
read
about
that
stuff
and
feeding
into
those
thoughts
there
no
need
to
feed
p22
even
when
they
believed
that
the
cause
was
external
most
accountability
still
saw
technology
companies
as
having
some
participants
proposed
number
of
different
parties
might
be
responsibility
and
role
to
play
in
addressing
the
issue
this
responsible
for
algorithmic
unfairness
and
sometimes
had
is
consistent
with
and
extends
plane
et
al
finding
that
differing
opinions
about
the
likely
underlying
cause
of
many
participants
held
both
the
advertiser
and
the
ad
unfairness
three
of
the
most
commonly
proposed
causes
network
responsible
regardless
of
which
was
explicitly
were
non-diverse
population
of
programmers
named
as
the
perpetrator
67
further
they
believed
that
prejudiced
online
behavior
by
members
of
society
and
companies
could
readily
resolve
many
of
the
problems
if
the
news
media
while
number
of
these
ideas
suggest
an
they
chose
to
do
so
understanding
of
algorithmic
fairness
that
goes
beyond
the
think
that
people
that
work
for
these
companies
they
can
make
the
technical
it
is
worth
noting
that
many
potential
causes
change
tonight
if
they
wanted
to
it
just
matter
of
how
are
they
commonly
raised
in
technical
circles
such
as
lack
of
going
to
meticulously
put
everything
so
it
will
still
benefit
them
in
some
aspect
p29
diverse
training
data
or
inequitable
accuracy
in
classifying
members
of
different
categories
44
were
raised
rarely
or
occasionally
in
specific
contexts
some
participants
not
at
all
indicated
that
they
did
not
feel
companies
could
or
should
take
action
the
most
prevalent
arguments
for
inaction
many
participants
held
the
programmer
accountable
for
an
were
freedom
of
expression
concern
about
censoring
algorithm
discrimination
not
necessarily
because
they
content
from
credible
news
sources
belief
that
user
is
thought
programmers
were
ill-intended
but
rather
because
personally
responsible
for
making
good
choices
in
their
their
perception
was
that
programmers
are
predominantly
online
activity
in
order
to
shape
what
they
see
or
belief
privileged
white
males
who
do
not
understand
the
that
there
was
not
feasible
technological
solution
perspective
of
more
diverse
users
they
felt
more
diverse
hiring
practices
would
help
as
company
like
google
you
have
to
respect
the
free
speech
what
could
you
do
it
would
be
very
difficult
decision
for
me
to
people
create
the
technology
to
do
these
things
so
that
why
say
it
have
to
make
p44
stems
from
the
writer
p29
sometimes
that
what
people
want
to
see
you
kind
of
got
to
give
when
you
lack
that
diversity
they
may
not
be
able
to
input
certain
them
what
they
want
to
see
unfortunately
it
scary
p24
things
into
that
equation
because
they
don
know
that
reality
because
the
people
that
are
writing
these
apps
are
probably
unless
google
owns
the
news
companies
think
it
kind
of
out
of
not
from
our
community
you
need
to
be
more
selective
diverse
or
their
hands
p37
whatever
in
who
you
re
hiring
p20
don
know
who
going
to
really
go
and
actually
keep
up
with
each
facilitator
does
anybody
else
have
any
thoughts
about
who
writing
controversial
racial
issue
that
comes
up
how
would
you
regulate
algorithms
how
would
you
know
that
these
things
would
eventually
come
up
p24
think
it
kind
of
assumed
that
it
is
white
males
you
just
check
every
damn
time
something
happened
you
just
kind
p17
ivy
league
people
of
look
and
you
kind
of
monitor
don
even
know
if
that
actually
p21
laughs
was
going
to
say
rich
white
men
feasible
p43
p24
mean
who
else
laughs
however
these
positions
were
less
common
tended
to
arise
p21
does
that
make
us
racist
when
we
say
that
for
fairly
specific
situations
and
were
often
in
opposition
to
much
more
commonly
expressed
positions
that
companies
participants
also
often
thought
that
much
of
the
can
and
should
act
to
reduce
unfairness
stereotyping
or
racism
was
coming
from
outside
of
technology
companies
frequently
calling
out
the
role
curation
society
played
in
creating
the
problem
some
participants
as
mentioned
in
the
previous
section
participants
also
emphasized
that
the
news
media
is
source
of
bias
expressed
certain
expectations
of
companies
regardless
of
the
source
of
unfairness
in
this
section
we
discuss
the
most
it
not
really
like
company
being
racist
it
really
just
machine
it
stats
it
counting
numbers
it
counting
what
we
are
prominent
themes
regarding
expectations
curatorial
all
looking
at
it
based
on
what
we
re
looking
at
not
what
google
position
on
representation
and
the
voice
of
the
company
wants
you
to
look
at
the
problem
is
us
and
what
we
have
in
our
minds
so
we
can
really
turn
around
and
be
like
oh
google
did
it
journalistic
standards
p02
participants
tended
to
hold
technology
companies
such
as
p06
hear
what
you
re
saying
and
totally
against
everything
search
engines
to
journalistic
standards
for
instance
they
that
going
on
but
the
only
reason
it
so
popular
is
because
expected
them
to
perform
careful
manual
fact
checking
everybody
clicking
on
it
and
people
are
making
it
popular
people
although
resonating
with
the
findings
above
regarding
have
put
that
in
there
doesn
mean
it
true
underestimation
of
scale
participants
tended
to
propose
p02
yeah
the
problem
not
really
the
search
engine
it
the
people
searching
wouldn
blame
google
or
anything
because
it
just
manual
human-scale
approaches
and
show
proven
facts
going
on
clicks
the
machine
not
deciding
whether
it
right
or
rather
than
opinions
or
biased
content
some
participants
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
indicated
the
news
media
do
not
always
meet
this
standard
feelings
of
betrayal
disappointment
or
anger
when
but
rather
sometimes
shows
harmful
biased
representations
companies
they
trusted
surfaced
societal
bias
or
prejudice
of
marginalized
populations
and
some
felt
that
technology
ve
used
google
lot
it
been
my
lifeline
almost
maybe
that
companies
could
compensate
for
this
why
even
more
offended
it
like
come
on
google
thought
we
were
better
than
that
p24
p10
would
only
allow
what
is
actual
fact
don
need
to
know
your
cousin
your
momma
said
this
that
and
the
other
just
include
when
go
on
google
like
the
company
and
expect
great
things
p07
the
truth
from
them
and
expect
facts
and
expect
not
to
see
stuff
like
that
and
p10
the
facts
don
want
my
child
to
see
it
because
it
such
great
company
p12
the
media
responsibility
google
has
that
responsibility
p28
just
need
the
news
on
it
it
makes
you
upset
when
you
see
that
all
however
when
participants
perceived
companies
were
the
time
about
any
person
pretty
much
that
has
been
in
the
news
for
protecting
them
from
unfairness
or
discrimination
it
greatly
being
brutalized
or
killed
would
prefer
for
it
to
be
just
official
enhanced
user
trust
and
strengthened
their
relationships
news
would
like
to
try
to
explore
on
my
own
make
my
own
with
those
companies
opinion
but
it
seems
like
my
opinion
is
already
kind
of
being
made
before
can
even
search
for
answers
p43
think
that
it
very
good
decision
that
google
decided
to
stop
running
tobacco
ads
and
stop
doing
the
payday
loans
because
it
lets
think
it
their
responsibility
to
not
do
that
they
don
have
to
me
know
that
as
consumer
they
are
taking
my
feelings
into
report
it
like
that
just
because
the
news
reports
it
like
that
p12
consideration
tell
my
son
to
search
google
all
the
time
and
so
now
on
related
note
many
participants
suggested
that
feel
more
confident
may
not
have
to
watch
over
his
shoulder
very
good
very
pleased
p43
predictive
search
feature
should
not
suggest
negative
information
for
individuals
particularly
minors
few
also
discussion
suggested
that
negative
information
should
be
as
human-computer
interaction
researchers
we
often
make
counterbalanced
with
positive
information
so
the
reader
arguments
to
stakeholders
about
how
and
why
they
can
could
learn
about
both
sides
of
an
argument
and
reach
their
change
technology
to
better
serve
users
and
or
improve
own
conclusions
society
in
the
case
of
algorithmic
fairness
stakeholders
such
as
regulators
lawmakers
the
press
industry
voice
of
the
company
practitioners
and
many
others
have
the
opportunity
to
take
participant
responses
suggest
that
in-product
information
positive
action
technology
companies
in
particular
have
processed
by
algorithms
can
give
the
impression
that
tremendous
leverage
to
improve
algorithmic
fairness
company
generated
or
endorses
message
for
example
because
they
are
immediately
proximate
to
many
of
the
predictive
search
actively
suggests
content
within
the
user
technical
issues
that
arise
and
they
are
uniquely
positioned
interface
and
some
participants
felt
this
gave
the
to
diagnose
and
develop
effective
solutions
to
complex
appearance
that
the
content
originated
with
the
company
problems
that
would
be
difficult
for
outsiders
to
address
that
produced
the
feature
participants
also
felt
that
the
accordingly
while
we
hope
it
is
apparent
that
our
findings
feature
could
make
it
too
easy
for
users
to
find
such
can
be
directly
leveraged
by
wide
variety
of
stakeholders
content
or
even
encourage
searching
for
it
and
suggested
especially
for
decisions
relating
to
product
categories
such
that
users
should
have
to
generate
the
negative
searches
as
social
media
and
search
engines
we
focus
here
on
three
themselves
best
practices
that
our
findings
suggest
apply
to
companies
feel
like
encouraging
this
type
of
searching
is
just
toxic
p22
across
the
technology
sector
if
there
were
any
negative
connotations
then
it
wouldn
pop
up
at
include
fairness
as
value
in
product
design
and
all
so
if
you
wanted
to
see
something
negative
you
would
have
to
spell
it
out
p08
development
similar
to
considerations
such
as
privacy
fairness
can
be
included
as
consideration
throughout
the
would
clear
off
all
the
negative
and
just
let
them
actually
type
in
what
they
wanted
to
know
about
the
person
instead
of
offering
product
life
cycle
many
positive
steps
can
be
taken
such
things
p38
as
ensuring
diverse
training
data
for
machine
learning
models
ensuring
that
designers
are
aware
of
inequalities
in
inaction
posed
the
risk
of
appearing
to
endorse
others
their
systems
so
they
can
consider
appropriate
action
discrimination
by
signal
boosting
it
15
49
and
including
diverse
populations
in
user
testing
you
guys
facebook
are
pretty
much
promoting
this
hate
and
promoting
this
deceit
that
not
doing
nothing
but
making
in
support
of
this
point
our
participants
cared
about
everybody
mad
p04
fairness
had
strong
ethical
expectations
of
companies
were
disappointed
when
companies
did
not
act
regardless
of
the
impact
on
user
trust
source
of
the
unfairness
and
greatly
valued
efforts
on
the
as
illustrated
in
the
preceding
sections
algorithmic
fairness
part
of
companies
to
ameliorate
societal
bias
and
make
their
connects
to
strong
emotions
and
in
many
cases
participants
products
as
inclusive
as
possible
therefore
it
is
likely
that
have
high
expectations
of
how
companies
will
ensure
measureable
gains
in
user
trust
and
engagement
can
result
fairness
in
their
products
consistent
with
the
philosophy
of
relationship
marketing
59
participants
linked
algorithmic
earlier
in
the
interview
we
told
the
participant
that
google
had
fairness
to
their
relationships
with
companies
expressing
established
policy
that
banned
ads
for
payday
loans
40
45
paper
656
page
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
from
incorporating
algorithmic
fairness
in
product
design
forward
will
result
from
strong
participation
of
multiple
our
findings
suggest
this
is
an
opportune
time
for
players
point
reinforced
by
lee
et
al
argument
that
companies
to
act
proactively
while
public
perception
of
algorithmic
service
design
support
multiple
stakeholder
this
complex
topic
is
still
evolving
algorithmic
fairness
perspectives
52
for
example
companies
can
partner
with
issues
are
challenging
both
technically
and
organizationally
community
groups
and
community
leaders
to
address
and
can
take
long
time
to
address
particularly
if
particular
challenges
as
airbnb
did
when
addressing
racism
mechanisms
are
not
already
in
place
so
it
is
strategically
on
its
platform
60
as
facebook
did
when
addressing
wise
to
take
positive
steps
before
additional
pressures
concerns
about
ethnic
affinity
marketing
29
and
as
apply
due
to
the
complexity
of
these
issues
it
is
also
wise
google
did
when
developing
its
policy
about
payday
to
proceed
thoughtfully
with
user
research
and
to
engage
lending
ads
40
45
our
research
underscores
the
stakeholders
to
represent
diverse
perspectives
we
discuss
importance
of
such
efforts
since
it
shows
that
traditional
these
in
turn
in
the
next
two
points
methods
of
user
testing
may
not
yield
complete
picture
of
different
groups
perspectives
on
this
computationally
and
design
user
studies
that
accommodate
diverse
socially
complex
issue
community
groups
and
leaders
are
perspectives
and
include
members
of
traditionally
experienced
in
considering
societal-scale
consequences
and
marginalized
populations
in
user
testing
the
workshop
representing
their
constituencies
on
range
of
issues
and
format
supported
and
encouraged
participants
exploration
are
well-positioned
to
contribute
to
such
discussions
and
development
of
diverse
nuanced
and
at
times
conflicting
positions
and
participants
reported
that
it
was
conclusions
and
future
work
empowering
to
take
the
perspective
of
decision-maker
at
one
way
to
make
social
change
is
to
bolster
pragmatic
technology
company
at
the
same
time
our
experience
arguments
for
corporations
to
do
good
by
demonstrating
reflects
both
the
value
and
challenges
of
user
research
on
that
societally
positive
actions
are
also
good
business
complex
computational
topics
complementing
other
work
practice
consider
for
example
how
green
to
gold
our
findings
suggest
that
participants
opinions
on
this
topic
effectively
argued
that
sustainable
business
practices
not
were
highly
contextual
often
varying
in
response
to
only
benefit
the
environment
but
can
yield
significant
situational
factors
specific
details
of
given
scenarios
financial
profit
32
in
this
paper
we
presented
novel
individual
factors
which
appears
to
resonate
with
variation
exploration
of
how
traditionally
marginalized
populations
reported
in
69
86
different
stakeholder
perspectives
as
perceive
algorithmic
fairness
while
our
findings
can
discussed
for
example
in
51
52
and
different
framings
of
inform
range
of
stakeholders
we
highlight
the
insight
that
fairness
for
example
an
emphasis
on
fair
division
as
in
company
handling
of
algorithmic
fairness
interacts
51
52
versus
social
justice
this
contextual
nature
may
significantly
with
user
trust
we
hope
this
insight
may
help
explain
why
research
on
this
topic
yields
results
that
provide
additional
motivation
for
companies
across
the
may
sometimes
appear
inconsistent
for
example
while
technology
sector
to
actively
pursue
algorithmic
fairness
many
of
our
findings
are
broadly
consistent
with
plane
et
future
work
could
fruitfully
explore
these
findings
with
al
objections
to
personalization
based
on
demographic
broader
population
noting
that
plane
et
al
study
offers
characteristics
and
the
expectation
that
technology
evidence
that
at
least
some
of
these
issues
may
resonate
companies
play
role
in
addressing
issues
caused
by
widely
67
we
also
suggest
further
exploring
concrete
external
forces
our
findings
differed
in
other
regards
such
actions
that
companies
can
take
regarding
algorithmic
as
the
fact
that
our
participants
appeared
to
favor
and
trust
fairness
such
as
making
specific
improvements
to
product
human
decision-making
over
algorithmic
decision-making
experiences
to
build
and
maintain
user
trust
finally
we
additional
research
could
yield
further
insights
that
account
suggest
further
research
on
how
stakeholders
across
the
for
such
variation
relatedly
we
caution
that
ecosystem
can
work
collectively
to
leverage
their
different
decontextualized
user
research
on
this
topic
may
yield
perspectives
and
skills
to
pursue
algorithmic
fairness
misleading
results
we
recommend
that
researchers
prepare
and
account
for
the
beliefs
and
knowledge
that
participants
acknowledgments
may
bring
to
the
research
environment
in
order
to
provide
we
thank
the
following
for
their
thoughtful
comments
and
an
inclusive
research
environment
for
all
participants
in
contributions
to
this
work
paul
aoki
ed
chi
charina
some
situations
it
will
also
be
valuable
to
use
ethnographic
choi
mark
chow
rena
coen
sunny
consolvo
jen
approaches
to
explore
participants
underlying
values
and
gennai
lea
kissner
brad
krueger
ali
lange
irene
tang
extrapolate
from
those
values
to
technological
implications
lynette
webb
jill
woelfer
and
the
anonymous
reviewers
see
26
for
additional
discussion
of
the
nature
of
analytic
references
knowledge
that
can
be
gained
in
ethnographic
studies
acm
us
public
policy
council
2017
statement
on
engage
with
community
groups
and
advocates
to
algorithmic
transparency
and
accountability
collaboratively
develop
solutions
as
is
common
with
retrieved
september
16
2017
from
wicked
problems
stakeholders
should
not
work
in
isolation
https://www.acm.org/binaries/content/assets/public-
to
address
the
complex
issues
posed
by
algorithmic
fairness
policy
2017_usacm_statement_algorithms
pdf
72
robust
understanding
of
the
goals
and
the
best
path
paper
656
page
10
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
airbnb
inc
airbnb
nondiscrimination
policy
häkkilä
kate
kuehl
valentina
nisi
nuno
jardim
retrieved
september
14
2017
from
nunes
nina
wenig
dirk
wenig
brent
hecht
and
https://www.airbnb.com/terms/nondiscrimination_polic
johannes
schöning
2017
the
geography
of
pokémon
go
beneficial
and
problematic
effects
on
places
and
mariam
asad
sarah
fox
and
christopher
le
movement
in
proceedings
of
the
2017
chi
dantec
2014
speculative
activist
technologies
conference
on
human
factors
in
computing
systems
proceedings
iconference
2014
chi
17
1179
1192
https://doi.org/10.9776/14074
https://doi.org/10.1145/3025453.3025495
paul
baker
and
amanda
potts
2013
why
do
white
16
anne
corden
and
roy
sainsbury
2006
using
people
have
thin
lips
google
and
the
perpetuation
of
verbatim
quotations
in
reporting
qualitative
social
stereotypes
via
auto-complete
search
forms
critical
research
university
of
york
york
uk
discourse
studies
10
187
204
17
tressie
mcmillan
cottom
2015
credit
scores
life
https://doi.org/10.1080/17405904.2012.744320
chances
and
algorithms
retrieved
september
15
solon
barocas
and
andrew
selbst
2016
big
data
2017
from
https://tressiemc.com/uncategorized/credit-
disparate
impact
california
law
review
104
671
scores-life-chances-and-algorithms
732
18
kate
crawford
2014
the
anxieties
of
big
data
the
david
beer
2009
power
through
the
algorithm
new
inquiry
participatory
web
cultures
and
the
technological
19
kate
crawford
2016
artificial
intelligence
white
unconscious
new
media
society
11
985
1002
guy
problem
the
new
york
times
https://doi.org/10.1177/1461444809336551
20
amit
datta
michael
carl
tschantz
and
anupam
hugh
beyer
and
karen
holtzblatt
1998
contextual
datta
2015
automated
experiments
on
ad
privacy
design
defining
customer-centered
systems
morgan
settings
in
proceedings
on
privacy
enhancing
kaufmann
publishers
inc
san
francisco
ca
usa
technologies
pets
2015
92
112
danah
boyd
and
kate
crawford
2012
critical
https://doi.org/10.1515/popets-2015-0007
questions
for
big
data
information
communication
21
michael
devito
jeremy
birnholtz
and
jeffery
society
15
662
679
hancock
platforms
people
and
perception
using
https://doi.org/10.1080/1369118x.2012.678878
affordances
to
understand
self-presentation
on
social
danah
boyd
karen
levy
and
alice
marwick
2014
media
in
proceedings
of
the
20th
acm
conference
on
the
networked
nature
of
algorithmic
discrimination
computer-supported
cooperative
work
social
in
data
and
discrimination
collected
essays
seeta
computing
cscw
17
740
754
peña
gangadharan
virginia
eubanks
and
solon
22
nicholas
diakopoulos
2015
algorithmic
barocas
eds
open
technology
institute
new
accountability
digital
journalism
398
415
america
foundation
washington
53
57
https://doi.org/10.1080/21670811.2014.976411
10
engin
bozdag
2013
bias
in
algorithmic
filtering
and
23
carl
disalvo
thomas
lodato
laura
fries
beth
personalization
ethics
and
information
technology
15
schechter
and
thomas
barnwell
2011
the
collective
209
227
https://doi.org/10.1007/s10676-013-9321-
articulation
of
issues
as
design
practice
codesign
185
197
11
taina
bucher
2017
the
algorithmic
imaginary
https://doi.org/10.1080/15710882.2011.630475
exploring
the
ordinary
affects
of
facebook
algorithms
24
carl
disalvo
illah
nourbakhsh
david
holstius
ayça
information
communication
society
20
30
44
akin
and
marti
louw
2008
the
neighborhood
12
jenna
burrell
2016
how
the
machine
thinks
networks
project
case
study
of
critical
understanding
opacity
in
machine
learning
algorithms
engagement
and
creative
expression
through
big
data
society
12
participatory
design
in
proceedings
of
the
tenth
https://doi.org/10.1177/2053951715622512
anniversary
conference
on
participatory
design
2008
pdc
08
41
50
13
kathleen
chaykowski
2016
facebook
to
ban
ethnic
affinity
targeting
for
housing
employment
credit
25
carl
disalvo
phoebe
sengers
and
hrönn
related
ads
forbes
brynjarsdóttir
2010
mapping
the
landscape
of
sustainable
hci
in
proceedings
of
the
sigchi
14
danielle
keats
citron
and
frank
pasquale
2014
the
conference
on
human
factors
in
computing
systems
scored
society
due
process
for
automated
chi
10
1975
1984
predictions
washington
law
review
89
https://doi.org/10.1145/1753326.1753625
15
ashley
colley
jacob
thebault-spieker
allen
yilun
26
paul
dourish
2006
implications
for
design
in
lin
donald
degraen
benjamin
fischman
jonna
proceedings
of
the
sigchi
conference
on
human
paper
656
page
11
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
factors
in
computing
systems
chi
06
541
550
39
bryce
goodman
and
seth
flaxman
2016
european
https://doi.org/10.1145/1124772.1124855
union
regulations
on
algorithmic
decision-making
and
27
benjamin
edelman
and
michael
luca
2014
digital
right
to
explanation
in
icml
workshop
on
human
discrimination
the
case
of
airbnb
com
harvard
interpretability
in
machine
learning
whi
2016
business
school
working
paper
14
054
40
david
graff
2016
an
update
to
our
adwords
policy
28
benjamin
edelman
michael
luca
and
daniel
svirsky
on
lending
products
google
public
policy
blog
2017
racial
discrimination
in
the
sharing
economy
retrieved
september
15
2017
from
evidence
from
field
experiment
american
https://www.blog.google/topics/public-policy/an-
economic
journal
applied
economics
22
update-to-our-adwords-policy-on
29
erin
egan
2016
improving
enforcement
and
41
anthony
greenwald
mahzarin
banaji
and
brian
promoting
diversity
updates
to
ethnic
affinity
nosek
2015
statistically
small
effects
of
the
marketing
facebook
newsroom
blog
retrieved
implicit
association
test
can
have
societally
large
september
14
2017
from
effects
journal
of
personality
and
social
psychology
https://newsroom.fb.com/news/2016/11/updates-to-
108
553
561
https://doi.org/10.1037/pspa0000016
ethnic-affinity-marketing
42
maya
gupta
andrew
cotter
jan
pfeifer
konstantin
30
pelle
ehn
1990
work-oriented
design
of
computer
voevodski
kevin
canini
alexander
mangylov
artifacts
erlbaum
associates
inc
hillsdale
nj
wojciech
moczydlowski
and
alexander
van
esbroeck
usa
2016
monotonic
calibrated
interpolated
look-up
tables
journal
of
machine
learning
research
17
31
motahhare
eslami
aimee
rickman
kristen
vaccaro
109
47
amirhossein
aleyasen
andy
vuong
karrie
karahalios
kevin
hamilton
and
christian
sandvig
43
bernard
harcourt
2007
against
prediction
2015
always
assumed
that
wasn
really
that
profiling
policing
and
punishing
in
an
actuarial
age
close
to
her
reasoning
about
invisible
algorithms
university
of
chicago
press
in
news
feeds
in
proceedings
of
the
33rd
annual
44
moritz
hardt
eric
price
and
nathan
srebro
2016
acm
conference
on
human
factors
in
computing
equality
of
opportunity
in
supervised
learning
in
systems
chi
15
153
162
advances
in
neural
information
processing
systems
https://doi.org/10.1145/2702123.2702556
nips
2016
3315
3323
32
daniel
esty
and
andrew
winston
2006
green
to
45
shin
inouye
2016
advocates
applaud
google
ban
gold
how
smart
companies
use
environmental
on
payday
loan
advertisements
the
leadership
strategy
to
innovate
create
value
and
build
conference
on
civil
and
human
rights
retrieved
competitive
advantage
yale
university
press
september
15
2017
from
33
executive
office
of
the
president
2016
big
data
http://civilrights.org/advocates-applaud-googles-ban-
report
on
algorithmic
systems
opportunity
and
civil
on-payday-loan-advertisements
rights
46
lucas
introna
and
helen
nissenbaum
2000
34
federal
trade
commission
2016
big
data
tool
for
shaping
the
web
why
the
politics
of
search
engines
inclusion
or
exclusion
understanding
the
issues
matters
the
information
society
16
169
185
https://doi.org/10.1080/01972240050133634
35
batya
friedman
and
helen
nissenbaum
1996
bias
in
computer
systems
acm
transactions
on
information
47
isaac
johnson
connor
mcmahon
johannes
schöning
systems
14
330
347
and
brent
hecht
2017
the
effect
of
population
and
https://doi.org/10.1145/230538.230561
structural
biases
on
social
media-based
algorithms
case
study
in
geolocation
inference
across
the
36
shaun
gabbidon
2003
racial
profiling
by
store
urban-rural
spectrum
in
proceedings
of
the
2017
clerks
and
security
personnel
in
retail
chi
conference
on
human
factors
in
computing
establishments
an
exploration
of
shopping
while
systems
chi
17
1167
1178
black
journal
of
contemporary
criminal
justice
19
https://doi.org/10.1145/3025453.3026015
345
364
https://doi.org/10.1177/1043986203254531
48
robert
jungk
norbert
müllert
and
institute
for
social
inventions
1987
future
workshops
how
to
create
37
marilyn
geewax
2015
the
tipping
point
most
desirable
futures
institute
for
social
inventions
americans
no
longer
are
middle
class
npr
london
38
tarleton
gillespie
2014
the
relevance
of
algorithms
49
matthew
kay
cynthia
matuszek
and
sean
in
media
technologies
essays
on
communication
munson
2015
unequal
representation
and
gender
materiality
and
society
university
press
stereotypes
in
image
search
results
for
occupations
scholarship
tarleton
gillespie
pablo
boczkowski
and
in
proceedings
of
the
33rd
annual
acm
conference
kirsten
foot
eds
mit
press
paper
656
page
12
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
on
human
factors
in
computing
systems
chi
15
human
factors
in
computing
systems
chi
15
207
3819
3828
https://doi.org/10.1145/2702123.2702520
210
https://doi.org/10.1145/2702123.2702514
50
jonathan
lazar
julio
abascal
simone
barbosa
58
claire
cain
miller
2015
when
algorithms
jeremy
barksdale
batya
friedman
jens
grossklags
discriminate
the
new
york
times
jan
gulliksen
jeff
johnson
tom
mcewan
loïc
59
robert
morgan
and
shelby
hunt
1994
the
martínez-normand
wibke
michalk
janice
tsai
commitment-trust
theory
of
relationship
marketing
gerrit
van
der
veer
hans
von
axelson
ake
walldius
journal
of
marketing
58
20
38
gill
whitney
marco
winckler
volker
wulf
elizabeth
https://doi.org/10.2307/1252308
churchill
lorrie
cranor
janet
davis
alan
hedge
harry
hochheiser
juan
pablo
hourcade
clayton
60
laura
murphy
2016
airbnb
work
to
fight
lewis
lisa
nathan
fabio
paterno
blake
reid
discrimination
and
build
inclusion
report
whitney
quesenbery
ted
selker
and
brian
wentz
submitted
to
airbnb
retrieved
september
15
2017
2016
human
computer
interaction
and
international
from
http://blog.atairbnb.com/wp-
public
policymaking
framework
for
understanding
content
uploads
2016
09
report_airbnbs-work-to
and
taking
future
actions
foundations
and
trends
in
fight-discrimination-and-build-inclusion
pdf
3c10be
human-computer
interaction
69
149
61
carey
nadeau
and
amy
glasmeier
2016
minimum
https://doi.org/10.1561/1100000062
wage
can
an
individual
or
family
live
on
it
51
min
kyung
lee
and
su
baykal
2017
algorithmic
retrieved
september
15
2017
from
mediation
in
group
decisions
fairness
perceptions
of
http://livingwage.mit.edu/articles/15-minimum-wage-
algorithmically
mediated
vs
discussion-based
social
can-an-individual-or-a-family-live-on-it
division
in
proceedings
of
the
20th
acm
conference
62
safiya
umoja
noble
2014
teaching
trayvon
race
on
computer-supported
cooperative
work
social
media
and
the
politics
of
spectacle
the
black
scholar
computing
cscw
17
1035
1048
44
12
29
52
min
kyung
lee
ji
tae
kim
and
leah
lizarondo
https://doi.org/10.5816/blackscholar.44.1.0012
2017
human-centered
approach
to
algorithmic
63
cathy
neil
2016
weapons
of
math
destruction
services
considerations
for
fair
and
motivating
smart
how
big
data
increases
inequality
and
threatens
community
service
management
that
allocates
democracy
crown
new
york
donations
to
non-profit
organizations
in
proceedings
64
jahna
otterbacher
jo
bates
and
paul
clough
2017
of
the
2017
chi
conference
on
human
factors
in
competent
men
and
warm
women
gender
computing
systems
chi
17
3365
3376
stereotypes
and
backlash
in
image
search
results
in
53
richard
lundman
and
robert
kaufman
2003
proceedings
of
the
2017
chi
conference
on
human
driving
while
black
effects
of
race
ethnicity
and
factors
in
computing
systems
chi
17
6620
6631
gender
on
citizen
self-reports
of
traffic
stops
and
https://doi.org/10.1145/3025453.3025727
police
actions
criminology
41
195
220
65
sarah
perez
2016
microsoft
silences
its
new
bot
https://doi.org/10.1111/j.1745-9125.2003.tb00986.x
tay
after
twitter
users
teach
it
racism
techcrunch
54
caitlin
lustig
and
bonnie
nardi
2015
algorithmic
66
sarah
pink
2014
doing
visual
ethnography
sage
authority
the
case
of
bitcoin
in
48th
hawaii
publications
international
conference
on
system
sciences
hicss
2015
743
752
67
angelisa
plane
elissa
redmiles
michelle
mazurek
https://doi.org/10.1109/hicss.2015.95
and
michael
tschantz
2017
exploring
user
perceptions
of
discrimination
in
online
targeted
55
caitlin
lustig
katie
pine
bonnie
nardi
lilly
irani
advertising
in
proceedings
of
the
2017
usenix
min
kyung
lee
dawn
nafus
and
christian
sandvig
security
symposium
2016
algorithmic
authority
the
ethics
politics
and
economics
of
algorithms
that
interpret
decide
and
68
bernadette
proctor
jessica
semega
and
melissa
manage
in
proceedings
of
the
2016
chi
conference
kollar
2016
income
and
poverty
in
the
united
extended
abstracts
on
human
factors
in
computing
states
2015
the
united
states
census
bureau
systems
chi
ea
16
1057
1062
retrieved
september
15
2017
from
https://doi.org/10.1145/2851581.2886426
https://www.census.gov/library/publications/2016/dem
p60-256
html
56
peggy
mcintosh
1990
white
privilege
unpacking
the
invisible
knapsack
independent
school
49
31
69
emilee
rader
and
rebecca
gray
2015
understanding
user
beliefs
about
algorithmic
curation
in
the
57
amanda
menking
and
ingrid
erickson
2015
the
facebook
news
feed
in
proceedings
of
the
33rd
heart
work
of
wikipedia
gendered
emotional
labor
annual
acm
conference
on
human
factors
in
in
the
world
largest
online
encyclopedia
in
computing
systems
chi
15
173
182
proceedings
of
the
33rd
annual
acm
conference
on
https://doi.org/10.1145/2702123.2702174
paper
656
page
13
chi
2018
paper
chi
2018
april
21
26
2018
montréal
qc
canada
70
matt
ratto
2011
critical
making
conceptual
and
79
clay
shirky
2011
speculative
post
on
the
idea
of
material
studies
in
technology
and
social
life
the
algorithmic
authority
retrieved
september
15
2017
information
society
27
252
260
from
http://www.shirky.com/weblog/2009/11/a-
https://doi.org/10.1080/01972243.2011.583819
speculative-post-on-the-idea-of-algorithmic-authority
71
noopur
raval
and
paul
dourish
2016
standing
out
80
kiley
sobel
katie
leary
and
julie
kientz
2015
from
the
crowd
emotional
labor
body
labor
and
maximizing
children
opportunities
with
inclusive
temporal
labor
in
ridesharing
in
proceedings
of
the
play
considerations
for
interactive
technology
19th
acm
conference
on
computer-supported
design
in
proceedings
of
the
14th
international
cooperative
work
social
computing
cscw
16
conference
on
interaction
design
and
children
idc
97
107
https://doi.org/10.1145/2818048.2820026
15
39
48
https://doi.org/10.1145/2771839.2771844
72
horst
rittel
and
melvin
webber
1973
81
derald
wing
sue
2010
microaggressions
in
everyday
dilemmas
in
general
theory
of
planning
policy
life
race
gender
and
sexual
orientation
wiley
sciences
155
169
82
astra
taylor
and
jathan
sadowski
2015
how
https://doi.org/10.1007/bf01405730
companies
turn
your
facebook
activity
into
credit
73
rosemary
rodriguez
2015
discovery
the
good
score
the
nation
wife
83
david
thomas
2006
general
inductive
74
daniela
rosner
saba
kawas
wenqi
li
nicole
approach
for
analyzing
qualitative
evaluation
data
tilly
and
yi-chen
sung
2016
out
of
time
out
of
american
journal
of
evaluation
27
237
246
place
reflections
on
design
workshops
as
research
https://doi.org/10.1177/1098214005283748
method
in
proceedings
of
the
19th
acm
conference
84
vanessa
thomas
christian
remy
mike
hazas
and
on
computer-supported
cooperative
work
social
oliver
bates
2017
hci
and
environmental
public
computing
cscw
16
1131
1141
policy
opportunities
for
engagement
in
proceedings
https://doi.org/10.1145/2818048.2820021
of
the
2017
chi
conference
on
human
factors
in
75
elizabeth
sanders
and
pieter
jan
stappers
2008
computing
systems
chi
17
6986
6992
co-creation
and
the
new
landscapes
of
design
https://doi.org/10.1145/3025453.3025579
codesign
18
85
blase
ur
pedro
giovanni
leon
lorrie
faith
cranor
https://doi.org/10.1080/15710880701875068
richard
shay
and
yang
wang
2012
smart
useful
76
christian
sandvig
kevin
hamilton
karrie
karahalios
scary
creepy
perceptions
of
online
behavioral
and
cedric
langbort
2015
can
an
algorithm
be
advertising
in
proceedings
of
the
eighth
symposium
unethical
in
65th
annual
meeting
of
the
international
on
usable
privacy
and
security
soups
12
communication
association
15
https://doi.org/10.1145/2335356.2335362
77
christian
sandvig
kevin
hamilton
karrie
karahalios
86
jeff
warshaw
nina
taft
and
allison
woodruff
2016
and
cedric
langbort
auditing
algorithms
research
intuitions
analytics
and
killing
ants
inference
literacy
methods
for
detecting
discrimination
on
internet
of
high
school-educated
adults
in
the
us
in
platforms
data
and
discrimination
converting
proceedings
of
the
twelfth
symposium
on
usable
critical
concerns
into
productive
inquiry
may
2014
privacy
and
security
soups
16
78
sassafras
tech
collective
2016
icebreaker
in
87
brian
wynne
1991
knowledges
in
context
science
exploring
social
justice
design
and
hci
workshop
technology
human
values
16
111
121
at
chi
2016
paper
656
page
14