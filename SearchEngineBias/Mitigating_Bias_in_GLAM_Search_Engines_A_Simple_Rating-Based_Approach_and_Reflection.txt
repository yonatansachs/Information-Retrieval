Mitigating Bias in GLAM Search Engines: A Simple Rating-Based
Approach and Reflection
Xinran Tian

Bernardo Pereira Nunes

Xinran.Tian@anu.edu.au
Australian National University
Canberra, ACT, Australia

Bernardo.Nunes@anu.edu.au
Australian National University
Canberra, ACT, Australia

Katrina Grant

Marco Antonio Casanova

Katrina.Grant@anu.edu.au
Australian National University
Canberra, ACT, Australia

Pontifícia Universidade Católica do Rio de Janeiro
Rio de Janeiro, Brazil
casanova@inf.puc-rio.br

ABSTRACT
Galleries, Libraries, Archives and Museums (GLAM) institutions are
increasingly opening up their digitised collections and associated
data for engagement online via their own websites/search engines
and for reuse by third parties. Although bias in GLAM collections
is inherent, bias in the search engines themselves can be rated. This
work proposes a bias rating method to reflect on the use of search
engines in the GLAM sector along with strategies to mitigate bias.
The application of this to an existing large art collection shows
the applicability of the proposed method and highlights a range of
existing issues.

CCS CONCEPTS
• Information systems → Information retrieval; Evaluation
of retrieval results; • Applied computing → Arts and humanities; Digital libraries and archives.

KEYWORDS

and accountability for biases present in digitised cultural collections
[9].
Much current work that aims to address bias in GLAM datasets
tends to look at the need for diversification of datasets used (adding
more diverse works to collections) or for changes to the cataloguing
process, including the enrichment of metadata to include more
diverse classifications and to reveal contested histories. Both these
approaches are worthwhile but require considerable resources and
time to undertake.
Our proposal is a simple method of addressing existing issues
within the search engine design itself that can offer a step towards
addressing and revealing bias in search results for the GLAM sector. We do not aim to correct bias — an impossible task for the
GLAM sector — but to develop a straightforward rating system
where search engine bias can be revealed and addressed, by both
developers and end-users. This paper serves as a reflection on the
use of search engines in the GLAM sector and offers strategies to
address the inherent bias of GLAM collections.

bias, search engines, GLAM, reflections

2

ACM Reference Format:
Xinran Tian, Bernardo Pereira Nunes, Katrina Grant, and Marco Antonio
Casanova. 2023. Mitigating Bias in GLAM Search Engines: A Simple RatingBased Approach and Reflection. In 34th ACM Conference on Hypertext and
Social Media (HT ’23), September 4–8, 2023, Rome, Italy. ACM, New York, NY,
USA, 5 pages. https://doi.org/10.1145/3603163.3609043

The problem of bias in datasets and the effect it has on search and
other computational applications is well-recognised. Gender [2]
and racial bias [1] have been extensively studied by researchers in
the AI field. The recognition of bias in GLAM collections, however,
requires a level of domain expertise to identify and, even when it is
identified, it is still challenging to address for historical and cultural
reasons. Biases are further introduced in the process of analogue
cataloguing, digitisation and the use of collection management
systems [5, 10]. Bias in metadata and categorisation can be created
by cataloguers and curators1,2 . These biases may include (but are
not limited to): vastly different levels of metadata for different types
of objects, presentation of object records as objective fact, which
obscures debates over origins and meaning, lack of data for certain
categories (for example, the gender of artists is rarely recorded),
and the use of diverse and idiosyncratic categorisation of objects
within individual collections [8, 14].
A recent study of the bias in the Google Arts and Culture platform
(GA&C)[9] pointed out that although GA&C includes art from
almost every country/region in the UN membership list, its data set
is still biased. Museum objects from the United States, the United

1 INTRODUCTION
GLAM collections exhibit bias due to a range of collection practices
over decades or even centuries that have privileged the collection
of art and objects from certain eras, regions, genders and socioeconomic classes [12]. This bias is impossible to eliminate from
any search of these collections. However, increasingly scholars and
experts from the GLAM sector are calling for more transparency
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
HT ’23, September 4–8, 2023, Rome, Italy
© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0232-7/23/09. . . $15.00
https://doi.org/10.1145/3603163.3609043

RELATED WORK

1 https://bpr.berkeley.edu/2017/05/16/the-conflict-of-curation/
2 https://bookofbadchoices.com/shorts/curation-means-bias/

HT ’23, September 4–8, 2023, Rome, Italy

Kingdom, the Netherlands, Italy and South Korea account for 93.4%
of the content posted on the GA&C platform.
Srivastava and Rossi (S&R, hence forth) proposed a simple approach to rate bias in the context of text translation using a two-step
test [16]. Their approach acts as an independent third-party rating
agency controlling the levels of bias in the data input to measure
and rate the system bias in the output.
This paper extends and applies the rating system proposed by
S&R to search engines used in the GLAM sector, a more complex
problem. Historical collections of art and material culture contain
different kinds of biases that are often different than those typically
expected in normal Web search or translation services. GLAM collections are also responsible for perpetuating bias in our society
[6]. There is bias in the collection based on decisions about what to
add to the collection and in the way collections are displayed (if an
art museum chooses to only collect or display works of art by men
they perpetuate the bias that men are more creative and important
in the art world) [3].

3

OUR SEARCH ENGINE BIAS RATING
APPROACH

Our approach is an extended 2-step test adapted from the Srivastava
and Rossi (S&R) bias rating system to identify bias in the context
of text translation [16]. Fig. 1 shows our 2-step approach and each
step is explained below.
Step 1: Unbiased Input. The first part of the first step compares
the unbiased input with the system’s output. Note that the “unbiased input" for search engines is a known subset of the entire
dataset used by the search engine that has no bias with regards
to at least one aspect (e.g. gender, region or religion). The bias in
the search engine is examined based on the content and interface
independently. For content, if the output is deemed to be biased, the
system is rated as a “Biased System - Content Bias (BS-CB)". That
means the content of the results returned by the search engine are
biased according to the aspects under consideration. If the content
is deemed not biased, then Step 2.1 is triggered. The same process
is carried out for the interface. We will focus on the way search
engines present the results to the end-users in the interface analyser instead of the content of all returned results. If the interface
presents bias, the system is rated as “Biased System - Interface Bias
(BS-IB). Otherwise, Step 2.2 is triggered.
Step 2: Biased Input. As with the S&R approach, we use biased
data as input and rate the system based on its ability to deal with the
biased input. If the output is deemed to be biased in Step 2.1, then
the system has no ability to recover from a biased input and is rated
as “Data-Sensitive Biased System - Content Biased (DSBS-CB)". If
it is deemed to be biased in Step 2.2, it is rated as “Data-Sensitive
Biased System - Interface Biased (DSBS-IB)". If bias is detected
neither in Step 2.1 nor in Step 2.2, the original rating “Content
Unbiased Compensated System (CUCS)" or “Interface Unbiased
Compensated System (IUCS)" is assigned to the results generated
by the system demonstrating its ability to remove/compensate content or interface bias in specific subsets generated by the search
engine system.

Tian et al.

To deal with the complexity of search engine systems, the proposed approach consists of modules similar to those of the S&R
approach and a few additional ones. Each module is described below.
The Data Generation module is responsible for generating biased and unbiased data as input based on one or more specific
data aspect (i.e., attribute), as explained in Step 1 of our proposed
approach. Given an attribute 𝑎𝑖 ∈ {𝑎 0, ..., 𝑎𝑛 } and a set of keywords
(query) 𝑞, this module will generate a domain 𝐷𝑖 = {𝑑 0, ...𝑑𝑚 } for
the attribute 𝑎𝑖 based on the items related to 𝑞 from the entire
dataset used by the search engine. Then, different proportion distributions 𝑝 are assigned to 𝐷 to control the degree of bias of the
Í
input, 𝐷𝑖 = {𝑑 0 : 𝑝𝑑0 , ...𝑑𝑚 : 𝑝𝑑𝑚 }, with 𝑚
𝑝 = 1. Finally, for
𝑘=0 𝑑𝑘
each 𝑎𝑖 , this module generates a balanced or unbalanced subset on
the domain 𝐷𝑖 based on its proportion distribution.
The Experiment Setting module decides the clicking model
[4, 17] and gives a threshold 𝜏. The clicking model is used to apply
weights to the results displayed at different positions in a search
engine interface, and tests for interface bias. 𝜏 is used to set the
number of results to be collected and analysed in the interface bias
analyser. Only the top 𝑁𝜏 results returned by the search engine will
be considered in the interface bias analyser.
The Experiment Executor module automatically runs predetermined queries on the search engine and collects its top 𝑁𝜏 output
results. All of the output results are sent to the content bias analysis module, and the collected results are sent to the interface bias
analysis module.
The Content Bias Measurement module computes the data
content imbalance for one or more given attributes in the data
input. We consider an attribute 𝑎𝑖 with its corresponding domain
𝐷𝑖 = {𝑑 0 : 𝑝 (𝑑 0 ), ...𝑑𝑚 : (𝑝𝑚 )} in the data generation module. Here,
the data imbalance is computed by comparing the distribution
Í
of different elements in 𝐷𝑖 , 𝑃 (𝑑𝑖 ) = 𝑁 (𝑅𝑑𝑖 )/ 𝑚
𝑗=0 𝑁 (𝑅𝑑 𝑗 ), where
𝑁 (𝑅𝑑𝑥 ) is the number of items related to an element 𝑑𝑥 in output results. A new distribution set is computed as 𝐶𝑖 = {𝑑 0 : 𝑃 (𝑑 0 ), ...𝑑𝑚 :
𝑃 (𝑑𝑚 )}, where 𝑑𝑖 is an element from attribute 𝑎𝑖 , and 𝑃 (𝑑𝑖 ) indicates the distribution of 𝑑𝑖 in all output results. By comparing
the 𝐶𝑖 with 𝐷𝑖 , we can compute the bias in the output result content.
The Interface Bias Measurement module calculates the weight
for different elements of a given attribute among collected results
to check for interface bias. For example, consider an attribute 𝑎𝑖
with domain 𝐷𝑖 = {𝑑 0, ...𝑑𝑚 }, the interface weight for each element
Í
in 𝐷𝑖 is computed as 𝑊 (𝑑𝑖 ) = (𝑤𝑒𝑖𝑔ℎ𝑡 (𝑅𝑑𝑖 )), where 𝑅𝑑𝑖 are all
results related to 𝑑𝑖 in top 𝑁𝜏 results, and 𝑤𝑒𝑖𝑔ℎ𝑡 () is the function
used to compute the weights for the results displayed at different
positions — this is the bases of our clicking model. The output of
this module is 𝐼𝑖 = {𝑑 0 : 𝑊 (𝑑 0 ), ...𝑑𝑚 : 𝑊 (𝑑𝑚 )}. We then compare
the weight of different elements to compute the interface imbalance.
The rating and analysing module is finally responsible for aggregating the results and assigning a final bias rating to the search

Mitigating Bias in GLAM Search Engines: A Simple Rating-Based Approach and Reflection

HT ’23, September 4–8, 2023, Rome, Italy

Figure 1: 2-step bias rating system for search engines.
engine system; the final bias rating includes two parts, one for the
content and one for the interface.

4 EXPERIMENTAL SETUP
4.1 Search Engine
To validate our approach, we used the Jina Search Engine3 , an
open-source deep learning-powered search framework for building
multimodal search systems. As Jina is open-source, it can be used
as testbed to measure the degree of bias to specific data sets under
different algorithms and language models. For our experiments, we
used three different language models: Distilbert [15], LaBSE [7] and
Roberta [11], as required by Jina.

4.2

Dataset

We collected approximately 40,000 resource items from the Metropolitan Museum of Art (MET), Google Images and Baidu Images.
The images from Google and Baidu were used to balance or imbalance the existing data in the MET museum collection.
We manually built three different subsets resources about Buddha, religious building, and deity/god/goddess respectively from
the MET collection, Google, and Baidu, test with five different
queries, and use them as input to our system to rate bias on three
attributes. These subsets were created to simulate a real search
engine used in a GLAM institution. Attribute 1 ‘Origin place of
Buddha collection (Region)’ has domain {‘China’, ‘India’, ‘Sri Lanka’,
‘Thailand’} and is tested with query ‘Buddha’; Attribute 2 ‘Religious
building (Religion)’ has domain {‘Church’, ‘Mosque’, ‘Temple’, ‘Synagogue’} and is tested with query ‘Place for worship’; Attribute 3
‘Name of deity/god/goddess’ has domain {‘Jesus’, ‘Nüwa’, ‘Temple’,
‘Synagogue’} and is tested with query ‘God’, ‘Deity’ and ‘Goddess’.

3 https://jina.ai

Unbiased input was generated based on distribution 𝑝 of each element in domain (see Table 1). To generate bias, the proportions of
the attribute domains must be asymmetric.

4.3

Clicking model

We used a simple clicking model approach based on the reading
habit of most western countries (top-down and left-to-right reading). The weights to each result were assigned following the formula
𝑤𝑒𝑖𝑔ℎ𝑡 (𝑅𝑒𝑠𝑢𝑙𝑡𝑖 𝑗 ) = 𝑁𝜏 − (𝑖 + 𝑗), where 𝑁𝜏 is the number of results
analysed returned by the search engine, and, 𝑖 and 𝑗 are the position
of an item presented in a grid/matrix. In our experiment, 𝑁𝜏 = 40.
Í
The weights are then normalised by 𝑊 (𝑅𝑑𝑖 )/ 𝑚
𝑗=0 𝑊 (𝑅𝑑 𝑗 ), where
𝑅𝑑𝑥 is defined in the content bias measurement module.

5

RESULTS

Table 2 presents the results for the three attributes, five queries,
and three models described in Section 4. We omit the interface bias
for some cases as the content bias is significant (see query 3 and
query 4).
Table 3 presents rating results based on our experiments. For the
first attribute, the bias rating assigned is “BS-IB" for the three models
indicating that, with unbiased input (𝑝 in Table 2 is equivalent to
𝑝 in Table 1), the system generates an unbiased content output,
but fails in displaying an unbiased list of items with respect to the
attribute (imbalanced interface weight). After inspecting the subset
used and the metadata descriptions of the resource items, we found
that the search engine uses the inclusion date as a sort criterion
leading to bias in the interface as resource items were inserted by
location batches.
As for the second attribute, the bias rating assigned is “BS-CB"
indicating that the content generated by unbiased input is biased.
The reason is that the output contains imbalanced results for the
different places of worship, in this case, “Synagogue" is less represented than “Church" (see 𝑝 distribution in Table 2). Note that, if a

HT ’23, September 4–8, 2023, Rome, Italy

Tian et al.

Table 1: Unbiased attribute domain distribution for each attribute in the test set.

Attribute 1
Origin place of Buddha collection (Region)
domain
p
China
0.25
India
0.25
Sri Lanka
0.25
Thailand
0.25

Attribute 2
Religious building (Religion)
domain
p
Church
0.25
Mosque
0.25
Temple
0.25
Synagogue
0.25

search engine is using a language model to retrieve resource items,
it must consider the bias added via the language model. For the last
experiment, we used three different queries with similar meanings
– “God", “Goddess", and “Deity" – on one subset.
The results obtained are interesting and worth further investigation, especially in the context of the GLAM sector, as strong biases
are identified. According to the results, when using the queries
‘God’ and ‘Deity’, the search engine shows a significant content
bias and, therefore, it is rated as “BS-CB". For all three models, the
contextual relatedness between ‘God’ and ‘Jesus’ is greater than between ‘God’ and ‘Nüwa’, ‘God’ and ‘Ptah’, and ‘God’ and ‘Brahma’.
Here, the language model plays a key role in the bias added to the
content; for instance, when using “Deity" as reference, the Distilbert
model leans to ‘Brahma’ and LaBSE model to ‘Jesus’.

6

Attribute 3
Name of deity/god/goddess (Religion)
domain
p
Jesus
0.25
Nüwa
0.25
Ptah
0.25
Brahma
0.25

while the small material culture objects may disappear from the
public’s view in a circle of continuous superimposed bias.
The solution to this requires the attention and expertise of developers and domain experts. By rating and analysing the bias in
search engines using the (similar) proposed system, domain experts
can generate feedback, which can be used as a guide for improving
both future development of search engines and metadata entry
by curators. The rating system could also be used as a guide for
end-users of these collection search engines to help them understand where results are demonstrating a level of bias and help them
make more informed decisions about their use of results from these
searches.
Search engines are an important tool for curators, researchers
and the general public. We hope this work can help open discussions
and reassess existing search engines used in the GLAM sector.

CONCLUSION AND REFLECTION

This work adapted and extended the method proposed by S&R,
which was limited to text translation services, to the more complex
context of search engines. The proposed method suggests the potential to create unbiased and biased input sets that can be used
to rate the ability of the search engine to deal with or compensate
for bias. For the experiments we used Jina, an open-search engine
framework, and a GLAM dataset extracted from the MET museum.
This method delivered relevant results that show how a search
engine could be designed to minimise bias in both content and
interface.
There needs to be support for a greater understanding from both
technical and domain experts about the factors that effect search
and discovery, especially as relates to bias. Developers need to to be
aware of diverse factors affecting metadata and the dataset which
will be used by the search engine. In the GLAM sector this means
they need to communicate and cooperate with domain experts,
which may include curators, registrars, collection managers and
researchers. This communication can be challenging [13] and the
proposed bias rating system presented here can support this communication between domain experts and search engine developers.
The results of this research have demonstrated the applicability
of the proposed method, but beyond that, it opens up several lines of
research to explore bias in the GLAM collections. This could address
issues, such as those noted above, where many factors impact the
digital metadata of the object. Richer metadata for well known
works of art may make it easier for the object to be displayed
to users when searching. This means that the objects that have
been more popular, such as the paintings of famous male European
painters, will gain more attention as they are exhibited more often,

REFERENCES
[1] Arifah Addison, Christoph Bartneck, and Kumar Yogeeswaran. 2019. Robots
Can Be More Than Black And White: Examining Racial Bias Towards Robots. In
Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu,
HI, USA) (AIES ’19). ACM, New York, NY, USA, 493–498. https://doi.org/10.1145/
3306618.3314272
[2] Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and
Evan Freitag. 2020. Quantifying Gender Bias in Different Corpora. In Companion
Proceedings of the Web Conference 2020 (Taipei, Taiwan) (WWW ’20). ACM, New
York, NY, USA, 752–759. https://doi.org/10.1145/3366424.3383559
[3] Pierre Bourdieu, Dominique. Schnapper, and Alain. Darbel. 1991. The love of art :
European art museums and their public / Pierre Bourdieu and Alain Darbel with
Dominique Schnapper ; translated by Caroline Beattie and Nick Merriman. Polity
Press Cambridge, UK. viii, 182 p. ; pages.
[4] Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. 2015. Click Models for
Web Search. Morgan & Claypool.
[5] Kelly Davis. 2019. Old metadata in a new world: Standardizing the Getty Provenance Index for linked data. Art Libraries Journal 44, 4 (Oct. 2019), 162–166.
https://doi.org/10.1017/alj.2019.24
[6] Carol Duncan. 2012. Art museums and the ritual of citizenship. In Interpreting
Objects and Collections, Susan Pearce (Ed.). Routledge, 291–298. https://doi.org/
10.4324/9780203428276
[7] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
2020. Language-agnostic BERT Sentence Embedding. arXiv:2007.01852 [cs.CL]
[8] Kathryn M. Hunter. 2017. Silence in Noisy Archives: Reflections on Judith Allen’s
‘Evidence and Silence – Feminism and the Limits of History’ (1986) in the Era of
Mass Digitisation. Australian Feminist Studies 32, 91-92 (April 2017), 202–212.
https://doi.org/10.1080/08164649.2017.1357009
[9] Inna Kizhner, Melissa Terras, Maxim Rumyantsev, Valentina Khokhlova, Elisaveta
Demeshkova, Ivan Rudov, and Julia Afanasieva. 2020. Digital cultural colonialism:
measuring bias in aggregated digitized content held in Google Arts & Culture.
Digital Scholarship in the Humanities (2020).
[10] Shirley Lim and Chern Li Liew. 2011. Metadata quality and interoperability of
GLAM digital images. Aslib Proceedings 63, 5 (01 Jan 2011), 484–498. https:
//doi.org/10.1108/00012531111164978
[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[12] Sharon Macdonald. 2011. A Companion to Museum Studies. Wiley, New York.

Mitigating Bias in GLAM Search Engines: A Simple Rating-Based Approach and Reflection

HT ’23, September 4–8, 2023, Rome, Italy

Table 2: Preliminary Results.

Attribute 1: Origin place of Buddha collection (Region)
Query

Model

Domain

China

India

Sri Lanka

Thailand

1. Buddha

Distilbert / LaBSE / Roberta

𝑝 distribution
Interface weight 𝑊

0.25
0.188

0.25
0.299

0.25
0.311

0.25
0.270

Attribute 2: Place for worship (Religion)
Query

Model

Domain

Church

Mosque

Temple

Synagogue

2. Place for worship

Distilbert

𝑝 distribution
Interface weight 𝑊
𝑝 distribution
Interface weight 𝑊
𝑝 distribution
Interface weight 𝑊

0.36
0.392
0.28
0.348
0.33
0.490

0.27
0.300
0.28
0.276
0.25
0.269

0.27
0.235
0.28
0.237
0.25
0.127

0.10
0.073
0.16
0.138
0.16
0.112

LaBSE
Roberta

Attribute 3: Name of deity/god/goddess (Religion)
Query

Model

Domain

Jesus

Nüwa

Ptah

Brahma

3. God

Distilbert / LaBSE / Roberta

𝑝 distribution

1.0

0

0

0

4. Deity

Distilbert
LaBSE
Roberta

𝑝 distribution
𝑝 distribution
𝑝 distribution

0
1.0
0

0
0
0

0
0
0

1.0
0
0

5. Goddess

Distilbert

𝑃 distribution
Interface weight 𝑊
𝑝 distribution
𝑝 distribution

0
0
0.5
0

0.33
0.428
0
0

0.33
0.350
0
0

0.33
0.222
0.5
1.0

LaBSE
Roberta

Table 3: Rating.

Attributes

distilbert-basenli-stsb-mean-tokens

LaBSE

roberta-largenli-stsb-mean-tokens

Origin place of Buddha collection (Region)

BS-IB

BS-IB

BS-IB

Place for worship (Religion)

BS-CB

BS-CB

BS-CB

Name of deity/god/goddess (Religion)

BS-CB

BS-CB

BS-CB

[13] Sean Minney. 2021. Digital Trasnformation in the Asutralian GLAM sector:
Staffing differences. https://metodhology.anu.edu.au/index.php/2021/01/14/
digital-transformation-in-the-australian-glam-sector-staffing-differences/
[14] Gaby Porter. 1990. Gender bias: Representations of work in history museums.
Continuum 3, 1 (Jan. 1990), 70–83. https://doi.org/10.1080/10304319009388150
[15] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
arXiv:1910.01108 [cs.CL]

[16] Biplav Srivastava and Francesca Rossi. 2018. Towards Composable Bias Rating
of AI Services. In Proceedings of the AAAI Conf. on AI, Ethics, and Society (New
Orleans, LA, USA). ACM, 284–289.
[17] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016.
Learning to Rank with Selection Bias in Personal Search. In Proc of the 39th Int’l
ACM SIGIR Conf. on Research and Development in Information Retrieval (Pisa,
Italy) (SIGIR ’16). ACM, 115–124.

