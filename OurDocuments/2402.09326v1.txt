Stability and Multigroup Fairness in Ranking with
Uncertain Predictions
Aleksandra Korolova∗
Princeton University
korolova@princeton.edu

arXiv:2402.09326v1 [cs.LG] 14 Feb 2024

Siddartha Devic
University of Southern California
devic@usc.edu
David Kempe∗
University of Southern California
david.m.kempe@gmail.com

Vatsal Sharan∗
University of Southern California
vsharan@usc.edu

Abstract
Rankings are ubiquitous across many applications, from search engines to hiring committees. In
practice, many rankings are derived from the output of predictors. However, when predictors trained for
classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented
in the derived rankings. Our work considers ranking functions: maps from individual predictions for a
classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to
perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an
important requirement for its own sake, but — as we show — it composes harmoniously with individual
fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside
from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions
of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve multigroup fairness
through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates
that UA rankings naturally interpolate between group and individual level fairness guarantees, while
simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.

1

Introduction

Rankings underpin many modern systems: companies rank job applications (Geyik et al., 2019; TurboHire,
2023), ad marketplaces rank ads to serve to a user (Google, 2023), and social media platforms and feeds
rank content (Meta, 2023). Rankings are also used to partially automate decision making in settings with
limited resources or attention span (such as job candidate interview selections or ad delivery). Rankings
are often derived from predictions generated by machine learning models designed and deployed on relevant
classification tasks. For example, a job advertisement platform may use a model which predicts an individual’s
relevance score for each job they apply to; say, on a scale from 1 to 3 (corresponding to irrelevant, suitable,
or extremely relevant); this score is then factored into the platform’s ranking of job applicants shown to a
company recruiter with a limited time budget.
In practice, machine learning models often predict distributions over classes instead of a single class. This
is because predictions correspond to a belief about what the future may possibly hold, but not a certainty
about what the future will look like. A plethora of recent research in model calibration (Guo et al., 2017;
Gupta and Ramdas, 2022; Minderer et al., 2021), conformal prediction (Bastani et al., 2022; Jung et al.,
2023), and uncertainty quantification (Angelopoulos and Bates, 2021) has tackled the issue of ensuring that
* Equal contribution; the order of these authors was randomized with https://www.random.org/.

1

the uncertainty estimates output by a model are meaningful, rather than artifacts of any particular training
regime.
With uncertainty inherent in predictions, we argue that it is essential to revisit the question of how to
meaningfully convert predictions (made in the form of distributions over classes) into rankings. Without any
uncertainty — for example, if one had access to an oracle for the future — it would usually be clear what a
ranking should look like: meritocracy would suggest that one always places the more suitable candidates
higher in the ranking. However, when given only predictions about suitability/merit with intrinsic uncertainty,
the approach for generating a meaningful ranking is less clear. After all, one must choose a ranking over
candidates, e.g., in order to make an interview decision, before witnessing the exact suitability of each
candidate, which is generally only observable after an individual works in the job for months or even years.
Since an uncertain prediction can be considered a prior (and typically imperfect) belief on the qualifications
or performance of any given individual, the fundamental task of designing a meaningful ranking algorithm
utilizing these predictions must be reexamined.
For a meaningful derivation of rankings from predictors, we consider the following two properties to be
essential requirements:
Anonymity. All individuals must be treated symmetrically a priori, i.e., if the predictions are permuted,
then the ranking is permuted according to the same permutation.
Stability. If the predictor’s distribution over classes changes only slightly (in Total Variation distance),
then the corresponding induced ranking should only change slightly.
The reason for requiring anonymity is self-evident: it rules out discrimination on the basis of the identity
of individuals. Stability is more nuanced: it articulates a desire to have rankings which are agnostic to small
amounts of noise in the predictions for each individual. In deployed applications, a small amount of variation
injected by a seeded training/test data set split or a randomized training procedure can introduce noise at
the level of individual predictions (Ganesh et al., 2023). Furthermore, there will also always be at least some
additional noise due to incomplete data entries, mistaken inputs, etc. (Nettleton et al., 2010). Rankings
should be generally agnostic to these sources of noise: if minute noise in predictions can induce large changes
in the derived ranking, the ranking cannot be very meaningful or fair to begin with. Stability can therefore
be interpreted as a way to combat micro-arbitrariness of rankings induced by learned predictors (Cooper
et al., 2024). For stability to be meaningful, we will need to shift our focus to distributions over rankings:
utilizing randomness to deal with uncertainty will be key in achieving stability.
Anonymity can be construed as a fairness notion, but it is a very minimal one. Fairness in a stronger
sense has been the focus of much recent work, both in the context of ranking (see, e.g., Singh and Joachims
(2018, 2019)) and in the context of classifiers/predictors (see, e.g., Awasthi et al. (2020); Caton and Haas
(2020); Dwork et al. (2012); Hardt et al. (2016)). As ML-based predictors are often used in order to ultimately
produce rankings (Wang and Chen, 2012), it is a natural desideratum that the ranking function preserve
fairness guarantees of the underlying predictor: this ensures that no additional unfairness is introduced
in post-processing the classifier’s output. As we will see, not all ranking functions satisfy such fairness
composition properties.

1.1

Our Contributions

We focus on scenarios in which individuals, scored by a predictor, must be presented to a decision maker in a
linear order or ranking. We assume that the predictions take the form of distributions over classes, modeling
inherent uncertainty in the underlying ground truth or data. In Section 2, we define a ranking function as a
map from such probabilistic predictions to a distribution over rankings. Figure 1 illustrates this setting with
an example.
Our first (and very immediate — see Section 3.1) result is that stability naturally composes with individual
fairness (Dwork et al., 2012): if the predictor is individually fair and the ranking function is stable, then the

2

Coding Work History
ability
3.0 excellent
applicable
3.5
poor
not applicable
4.0 excellent
applicable

Rank 1

Student GPA

(a) Observations

Rank 2

Prob.

ML Algorithm

irrelevant

suitable

extremely relevant

Candidate relevance classes

(b) Uncertain predictions

Ranking
Function
(this work)

Rank 3
Prob.

0.6

0.3

0.07

0.03

(c) Distribution over rankings

Figure 1: An overview of our setting, using students being ranked by an employer for potential interviews.
Observations (a), given by the students’ resumés and coding abilities, are fed into a machine learning algorithm
which produces distributions (b) over the relevance classes {irrelevant, suitable, extremely relevant} for each
candidate. Then, a ranking function takes as input these uncertain predictions to produce a distribution (c)
over rankings of the three candidates. Although it may appear that x3 is the most qualified or relevant, due
to inherent uncertainty in observations, a ranking function may place x1 or x2 at rank one with non-zero
probability (c).
composition satisfies a natural generalization of individual fairness to rankings. This result further confirms
that stability is a desirable property for a ranking function.
In light of the desirability of stability, we next investigate which ranking functions are stable. Deterministic
ranking functions are natural and popular; unfortunately, we show (Section 3.2) that the only stable
deterministic ranking functions are constant, i.e., trivial functions that output the same ranking regardless
of the predictions. Further, deterministic ranking functions cannot be anonymous. Thus, one must choose
between stability/anonymity and determinism, providing significant evidence in favor of randomization.
With randomization, stability and anonymity both become achievable: we show (Section 3.3) that a natural
adaptation of the Uncertainty Aware (UA) ranking functions of Devic et al. (2023); Singh et al. (2021) to the
case of multiclass predictions of the classifier is indeed anonymous and stable.
We then investigate the fairness guarantees of UA ranking in more depth, and prove (Section 4) our main
result: UA ranking naturally preserves multiaccuracy and multicalibration guarantees1 (Hébert-Johnson
et al., 2018; Kim et al., 2019). We show that when the predictor is multiaccurate (or multicalibrated), then
the ranking distribution output by UA ranking satisfies a natural generalization of multiaccuracy (resp.
multicalibration) towards the same groups. This result can be interpreted as an interpolation between
individual and group fairness notions for ranking: as the set of subgroups the predictor is multicalibrated
against becomes more refined, the UA ranking for predictions more accurately reflects the UA ranking induced
by the unknown ground truth classes of individuals.
To investigate the tradeoff between fairness, stability, and utility, in Section 5, we introduce a standard
ranking utility model, and show that the utility optimal ranking function cannot hope to achieve stability
or fairness guarantees similar to UA. We also investigate a ranking function which provides a guaranteed
tradeoff between stability/fairness and utility. We believe that this will be useful to practitioners interested
in employing stable rankings in practice. Finally, in Section 6, we corroborate our theoretical results with
experimental evidence.
While various notions of stability in rankings have been proposed before (see, e.g., Asudeh et al. (2018)),
our framework is unique in that it frames the rankings as induced by predictions of some machine learning
algorithm — this ties our work more closely to modern applications. Another benefit of our definition of
stability is that it makes progress towards the broader goal of rankings which compose with fair predictors.

1 Multiaccuracy requires that the uncertainty estimates of a predictor are unbiased over a set of subgroups (combating discrimination

between groups); multicalibration guarantees that the estimates are also calibrated over subgroups (combating discrimination
between and within groups). These are arguably the most popular notions of fairness in settings with uncertain predictions
where predictors output uncertainty estimates, since obtaining meaningful or accurate estimates at the level of individuals is
usually computationally and statistically infeasible.

3

1.2

Related Work

Fairness in Ranking. By far the most relevant related work is of Dwork et al. (2019), who are also
interested in fair rankings induced by predictors, but importantly restrict their focus to only deterministic
rankings (where a better prediction means that an individual will always receive a higher rank) induced by
probabilistic binary predictors. Indeed, their motivating example is a setting in which small perturbations to
a predictor can massively impact an induced ranking. By requiring stability of ranking functions, we approach
this problem fundamentally differently: we allow (and indeed, require) non-deterministic rankings. The
multiaccuracy and multicalibration guarantees of Dwork et al. (2019) for induced rankings from predictors are
similar in flavor to ours; however, a fundamental difference is that we show this guarantee to be compatible
with stability, and, furthermore, that our guarantees hold for each position k in the ranking.
At the intersection of group and individually fair rankings, the work of Gorantla et al. (2023) is most
similar to ours. They show that one can sample from a distribution over rankings which is simultaneously
individually and group fair (in a proportional representation sense) for laminar groups. In contrast, our
group fairness hinges on the group-level statistical constraints of multicalibration imposed on the underlying
predictor, which instead allow for potentially arbitrarily overlapping groups. Garcı́a-Soriano and Bonchi
(2021) also work at the intersection of group and individual fairness in rankings, although their group fairness
constraints require that certain groups get representation amongst the top-k positions in the ranking, for
all k ∈ [n]. Both of these works and ours more broadly explore the interplay between group and individual
fairness constraints.
There is far too rich a literature on group and individually fair rankings to cover here, so we restrict
attention to only works related to uncertainty and fairness; for a more comprehensive overview, the interested
reader is referred to the survey of Zehlike et al. (2021).
Uncertainty in Rankings. Rastogi and Joachims (2023) investigate fairness in uncertainty aware rankings
when the uncertainty estimates themselves may be biased for different subgroups. We work in the simpler
setting in which we assume that uncertainty estimates are themselves unbiased. Mehrotra and Celis (2021)
and Mehrotra and Vishnoi (2022) investigate uncertain protected attributes in the settings of subset selection
and ranking, respectively. We do not assume that anything is known about individuals’ protected attributes;
instead, we only require utilizing the output of a group-fair (multiaccurate) predictor in Section 4. Training
such a predictor, however, will require certain knowledge of protected attributes (see, e.g., Kim et al. (2019)).
Independently of the line of work on UA rankings (Devic et al., 2023; Singh et al., 2021), Shen et al. (2023)
propose ranked proportionality, which shares a similar definition. Their work is in the more general setting of
the assignment problem with uncertain priorities, and they focus on algorithmic approaches for achieving a
variety of fairness notions simultaneously. Tang et al. (2023) also consider the (fair) assignment problem
and its connections with calibration. Our work is instead focused on proving certain properties of rankings
induced by predictors (predictors which, when stated in the language of Shen et al. (2023), may induce
uncertain priorities). More generally in fairness in uncertain decision making, Tahir et al. (2023) consider how
different sources of uncertainty can impact fairness. Guo et al. (2023) utilize conformal prediction techniques
to (feasibly) train fair learn-to-rank models, and are also partially interested in a similar notion of stability as
ours.
Cohen et al. (2021); Guiver and Snelson (2008); Penha and Hauff (2021); Soliman and Ilyas (2009); Yang
et al. (2022) all also work in the area of ranking with uncertain scores or preferences. In contrast to these
works, we simultaneously consider uncertainty, fairness, and stability of rankings. Heuss et al. (2023) also
model uncertainty with a Bayesian framework that allows them to apply their method post-hoc to arbitrary
retrieval models in hopes of reducing bias. Perhaps most relevant is the work of Yang et al. (2023), who
examine rankings, utility, fairness, and uncertainty simultaneously. They find that modeling uncertainty can
actually improve utility in some cases, relative to other fair ranking metrics.
Calibration and Ranking. In Section 4, we work with (multi)-calibrated predictors. Within the ranking
community, there has been some investigation into the impact of calibration of ranking models. Menon
et al. (2012) initiated this study, attempting to obtain predicted probabilities based on the score output of a
4

ranking model. Kweon et al. (2022) work in a similar setting, but refine the method of obtaining predicted
probabilities. Yan et al. (2022) work in the score-and-sort model where a scoring function is learned to
score each individual, and a ranking function is derived by sorting the individuals according to their scores.
Yan et al. (2022) aim to ensure that the scoring model is calibrated with respect to some external property.
These works all attempt to infer uncertainty from the scoring function, whereas we assume that uncertainty
is given in the form of machine-learned predictions. Busa-Fekete et al. (2011) show that calibration for
ranking functions can help increase diversity of rankings. More recently, DiCiccio et al. (2023) show that
conditional predictive parity (a notion which appears to be related to multicalibration) can help decrease
bias in rankings. These works all highlight the benefits of using calibrated predictive models for ranking,
outside of the guarantees that we provide. Korevaar et al. (2023) relate calibration and exposure in rankings
by comparing the rankings attained by subgroups with similar score distributions.
Stability in Rankings. In the information retrieval literature, Asudeh et al. (2018) also study the notion
of stability for rankings. They work in a setting where a score is calculated based on a weighted sum of
features of each item, and stability is then with respect to small changes of these weights. However, their
notion of stability is based on geometric intuition for their scoring function and its dual, and only holds
for any fixed data set. They furthermore state that stability is not a property of their “scoring function”
(particular weighted sum over features). In contrast, we are explicitly defining stability as a property of our
ranking function, which maps from any set of predictions (data set) to a randomized ranking. Oh et al. (2022)
also study the sensitivity of rankings; however, their context is slightly different: they examine stability with
respect to user interactions with, e.g., a recommendation system. In a very recent followup work (Oh et al.,
2024), the same authors also provide an algorithm to empirically achieve stability in that setting. Bruch et al.
(2020) provide experimental evidence showing that randomization can help stability (which they define as
robustness) during the training of learning-to-rank models. Our theoretical results are complementary and
corroborate the empirical evidence of Bruch et al. (2020) that randomized rankings are more robust to noise
than deterministic ones.
Finally, in terms of the interplay between prediction systems and rankings, the work of Narasimhan et al.
(2020) is perhaps most relevant; they show that the ranking problem can be considered as a pairwise binary
classification problem between items to determine which item should be placed at a higher rank.

2

Notation and Preliminaries

We write vectors in boldface. We use the standard notation x−i to denote the vector x with the ith coordinate
removed. For a random event E, we write 1E for the indicator function which is 1 if E happens, and 0 otherwise.
The total variation distance of two measures µ, ν is defined as the maximum difference in probability for
1
any event E under the two measures,
P i.e., dTV (µ, ν) := maxE |µ(E) − ν(E)| = 2 ||µ − ν||1 . We will use the
2
entry-wise matrix norms ||M ||1 = i,j |Mi,j | and ||M ||∞ = maxi,j |Mi,j |.
X denotes a set of individuals; it contains humans, ads, service requests, or other entities towards whom
fairness is desired. The elements of X can be labeled with labels from the label set [L] = {1, 2, . . . , L}. We
work in the multiclass “ordinal” classification setting where the labels are sorted from most to least preferred
as L ≻ L − 1 ≻ · · · ≻ 2 ≻ 1. This notation corresponds with the intuition that possessing a higher merit
score/class is valued more by a decision maker. A common special case is L = 2, i.e., binary labels, where
label 1 might represent irrelevant/unsuitable, while label 2 represents relevant/suitable.

2.1

Predictors

We focus on predictors in the multiclass setting which output distributions over L labels. Let ∆L denote
the set of all distributions on [L]. A (probabilistic) predictor f : X → ∆L is a function mapping data points
to distributions over labels. We let p = f (x) denote the vector of probabilities that the predictor f assigns
2 rather than induced norms, which are typically described by the same notation.

5

to individual x ∈ X . For any class ℓ ∈ [L], pℓ denotes the probability of that class. As an example, for a
probabilistic binary predictor f in the context of determining whether a candidate is qualified for a job, p2
would capture the probability that the individual x is qualified, while p1 = 1 − p2 is the probability that x is
unqualified.
Rankings involve multiple individuals, and hence multiple predictions. A prediction for n individuals P is
an n × L matrix where each row corresponds to the distribution over labels for a particular individual. We
define Pn,L to be the set of all predictions for n individuals, i.e., the set of all n × L matrices where each
row is a distribution. We will frequently consider the case in which a predictor f for single individuals is
applied to each of n individuals separately. For a vector x = (x1 , . . . , xn ) ∈ X n of n individuals, we write
f (x) = (f (x1 ), . . . , f (xn )) for the n × L matrix of predictions for all of the n individuals.
We use the random variable λx to denote the (random) label of individual x; when
Pwe specifically consider
an individual xi in a vector of individuals, we abbreviate λi := λxi . We write N ℓ = i 1λi =ℓ for the random
variable that is the number of individuals with label ℓ; when we
use this notation, the domain of i will be
PL
′
clear from the context. We extend this notation to write N ≥ℓ = ℓ′ =ℓ N ℓ for the number of individuals with
>ℓ
label ℓ or better, and similarly
Pfor N . We will sometimes restrict the count to individuals in a particular
ℓ
set S, and then write NS = i∈S 1λi =ℓ , and similarly for the derived notation. In particular, we use the
ℓ
ℓ
notation N−i
= N[n]\{i}
for the number of individuals other than i with a particular label ℓ.

2.2

Rankings and Ranking Functions

A principal would like to use predictions provided by a predictor to output a (distribution over) rankings. As
examples, consider a site or service such as LinkedIn providing an employer with a ranked list of applicants to
interview (Geyik et al., 2019), or an online platform deciding on the order in which to display ads or vendors
to a visitor. In these settings, because attention is a limited resource, a common approach would have the
principal rank the items in question based on some function of the predictions.
A ranking is a total order on n individuals. A randomized ranking is a distribution over rankings. Let
n×n
Mn×n
DS denote the set of all n×n doubly stochastic matrices. Each matrix M ∈ MDS represents a randomized
ranking 3 over n individuals, where Mi,k is the probability with which individual i is ranked in position k.
When reasoning about random rankings, we use Ri,k to denote the random event that individual i receives
position k in the ranking.
We refer to mappings from predictions to (randomized) rankings as ranking functions:
Definition 1. A ranking function r : Pn,L → Mn×n
DS maps predictions over L labels on a data set of n
individuals to a randomized ranking of those individuals.
By focusing on ranking functions, we implicitly state that the principal interacts with the data set only
through the predictions over labels. That is, we do not consider listwise learning-to-rank schemes — such as
Cao et al. (2007); Xu and Li (2007) — in which the principal directly learns a function mapping data sets of
individuals’ features to rankings.

2.3

Desiderata of Ranking Functions

While ranking functions could be very general, there are natural requirements that make them “reasonable”
to be used. In particular, we focus on the following basic properties.
Definition 2 (Anonymity). A ranking function r : Pn,L → Mn×n
DS is anonymous if every permutation
σ : [n] → [n] of the predictions for individuals results in an identical permutation of the individuals’ ranks.
Anonymity states that the outcome for an individual depends only on their (and everyone else’s) prediction,
but not on the index at which the individual appeared in the data set, i.e., on their identity. As such, it is an
essential fairness requirement in virtually all settings.
3 More precisely, it represents the marginal probabilities of the distribution, which can typically be implemented by many different

distributions over rankings. We assume that individuals care only about the probabilities with which they are ranked in each
position, in which case marginal distributions sufficiently capture fairness.

6

A second essential property of ranking functions is stability: that small changes in the predictions only
lead to small changes in the rankings.
′
Definition 3 (Stability). Fix n and L. A ranking function r : Pn,L → Mn×n
DS is γ-stable if ||r(P )−r(P )||∞ ≤
′
′
γ · ||P − P ||1 for all predictions P, P ∈ Pn,L .

This is particularly important when the predictions are the result of ML-based training methods, which
will always contain non-trivial amounts of noise. Indeed, the lack of stability is a well-documented and
problematic aspect of many ML-systems, and has been shown not only within the fairness literature (Cooper
et al., 2024), but has long been a concern for image classification models (Goodfellow et al., 2015) and more
recently also LLMs (Zou et al., 2023).

3

Predictions and Rankings

We first show useful fairness consequences of stability: combining a stable ranking function with an individually
fair predictor results in fair ranking outcomes. We then show that stability and anonymity are fundamentally
at odds with determinism: only constant deterministic ranking functions are stable, and no deterministic
ranking function is anonymous. This establishes that randomization is inherently necessary for a ranking
function to be meaningful, anonymous, and stable. We then present our adaptation of the UA ranking
function of Singh et al. (2021), and show that it is anonymous and stable.

3.1

Consequences of Stability

Stability implies that small changes in predictions do not change the distribution over rankings much. This
has two immediate but noteworthy consequences: (1) if the predictions are made by an individually fair
predictor, then similar individuals will be ranked similarly, and (2) as the predictions approach ground truth,
the ranking distribution produced by the ranking function approaches the rankings under the ground truth.
To formalize the first claim, we recall the seminal definition of an individually fair predictor (Dwork et al.,
2012). This notion assumes a metric d defined on X capturing a relevant measure of similarity between
individuals. For β > 0, a probabilistic predictor f is (β, d)-individually fair if ||f (x) − f (x′ )||1 ≤ β · d(x, x′ )
for all x, x′ ∈ X .
Proposition 4. Let f : X → ∆L be a (β, d)-individually fair predictor, and r : Pn,L → Mn×n
DS an anonymous
and γ-stable ranking function. Given a data set of individuals (xi )i∈[n] and their associated predictions P =
(f (xi ))i∈[n] ∈ Pn,L , let q, q ′ be the ith and j th rows of r(P ), respectively. Then, ∥q − q ′ ∥∞ ≤ (2βγ) · d(xi , xj ).
Proof. The proof is a straightforward application of γ-stability with respect to the given prediction matrix P
and a matrix P ′ , where P ′ is exactly P but with rows i and j swapped (requiring the anonymity condition).
This, combined with the definition of (β, d)-individual fairness for i, j, completes the proof.
The result can be interpreted as a composition guarantee for anonymous and stable rankings with
individually fair predictors: if x, x′ are simultaneously in a data set, the difference in their distributions over
rankings can be at most proportional to their dissimilarity under the metric d. Another interpretation is the
following: Stability and individual fairness are both Lipschitz conditions, and composition of Lipschitzness
implies that an individually fair predictor combined with a stable ranking will induce an individually fair
ranking.
Another very straightforward but desirable consequence of stability is obtained by considering one
prediction to be ground truth and the other obtained from a learned classifier.
Corollary 5. Let f ∗ : X → ∆L be the ground truth label distribution for individual x, and f : X → ∆L the
learned predictor. Assume that f is ϵ-accurate, satisfying that ∥f (x) − f ∗ (x)∥1 ≤ ϵ for all x ∈ X . Then, any
γ-stable ranking function r guarantees that ∥r(f (x)) − r(f ∗ (x))∥∞ ≤ γ · nϵ for all x ∈ X n .

7

Put differently, for any stable ranking function, accurate individual level uncertainty estimates (relative to
a ground truth f ∗ ) will induce accurate individual level rankings. Although somewhat obvious, we highlight
this property of stability since the “ground truth” approach is often a central assumption in the study of
machine learned predictors (Shalev-Shwartz and Ben-David, 2014).

3.2

Stability and Determinism are Incompatible

A third property which most rankings used in practice possess, and which is often considered desirable
by practitioners, is determinism: that for given inputs, only one ranking (rather than a distribution over
rankings) can result.
Definition 6 (Determinism). A ranking function r : Pn,L → Mn×n
DS is deterministic iff for all P ∈ Pn,L , the
resulting distribution over rankings r(P ) has only entries in {0, 1}.
Perhaps the most well-known deterministic ranking function is given by the Probability Ranking Principle
(PRP) of Robertson (1977). In the setting with binary predictions, this ranking function sorts individuals by
decreasing probability of belonging to class 2, i.e., being qualified.
Naturally, one may ask whether a deterministic ranking function like the PRP can be stable or anonymous.
Unfortunately, neither is possible, as captured by the following.
Proposition 7. No deterministic ranking function r : Pn,L → Mn×n
DS is anonymous. Furthermore, any
deterministic and stable ranking function must be constant, in the sense that | Im(r)| = 1, i.e., the ranking
function outputs the same ranking for all input predictions.
Proof. To prove the impossibility of anonymity, consider any prediction matrix P = (p)i∈[n] with identical
predictions p for each individual (e.g. p = (1, 0, . . . , 0) ∈ ∆L ). Then, any deterministic ranking function r
must order the individuals based only on their indices in P , since they all have identical predictions. For
any permutation σ : [n] → [n], let Pσ represent applying permutation σ to the rows of P . Since the ranking
function can only depend on the input matrix, and r is deterministic, we have that r(P ) = r(Pσ ). However,
by the definition of anonymity (Definition 2), the permutation σ on the rows of P should produce the
permutation r(P )σ on the individuals in the resulting ranking, which is a contradiction. Therefore, r is not
anonymous.
To prove the instability result, we prove the contrapositive. Let r be deterministic and non-constant.
We will show that r is not stable. Because r is non-constant, there exist P, P ′ ∈ Pn,L with r(P ) ̸= r(P ′ ).
Consider the straight line Q(β) = βP + (1 − β)P ′ , for β ∈ [0, 1]. Because Pn,L is convex, Q(β) ∈ Pn,L for all
β ∈ [0, 1]. Let β ∗ = inf {β | Q(β) = P ′ }; β ∗ is well-defined because Q(1) = P ′ . By definition, Q(β) ̸= P ′ for
all β < β ∗ , and if β ∗ = 0, then Q(β ∗ ) = P =
̸ P ′ . On the other hand, by the definition of the infimum, for
′
∗
every δ > 0, there is a δ < δ with Q(β + δ ′ ) = P ′ . Thus, we obtain arbitrarily close pairs β ′ ≤ β ∗ < β ′′
with Q(β ′ ) ̸= Q(β ′′ ). Because r is deterministic, all entries of r(Q(β ′ )) and r(Q(β ′′ )) are in {0, 1}, implying
that ||r(Q(β ′ )) − r(Q(β ′′ ))||∞ ≥ 1. On the other hand, ||Q(β ′ ) − Q(β ′′ )||1 ≤ ||2δ(P ′ − P )||1 → 0 as δ → 0.
This implies that r is not stable, completing the proof.
We remark that the instability portion of the proof did not rely on considering a straight line. By
considering any path (curve in Rn×L ) connecting P, P ′ and its parametrization by β ∈ [0, 1], the exact
same proof still works. This shows that even if we consider only a subset of possible predictions, so long
as the subset is path-connected4 , a deterministic stable ranking function must be constant. This extends
the proposition to settings where prediction strategies may output only certain (path-connected) subsets of
predictions, due to, for example, intrinsic preferences or implicit bias of a particular learning algorithm.
Proposition 7 formalizes the intuition that randomness is required to achieve stability. Indeed, the main
results of our work also show that randomization and the resulting stability are crucial for achieving desirable
fairness guarantees.
4 Recall that a set A is path-connected if for every pair of elements x, y ∈ A there exists a continuous path between x and y which

is entirely contained within A.

8

3.3

Uncertainty Aware Rankings

Meaningful deterministic ranking functions cannot be stable; in fact, it is not immediate that there exist
(non-constant) stable ranking functions. We now show that Uncertainty Aware (UA) Rankings, introduced
by Singh et al. (2021), are anonymous and stable.
UA rankings were originally introduced via an axiomatization of when a probabilistic ranking should
be considered “fair” for given merit distributions. Devic et al. (2023) further refined this axiomatization
by combining notions of meritocracy and lifting deterministic decision making to decision making under
uncertainty.
The definition of Singh et al. (2021) assumed that merit distributions were continuous and ties occurred
with probability 0. Motivated by predictors which output distributions over discrete label sets such as 1–5 or
{irrelevant, suitable, extremely relevant} with a corresponding total order, we adapt the definition of UA
rankings:
Definition 8 (Uncertainty Awareness (Singh et al., 2021)). A randomized ranking M ∈ Mn×n
DS is uncertainty
aware for a prediction P ∈ Pn,L if for each individual i and position k, the entry Mi,k is the probability that
i has the k th highest label if all labels λi ∼ pi are sampled independently from the respective distributions
pi ∈ ∆L , and ties are broken uniformly. Formally, conditioned on the drawn labels λi of all individuals i,
which entail the counts N ℓ for all labels, the probability for individual i to obtain rank k is
(
1
if N >ℓ < k ≤ N ≥ℓ
1
L
P[Ri,k | λi = ℓ, N , . . . , N ] = N ℓ
(1)
0
otherwise.
A ranking function r : Pn,L → Mn×n
DS is uncertainty aware if r(P ) is uncertainty aware for all P ∈ Pn,L .
Because the definition of uncertainty awareness fully prescribes the ranking distribution M for a given
prediction P (as shown in Lemma 4.2 of Singh et al. (2021)), there is a unique uncertainty aware ranking
function r for any given n, L; we henceforth denote it by rUA .
Intuitively, the fairness of UA can be interpreted through a possible futures viewpoint. Given two
individuals A, B, if A has more merit than B in 60% of futures (when the merits/labels of both A and B are
sampled from their respective distributions), then UA implements the requirement that the allocation in the
present should respect this uncertainty and give A the better rank at least 60% of the time (and B at least
40% of the time); this entails the need for randomization. We refer the reader to Devic et al. (2023); Singh
et al. (2021) for a formal argument on the fairness of UA ranking.
Our first key insight for proving properties of UA rankings is that by taking into account the randomness
of the draws of labels and the tie breaking, the rank distribution produced by UA ranking can be summarized
as follows:
Proposition 9. Let P ∈ Pn,L be a prediction, and rUA (P ) the ranking distribution produced by rUA for P .
Then, the probability of individual i ∈ [n] being ranked in position k ∈ [n] is:
n−1
X

1
ℓ
>ℓ
· PP [N−i
= j and k − (j + 1) ≤ N−i
< k],
j
+
1
j=0
X
PrUA (P ) [Ri,k ] =
pi,ℓ · PrUA (P ) [Ri,k | λi = ℓ].

PrUA (P ) [Ri,k | λi = ℓ] =

(2)
(3)

ℓ

Proof. For the first part, we observe that the probability of Ri,k in (1) depends only on N ℓ and N >ℓ . By
considering all the possible values of N ≥ℓ for which (1) gives a non-zero probability, we obtain that

PrUA (P ) [Ri,k | λi = ℓ] =

n
X
1

j
j=1

· PP [N ℓ = j and N >ℓ < k ≤ N ≥ℓ | λi = ℓ].
′

′

ℓ
ℓ
The result is then obtained by noticing that conditioned on λi = ℓ, we have N−i
= N ℓ − 1 and N−i
= N ℓ for
′
all ℓ ̸= ℓ. The second part of the proposition simply states the law of total probability.

9

We also remark that the definition of Singh et al. (2021), in contrast to Definition 8, did not require
the labels/merits of different individuals to be sampled independently from their respective distributions
Pi . We add this independence requirement to facilitate connections with learning algorithms for predictors.
An added benefit of the independence assumption is that it makes it possible to explicitly compute rUA (P )
in polynomial time, as captured by the following proposition. Note that this is in contrast to the case of
possibly correlated labels/merits from previous work (Devic et al., 2023; Singh et al., 2021); indeed, a main
technical contribution of these works was analyzing the loss in fairness/utility incurred due to imperfectly
approximating rUA (P ) via sampling.
Proposition 10. There exists an algorithm which, given P ∈ Pn,L , exactly computes rUA (P ) in time
O(n4 + n3 L).
Proof. The two parts of Proposition 9 combined imply that in order to compute row i of rUA (P ), it is
>ℓ
ℓ
sufficient to compute PP [N−i
= j and k − (j + 1) ≤ N−i
< k] for all pairs (j, k) ∈ [m]. This is accomplished
by a dynamic program similar to a standard undergraduate exercise, which is to compute a Poisson Binomial
distribution explicitly.
For notational convenience, assume that i = n; this is solely to avoid a special case in the recurrence, and
also without loss of generality by anonymity of the UA rule. For any t, j, j ′ ∈ {0, 1, . . . , m − 1}, let
ℓ
>ℓ
A(t, j, j ′ ) = PP [N{1,...,t}
= j and N{1,...,t}
= j′]

be the probability that among the first t individuals, exactly j have label ℓ, and exactly j ′ have a label strictly
better than ℓ. From these values, we can then construct the necessary quantities as
ℓ
>ℓ
PP [N−n
= j and k − (j + 1) ≤ N−n
< k] =

k−1
X

A(n − 1, j, j ′ ).

j ′ =k−(j+1)

We give the recurrence relationship for the A(t, j, j ′ ). The base cases are that
A(0, 0, 0) = 1
A(0, j, j ′ ) = 0 if j + j ′ ̸= 0,
because with no individuals, the only possible numbers of individuals with given labels is 0.
Now consider A(t, j, j ′ ) for t ≥ 1. With probability pt,ℓ , individual t has label ℓ, in which case the desired
′
event happens when j − 1
Pindividuals among the first t − 1 have label ℓ, and j have labels strictly better
than ℓ. With probability ℓ′ >ℓ pt,ℓ′ , individual t has a label strictly better than ℓ, in which case the desired
′
event happens when j individuals
Pamong the first t − 1 have label ℓ, and j − 1 have labels strictly better
than ℓ. Finally, with probability ℓ′ <ℓ pt,ℓ′ , individual t has a label strictly worse than ℓ, in which case the
desired event happens when j individuals among the first t − 1 have label ℓ, and j ′ have labels strictly better
than ℓ. These three cases disjointly cover all possibilities for the label of t, so we have derived the following
recurrence:
A(t, j, j ′ ) = pt,ℓ · A(t − 1, j − 1, j ′ ) +

X

pt,ℓ′ · A(t − 1, j, j ′ − 1) +

ℓ′ >ℓ

X

pt,ℓ′ · A(t − 1, j, j ′ ).

ℓ′ <ℓ
′

Here, to avoid case distinctions for whether j and/or j are 0, we treat A(t, j, j ′ ) = 0 whenever j or j ′ are
negative.
P
Notice that for any fixed t, all values ℓ′ >ℓ pt,ℓ′ , being prefix sums, can be pre-computed in time O(L).
Thus, for all ℓ, t, the precomputation can be performed in time O(nL).
Then, any one entry A(t, j, j ′ ) can be computed in constant time from previously computed values. Because
the table has size O(n3 ), the total computation takes time O(n3 ). Summing over all possible values of i, the
total time to compute the entire ranking distribution rUA (P ) is O(n4 + n2 L). Finally, the post-processing of
>ℓ
ℓ
computing the PP [N−i
= j and k − (j + 1) ≤ N−i
< k] for all k, for fixed i, ℓ, j, can be implemented in time
O(n) by using differences of prefix sums; thus, all values can be computed in time O(n3 L). This gives a total
time of O(n4 + n3 L).
10

While the notion and use of uncertainty aware rankings may appear to be of primarily theoretical interest,
it is in fact used in practice. For example, the NBA draft lottery can be understood through the lens of
uncertainty aware rankings. The merit of a team is its need for better choice picks, which can be (imperfectly)
inferred from the team’s performance in the previous season. The draft order is then obtained by a weighted
lottery based on these uncertain merits.
We now present the central result of this section: that the uncertainty aware ranking function is anonymous
and stable.
Theorem 11. Let rUA : Pn,L → Mn×n
DS be the UA ranking function for n individuals and L labels. rUA is
anonymous and 1-stable.
The following lemma is a key part of the proof of stability; it bounds how different the probabilities for
individual i obtaining rank k can be under two different prediction matrices, as a function of how similar
these matrices are:
Lemma 12. Let P, Q ∈ Pn,L be two different prediction matrices. For any individual i, let pi , qi be the ith
rows of P, Q, respectively, i.e., the label distributions of individual i under the two predictions. Let i be an
individual, k ∈ [n] a position, and ℓ ∈ [L] a label. Then,
X
PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ] ≤ 2 ·
dTV (pi′ , qi′ ).
i′ ̸=i

We defer the proof of Lemma 12 to Appendix A.
Proof of Theorem 11. First, the UA ranking rule is obviously anonymous, simply by its (symmetric) definition
which treats all indices identically. Thus, we focus on proving stability.
Now, let
P any individual i and rank k be given. By Equation (3) in the second part of Proposition 9,
P[Ri,k ] = ℓ pi,ℓ · P[Ri,k | λi = ℓ]. Now consider two different predictors P, Q. We bound the difference in
probabilities for i to be ranked in position k as follows:

PrUA (P ) [Ri,k ] − PrUA (Q) [Ri,k ]
≤

X

pi,ℓ · PrUA (P ) [Ri,k | λi = ℓ] − qi,ℓ · PrUA (Q) [Ri,k | λi = ℓ]

ℓ

≤

X

|pi,ℓ − qi,ℓ | · PrUA (P ) [Ri,k | λi = ℓ] + qi,ℓ · PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ]

ℓ

≤

X

|pi,ℓ − qi,ℓ | + qi,ℓ · PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ] .

(4)

ℓ

P
By Lemma 12, we can bound PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ] ≤ 2 · i′ ̸=i dTV (pi′ , qi′ ).
Substituting this bound back into (4), we now obtain that
X
X
X
|PrUA (P ) [Ri,k ] − PrUA (Q) [Ri,k ]| ≤
|pi,ℓ − qi,ℓ | +
qi,ℓ · 2 ·
dTV (pi′ , qi′ )
ℓ

=

ℓ

1
dTV (pi , qi ) + 2 ·
2
′

X

i′ ̸=i

dTV (pi′ , qi′ )

i ̸=i

≤ ||P − Q||1 .
In the final step, we absorbed the total variation distance for i into the sum for i′ ̸= i (which has a larger
coefficient), and used that the 1-norm is exactly twice the total variation distance.
As the number of labels increases, the predictor can provide the ranking function with more fine-grained
information, which should allow the UA ranking function to produce a wider class of distributions over
rankings. The next proposition shows that this is indeed the case:
11

Proposition 13. The expressivity of uncertainty aware ranking functions is strictly increasing in L. More
n×n L−1
L
formally, let n ≥ L, and rUA
: Pn,L → MDS
, rUA : Pn,(L−1) → Mn×n
DS be the corresponding UA ranking
L−1
L
functions. Then, rUA (Pn,L ) ⊋ rUA (Pn,(L−1) ).
Proof. First, to see monotonicity, notice that adding a column of all 0 entries, i.e., an unused label L, does
not change the behavior of rUA . For any P ∈ Pn,(L−1) , writing [P, 0] ∈ Pn,L for this matrix, we have
L−1
L−1
L
L
rUA
([P, 0]) = rUA
(P ), implying that rUA
(Pn,L ) ⊇ rUA
(Pn,L−1 ).
To prove strictness of inclusion, consider a prediction P = Jn over n = L individuals. Here, Jn is the
n × n row-reversed identity matrix with ones along the anti-diagonal, so individual i is deterministically
known to have label L − i + 1. Then rUA (P ) = In for the n × n identity matrix In , i.e., individual i is ranked
L
deterministically in position i, and we have proved that In ∈ rUA
(Pn,L ). Note that due to the tie breaking of
rUA , to achieve a deterministic ranking, no two individuals must ever have the same label, i.e., the supports
of the n = L rows of any prediction matrix Q yielding rUA (Q) = In must be disjoint. This implies that Q
L−1
must have at least L columns, i.e., In ∈
/ rUA
(Pn,L−1 ), completing the proof of strictness of inclusion.
Using Proposition 13, we can show that our stability analysis is essentially tight up to a factor of 2, ruling
out the possibility of, for example, n1 -stability for UA rankings:
Proposition 14. For any L ≥ 3 and n ≥ 2, rUA is not γ-stable for any γ < 21 .
Proof. Let n ≥ 2 be given. We only consider L = 3; for any L > 3, it suffices by Proposition 13 to embed the
following instance and ignore the extra labels. Consider the prediction matrix P with individual 1 having
prediction p1 = (1/2, 0, 1/2), and individuals 2 through n having prediction p2 = (0, 1, 0). Similarly, the
prediction matrix P ′ will have individual 1 with prediction p′1 = (1, 0, 0) and individuals 2 through n with
prediction p2 . That is, P and P ′ are identical except for individual 1.
Let M = rUA (P ), M ′ = rUA (P ′ ) be the resulting probabilities for placing individuals in specific positions.
Since all individuals except individual 1 have deterministic qualifications, under P , there is a 50% probability
′
that individual 1 is ranked last, so M1,n = 1/2, whereas M1,n
= 0. Therefore, we have the following.
′
∥rUA (P ) − rUA (P ′ )∥∞ ≥ |M1,n − M1,n
| = 1/2,

∥P − P ′ ∥1 = 1.

Thus, rUA cannot be γ-stable for any γ < 1/2.

4

Multigroup Fairness Guarantees

We now present our main result: UA rankings naturally compose with multiaccurate and multicalibrated
predictors.
We have shown that UA rankings are stable, and, furthermore, that stable rankings compose harmoniously
with individually fair predictors. Corollary 5 demonstrates that an accurate predictor, at the individual level,
can combine with a stable ranking function (such as UA) to induce a ranking which is close to that of the
underlying ground truth. In practice, however, obtaining accurate uncertainty estimates at the individual
level is too strong of an assumption for arbitrary data sets of individuals. This is because such a requirement
is equivalent to learning the Bayes optimal predictor (the true distribution over the labels conditioned on the
features of an individual) (Shalev-Shwartz and Ben-David, 2014, Section 3.2.1), which generally requires the
number of samples or running time to be exponential in the dimensionality of the features used for prediction,
which can be statistically or computationally infeasible.
Instead, we focus on obtaining a coarser guarantee for UA rankings at the level of subgroups of the domain
for data sets sampled i.i.d. from a distribution D over individuals (instead of arbitrary data sets). The
i.i.d. assumption is a standard setting for machine learning, and has proven useful in many practical settings.
Our contributions are tightly connected to the statistical group-fairness conditions of multiaccuracy and
multicalibration (Hébert-Johnson et al., 2018; Kim et al., 2019). Our guarantees will be meaningful since they
directly imply that, relative to an underlying ground truth, unbiased predictors will induce unbiased rankings.

12

4.1

Group-wise Accuracy Guarantees

We first recall the definitions of multiaccuracy and multicalibration from the fair machine learning literature.
Then, we state our result on the average accuracy of rankings induced by multiaccurate and multicalibrated
predictors when compared to rankings induced from nature.
Definition 15 (Multiaccuracy/Multicalibration (Hébert-Johnson et al., 2018; Kim et al., 2019)). Let D be
a distribution over individuals X . Let f ∗ be the ground truth distribution of labels, i.e., f ∗ (x) is the true
distribution of labels for individual x, while f is the predictor, so f (x) is the predicted label distribution.
Let C be a collection of sets for which the predictor is to be multiaccurate/multcalibrated. Let α ≥ 0 be a
parameter for how far from fully accurate/calibrated the predictor is allowed to be. When writing E [v] for a
vector-valued quantity v, we mean the coordinate-wise expectations. Then we have that:
1. f is (C, α)-multiaccurate if for every set S ∈ C,
∥Ex∼D [1x∈S · (f ∗ (x) − f (x))] ∥∞ ≤ α.
That is, for each of the given groups S ∈ C and each label, the expected probability mass on that label is
approximately the same for the predictor as for the ground truth.
2. Let some interval width δ ∈ (0, 1] be given, such that 1/δ is an integer. f is (C, α, δ)-multicalibrated if
L
for every set S ∈ C and vector (j1 , j2 , . . . , jL ) ∈ {0, 1, . . . , 1/δ − 1} ,


∥Ex∼D 1x∈S · 1∀ℓ, f (x)ℓ ∈[jℓ ·δ,(jℓ +1)·δ) · (f ∗ (x) − f (x)) ∥∞ ≤ α.
That is, in addition to fixing a group, even if we also fix a (rough) interval within which the predicted
probability mass must lie, the predictor still has to be close to the ground truth, for each possible label.
Each set S ∈ C represents a protected group of import in the underlying population X . The sets in C can
be complex, overlapping, nested, laminar, etc., since both multiaccuracy and multicalibration with respect to
C are — in contrast to other notions of group fairness in supervised learning such as equalized odds (Awasthi
et al., 2020) or equality of opportunity (Hardt et al., 2016) — statistically sound in that they are consistent
with the underlying ground truth f ∗ . There are a variety of learning and post-processing algorithms which,
in terms of sample and time complexity, efficiently achieve multiaccuracy/multicalibration (Gopalan et al.,
2022; Haghtalab et al., 2023; Hébert-Johnson et al., 2018; Kim et al., 2019).
We now present the central contribution of this section: multicalibration and multiaccuracy for a predictor
f guarantee that on a per group basis, UA rankings derived from f will be close to UA rankings derived
from the ground truth f ∗ .
Theorem 16. Let D be a distribution over individuals X . Let f ∗ be the ground truth distribution of labels,
and f a predictor. For any n, let Dn be the distribution obtained from drawing a vector of n i.i.d. samples
from D.
Let C be a collection of sets with X ∈ C, the sets of individuals for which the predictor f will be assumed
to be multiaccurate/multcalibrated. Let α ≥ 0 be a parameter for how far from fully accurate/calibrated the
predictor is allowed to be. Then, we have that:
1. If f is (C, α)-multiaccurate, then the following holds for all sets S ∈ C and k ∈ [n]:


Ex∼Dn ,i∼Unif([n]) 1xi ∈S · PrUA (f ∗ (x)) [Ri,k ] − PrUA (f (x)) [Ri,k ] ≤ Lnα.
2. Let some interval width δ ∈ (0, 1] be given, such that 1/δ is an integer. If f is (C, α, δ)-multicalibrated
L
and ({X } , α)-multiaccurate, then for every set S ∈ C, vector (j1 , j2 , . . . , jL ) ∈ {0, 1, . . . , 1/δ − 1} , and
k ∈ [n]:
h
i
Ex∼Dn ,i∼Unif([n]) 1xi ∈S · 1f (xi )ℓ ∈[jℓ ·δ,(jℓ +1)·δ) for all ℓ · PrUA (f ∗ (x)) [Ri,k ] − PrUA (f (x)) [Ri,k ]
≤ Lnα.
13

Proof. Fix a set S ∈ C and position k ∈ [n]. The proofs of both parts of the theorem are essentially
identical, with only minor (practically syntactic) changes, and we will give both proofs simultaneously,
pointing out the few places where differences occur. For the proof of the second part of the theorem (for
the case of a multi-calibrated predictor), let δ ∈ (0, 1] be given, such that 1/δ is an integer, and fix a vector
L
(j1 , j2 , . . . , jL ) ∈ {0, 1, . . . , 1/δ − 1} . We use the notation 1(x) to denote 1(x) := 1x∈S for the proof under
a multiaccurate predictor f , and to denote 1(x) := 1x∈S · 1f (x)ℓ ∈[jℓ ·δ,(jℓ +1)·δ) for all ℓ for the proof under a
multicalibrated predictor f .
First, we observe that we can simplify notation by using that all draws of the xj are i.i.d., and we can
apply the law of total probability:


Ex∼Dn ,i∼Unif([n]) 1(xi ) · PrUA (f ∗ (x)) [Ri,k ] − PrUA (f (x)) [Ri,k ]


= Ex∼Dn 1(xn ) · PrUA (f ∗ (x)) [Rn,k ] − PrUA (f (x)) [Rn,k ]



= Exn ∼D 1(xn ) · Ex−n ∼Dn−1 PrUA (f ∗ (x−n ,xn )) [Rn,k ] − PrUA (f (x−n ,xn )) [Rn,k ] .
Next, we sum over all possible labels λn = ℓ, condition on those labels for individual n, and then omit
from the probabilities those random variables that do not affect the probability, to rewrite the preceding
expression as
"

Exn ∼D 1(xn ) · Ex−n ∼Dn−1

"
X

Pf ∗ (x−n ,xn ) [λn = ℓ] · PrUA (f ∗ (x−n ,xn )) [Rn,k | λn = ℓ]

ℓ

− Pf (x−n ,xn ) [λn = ℓ] · PrUA (f (x−n ,xn )) [Rn,k | λn = ℓ]
"

= Exn ∼D 1(xn ) · Ex−n ∼Dn−1

"
X

##

Pf ∗ (xn ) [λn = ℓ] · PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ]

ℓ

##

− Pf (xn ) [λn = ℓ] · PrUA (f (x−n )) [Rn,k | λn = ℓ]
"
≤ Exn ∼D

1(xn ) · Ex−n ∼Dn−1

"
X


Pf ∗ (xn ) [λn = ℓ] − Pf (xn ) [λn = ℓ] · PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ]

##

ℓ

"
+ Exn ∼D

1(xn ) · Ex−n ∼Dn−1

"
X

Pf (xn ) [λn = ℓ] · PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] − PrUA (f (x−n )) [Rn,k | λn = ℓ]

##


ℓ

=

X





Exn ∼D 1(xn ) · Pf ∗ (xn ) [λn = ℓ] − Pf (xn ) [λn = ℓ]
· Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ]

ℓ

(5)
+

X





Exn ∼D 1(xn ) · Pf (xn ) [λn = ℓ] · Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] − PrUA (f (x−n )) [Rn,k | λn = ℓ] .

ℓ

(6)
We bound the two sums separately. For the first sum, we simply bound
X


(5) ≤
Exn ∼D 1(xn ) · Pf ∗ (xn ) [λn = ℓ] − Pf (xn ) [λn = ℓ] ≤ L · α.
ℓ

In the final step, we applied the multi-accuracy guarantee for S for every label ℓ under the sum for the first
part of the theorem, and the multi-calibration guarantee for S and (j1 , . . . , jL ) for every label ℓ for the second
part of the theorem.
14

For the second term, we apply the triangle inequality, and bound
X




(6) ≤
Exn ∼D Pf (xn ) [λn = ℓ] · Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] − PrUA (f (x−n )) [Rn,k | λn = ℓ] .
ℓ

First, by Equation (2) in the first part of Proposition 9,
X

 n−1
Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] =



1
ℓ
>ℓ
· Ex−n ∼Dn−1 Pf ∗ (x−n ) [N−n
= j and k − (j + 1) ≤ N−n
< k] .
j+1
j=0

Because the label of i under f ∗ is drawn by first drawing xi ∼ D, then drawing λi ∼ f ∗ (xi ), the label
distribution of i under f ∗ is exactly p := Ex∼D [f ∗ (x)] (where we again take expectation of vectors componentwise). Writing P ∈ Pn−1,L for the matrix in which each of the n − 1 rows is pi = p, we can therefore
write
X 1

 n−1
ℓ
>ℓ
Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] =
· PP [N−n
= j and k − (j + 1) ≤ N−n
< k]
j
+
1
j=0
= PrUA (P ) [Rn,k | λn = ℓ].
An identical argument applies for f instead of f ∗ . Writing q := Ex∼D [f (x)] and Q ∈ Pn−1,L for the matrix
in which each of the n − 1 rows is qi = q, we can write


Ex−n ∼Dn−1 PrUA (f (x−n )) [Rn,k | λn = ℓ] = PrUA (Q) [Rn,k | λn = ℓ].
Combining both of these calculations, then applying Lemma 12, we can bound


Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] − PrUA (f (x−n )) [Rn,k | λn = ℓ]
= PrUA (P ) [Rn,k | λn = ℓ] − PrUA (Q) [Rn,k | λn = ℓ]
≤2·

n−1
X

dTV (pi , qi )

i=1

= 2(n − 1) · dTV (p, q).
X
= (n − 1) ·
|pℓ − qℓ |
ℓ

= (n − 1) ·

X

|Ex∼D [f ∗ (x)ℓ − f (x)ℓ ]| .

ℓ

If f is (C, α)-multiaccurate, because X ∈ C, we bound
X
X
|Ex∼D [f ∗ (x)ℓ − f (x)ℓ ]| =
|Ex∼D [1x∈X · (f ∗ (x)ℓ − f (x)ℓ )]| ≤ L · α.
ℓ

ℓ

The identical bound holds for the second part of the theorem, because we assumed f to also be ({X } , α)multiaccurate.
Finally, we combine all of these bounds, to obtain that
X





Exn ∼D 1xn ∈S · Pf ∗ (xn ) [λn = ℓ] − Pf (xn ) [λn = ℓ]
· Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ]

ℓ

+

X





Exn ∼D 1xn ∈S · Pf (xn ) [λn = ℓ] · Ex−n ∼Dn−1 PrUA (f ∗ (x−n )) [Rn,k | λn = ℓ] − PrUA (f (x−n )) [Rn,k | λn = ℓ]

ℓ

≤L·α+

X



Exn ∼D Pf (xn ) [λn = ℓ] · (n − 1) · L · α

ℓ

= L · α + (n − 1) · L · α = nLα,
completing the proof.
15

Theorem 16 can intuitively be thought of as the following: a predictor which is unbiased on average
over a collection of subgroups C will induce an uncertainty aware ranking which, for those subgroups, has a
similar outcome to the (usually inaccessible) uncertainty aware ranking induced by the ground truth label
distribution. The multicalibration guarantee refines this to hold for not only subgroups, but intervals of
predictions of the predictor f within that subgroup. Part (2) of Theorem 16 requires f be simultaneously
(C, α, δ)-multicalibrated and ({X } , α)-multiaccurate; that is, f is unbiased on average across individuals
sampled from D. This combination of properties can be achieved by, e.g., the algorithm of Gopalan et al.
(2022).

4.2

Multicalibration as Interpolating between Group and Individual Fairness

In contrast to learning accurate individual-level estimates (the Bayes optimal predictor), multiaccuracy/multicalibration can be achieved in time and samples polynomial in the number of sets in C, or, more generally,
polynomial in measures of complexity of C such as its VC-dimension (Gopalan et al., 2022; Hébert-Johnson
et al., 2018).
Notice that if we define CBayes = {{x} | x ∈ X } to be the set of all singleton groups, then (CBayes , α)multiaccuracy guarantees increasingly accurate predictions for all individuals as α → 0, and α = 0 recovers
the Bayes optimal classifier, i.e., the ground truth f ∗ . By varying the level of granularity of the collection C,
the learned (C, α)-multiaccurate or multicalibrated predictor represents a finer or coarser approximation of
the Bayes optimal classifier; thus, Theorem 16 guarantees that the induced ranking effectively interpolates
between individual and group-level fair rankings at the granularity defined by C.
Nonetheless, as previously noted, it is usually unreasonable to expect multiaccuracy (or multicalibration)
at the level of (CBayes , α) as α → 0. This is due to information and computational constraints: multicalibration
algorithms must use Ω(poly(|C|, 1/δ, 1/α)) samples to learn a multiaccurate/multicalibrated predictor (Shabat
et al., 2020). In addition to the sample complexity requirements, the class C must be agnostic PAC learnable
(Shalev-Shwartz and Ben-David, 2014, Section 3.2). This is a stringent requirement which rarely holds for
complex collections such as CBayes . In practice, we envision Theorem 16 to be used with sufficiently simple
classes C, such as conjunctions of categorical features and intervals of numeric features (e.g., “women in the
age range 45–65”). Working at this level of granularity not only permits efficient algorithms for obtaining
multiaccurate/multicalibrated predictors, but also guarantees that the derived rankings will be unbiased for
meaningful protected groups of individuals.

5

Ranking Functions and Utility

Most online marketplaces utilizing rankings and ranking functions are also concerned with utility or revenue.
In this section, we introduce a natural class of utility models inspired by the literature standard (Taylor
et al., 2008), and prove that the optimal utility ranking function cannot achieve the stability or group
ϕ
fairness guarantees that UA rankings enjoy. We then show that a simple ranking function rmix
which is
a randomization between the utility-optimal and UA ranking function satisfies an approximate notion of
stability and fairness while simultaneously retaining a utility guarantee. This may be of use to practitioners
more broadly.
Definition 17 (Utility Model). Let w1 ≥ w2 ≥ · · · ≥ wn ≥ 0 be position weights, and τ : ∆L → R≥0 a
function we call the class utility map, which determines how predictions are mapped to utilities. The utility
U (P, r) of a prediction matrix P ∈ Pn,L under a ranking function r is
" n
#
n
n X
X
X
Eσ∼r(P )
wk · τ (pσ(k) ) =
[r(P )i,k · wk · τ (pi )] .
i=1 k=1

k=1

PL
A particularly natural and common type of class utility map is the expected utility τ (p) = ℓ=1 vℓ · pℓ ,
where vL > vL−1 > · · · > v1 ≥ 0 are the utilities for labels ℓ ∈ [L]. Combined with the weights wk =
1/ log2 (1 + k), this class captures DCG (Järvelin and Kekäläinen, 2002), for example.
16

τ
The ranking function which achieves optimal utility will clearly depend on τ ; we denote it by ropt
. It
can be simply described as the ranking function which deterministically orders the individuals by decreasing
τ
values τ (pi ); recall that pi is the ith row of P ∈ Pn,L . We now show that in general, ropt
is not stable, which
demonstrates a necessity to trade off notions of utility and stability.
τ
Proposition 18. Even for binary labels (L = 2) and expected utility map τ , the utility-maximizing map ropt
is unstable.

Proof. The example is standard in the literature. Assume that v2 > v1 . For any ϵ ∈ (0, 12 ), define
1

1

1
+ ϵ 12 − ϵ
′
2 −ϵ
2 +ϵ .
P
=
Pϵ = 21
1
1
1
ϵ
2 −ϵ
2 +ϵ
2 +ϵ
2 −ϵ
τ
τ
Then, ropt
(Pϵ ) deterministically ranks 2 ahead of 1, while ropt
(Pϵ′ ) deterministically ranks 1 ahead of 2. As a
τ
τ
′
′
τ
result, ∥ropt (Pϵ ) − ropt (Pϵ )∥∞ = 1, while ∥Pϵ − Pϵ ∥1 = 8ϵ → 0 as ϵ → 0. This proves instability of ropt
.

Next, we show that for an extremely simple class of instances, namely, when there are two types of
individuals with uniform distribution, binary labels, identical uniform ground truth distribution over the
two labels for both types, and groups which are just the two singleton types, the utility-maximizing ranking
τ
function ropt
cannot approach optimal multigroup fairness, never mind how close to perfectly multiaccurate
the predictor gets.
Proposition 19. Let X = {1, 2} be a domain of two “types” of individuals. Let D be the uniform distribution
over those two types. Let L = 2, i.e., we consider binary labels, and the ground truth label distribution is
f ∗ (x) = ( 12 , 12 ) for both x ∈ {1, 2}, i.e., under the ground truth, both types are equally likely to be good and
bad. Let C = {{1} , {2}} be the collection of singleton subgroups.
For any α ∈ (0, 12 ), let fα be the predictor with predictions fα (1) = ( 12 −α, 12 +α) and fα (2) = ( 12 +α, 12 −α).
(That is, fα slightly overestimates the quality of type 1, and slightly underestimates the quality of type 2.) Let τ
be any utility map strictly preferring higher labels, i.e., any utility map with τ (( 12 −α, 12 +α)) > τ (( 12 +α, 21 −α)).
τ
Let ropt
be any utility-maximizing ranking function for the utility map τ .
τ
Then, fα is (C, α/2)-multiaccurate, yet for every number n of individuals, the group fairness under ropt
towards the group S = {1} for assignment to the top (most valuable) position in the ranking is the following:
h

Ex∼Dn ,i∼Unif([n]) 1xi ∈{1} ·



τ (f ∗ (x)) [Ri,1 ] − Pr τ (f (x)) [Ri,1 ]
Propt
α
opt

i

1
= ·
n




1
−n
−2
.
2

(7)

In particular, for any fixed n, the quantity stays bounded away from 0, even as α → 0.
The intuition for Proposition 19 is similar to that for Proposition 7: for the utility-maximizing ranking
function, an arbitrarily small (but non-zero) predictive mistake can induce large variations in the resulting
ranking distribution, preventing it from preserving group fairness of its predictor.
Proof. We first verify that fα is (C, α/2)-multiaccurate. For S = {1} or S = {2}, we have that
∥Ex∼D [1x∈S · (fα (x) − f ∗ (x))] ∥∞ ≤

1
1
1
1
1
· max(| − ( − α)|, | − ( + α)|) = α/2.
2
2
2
2
2

The rest of the proof focuses on the group fairness analysis, i.e., proving Equation (7). We first consider the
1
∗
∗
τ (f ∗ (x)) [Ri,1 ]. First observe that under the ground truth classifier f , we have that f (x) =
term Propt
2 · 1n×2
τ
for all x; here 1n×2 denotes the n × 2 all-ones matrix. Under this input matrix, ropt
must have some
distribution q = (q1 , . . . , qn ) over which individual is assigned the top rank; crucially for our analysis, because
the ranking function only observes this matrix 12 · 1n×2 , it must use the same distribution for all type vectors
τ (f ∗ (x)) [Ri,1 ] = qi for all type vectors x.
x. We thus conclude that Propt
τ (f (x)) [Ri,1 ]. Focus on any type vector x ̸= (2, 2, 2, . . . , 2), i.e., a vector
Next, we consider the term Propt
α
τ
that has at least one individual of type 1. Because τ (fα (1)) > τ (fα (2)), and ropt
is utility-maximizing for the
17

τ
utility map τ , ropt
(fα (x)) must rank all individuals
Pn of type 1 (of whom there is at least one) ahead of all
τ (f (x)) [Ri,1 ] = 1 for all x ̸= (2, 2, 2, . . . , 2).
individuals of type 2. From this, we obtain that i=1 1xi ∈S · Propt
α
Next, we write out the expectation from Equation (7). We use that the terms 1xi ∈S = 0 for all i when
x = (2, 2, . . . , 2), which allows us to drop this term from the sum. We then use that each x under the
i.i.d. uniform type distribution is drawn with probability 2−n , and substitute our preceding calculations for
the probabilities. This gives us the following:

h

i
τ (f ∗ (x)) [Ri,1 ] − Pr τ (f (x)) [Ri,1 ]
Ex∼Dn ,i∼Unif([n]) 1xi ∈S · Propt
opt
n
XX



1
τ (f ∗ (x)) [Ri,1 ] − Pr τ (f (x)) [Ri,1 ]
· 1xi ∈S · Propt
opt
x∼D
n
x i=1
!
n
n
X
X
X
−n 1
τ (f (x)) [Ri,1 ]
= 2 · ·
1xi ∈S · qi −
1xi ∈S · Propt
n
i=1
i=1
=

Pr n [x] ·

x̸=(2,2,...,2)

−n

=2

1
· ·
n

X

n
X

x̸=(2,2,...,2)

i=1


n
X
1
= 2−n · · 
qi ·
n
i=1
(⋆)

= 2

−n

1
· ·
n

n
X

!

!

1xi ∈S · qi − 1


1xi ∈S  − (2n − 1)

X
x̸=(2,2,...,2)

!
qi · 2

n−1

− (2n − 1)

i=1

1
= 2−n · · 2n−1 − (2n − 1)
n
1 1
= · ( − 2−n ).
n 2
In the step labeled (⋆), we used that there are exactly 2n−1 vectors with xi = 1; the following step used that
the qi , defining a probability distribution, sum to 1. This completes the proof.

5.1

Utility-Stability Tradeoffs

In both theory and practice, it is often necessary to trade off utility against other desiderata, such as fairness
or stability (see, for example, Pitoura et al. (2022); Singh and Joachims (2019)): if achieving fairness/stability
comes at a huge price in utility, it may become economically infeasible to implement fair or stable rankings.
ϕ
In this section, we introduce and discuss a class rmix
of ranking functions which provide a quantifiable tradeoff
ϕ
τ
between the objectives. rmix linearly interpolates between rUA and ropt
with a trade-off parameter ϕ ∈ [0, 1],
ϕ
chosen by the ranking/mechanism designer. We show that this interpolation naturally leads to rmix
satisfying
approximate stability and fairness, while providing lower-bound guarantees on the utility. While the proofs
are relatively straightforward, we believe that practitioners may find this class of ranking functions useful in
practical applications where the stringent requirement of 1-stability may not be necessary. The interested
reader is referred to Singh et al. (2021) for additional discussion on how to choose ϕ appropriately5 .
We first formally define the notion of approximate stability.
Definition 20. Fix n and L. A ranking function r : Pn,L → Mn×n
DS is (γ, α)-approximately stable if
||r(P ) − r(P ′ )||∞ ≤ γ · ||P − P ′ ||1 + α for all predictions P, P ′ ∈ Pn,L .
5We note that our approximate fairness guarantee in Proposition 22 on multiaccuracy/multicalibration with an additive slack is

different from the ϕ-approximate fairness of Singh et al. (2021), which is a multiplicative notion. Indeed, both hold simultaneously
ϕ
for rmix
.

18

Notice that (γ, 0)-approximate stability recovers our stability notion from Definition 3. Approximate
stability is a relaxation which allows for additive slack in the dependence on ∥P − P ′ ∥1 . An additive slack
relaxation is natural and akin to, for example, (ϵ, δ)-differential privacy (when compared to pure differential
privacy). We show that a simple mixture of UA and the optimal utility ranking satisfies the following
approximate stability and utility guarantee.
ϕ
Proposition 21. Fix a utility map τ . Let rmix
be the ranking function which randomizes between rUA (with
ϕ
τ
probability ϕ), and ropt (with probability 1 − ϕ). Then rmix
is (ϕ, 1 − ϕ)-approximately stable. Furthermore,
ϕ
τ
for any P ∈ Pn,L , we have that U (P, rmix ) = ϕ · U (P, rUA ) + (1 − ϕ) · U (P, ropt
).

Proof. We first show approximate stability. For any P, P ′ ∈ Pn,L , we have the following.
ϕ
ϕ
τ
τ
∥rmix
(P ) − rmix
(P ′ )∥∞ = ∥ϕrUA (P ) + (1 − ϕ)ropt
(P ) − ϕrUA (P ′ ) − (1 − ϕ)ropt
(P ′ )∥∞
τ
τ
≤ ϕ∥rUA (P ) − rUA (P ′ )∥∞ + (1 − ϕ)∥ropt
(P ) − ropt
(P ′ )∥∞

≤ ϕ∥P − P ′ ∥1 + (1 − ϕ).
The last line used the 1-stability of rUA (proved in Theorem 11), as well as the fact that the ∥ · ∥∞ -norm
difference of doubly stochastic matrices is at most 1.
The claim about utility is simply linearity of expectations.
ϕ
It is straightforward to show that a similar approximate fairness guarantee holds for rmix
, again due to its
linearity.

Proposition 22. Let D be a distribution over individuals X . Let f ∗ be the ground truth distribution of labels,
and f a predictor. For any n, let Dn be the distribution obtained from drawing a vector of n i.i.d. samples
from D.
Let C be a collection of sets with X ∈ C, the sets of individuals for which the predictor f will be assumed
to be multiaccurate/multcalibrated. Let α ≥ 0 be a parameter for how far from fully accurate/calibrated the
predictor is allowed to be.
1. If f is (C, α)-multiaccurate, then the following holds for all sets S ∈ C and k ∈ [n]:
h

i
Ex∼Dn ,i∼Unif([n]) 1xi ∈S · Prϕ (f ∗ (x)) [Ri,k ] − Prϕ (f (x)) [Ri,k ]
≤ ϕLnα + 1 − ϕ.
mix

mix

2. Let some interval width δ ∈ (0, 1] be given, such that 1/δ is an integer. If f is (C, α, δ)-multicalibrated
L
and ({X } , α)-multiaccurate, then for every set S ∈ C, vector (j1 , j2 , . . . , jL ) ∈ {0, 1, . . . , 1/δ − 1} , and
k ∈ [n]:
h
i
Ex∼Dn ,i∼Unif([n]) 1xi ∈S · 1f (xi )ℓ ∈[jℓ ·δ,(jℓ +1)·δ) for all ℓ · PrUA (f ∗ (x)) [Ri,k ] − PrUA (f (x)) [Ri,k ]
≤ ϕLnα + 1 − ϕ.
ϕ
Proof. The proof follows from the following computation due to linearity of rmix
:
τ
∗
∗
Prmix
ϕ
ϕ
(f ∗ (x)) [Ri,k ] − Prmix
(f (x)) [Ri,k ] = ϕ·PrUA (f (x)) [Ri,k ] + (1 − ϕ) · Propt (f (x)) [Ri,k ]



τ (f (x)) [Ri,k ]
− ϕ · PrUA (f (x)) [Ri,k ] + (1 − ϕ) · Propt
Applying this decomposition, then applying linearity of expectation, applying the triangle inequality, and
then using Theorem 16 completes the proof of both parts.

19

6

Experiments on Stability and Utility

We ran experiments on the US census data set ACS curated by Ding et al. (2021) and the student dropout
task Enrollment introduced by Martins et al. (2021) in the UCI data set Repository.
In our experiments, we demonstrate empirically that the stability guarantees of UA rankings hold when
using multiclass predictions. Furthermore, we find that in practice, the stability guarantees offered by UA
ranking may be much better than 1-stability (or the 1/2-stability worst-case lower bound in Proposition 14),
and show that the utility loss suffered by rUA is reasonable. Although our experiments are relatively simplistic
and are not the main focus of our work, they demonstrate that UA rankings have relatively good performance
in terms of utility: they outperform not only a uniformly random baseline ranking, but even the Plackett-Luce
distribution. At the same time, they also retain the provable fairness and stability properties.
Related Experiments. Previous work (Devic et al., 2023; Singh et al., 2021) also contain experiments
demonstrating the utility and utility-fairness tradeoff of UA ranking functions. This past work assumed
real-valued predictions (as opposed to the multiclass predictions in our work). Singh et al. (2021) actually
deployed a paper recommendation system at a large computer science conference using UA rankings to
demonstrate the viability of the method in practice. Furthermore, their experiments on the MovieLens data
set (Harper and Konstan, 2015) demonstrate that UA ranking — corresponding to a fairness parameter of
τ
ϕ = 1 in their work — can achieve nearly 99% of the optimal utility given by ropt
in some applications. Devic
et al. (2023) show the viability of UA rankings in a matching setting, running experiments on an online
dating data set.

6.1

Stability Against SGD Noise in Neural Network Training

Given our focus on the combination of ranking functions with noisy predictions derived from ML-based
classifiers, we first investigate experimentally the stability of UA and utility-maximizing rankings under a
natural model of prediction noise. In particular, one of the most common sources of noise in predictions is
the randomness in the training procedure (such as SGD). We designed a natural experiment by comparing
the behavior of ranking functions under predictors learned with different randomly seeded SGD training
runs. Our focus is on understanding if or to what extent the stability of UA and other ranking functions will
exceed the worst-case theoretical guarantees in such quasi-realistic settings.
First, we describe the data sets. In ACS, the prediction target is the binary variable of whether a person is
employed or not (after filtering to individuals in the age range 16–90). For computational reasons, we restrict
our experiments to a subset of the data for California with parameters survey year=‘2018’, horizon=‘1Year’ and survey=‘person’. These parameters are standard when using ACS for testing algorithmic fairness
methods, due to the large amount of available data. (See, e.g., the GitHub repository of Ding et al. (2021).)
We are left with 378,817 entries, and use an 80/20 train/test split. In Enrollment, the target is a multiclass
variable for whether an individual is an enrolled, graduated, or dropout student. After cleaning the data, we
are left with 4,424 entries, on which we use an 80/20 train/test split.
τ
Since we want to compare the stability of rUA against that of ropt
, we next define simple and natural
utilities. For ACS, we take class 2 to correspond to employment, and class 1 to correspond to unemployment
(class 2 ≻ class 1). We define τ (p) = p2 , i.e., the probability of employment. For Enrollment, we take class 1
to be that the student has dropped out, class 2 to be enrolled, and class 3 to be graduated (class 3 ≻ class 2
≻ class 1), and define τ (p) = p1 + 2 · p2 + 3 · p3 .
We train 30 simple three-layer MLP neural networks on the ACS data set, which we divide into 15
pairs of networks. Each pair of networks is initialized with the same (random) weight matrix, then trained
separately with SGD. This introduces noise into the final trained neural network weights, and consequently
the predictions. That is, each pair of networks has similar test accuracy, but will output different probabilities
on some (or all) individuals. On ACS, all networks achieve between 75–80% train and test accuracy. We
perform the identical procedure for the Enrollment data set, where the networks all achieve between 70–75%
train and test accuracy (due to less data being available).

20

Let (fi , gi ), for i = 1, . . . , 15, be the classifiers corresponding to a given pair of networks trained from the
same initialization but with different noise due to SGD. To test the stability of the ranking functions rUA and
τ
ropt
, for each pair (fi , gi ), we randomly select 30 individuals from the test set as the data set of individuals x,
τ
and obtain the (probabilistic) predictions P = fi (x) and P ′ = gi (x). We then run ropt
and rUA on these
two prediction matrices, logging the deviations of the rankings and the resulting value of ∥P − P ′ ∥1 . For
each pair of networks (fi , gi ), we repeat this procedure 10 times with different randomly selected subsets of
individuals. In Table 1, we report the average and standard deviation of this experiment.
Quantity

ACS

Enrollment

∥rUA (P ) − rUA (P ′ )∥∞
τ
τ
∥ropt
(P ) − ropt
(P ′ )∥∞
′
∥P − P ∥1

0.011 ± 0.002
0.947 ± 0.224
0.971 ± 0.582

0.012 ± 0.002
0.680 ± 0.466
0.653 ± 0.453

Table 1: Measured stability over 30 neural network training runs (15 pairs of networks) for 10 data sets of
n = 30 individuals each. ∞-norm of UA deviation being bounded above by ∥P − P ′ ∥1 confirms stability of UA
τ
(Theorem 11). Instability of the ranking ropt
is also demonstrated (Proposition 18).
The results in Table 1 demonstrate that ∥P − P ′ ∥1 dominates the value of ∥rUA (P ) − rUA (P ′ )∥∞ ; this
behavior persisted through all of our many training runs. We conclude that UA rankings are extremely stable
in the face of noise introduced during the learning of a predictor (drastically surpassing our 1-stability bound).
τ
Our results also confirm that instability of the optimal ranking function ropt
is not only a theoretical possibility,
τ
τ
but prevalent when working with real data. To see this, notice that the mean value of ∥ropt
(P ) − ropt
(P ′ )∥∞
is two orders of magnitude larger than for rUA . For the Enrollment data set, it even exceeds the mean
value of ∥P − P ′ ∥1 , implying that γ ≤ 1 is impossible. For the ACS data set, the norms are very comparable,
meaning that γ ≪ 1 is impossible; in fact, consistent with the large standard deviations, there are multiple
instances illustrating that γ must be significantly larger than 1 for both data sets.

6.2

Utility

Next, we measure the utility attained by the different ranking functions. The utility map τ for both data
sets is the same one as in Section 6.1. In addition to the two rankings functions of primary interest, we
also consider the following two baselines: (1) runif , the ranking function which places the n individuals in
uniformly random order; and (2) rPL , the Plackett-Luce (PL) ranking defined by Luce’s axiom (Luce, 1959;
Plackett, 1975).
The PL model, similar to UA, defines a distribution over rankings. At a high level, in each iteration
i, the item for the ith position is sampled based on a softmax mapping of all remaining items’ relevance
scores. More precisely, in each iteration t, let Mt be the set of individuals not yet placed in the ranking, with
M1 = [n] in the first iteration. Then, in each iteration t = 1, . . . , n, individual i ∈ Mt is placed in position t
with probability
exp (τ (pi ))
P[Ri,t ] = P
.
j∈Mt exp (τ (pj ))
To efficiently compute the PL ranking, we use the (now standard) Gumbel trick from Bruch et al. (2020).
That is, to sample one ranking from the PL ranking distribution rPL (P ), we sort the individuals in decreasing
order of τ (pi ) + γi , where each γi ∼ Gumbel(0, 1) independently. We average over 100k samples from the PL
ranking distribution to compute rPL (P ).
To measure utility, we use the DCG position weights wk = 1/ log2 (k + 1). In order to make the scales
of the utilities more meaningful in our comparisons, we normalize all utilities to lie in [0, 1]. Thereto, let
τ
rmin
be the worst-utility ranking, obtained by ordering the individuals by increasing relevance score (i.e.,
the individual of lowest utility is deterministically placed first). For a ranking function r, we compute the

21

normalized utility score as follows:
Ũ (P, r) =

τ
r(P ) − rmin
(P )
τ (P ) − r τ (P ) .
ropt
min

In Table 2, we report the mean and standard deviation over 30 neural network training runs of the
normalized utility Ũ for each of the ranking functions discussed above. For each neural network and associated
prediction function f , we randomly sample n = 20, 40, and 60 individuals from the test set. Then, we
construct the prediction matrix P = (f (xi ))i∈[n] , and report Ũ (P, r) for each ranking function r. We find
that UA ranking outperforms the uniform and PL ranking in each experimental instance. However, UA
ranking is not guaranteed to always outperform the uniform ranking. One can carefully construct instances
in which the “safe bet” individual provides more utility than an individual who has a low probability of being
a “moonshot” candidate (further discussed in Singh et al. (2021)). Such an instance crucially depends on the
specific choice of τ .
n = 20

n = 40

n = 60

rUA
rPL
runif

0.726 ± 0.027
0.616 ± 0.038
0.540 ± 0.043

0.724 ± 0.027
0.621 ± 0.028
0.548 ± 0.029

0.727 ± 0.020
0.624 ± 0.023
0.550 ± 0.030

rUA
rPL
runif

0.852 ± 0.030
0.755 ± 0.041
0.552 ± 0.052

0.860 ± 0.023
0.767 ± 0.027
0.561 ± 0.037

0.857 ± 0.018
0.767 ± 0.025
0.562 ± 0.033

Table 2: Normalized utility achieved by rUA , runif , and rPL for n = 20, 40, and 60 random individuals from
the test set of ACS (top 3 rows) and Enrollment (bottom 3 rows). Mean/std taken across 30 neural network
training runs. UA outperforms the uniform and PL ranking.

7

Conclusions and Future Work

Stability of ranking functions is a natural desideratum to prevent large deviations arising in rankings from
noise in learned classifiers; combined with individually fair predictions, it results in fair rankings. Stability
is achieved by the natural Uncertainty Aware Ranking Functions, which also preserve multigroup fairness
guarantees of their underlying classifiers.
An interesting direction for future work is to find a more general sufficient condition for ranking functions
which allows them to inherit properties of multiaccurate/multicalibrated predictors. In the proof of Theorem 16,
we crucially make use of the fact that an individual i, when competing against a sampled dataset, can be
thought of as competing against n − 1 other individuals sampled from a single conditional distribution. We
call this property individual interpolation, since it allows for the linear properties of multiaccuracy and
multicalibration (Noarov and Roth, 2023) to compose with the ranking function. It would be desirable to
characterize which other ranking functions satisfy individual interpolation, and whether the property is
necessary for guarantees in the vein of Theorem 16.
Another important extension is to consider correlations between the sampled labels of different individuals,
and whether analogous individual/group fairness guarantees can still be provided in this case. In practice,
predictors used in rankings in applications such as LinkedIn are trained using highly correlated data due to
consumer click-through behavior (DiCiccio et al., 2023). Investigating ways to train group-fair predictors (in
the multiaccurate sense) while only relying on non-i.i.d. examples, and furthermore applying these predictors
in rankings with correlated merit distributions, is an important avenue for future work.

22

Acknowledgements
SD was supported by the Department of Defense (DOD) through the National Defense Science & Engineering
Graduate (NDSEG) Fellowship Program. This work was also funded in part by NSF awards #1916153,
#2333448, #1956435, #1943584, #2344925, and #2239265, and an Amazon Research Award.

References
A. N. Angelopoulos and S. Bates. A gentle introduction to conformal prediction and distribution-free
uncertainty quantification. Preprint, 2021. 1
A. Asudeh, H. V. Jagadish, G. Miklau, and J. Stoyanovich. On obtaining stable rankings. In Proc. 44th Intl.
Conf. on Very Large Data Bases, volume 12, pages 237–250, 2018. 3, 5
P. Awasthi, M. Kleindessner, and J. Morgenstern. Equalized odds postprocessing under imperfect group
information. In Proc. 23rd Intl. Conf. on Artificial Intelligence and Statistics, pages 1770–1780. PMLR,
2020. 2, 13
O. Bastani, V. Gupta, C. Jung, G. Noarov, R. Ramalingam, and A. Roth. Practical adversarial multivalid
conformal prediction. In Proc. 36th Advances in Neural Information Processing Systems, 2022. 1
S. Bruch, S. Han, M. Bendersky, and M. Najork. A stochastic treatment of learning to rank scoring functions.
In Proc. 13th ACM Intl. Conf. on Web Search and Data Mining, pages 61–69, 2020. 5, 21
R. Busa-Fekete, B. Kégl, T. Éltetö, and G. Szarvas. Ranking by calibrated adaboost. In Proceedings of the
Learning to Rank Challenge, pages 37–48. PMLR, 2011. 5
Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise
approach. In Proc. 24th Intl. Conf. on Machine Learning, pages 129–136, 2007. 6
S. Caton and C. Haas. Fairness in machine learning: A survey. ACM Computing Surveys, 2020. 2
D. Cohen, B. Mitra, O. Lesota, N. Rekabsaz, and C. Eickhoff. Not all relevance scores are equal: Efficient
uncertainty and calibration modeling for deep retrieval models. In Proc. 44th Intl. Conf. on Research and
Development in Information Retrieval (SIGIR), pages 654–664, 2021. 4
A. F. Cooper, K. Lee, S. Barocas, C. De Sa, S. Sen, and B. Zhang. Is my prediction arbitrary? Measuring
self-consistency in fair classification. In Proc. 38th AAAI Conf. on Artificial Intelligence, 2024. 2, 7
S. Devic, D. Kempe, V. Sharan, and A. Korolova. Fairness in matching under uncertainty. In Proc. 40th Intl.
Conf. on Machine Learning, volume 202, pages 7775–7794, 2023. 3, 4, 9, 10, 20
C. DiCiccio, B. Hsu, Y. Yu, P. Nandy, and K. Basu. Detection and mitigation of algorithmic bias via predictive
parity. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages
1801–1816, 2023. 5, 22
F. Ding, M. Hardt, J. Miller, and L. Schmidt. Retiring adult: New datasets for fair machine learning. In
Proc. 35th Advances in Neural Information Processing Systems, 2021. 20
C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proc. 3rd
Innovations in Theoretical Computer Science, pages 214–226, 2012. 1, 2, 7
C. Dwork, M. P. Kim, O. Reingold, G. N. Rothblum, and G. Yona. Learning from outcomes: Evidence-based
rankings. In Proc. 60th IEEE Symp. on Foundations of Computer Science, pages 106–125. IEEE, 2019. 4

23

P. Ganesh, H. Chang, M. Strobel, and R. Shokri. On the impact of machine learning randomness on group
fairness. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages
1789–1800, 2023. 2
D. Garcı́a-Soriano and F. Bonchi. Maxmin-fair ranking: individual fairness under group-fairness constraints.
In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages
436–446, 2021. 4
S. C. Geyik, S. Ambler, and K. Kenthapadi. Fairness-aware ranking in search & recommendation systems
with application to LinkedIn talent search. In Proc. 25th Intl. Conf. on Knowledge Discovery and Data
Mining, pages 2221–2231, 2019. 1, 6
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Y. Bengio
and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, 2015. URL http://arxiv.org/abs/1412.6572. 7
Google. Ads help: About ad rank, 2023. URL https://support.google.com/google-ads/answer/
1722122?hl=en. Accessed: 2023-09-28. 1
P. Gopalan, M. P. Kim, M. A. Singhal, and S. Zhao. Low-degree multicalibration. In Proc. 35th Conference
on Learning Theory, pages 3193–3234. PMLR, 2022. 13, 16
S. Gorantla, A. Mehrotra, A. Deshpande, and A. Louis. Sampling individually-fair rankings that are always
group fair. In F. Rossi, S. Das, J. Davis, K. Firth-Butterfield, and A. John, editors, Proceedings of the
2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2023, pages 205–216. ACM, 2023. 4
J. Guiver and E. Snelson. Learning to rank with softrank and gaussian processes. In Proc. 31st Intl. Conf.
on Research and Development in Information Retrieval (SIGIR), pages 259–266, 2008. 4
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In Proc. 34th
Intl. Conf. on Machine Learning, pages 1321–1330. PMLR, 2017. 1
R. Guo, J.-F. Ton, and Y. Liu. Fair learning to rank with distribution-free risk control. Preprint, 2023. 4
C. Gupta and A. Ramdas. Top-label calibration and multiclass-to-binary reductions. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=WqoBaaPHS-. 1
P. A. Gutiérrez, M. Perez-Ortiz, J. Sanchez-Monedero, F. Fernandez-Navarro, and C. Hervas-Martinez.
Ordinal regression methods: survey and experimental study. IEEE Transactions on Knowledge and Data
Engineering, 28(1):127–146, 2015.
N. Haghtalab, M. I. Jordan, and E. Zhao. A unifying perspective on multi-calibration: Unleashing game
dynamics for multi-objective learning. Proc. 37th Advances in Neural Information Processing Systems, 36,
2023. 13
M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. Proc. 30th Advances in
Neural Information Processing Systems, 29, 2016. 2, 13
F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell.
Syst., 2015. 20
U. Hébert-Johnson, M. Kim, O. Reingold, and G. Rothblum. Multicalibration: Calibration for the
(computationally-identifiable) masses. In Proc. 35th Intl. Conf. on Machine Learning, pages 1939–1948.
PMLR, 2018. 3, 12, 13, 16

24

M. Heuss, D. Cohen, M. Mansoury, M. d. Rijke, and C. Eickhoff. Predictive uncertainty-based bias mitigation
in ranking. In Proceedings of the 32nd ACM International Conference on Information and Knowledge
Management, pages 762–772, 2023. 4
K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on
Information Systems (TOIS), 20(4):422–446, 2002. 16
C. Jung, G. Noarov, R. Ramalingam, and A. Roth. Batch multivalid conformal prediction. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023.
1
M. P. Kim, A. Ghorbani, and J. Zou. Multiaccuracy: Black-box post-processing for fairness in classification.
In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247–254, 2019. 3, 4,
12, 13
H. Korevaar, C. McConnell, E. Tong, E. Brinkman, A. Shine, M. Abbas, B. Metevier, S. Corbett-Davies, and
K. El-Arini. Matched pair calibration for ranking fairness. arXiv preprint arXiv:2306.03775, 2023. 5
W. Kweon, S. Kang, and H. Yu. Obtaining calibrated probabilities with personalized ranking models. In
Proc. 36th AAAI Conf. on Artificial Intelligence, volume 36, pages 4083–4091, 2022. 5
R. D. Luce. Individual choice behavior. Courier Corporation, 1959. 21
M. V. Martins, D. Tolledo, J. Machado, L. M. Baptista, and V. Realinho. Early prediction of student’s
performance in higher education: A case study. In Trends and Applications in Information Systems and
Technologies: Volume 1 9, pages 166–175. Springer, 2021. 20
A. Mehrotra and L. E. Celis. Mitigating bias in set selection with noisy protected attributes. In Proceedings
of the 2021 ACM conference on fairness, accountability, and transparency, pages 237–248, 2021. 4
A. Mehrotra and N. Vishnoi. Fair ranking with noisy protected attributes. Proc. 36th Advances in Neural
Information Processing Systems, 35:31711–31725, 2022. 4
A. K. Menon, X. J. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabilities with
a ranking loss. In Proc. 29th Intl. Conf. on Machine Learning, volume 2012, page 703, 2012. 4
Meta. Our approach to facebook feed ranking, 2023. URL https://transparency.fb.com/features/
ranking-and-content/. Accessed: 2023-09-28. 1
M. Minderer, J. Djolonga, R. Romijnders, F. Hubis, X. Zhai, N. Houlsby, D. Tran, and M. Lucic. Revisiting
the calibration of modern neural networks. Proc. 35th Advances in Neural Information Processing Systems,
34:15682–15694, 2021. 1
H. Narasimhan, A. Cotter, M. Gupta, and S. Wang. Pairwise fairness for ranking and regression. In Proc.
34th AAAI Conf. on Artificial Intelligence, volume 34, pages 5248–5255, 2020. 5
D. F. Nettleton, A. Orriols-Puig, and A. Fornells. A study of the effect of different types of noise on the
precision of supervised learning techniques. Artificial intelligence review, 33:275–306, 2010. 2
G. Noarov and A. Roth. The statistical scope of multicalibration. In Proc. 40th Intl. Conf. on Machine
Learning, volume 202, pages 26283–26310, 2023. 22
S. Oh, B. Ustun, J. McAuley, and S. Kumar. Rank list sensitivity of recommender systems to interaction
perturbations. In Proceedings of the 31st ACM International Conference on Information & Knowledge
Management, pages 1584–1594, 2022. 5
S. Oh, B. Ustun, J. McAuley, and S. Kumar. Finest: Stabilizing recommendations by rank-preserving
fine-tuning. arXiv preprint arXiv:2402.03481, 2024. 5
25

G. Penha and C. Hauff. On the calibration and uncertainty of neural learning to rank models for conversational
search. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, pages 160–170, 2021. 4
E. Pitoura, K. Stefanidis, and G. Koutrika. Fairness in rankings and recommendations: an overview. In Proc.
48th Intl. Conf. on Very Large Data Bases, 2022. 18
R. L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied
Statistics, 24(2):193–202, 1975. 21
R. Rastogi and T. Joachims. Fair ranking under disparate uncertainty. Preprint, 2023. 4
S. E. Robertson. The probability ranking principle in ir. Journal of documentation, 33(4):294–304, 1977. 8
E. Shabat, L. Cohen, and Y. Mansour. Sample complexity of uniform convergence for multicalibration. Proc.
34th Advances in Neural Information Processing Systems, 33:13331–13340, 2020. 16
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge
university press, 2014. 8, 12, 16
Z. Shen, Z. Wang, X. Zhu, B. Fain, and K. Munagala. Fairness in the assignment problem with uncertain
priorities. In Proc. 22nd Intl. Conf. on Autonomous Agents and Multiagent Systems, page 188–196, 2023. 4
A. Singh and T. Joachims. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining, pages 2219–2228, 2018. 2
A. Singh and T. Joachims. Policy learning for fairness in ranking. Proc. 33rd Advances in Neural Information
Processing Systems, 32, 2019. 2, 18
A. Singh, D. Kempe, and T. Joachims. Fairness in ranking under uncertainty. In Proc. 35th Advances in
Neural Information Processing Systems, pages 11896–11908, 2021. 1, 3, 4, 7, 9, 10, 18, 20, 22
M. A. Soliman and I. F. Ilyas. Ranking with uncertain scores. In 2009 IEEE 25th international conference
on data engineering, pages 317–328. IEEE, 2009. 4
A. Tahir, L. Cheng, and H. Liu. Fairness through aleatoric uncertainty. Preprint, 2023. 4
B. Tang, Ç. Koçyiğit, E. Rice, and P. Vayanos. Learning optimal and fair policies for online allocation of
scarce societal resources from data collected in deployment. arXiv preprint arXiv:2311.13765, 2023. 4
M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In Proc.
1st ACM Intl. Conf. on Web Search and Data Mining, pages 77–86, 2008. 16
TurboHire. Unleashing the power of ai & automation to effortlessly discover the best talent, 2023. URL
https://turbohire.co/features/talent-screening/#candidate-scoring. Accessed: 2023-10-09. 1
C.-J. Wang and H.-H. Chen. Learning to predict the cost-per-click for your ad words. In Proceedings of the
21st ACM international conference on Information and knowledge management, pages 2291–2294, 2012. 2
J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. In Proc. 30th Intl. Conf. on
Research and Development in Information Retrieval (SIGIR), pages 391–398, 2007. 6
L. Yan, Z. Qin, X. Wang, M. Bendersky, and M. Najork. Scale calibration of deep ranking models. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages
4300–4309, 2022. 5
T. Yang, C. Luo, H. Lu, P. Gupta, B. Yin, and Q. Ai. Can clicks be both labels and features? unbiased
behavior feature collection and uncertainty-aware learning to rank. In Proc. 45th Intl. Conf. on Research
and Development in Information Retrieval (SIGIR), pages 6–17, 2022. 4
26

T. Yang, Z. Xu, Z. Wang, A. Tran, and Q. Ai. Marginal-certainty-aware fair ranking algorithm. In Proc.
16th ACM Intl. Conf. on Web Search and Data Mining, pages 24–32, 2023. 4
M. Zehlike, K. Yang, and J. Stoyanovich. Fairness in ranking: A survey. Preprint, 2021. URL https:
//arxiv.org/abs/2103.14000. 4
A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned
language models. Preprint, 2023. 7

27

A

Proof of Key Lemma for Uncertainty Aware Ranking

The proof of Lemma 12 uses the following lemma, bounding the total variation distance of sums of random
variables in terms of the total variation distances of the individual variables.
Lemma
23. Let
and X =
Pn
Pn Xi ∼ pi , Yi ∼ qi for i = 1, . . . , n be independent categorical random variables,
Pn
X
,
Y
Y
=
.
Let
p,
q
be
the
respective
distributions
of
X,
Y
.
Then,
d
(p,
q)
≤
d
TV
i=1 i
i=1 i
i=1 TV (pi , qi ).
Proof. Consider a maximal coupling between each Xi and the corresponding Yi . By the Coupling Lemma,
we then have that P[Xi ̸= Yi ] = dTV (pi , qi ), and dTV (p, q) ≤ P[X =
̸ Y ]. Now, by a union bound over all i,
we obtain that
X
X
dTV (p, q) ≤ P[X ̸= Y ] ≤
P[Xi ̸= Yi ] =
dTV (pi , qi ),
i

i

completing the proof.
Proof of Lemma 12. First, by Equation (2) in the first part of Proposition 9,

P[Ri,k | λi = ℓ] =

n−1
X

1
ℓ
>ℓ
· P[N−i
< k].
= j and k − (j + 1) ≤ N−i
j
+
1
j=0

Let

ℓ
>ℓ
ℓ
>ℓ
B = j | PP [N−i
= j and k − (j + 1) ≤ N−i
< k] ≥ PQ [N−i
= j and k − (j + 1) ≤ N−i
< k] ;
note that B is not a random variable, but simply determined by the distributions P, Q.
We substitute the characterization (2) for both P and Q, and use the triangle inequality as well as the
1
fact that j+1
≤ 1, to give us that

PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ]
≤

n−1
X

ℓ
>ℓ
ℓ
>ℓ
PP [N−i
= j and k − (j + 1) ≤ N−i
< k] − PQ [N−i
= j and k − (j + 1) ≤ N−i
< k]

j=0

=

n−1
X

ℓ
>ℓ
ℓ
>ℓ
PP [N−i
= j and k − (j + 1) ≤ N−i
< k] − PQ [N−i
= j and k − (j + 1) ≤ N−i
< k]



j=0,j∈B

+

n−1
X

ℓ
>ℓ
ℓ
>ℓ
PQ [N−i
= j and k − (j + 1) ≤ N−i
< k] − PP [N−i
= j and k − (j + 1) ≤ N−i
< k]



j=0,j ∈B
/
ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
PP [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k] − PQ [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k]

ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
+ PQ [N−i ∈
/ B and k − (N−i + 1) ≤ N−i < k] − PP [N−i ∈
/ B and k − (N−i + 1) ≤ N−i
< k]
ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
= PP [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k] − PQ [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k]
ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
+ PP [N−i ∈
/ B and k − (N−i + 1) ≤ N−i < k] − PQ [N−i ∈
/ B and k − (N−i + 1) ≤ N−i
< k] .



=

(8)
>ℓ
ℓ
Consider the (vector-valued) random variable (N−i
, N−i
), and let µ, ν denote its distribution under P, Q,
respectively.
>ℓ
>ℓ
ℓ
ℓ
ℓ
ℓ
Because [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k] and [N−i
∈
/ B and k − (N−i
+ 1) ≤ N−i
< k] are events
that can be expressed in terms of this random variable, the definition of total variation distance implies that
ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
PP [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k] − PQ [N−i
∈ B and k − (N−i
+ 1) ≤ N−i
< k] ≤ dTV (µ, ν),
ℓ
ℓ
>ℓ
ℓ
ℓ
>ℓ
PP [N−i ∈/ B and k − (N−i + 1) ≤ N−i < k] − PQ [N−i ∈/ B and k − (N−i + 1) ≤ N−i < k] ≤ dTV (µ, ν).

28

′
To bound dTV (µ, ν), associate with
P each individual i ̸= i the 2-dimensional (random) vector vi′ =
>ℓ
ℓ
(1λi′ >ℓ , 1λi′ =ℓ ). Then, (N−i , N−i ) = i′ ̸=i vi′ .
For a fixed i′ ̸= i, consider the distribution of vi′ under pi′ and qi′ . The total variation distance between
these distributions is at most dTV (pi′ , qi′ ), because
the vectors can differ only when the labels of i′ differ. By
P
Lemma 23, we thus obtain that dTV (µ, ν) ≤ i′ ̸=i dTV (pi′ , qi′ ).
Substituting this bound back into (8), we now obtain that
X
PrUA (P ) [Ri,k | λi = ℓ] − PrUA (Q) [Ri,k | λi = ℓ] ≤ 2 ·
dTV (pi′ , qi′ ),
i′ ̸=i

completing the proof.

29

