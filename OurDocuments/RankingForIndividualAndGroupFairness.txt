Ranking for Individual and Group Fairness Simultaneously
Sruthi Gorantla⋆ , Amit Deshpande† , and Anand Louis⋆

arXiv:2010.06986v1 [cs.IR] 24 Sep 2020

⋆

Indian Institute of Science, Bangalore, India. {gorantlas, anandl}@iisc.ac.in
†
Microsoft Research, Bangalore, India. amitdesh@microsoft.com

Abstract
Search and recommendation systems, such as search engines, recruiting tools, online marketplaces, news,
and social media, output ranked lists of content, products, and sometimes, people. Credit ratings, standardized
tests, risk assessments output only a score, but are also used implicitly for ranking. Bias in such ranking systems,
especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce
stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as
favoring group-fair outcomes over meritocracy.
In this paper, we study a trade-off between individual fairness and group fairness in ranking. We define
individual fairness based on how close the predicted rank of each item is to its true rank, and prove a lower
bound on the trade-off achievable for simultaneous individual and group fairness in ranking. We give a fair
ranking algorithm that takes any given ranking and outputs another ranking with simultaneous individual and
group fairness guarantees comparable to the lower bound we prove. Our algorithm can be used to both preprocess training data as well as post-process the output of existing ranking algorithms. Our experimental results
show that our algorithm performs better than the state-of-the-art fair learning to rank and fair post-processing
baselines.

1 Introduction
Search, and recommendation systems have revolutionized the way we consume an overwhelming amount of data
and find relevant information quickly [BP98, AT05]. They help us find relevant documents, news, media, people,
places, products and rank them based on our interests and intent [KLH16, PZZ+ 19]. Examples of these include
rankings and recommendations in online marketplaces, recruitments, college admissions, news, and social media.
Rankings can also be implicit when systems only screen or rate people, products, places as well as the social and
economic exchange of goods, money, information, while a downstream application uses their scores or ratings for
ranking. Credit ratings, standardized test scores, health risk assessment scores are some common examples of the
above.
Information presented through ranked lists influences our worldview [Par11, Tav20]. Rankings not only influence
the users who consume them but also act as vehicles of opportunities for the items being ranked. Biased ranking
of news, people, products raises ethical concerns and can potentially cause long-term economic and societal harm
to demographics and businesses [Nob18]. Many state-of-the-art rankings that maximize utility or relevance reflect
existing societal biases and are often oblivious to the societal harm they may cause by amplifying such biases.
When these systems amplify societal biases observed in their training data, they worsen social and economic
inequalities, polarize opinions, and reinforce stereotypes [O’N16].
1

In addition to ethical concerns, there are also legal obligations to remove bias. Disparate impact laws prohibit even
unintentional but biased outcomes in employment, housing, and many other areas if one group of people belonging
to a protected group is adversely affected compared to another [BS16]. Protected groups could vary for specific
statutes and include race, gender, age, religion, national origin, etc. Group-fairness in machine learning literature
has focused on outcome-based or proportion-based definitions of fairness (e.g., demographic parity, equality of
opportunity) in classification, ranking, and selection problems [HPS16, BHN19].
Another notion of fairness studied in fairness literature is individual fairness. Achieving individual fairness in
classification often means similar predictions for two similar individuals or two similar data points in terms of
their features or risks [DHP+ 12, CDPF+ 17]. Group-fairness is a desirable goal but arbitrary corrections to achieve
group-fairness can cause further harm if they are perceived as individually unfair [Cro04]. In classification as
well as ranking, if we consider individual fairness on average, then it is closely tied to the overall accuracy or
relevance. However, if we remove the aggregation or averaging to focus on the parts where individual-fairness
really matters, it is not always the same as accuracy or relevance. Recent work has pointed out these subtleties
between group-fairness and individual-fairness in both classification and ranking [BGW18, Bin20, KRW17].

Fairness in ranking. Fairness in ranking has three broad requirements: sufficient presence of items belonging
to different groups, consistent treatment of similar individuals or individual fairness, and proper representation to
avoid representational harm to members of protected groups [Cas19a]. The first and the third requirements are
about group-fairness, whereas the second requirement is about individual-fairness. For example, diversity alone
in top ranks satisfies sufficient presence for the user who consumes the ranking but need not provide consistent
treatment and proper representation in the way items are ranked. Fair ranking algorithms can be divided into
two categories. First, re-ranking algorithms that modify a given ranking of high utility to incorporate fairness
constraints while trying to preserve the utility. Second, learning-to-rank algorithms that incorporate fairness and
utility objectives into learning a ranker from training data. Re-ranking can be used to post-process the prediction
of any given ranker as well as pre-process the training data of any given ranker. We survey previous work on fair
ranking, with the above distinction in mind.
Fair ranking can be framed as an integer optimization problem [CSV18]. Given a set of n items along with their
group memberships (where a single item can belong to multiple groups), and a matrix W whose entries Wij indicate the utility of assigning rank j to item i, the objective is to maximize the total utility of rank assignments while
satisfying the given group-fairness constraints for the top k positions. They consider group-fairness constraints as
lower and upper bounds on the group-wise utilities in the top k positions and allow such constraints for all values
of k. For W matrices corresponding to most practical utility metrics, e.g., Discounted Cumulative Gain (DCG), a
greedy assignment of the highest valued item available at each rank maximizes the total utility. If ∆ is the maximum number of groups an item belongs to, then a fair and greedy re-ranking gives (∆ + 2)-approximation to the
group-fair ranking of maximum utility [CSV18].
The fair top-k selection problem gives another formulation of fair re-ranking [ZBC+ 17]. For a given k ≪ n, and
list of n items with a numerical quality value assigned to each item, the objective of fair top-k selection problem
is to select k items to maximize utility while ensuring a minimum proportion from a protected group in the topl ranks, for all l ⩽ k. The authors divide utility into two objectives, selection and ordering. Selection utility
quantifies if every candidate in the top-k is more qualified than the rest, and ordering utility quantifies if every pair
in the top-k is ranked according to their numerical quality values. They give an efficient algorithm called FA*IR to
solve the fair top-k selection problem by re-ranking a given true or color-blind ranking, which orders all the items
by their numerical quality values.
Fair ranking problem can also be defined in the learning-to-rank (LTR) setting, where a model is trained to maxi2

mize utility subject to fairness constraints. In LTR setting, the ranking is probabilistic, and the fairness guarantees
are often on average. Given a query-document pair, the probability of each document being ranked at top-1 is called
its exposure. ListNet is a neural network model trained to rank a list of documents by minimizing a loss function
based on their true and predicted exposure [CQL+ 07]. Building upon this, DELTR [ZC20] learns fair ranking via a
multi-objective optimization that maximizes utility and minimizes disparate exposure for different groups of items
for group-fairness or different items for individual-fairness. This general learning-to-rank framework facilitates
optimizing multiple utility metrics while satisfying equal exposure, and Fair-PG-LTR [SJ19] learns a ranking that
satisfies fairness of exposure. Experimental results on the above fair LTR algorithms give better fairness and utility
both when compared to post-processing algorithms on real-world datasets, e.g., Yahoo! LTR and GermanCredit
data.
There is related work on defining and maximizing various group-fairness metrics overall top-l prefixes of the top-k
ranks [YS17], for a given k, using an optimization algorithm to learn fair representations [ZWS+ 13]. There are
also other measures of group-fairness in ranking based on pairwise comparisons [NCGW20, BCD+ 19]. Recent
work has also studied fairness-aware ranking in search and recommendations for real-world recruitment tools using fairness metrics based on skew in the top-k and Normalized Discounted KL-divergence (NDKL) divergence
[GAK19]. Intersectional fairness, where the items belong to more than one group, and counterfactually fair ranking, measured for their group fairness (demographic parity at top-k, equal opportunity at top-k) and ranking utility
(utility loss at top-k, average precision at top-k) are studied in [YLS20].
To the best of our knowledge, all existing fair ranking algorithms guarantee group fairness but can provide only an
aggregate guarantee for individual fairness. They study trade-offs between group fairness and utility of fair rankings but do not provide guarantees for the worst-case individual fairness. In this work, we address this gap in the
fair ranking literature. Our group-fairness definition ensures sufficient presence of all groups, similar to previous
work, but we give a new, natural definition of individual-fairness. Our main contributions can be summarized as
follows.
• We define individual-fairness based on the worst-case deviation of re-ranking from the true merit-based (or
color-blind) ranking for the top-k items, for any k. This directly captures the loss of visibility suffered by
items of high merit that may get ranked lower in order to achieve high group-fairness. We prove a lower
bound on the trade-off achievable between individual-fairness and group-fairness simultaneously.
• We propose a Fair Individual and Group-fair Ranking (FIGR) algorithm that takes a given merit-based (or
color-blind) ranking and outputs another ranking with simultaneous individual and group-fairness guarantees, comparable to the lower bound mentioned above. Our algorithm can be used to both pre-process the
training data as well as post-process the output of the existing ranking algorithms.
• We do extensive experiments to show that our algorithm performs better than the state-of-the-art fair LTR
and fair post-processing baselines on standard real-world datasets such as COMPAS recidivism, German
credit risk, and ChileSAT used in fair ranking literature.

2 Individual and Group Fair Rankings
In the rest of this paper, we say that rank i is “lower” than rank j if i < j, and we say that rank i is “higher” than
rank j if i > j.

3

We now formally define the notion of group fairness; this definition is similar to the notions studied in the literature
[Cas19a, CSV18] .
Definition 2.1 (Group Fairness). A ranking is said to satisfy (α, k) group fairness if any k consecutive ranks have
at most αk items from any group.
This notion of group fairness has the desirable property that even if a few low ranked items are removed from the
ranking, the remaining ranking still satisfies the group fairness conditions. In case the given “true ranking” of the
items doesn’t already satisfy group fairness conditions, the re-ranking algorithms rearrange the items in the true
ranking such that the group fairness conditions are satisfied. Using the notion of individual fairness, we would like
to capture how much an item has been displaced from its true rank during re-ranking for group fairness.
Definition 2.2 (Individual Fairness). A ranking is said to satisfy α individual fairness if the rank of each item is at
most 1/α times its true rank.
We remark that unless the true ranking satisfies the group fairness conditions, some items with high merit must
suffer a loss of visibility during the process of re-ranking for group fairness. That is, the output group fair ranking
has strictly less than 1 individual fairness. This manifests the trade-off between the group fairness and the individual fairness in ranking. We also note that a true ranking is not always available for the real-world datasets. In our
experiments, we use some natural substitutes for the true ranking; see Section 3 for details.
Closely related to individual fairness is the well studied notion of PRECISION@K of ranking [JK00, MRS08,
ZC20]. For a given ranking, PRECISION@K is defined as the number of items in the top K ranks of the true
ranking which also appear in the top K ranks of the given ranking. We get the following relation between individual
fairness and PRECISION@K.
Corollary 2.3. A ranking satisfying α individual fairness also has PRECISION@K at least ⌊αK⌋, ∀K ∈ Z+ .
Proof. Fix a ranking having α individual fairness. By definition, the top ⌊αK⌋ items in the true ranking get
displaced at most to the rank ⌊αK⌋ /α ⩽ K. Hence, at least the top ⌊αK⌋ items in the true ranking are also in the
top K ranks in an α individually fair ranking. Therefore, PRECISION@K is at least ⌊αK⌋.
Our first main result is a lower bound on the trade-off achievable for simultaneous individual and group fairness in
ranking.
Theorem 2.4. Fix α ∈ [1/2, 1] ∩ Q and k ∈ Z+ . For every n0 ∈ Z+ , there exists an n such that n ⩾ n0 , and
there exists a true ranking of 2n items grouped into two groups of n items each, such that the following holds. Any
ranking satisfying α individual fairness (w.r.t. the true ranking) and (β, k) group fairness in the first n/α ranks
must have β ⩾ α.
Proof. Let α = a/b where a, b ∈ Z+ . Set n to be any integer multiple ak such that n ⩾ n0 . Consider a true
ranking where all the n items from group 1 are placed in first n ranks followed by the n items from group 2.
Now, consider any ranking of these items satisfying α individual fairness and (β, k) group fairness in the first n/α
ranks. Observe that by our choice of parameters, n/α is an integer. By the definition of individual fairness, we
get that the first n/α ranks must contain all the n items from group 1. Since the ranking satisfies (β, k) group
fairness, any k consecutive ranks have at most βk items from group 1. This implies that for any c ∈ Z+ , any
consecutive ck ranks have at most βck items from group 1. By our choice of parameters, n/(αk) is an integer; let
4

Algorithm 1: FIGR Algorithm
Input: A ranking
of the

 N items and parameters α, k satisfying the conditions in Theorem 2.5.
1
2
;
1 Set ǫ := k 1 +
α− 1
ℓ

for i = ⌈N/ ⌊α ⌊ǫk/2⌋⌋⌉ down to 1 do
3
for j = 1 to min{⌊α ⌊ǫk/2⌋⌋ , N − (i − 1) ⌊α ⌊ǫk/2⌋⌋} do
4
Move item at rank (i − 1) ⌊α ⌊ǫk/2⌋⌋ + j to rank (i − 1) ⌊ǫk/2⌋ + j.
5
end
6 end
7 for i = 1 to ⌈N/ ⌊α ⌊ǫk/2⌋⌋⌉ do
8
for j = (i − 1) ⌊ǫk/2⌋ + 1 to i ⌊ǫk/2⌋ do
9
if rank j is unoccupied then
10
move the first item at a rank higher than j that can be moved to rank j without violating the
(α, ⌊ǫk/2⌋) group fairness constraints among the items in ranks (i − 1) ⌊ǫk/2⌋ + 1 to i ⌊ǫk/2⌋,
if any such element is available.
11
end
12
end
13 end
14 for i = 1 to N do
15
if rank i is unoccupied then
16
move to rank i, the first item at rank higher than i .
17
end
18 end
19 Output final ranking;
2

c = n/(αk). Therefore, the first ck = n/α ranks contain at most βck = βn/α items from group 1. If β < α, then
first n/α ranks contain strictly less than n elements from group 1, which is a contradiction. Therefore, we must
have β ⩾ α.
Our next main result is a fair ranking algorithm that takes any given ranking and outputs another ranking with
individual and group fairness guarantees comparable to that of Theorem 2.4.
Theorem 2.5. Given a true ranking of N items grouped into ℓ disjoint groups, with each group having at least n
items, and fairness parameters α ∈ (1/ℓ, 1] and k ∈ Z+ , there exists a polynomial time algorithm to compute a
ranking satisfying


1. α − 
2.

1
1+

1
α− 1
ℓ



  individual fairness.

 

 
j
k
α 1 + k2 1 + α−1 1
, k group fairness in the first ⌊n/α⌋ − 1 + α−1 1 ranks.
ℓ

ℓ



For the rest of the section, let ǫ := k2 1 + α−1 1 . Let the ith “block” of ranks refer to the ranks (i − 1) ⌊ǫk/2⌋ + 1
ℓ
to i ⌊ǫk/2⌋.

5



1
Lemma 2.6. The ranking output by Algorithm 1 satisfies α − ⌊ǫk/2⌋
individual fairness.
Proof. Fix an item having true rank j ∈ [N ]. At the end of step 4, its rank is





 

j
j
j
⌊ǫk/2⌋ + j −
⌊α ⌊ǫk/2⌋⌋ =
(⌊ǫk/2⌋ − ⌊α ⌊ǫk/2⌋⌋) + j
⌊α ⌊ǫk/2⌋⌋
⌊α ⌊ǫk/2⌋⌋
⌊α ⌊ǫk/2⌋⌋
j
j
⌊ǫk/2⌋
j
⩽
.
(⌊ǫk/2⌋ − ⌊α ⌊ǫk/2⌋⌋) + j = j
< α⌊ǫk/2⌋−1 =
1
⌊α ⌊ǫk/2⌋⌋
⌊α ⌊ǫk/2⌋⌋
α − ⌊ǫk/2⌋
⌊ǫk/2⌋

Subsequent steps do not increase the ranking of any item.
Lemma 2.7. At the end of step 13, no positions in the first ⌊n/α⌋ − ⌊ǫk/2⌋ ranks will be empty.
Proof. Consider step 10 of the algorithm. A rank j will be left unoccupied if either (i) each group already has
⌊α ⌊ǫk/2⌋⌋ elements in the block containing rank j, or (ii) the groups which have less than ⌊α ⌊ǫk/2⌋⌋ elements
in the block containing rank j do not have any elements in ranks higher than j. By our choice of parameters, we
have


1
⩾1
⌊ǫk/2⌋ = 1 +
α − 1ℓ
and
2
ǫ=
k



1
1+
α − 1ℓ



=⇒ (ǫk/2 − 1) =

1
1
1 =⇒ ⌊ǫk/2⌋ >
α− ℓ
α − 1ℓ

=⇒ ℓ (α ⌊ǫk/2⌋ − 1) > ⌊ǫk/2⌋ =⇒ ℓ ⌊α ⌊ǫk/2⌋⌋ > ⌊ǫk/2⌋ .
Therefore, if each group has ⌊α ⌊ǫk/2⌋⌋ elements in the block containing rank j, then every rank in this block has
to be occupied. Therefore, case (i) can not happen.
For any block i ∈ Z+ , the first i ⌊ǫk/2⌋ elements in any intermediate ranking (including the final ranking) contain
at most i ⌊α ⌊ǫk/2⌋⌋ items from each group. Since there are at least n elements from group, we have that as long
as i satisfies i ⌊α ⌊ǫk/2⌋⌋ ⩽ n, there will be at least one element available from each group to move into an empty
spot in the first i blocks without violating (α, ⌊ǫk/2⌋) group fairness constraints for any of the first i blocks. Thus,
the first i blocks will be filled at the end of step 13. Therefore, the number of ranks filled is at least




n
n
⌊ǫk/2⌋ >
− 1 ⌊ǫk/2⌋
i ⌊ǫk/2⌋ =
⌊α ⌊ǫk/2⌋⌋
⌊α ⌊ǫk/2⌋⌋


n
− 1 ⌊ǫk/2⌋ = n/α − ⌊ǫk/2⌋ ⩾ ⌊n/α⌋ − ⌊ǫk/2⌋ .
⩾
α ⌊ǫk/2⌋


Therefore, case (ii) will not happen for the first ⌊n/α⌋ − ⌊ǫk/2⌋ ranks. Thus, at the end of step 13, no positions in
the first ⌊n/α⌋ − ⌊ǫk/2⌋ ranks will be empty.
Lemma 2.8. At the end of step 13, each block has at most ⌊α ⌊ǫk/2⌋⌋ items from any particular group.
6

Proof. For any block i, we observe that at the end of step 4, block i of size ⌊ǫk/2⌋ has at most ⌊α ⌊ǫk/2⌋⌋ nonempty positions and therefore has at most ⌊α ⌊ǫk/2⌋⌋ items from any particular group. Step 10 ensures that when
the algorithm terminates, each block has at most ⌊α ⌊ǫk/2⌋⌋ items from any particular group.
Lemma 2.9. The ranking output by Algorithm 1 satisfies (α (1 + ǫ) , k) group fairness in the first ⌊n/α⌋ − ⌊ǫk/2⌋
ranks.
Proof. Lemma 2.7 shows that none of the first ⌊n/α⌋ − ⌊ǫk/2⌋ ranks will be empty at the end of step 13; therefore,
these ranks will remain unchanged in the steps after step 13.
def

def

Consider any k consecutive ranks j, . . . , j + k − 1. Let i1 = ⌊j/ ⌊ǫk/2⌋⌋ and i2 = ⌊(j + k − 1)/ ⌊ǫk/2⌋⌋. By
construction, the blocks i1 + 1, . . . , i2 − 1 are fully contained in the ranks {j, j + 1, . . . , j + k − 1}. For any
l ∈ [ℓ], the number of items from group l in ranks j to j + k − 1 is at most the number of items from group l in
blocks i1 to i2 . Using Lemma 2.8 we get that this is at most
⌊αk⌋ + 2 ⌊α ⌊ǫk/2⌋⌋ ⩽ α(1 + ǫ)k.
We note that this bound also holds for cases when i2 = i1 + 1 or i2 = i1 .
Proof of Theorem 2.5. Follows from the choice of ǫ and from Lemma 2.6, Lemma 2.7 and Lemma 2.9.
We also obtain slightly stronger guarantees if we only need group fairness in “blocks” of size k instead of group
fairness guarantees for any k consecutive ranks.
Theorem 2.10. Given a true ranking of N items grouped into ℓ disjoint groups, with each group having at least
n items, and fairness parameters α ∈ (1/ℓ, 1] and k ∈ Z+ such that k ⩾ α−1 1 , there exists a polynomial time
ℓ
algorithm to compute a ranking satisfying

1. α − k1 individual fairness.
2. (α, k) group fairness in each of the first

j

n
⌊αk⌋

k

blocks of size k.

Proof. We use Algorithm 1 with ǫ := 2. Now, the ith “block” is of size

 ǫk 
2

= k.

Fix an item j ∈ [N ] in the true ranking. The proof of Lemma 2.6 shows that its final rank will be at most
j
1
α − ⌊ǫk/2⌋

=

j
.
α − k1

Here, the
 equality follows from our choice of ǫ = 2. Hence, the ranking output by Algorithm 1 with ǫ = 2 satisfies
α − k1 individual fairness.

Lemma 2.8 shows that at the end of step 13, each block has at most ⌊αk⌋ items from any particular group. For
k ⩾ α−1 1 we have,
ℓ

k⩾

1
=⇒ k (αℓ − 1) ⩾ ℓ =⇒ ℓ (αk − 1) ⩾ k =⇒ ℓ ⌊αk⌋ > k.
α − 1ℓ
7

Therefore, if a block contains ⌊αk⌋ items from each of the ℓ groups, it can not have any empty ranks. Consequently,
as long as i ⌊αk⌋ items fom each group are available, blocks 1 to i will not contain empty ranks. Since there are at
least n items from each group, no rank in the first i blocks will be empty, where i satisfies i ⌊αk⌋ ⩽ n. Hence, for
n
, blocks 1 to i each of size k contain at most αk items from each group. That is, the first
i ∈ Z+ such that i ⩽ ⌊αk⌋
j
k
n
⌊αk⌋ blocks each of size k satisfy (α, k) group fairness.

3 Experimental Validation
In this section, we study the trade-off between individual and group fairness achieved by FIGR on various realworld datasets. We also compare our results with the fair post-processing and fair LTR baselines.

3.1

Datasets

We consider two types of datasets in our experiments. The first type has a global ranking on the entire dataset
calculated using a subset of attributes. Bias in one or more of these attributes might introduce bias in the ranking
output. Hence, a fair post-processing algorithm can be used to correct these biases in the global ranking. However,
a listwise LTR model, such as ListNet [CQL+ 07], requires that each training sample consists of a list of items on
which a ranking is available. Hence, the experiments based on LTR model are performed on the second type of
datasets. These datasets have a query-document format and support training of a listwise LTR model.
German credit risk dataset. This is a dataset of credit risk scoring of adult German residents [DG17]. It consists
of a set of 1000 candidates from various demographics applying for a loan. Features of the candidate include
demographic information such as personal status, gender, age, etc. as well as financial status such as credit history,
property, housing, job etc. Schufa scores of these individuals is used to get a global ranking on the dataset similar
to [ZBC+ 17] and [YS17]. [Cas19b] observed that Schufa scoring is biased against young adults. Hence, we divide
the dataset into protected and non-protected groups based on age. We consider two such cases (i) age < 25 as
protected group, and (ii) age < 35 as protected group similar to [ZBC+ 17].
COMPAS recidivism dataset. This dataset consists of violent recidivism assessment of nearly 7000 criminal
defendants by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool based
on answers to a questionnaire consisting of 137 questions. This dataset is curated by [ALMK16] to analyze the
biases in the tool. Their study points out racial as well as gender biases in the predictions of COMPAS. In our
experiments, we consider ranking based on the recidivism score (individual with highest ¬recidivism is ranked at
top-1). The protected groups are (i) gender (=female)1 and (ii) race (=African American) similar to [ZBC+ 17].
ChileSAT dataset. This dataset consists of a Chilean University’s admission test scores of the students admitted,
their highschool grades, and a score based on their academic performance after one year at the university. Given
these test scores and their highschool grades, an LTR model has to predict in advance, a ranking over these students
about their performance at the end of the first academic year. This dataset has a query-document format since each
academic year can be considered as a query and students in that academic year are the list of candidates to be
ranked. There are around 450 to 600 student records in each of the 5 academic years in the dataset. In the
experiments, we perform a 5-fold cross validation with four academic years for training and one for validating the
1 Non-binary genders were not annotated in any of the datasets used in this paper.

8

LTR models. We consider two protected groups in the dataset, gender (=female) and high-school type (=public) to
study the behaviour of FIGR when two different types of bias exist. These biases are also studied by [ZC20].
In our experiments, we use the processed subsets of ChileSAT (Engineering Students)2 , German credit risk and
COMPAS recidivism datasets3 .

3.2

Experiments

We compare our results with baselines, such as (i) ListNet [CQL+ 07], a listwise LTR model that ranks a list of
documents based on their relevance scores with respect to the query, (ii) DELTR [ZC20], an in-processing LTR
model that is trained with ListNet’s objective along with the group fairness constraint – fairness of exposure, and
(iii) FA*IR [ZBC+ 17], a post-processing algorithm that re-ranks a given ranking to maintain significant proportions of the protected group in every prefix of the ranking. For brevity, we omit detailing the intricacies of these
baseline algorithms. Following is a brief explanation of our experimental setup4 ,
1. color-blind LTR. ListNet trained in a ‘colorblind’ fashion – no access to protected (or sensitive) attributes.
2. color-aware LTR. ListNet trained on all the attributes.
3. DELTR. Trained with γ = 100K. (refer [ZC20] for more details).
4. FA*IR pre p. FA*IR with parameter p used to pre-process the training data. ListNet is trained on this data.
5. FA*IR post p. ListNet’s predictions on test data post-processed using FA*IR with parameter p.
6. FA*IR p. FA*IR with parameter p used to re-rank the global ranking in the COMPAS recidivism and
German credit risk datasets.
7. FIGR pre p. FIGR with parameter α = max{1 − p, 1ℓ + δ} used to pre-process the training data. We add a
small number δ to 1/ℓ since FIGR requires that α is strictly greater than 1/ℓ. ListNet is trained on this data.
8. FIGR post p. ListNet’s predictions on test data post-processed using FIGR with parameter α = max{1 −
p, 1ℓ + δ}.
9. FIGR p. FIGR with parameter α = max{1 − p, 1ℓ + δ} used to re-rank the global ranking in the COMPAS
recidivism and German credit risk datasets.
Training details. For FA*IR, the parameter p is chosen from {p∗ , p+ , p− }, where p∗ is the proportion of the
protected group, p+ = p∗ + 0.1 and p− = p∗ − 0.1. This parameter setting is adopted from [ZC20]. In all the
experiments, the number of groups are 2 (protected and non-protected). Hence, we use ℓ = 2 and δ = 0.01 in
FIGR. For the values of p, ℓ and δ, the parameter α in FIGR will be α = max {1 − p, 0.51}. For all the datasets
in the experiments, we set the value of k to 100. This is a reasonable choice since the number of ranked items
is at least 500 in all the datasets. For the LTR models – ListNet and DELTR – we use the same parameter and
hyper-parameter settings as [ZC20] and report average results across 5-folds of the ChileSAT dataset. Further, to
account for variability in learning the parameters for LTR, we run all the LTR based expreiments 5 times and report
the final average5 .
2 https://github.com/MilkaLichtblau/DELTR-Experiments/tree/master/data/EngineeringStudents
3 https://github.com/DataResponsibly/FairRank/tree/master/datasets
4 Code for FIGR and baselines is available at https://github.com/sruthigorantla/FIGR
5 Our runs showed minor variations in the learned LTR parameters with almost same ranking predictions.

9

Protected group = age < 35
Proportion of the protected group

Proportion of the protected group

Protected group = age < 25

0.6
0.5
0.4
0.3
0.2
0.1

0.6
0.5
0.4
0.3
0.2
0.1

100 200 300 400 500 600 700 800 900 1000
(a)

top-k

0.1

FIGR p∗
FA*IR p∗

100 200 300 400 500 600 700 800 900 1000
FIGR p−

FA*IR p+

FA*IR p−

True
y = p∗

3

8

1.0

min
min α

1.0

min
min α

top-k

(c)
FIGR p+

0.8

0.8

0.6

0.6

1

2
(b)

3

4
5
6
7
8
Item
binsofofsize
size100
100
Blocks

9

1

10

2
(d)

4
5
6
7
Blocks of size 100

9

10

Figure 1: Trade-offs between individual and group fairness in the German credit risk dataset. In all the experiments with FIGR
p, the parameter settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}.

Reading the plots. For every combination of a dataset and a protected group, we show a pair of plots to understand
better the trade-off between group and individual fairness in the real-world datasets. For example, Figure 1(a) and
Figure 1(b) show group and individual fairness respectively on the German credit risk dataset with age < 25 as
protected group. In Figure 1(a), X-axis represents the rank k and Y-axis shows the proportion of protected group
items in the top-k ranks. In this plot, the line y = p∗ shows the proportion of the protected group in the entire
(training) data, whereas the line ‘True’ represents the proportion of the protected group in the top-k ranks of the
true ranking. These two lines serve as guidelines to understand the behavior of various algorithms and pick the
best performing algorithm. In Figure 1(b), X-axis represents blocks of size 100 – ith block consists of items
j
100(i − 1) + 1 to 100i of the true ranking. For any item j in the block, αj = pred(j)
, where pred(j) is the
rank of item j in the output ranking. Y-axis shows the minimum αj among the items in the block. We call this
min α. Higher the value of min α, higher the individual fairness, according to the Definition 2.2. Since we measure
individual fairness with respect to the true ranking, the line ‘True’ has 1-individual fairness. We say that there is
a trade-off between group and individual fairness when the algorithms placing higher proportions of protected
groups in the top-k ranks consistently suffer from lesser individual fairness in these ranks. In the following section
we study the trade-offs achieved by FIGR and the baselines on the real-world datasets.

10

Protected group = African American
Proportion of the protected group

Proportion of the protected group

Protected group = female

0.6
0.5
0.4
0.3
0.2
0.1

0.6
0.5
0.4
0.3
0.2
0.1

100 200 300 400 500 600 700 800 900 1000
(a)
top-k
FIGR p∗
FA*IR p∗

0.1

100 200 300 400 500 600 700 800 900 1000
top-k
(c)

FIGR p+

FIGR p−

FA*IR p+

FA*IR p−

1.0

min
min α

1.0

min
min α

True
y = p∗

0.8

0.8

0.6

0.6

1

2
(b)

3

4
5
6
7
8
Item
binsofofsize
size 100
100
Blocks

9

1

10

2
(d)

3

4
5
6
7
8
Item
binsofofsize
size 100
100
Blocks

9

10

Figure 2: Trade-offs between individual and group fairness in the COMPAS recidivism dataset. In all the experiments with
FIGR p, the parameter settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}. We show the reults of the
top 1000 ranks.

3.3

FIGR vs. Fair Post-Processing Methods on German Credit Risk and COMPAS
Datasets

Figures 1(a)- 1(d) show the experimental results on the German credit risk dataset. The protected group age < 25
(younger adults) is significantly underrepresented in the dataset with p∗ = 0.15 (see Figure 1(a)). Moreover, their
‘True’ proportions given by the Schufa score based ranking in the top-400 ranks is even less. This indicates bias
against younger adults in the ranking. All three variants of FIGR allocate almost 1.5-2 times the true proportion
and increase the representation of younger adults in the top-400 ranks. FIGR also achieves approximately 0.70individual fairness, which is a reasonable trade-off for group fairness in this case. FA*IR on the other hand fails to
correct biases with p∗ and p− . Even with p+ , it fails to achieve significant improvement in the representation of
the younger adults.
In case of the protected group age < 35 (young adults), although their representation in the entire dataset is almost
11

1.0

0.45

0.8

min
min α

Proportion of the protected group

Pre-processing vs. DELTR vs. LTR

0.50

0.40

0.6

0.35

0.4

0.30

0.2

100
(a)

0.45
0.45
0.40

200

300

400

1

500

color-blind LTR
color-aware LTR
DELTR

2
(b)

top-k
FIGR pre p∗
+

FIGR pre p
FIGR pre p−

3
4
Blocks
of
size
100
0.50

FA*IR pre p∗
FA*IR pre
p+
0.45
FA*IR∗ pre p−

5

True
y = p∗

Figure 3: Trade-offs between individual and group fairness in the ChileSAT dataset with public school as the protected group.
Comparision of pre-processig, in-processing and standard LTR methods. In all the experiments with FIGR p, the parameter
settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}. See Table 1 in Appendix A for tabulated results.

half (p∗ = 0.55), they are all ranked in the tail of the true ranking. This shows a strong bias against young adults
(see Figure 1(c)). FA*IR p− stays too close to the ‘True’ proportions. On the other hand, with slightly larger
values of p, FA*IR p+ and p∗ clearly overcompensate for the lack of representation of the protected group at the
cost of very low individual fairness (0.50-0.65). This also reduces the proportion of the non-protected group to
much lower than it’s true proportion, leading to inversion of bias. FIGR, by design, avoids this problem. For
values of p higher than 1/ℓ, α is set to 1ℓ + δ, where δ is a small value (δ = 0.01 in our experiments). Hence, the
representation of both protected and non-protected groups is bounded (see Theorem 2.5). This is also evident in
our results. With p+ and p∗ , FIGR achieves same results because α in both the cases is set to 0.51. FIGR with
each of these parameter settings finds the best trade-off between group fairness (45% in the top 100 positions) and
individual fairness (0.75 in the top 100 positions).
Figures 2(e)- 2(h) show experimental results on the COMPAS recidivism dataset. Females in the dataset (p∗ =
0.19) face bias in the top-200 ranks due to the biases introduced by the COMPAS tool (see Figure 2(e)). Once
again, FA*IR with p∗ and p− stays close to the ‘True’ proportions. With p+ , it improves the female representation
in the top-400 ranks. FIGR p∗ also shows same trends as FA*IR p+ . Both FIGR p+ and p− trade off individual
fairness for group fairness. Nevertheless, this doesn’t cause an inversion of bias in the output ranking.
The protected group race = African American is substantially underrepresented in the dataset in the top-500 positions even though their representation in the entire data is high, p∗ = 0.51 (see Figure 2(g)). This shows bias
against the protected group in the true ranking based on the recidivism score. Since this case of bias is similar
to the bias towards young adults in the German credit risk dataset, we see similar results. FIGR improves their
representation in the top-500 ranks while achieving reasonable individual fairness. FA*IR p− is a close competitor
to FIGR p− for the best choice of algorithm. However, FA*IR p+ and p∗ once again overcompensate for the lack
of representation of African-Americans in the top-1000 positions since they actually are the majority group in the
data.

12

1.0

0.45

0.8

min
min α

Proportion of the protected group

Post-processing vs. DELTR vs. LTR

0.50

0.40

0.6
0.4

0.35

0.2

0.30

100
1

200
2

300
3

400
4

(a)

top-k

0.45
0.45

color-blind LTR
color-aware LTR
DELTR

0.40

1
(b)

500
5
FIGRpost
pre p∗∗
FIGR
+
FIGR
FIGRpost
pre p+
−
FIGR
FIGRpost
pre p −

2

3
4
Blocks of size 100

5

0.50

FA*IR pre
postp∗p∗
FA*IR
FA*IR post
pre
pp++
0.45
FA*IR∗ post
pre pp−−

True
y = p∗

∗

Figure 4: Trade-offs between individual and group fairness in the ChileSAT dataset with public school as the protected group.
Comparision of post-processig, in-processing and standard LTR methods. In all the experiments with FIGR p, the parameter
settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}. See Table 1 in Appendix A for tabulated results.

3.4

FIGR vs. Fair LTR Methods on the ChileSAT Dataset

Figure 3 and Figure 5 show the group and individual fairness trade-offs achieved by the LTR models, DELTR and
pre-processing training data with FIGR and FA*IR. Whereas Figure 4 and Figure 6 show these results for same
LTR and DELTR models compared with post-processing methods applied on the ListNet predictions.
Representation of the students from public school in the top-300 ranks is higher than their total representation
(p∗ = 0.34) (see Figure 3(a)). This means that the students from public schools having same test scores as private
school students indeed have caliber to perform better at the university. Hence, it is evident that the test scores
are biased against the students from the public school. Such biases are naturally adjusted by a color-aware LTR,
whereas a color-blind LTR re-inforces these biases (see Figure 3(a)). DELTR does not further adjust the ranks.
Interestingly, pre-processing with all three variants of FIGR and FA*IR with p+ rank higher proportions of the
public school students in the top-200 ranks and at the same time achieve high individual fairness compared to
the LTR models (see Figure 3(b)). Although color-aware LTR corrects biases, its impact is limited. DELTR, on
the other hand, limits itself from achieving higher group fairness as well as individual fairness since fairness of
exposure is already satisfied. In the case of post-processing the ListNet predictions, only FIGR with p∗ achieves
similar results (see Figure 4(a)), and still has as much individual fairness as others.
Female students are substantially under-represented in the dataset (p∗ = 0.20). The color-blind LTR ranks almost
the same proportion of the females in all top-k ranks as that of ‘True’ proportions (see Figure 5(a)). This means
that the academic performance of females both before and after the first year at the university is bad. In this case, a
color-aware LTR will further learn to discriminate against the females because in this case, placing females in the
top few ranks would hurt the utility. As expected, the color-aware LTR stays below the color-blind LTR. DELTR
acts on the difference in the exposure of the groups. Hence, it places more number of females in the top-200 ranks
purely to achieve fairness of exposure. But, DELTR is oblivious to its impact on the individual fairness. Although
13

0.35

1.0

0.30

0.8

min
min α

Proportion of the protected group

Pre-processing vs. DELTR vs. LTR

0.25
0.20

0.6
0.4

0.15
0.2

0.10
1
100

2
200

3
300

4
400

(a)

top-k

0.45
0.45

color-blind LTR
color-aware LTR
DELTR

0.40

5
500
FIGR pre p∗
FIGR pre p+
FIGR pre p−

1

2
(b)

3
4
Blocks of size 100

5

0.50

FA*IR pre p∗
FA*IR pre
p+
0.45
FA*IR∗ pre p−

True
y = p∗

Figure 5: Trade-offs between individual and group fairness in the ChileSAT dataset with female as the protected group.
Comparision of pre-processig, in-processing and standard LTR methods. In all the experiments with FIGR p, the parameter
settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}. See Table 2 in Appendix A for tabulated results.

pre-processing with FIGR achieves less individual fairness than DELTR, post-processing with FIGR places the
highest number of females in the top-200 while achieving as much individual fairness as DELTR (see Figure 6).
In both the cases, FIGR with most of the parameter settings achieves the best group fairness as well as individual
fairness. Even in cases where it loses some individual fairness compared to the baselines, the trade-off is minimal.
In contrast, the fair LTR and fair post-processing baselines do not achieve these trade-offs.

4 Conclusion
Fair ranking is crucial to search and recommendations, and has been a matter of global concern in the quest
towards responsible AI. We studied group and individual fairness notions in ranking. We defined individual fairness
based on how close the predicted rank of each item is to its true rank, and proved a lower bound on the trade-off
achievable for simultaneous individual and group fairness in ranking. While other works (e.g., [CSV18], etc.)
have studied “aggregate forms” of individual fairness, to the best of our knowledge, our work is the first to give
provable guarantees on the worst-case displacement of an item in the output ranking with respect to its true rank.
We presented the first (to the best of our knowledge) algorithm that takes any given ranking and outputs another
ranking with simultaneous individual and group fairness guarantees comparable to the lower bound we proved.
Our algorithm performed better than the state-of-the-art fair learning to rank and fair post-processing baselines.
One limitation of our work (and other re-ranking algorithms) is that it requires the true ranking as input. All our
theoretical guarantees are with respect to this true ranking; in practice, a true merit-based ranking may be debatable
or unavailable due to incomplete data, unobserved features, legal and ethical considerations behind the downstream
application of these rankings, etc.
14

0.35

1.0

0.30

0.8

min
min α

Proportion of the protected group

Post-processing vs. DELTR vs. LTR

0.25
0.20

0.6
0.4

0.15

0.2
0.10
1
100

2
200

3
300

4
400

5
500

1

0.45
0.45
0.40

color-blind LTR
color-aware LTR
DELTR

2
(b)

top-k

(a)

FIGRpost
pre p∗∗
FIGR
+
FIGR
FIGRpost
pre p+
−
FIGR
FIGRpost
pre p −

3
4
Blocks of size 100

5

0.50

FA*IR pre
postp∗p∗
FA*IR
FA*IR post
pre
pp++
0.45
∗
FA*IR post
pre pp−−

True
y = p∗

Figure 6: Trade-offs between individual and group fairness in the ChileSAT dataset with female as the protected group.
Comparision of post-processig, in-processing and standard LTR methods. In all the experiments with FIGR p, the parameter
settings are ℓ = 2, δ = 0.01 and k = 100. Then, α = max {1 − p, 0.51}. See Table 2 in Appendix A for tabulated results.

Acknowledgements. AL was supported in part by SERB Award ECR/2017/003296 and a Pratiksha Trust Young
Investigator Award. AL is also grateful to Microsoft Research for supporting this collaboration.

References
[ALMK16] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias, 2016.
[AT05]

G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a survey of
the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering,
17(6):734–749, 2005.

[BCD+ 19] Alex Beutel, J. Chen, T. Doshi, H. Qian, L. Wei, Y. Wu, L. Heldt, Zhe Zhao, L. Hong, Ed Huai
hsin Chi, and Cristos Goodrow. Fairness in recommendation ranking through pairwise comparisons.
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, 2019.
[BGW18]

Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. Equity of attention: Amortizing individual
fairness in rankings. In The 41st International ACM SIGIR Conference on Research & Development
in Information Retrieval, SIGIR ’18, page 405414, New York, NY, USA, 2018. Association for Computing Machinery.

[BHN19]

Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019. http://www.fairmlbook.org.

15

[Bin20]

Reuben Binns. On the apparent conflict between individual and group fairness. In FAT* ’20: Conference on Fairness, Accountability, and Transparency, Barcelona, Spain, January 27-30, 2020, pages
514–524. ACM, 2020.

[BP98]

Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine. In
Proceedings of the Seventh International Conference on World Wide Web 7, WWW7, page 107117,
NLD, 1998. Elsevier Science Publishers B. V.

[BS16]

Solon Barocas and Andrew D. Selbst.
104(3):671–732, 2016.

[Cas19a]

Carlos Castillo. Fairness and transparency in ranking. SIGIR Forum, 52(2):6471, January 2019.

[Cas19b]

Carlos Castillo. Fairness and transparency in ranking. volume 52, page 6471, New York, NY, USA,
January 2019. Association for Computing Machinery.

Big data’s disparate impact.

California Law Review,

[CDPF+ 17] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision
making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’17, page 797806, New York, NY, USA, 2017.
Association for Computing Machinery.
[CQL+ 07]

Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: From pairwise
approach to listwise approach. In Proceedings of the 24th International Conference on Machine
Learning, ICML 07, page 129136, New York, NY, USA, 2007. Association for Computing Machinery.

[Cro04]

F.J. Crosby. Affirmative Action is Dead: Long Live Affirmative Action. Current perspectives in
psychology. Yale University Press, 2004.

[CSV18]

L. Elisa Celis, Damian Straszak, and Nisheeth K. Vishnoi. Ranking with fairness constraints. In
Ioannis Chatzigiannakis, Christos Kaklamanis, Dániel Marx, and Donald Sannella, editors, 45th International Colloquium on Automata, Languages, and Programming, ICALP 2018, July 9-13, 2018,
Prague, Czech Republic, volume 107 of LIPIcs, pages 28:1–28:15. Schloss Dagstuhl - LeibnizZentrum für Informatik, 2018.

[DG17]

Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.

[DHP+ 12]

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS
’12, page 214226, New York, NY, USA, 2012. Association for Computing Machinery.

[GAK19]

Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. Fairness-aware ranking in search
& recommendation systems with application to linkedin talent search. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page
22212231, New York, NY, USA, 2019. Association for Computing Machinery.

[HPS16]

M. Hardt, E. Price, and Nathan Srebro. Equality of opportunity in supervised learning. In NIPS,
2016.

[JK00]

Kalervo Järvelin and Jaana Kekäläinen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’00, page 4148, New York, NY, USA, 2000. Association for Computing Machinery.
16

[KLH16]

Christoph Kofler, Martha Larson, and Alan Hanjalic. User intent in multimedia search: A survey of
the state of the art and future challenges. ACM Comput. Surv., 49(2), August 2016.

[KRW17]

Michael Kearns, Aaron Roth, and Zhiwei Steven Wu. Meritocratic fairness for cross-population
selection. volume 70 of Proceedings of Machine Learning Research, pages 1828–1836, International
Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.

[MRS08]

Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information
Retrieval. Cambridge University Press, USA, 2008.

[NCGW20] Harikrishna Narasimhan, Andy Cotter, Maya Gupta, and Serena Lutong Wang. Pairwise fairness for
ranking and regression. In 33rd AAAI Conference on Artificial Intelligence, 2020.
[Nob18]

Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press,
2018.

[O’N16]

Cathy O’Neil. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens
Democracy. Crown Publishing Group, USA, 2016.

[Par11]

Eli Pariser. The Filter Bubble: What the Internet Is Hiding from You. Penguin Group , The, 2011.

[PZZ+ 19]

Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian Wu, Peng Jiang,
Junfeng Ge, Wenwu Ou, and Dan Pei. Personalized re-ranking for recommendation. In Proceedings
of the 13th ACM Conference on Recommender Systems, RecSys ’19, page 311, New York, NY, USA,
2019. Association for Computing Machinery.

[SJ19]

Ashudeep Singh and Thorsten Joachims. Policy learning for fairness in ranking. In NeurIPS, 2019.

[Tav20]

Herman Tavani. Search Engines and Ethics. In Edward N. Zalta, editor, The Stanford Encyclopedia
of Philosophy. Metaphysics Research Lab, Stanford University, fall 2020 edition, 2020.

[YLS20]

Ke Yang, Joshua R. Loftus, and Julia Stoyanovich. Causal intersectionality for fair ranking. ArXiv,
abs/2006.08688, 2020.

[YS17]

Ke Yang and Julia Stoyanovich. Measuring fairness in ranked outputs. In Proceedings of the 29th
International Conference on Scientific and Statistical Database Management, SSDBM 17, New York,
NY, USA, 2017. Association for Computing Machinery.

[ZBC+ 17]

Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo
Baeza-Yates. Fa*ir: A fair top-k ranking algorithm. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, CIKM 17, page 15691578, New York, NY, USA, 2017.
Association for Computing Machinery.

[ZC20]

Meike Zehlike and Carlos Castillo. Reducing disparate exposure in ranking: A learning to rank
approach. Proceedings of The Web Conference 2020, 2020.

[ZWS+ 13] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference
on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 325–333,
Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.

A

Appendix for Experimental Results
17

top-k ranks (→)
Method (↓)
True
color-blind LTR
color-aware LTR
DELTR
FA*IR pre p∗
FA*IR pre p+
FA*IR pre p−
FIGR pre p∗
FIGR pre p+
FIGR pre p−
FA*IR post p∗
FA*IR post p+
FA*IR post p−
FIGR post p∗
FIGR post p+
FIGR post p−

k = 100
Grp. Ind.
0.42
1
0.32 0.17
0.38 0.18
0.41 0.19
0.39 0.18
0.45 0.2
0.39 0.18
0.45 0.2
0.49 0.21
0.43 0.19
0.38 0.18
0.4 0.17
0.38 0.18
0.41 0.19
0.45 0.18
0.41 0.18

k = 200
Grp. Ind.
0.38
1
0.32 0.46
0.38 0.4
0.41 0.42
0.39 0.4
0.45 0.54
0.39 0.4
0.45 0.54
0.49 0.52
0.44 0.54
0.38 0.4
0.41 0.46
0.38 0.4
0.41 0.47
0.44 0.46
0.41 0.47

k = 300
Grp. Ind.
0.36
1
0.32 0.63
0.37 0.72
0.39 0.76
0.38 0.64
0.43 0.65
0.38 0.61
0.43 0.63
0.45 0.65
0.41 0.62
0.37 0.72
0.41 0.7
0.37 0.72
0.41 0.67
0.45 0.71
0.4 0.65

k = 400
Grp. Ind.
0.35
1
0.33 0.88
0.36 0.87
0.36 0.77
0.36 0.87
0.38 0.82
0.36 0.87
0.38 0.82
0.39 0.82
0.37 0.82
0.35 0.87
0.41 0.84
0.35 0.87
0.39 0.83
0.41 0.87
0.39 0.83

k = 500
Grp. Ind.
0.31
1
0.3
1
0.31
1
0.31
1
0.31
1
0.32
1
0.31
1
0.32
1
0.32
1
0.32
1
0.31
1
0.34
1
0.31
1
0.33
1
0.34
1
0.33
1

Table 1: Individual and group fairness results for the ChileSAT dataset with public school as the protected group.
The plots for these results are as shown in Figure 3 and Figure 4. The column Grp. shows the proportion of the
protected group items in the top-k ranks and the column Ind. shows min α in the ranks k − 99 to k. Note that
k
k − 99 to k is the 100
th block of size 100 shown in the plots.

18

top-k ranks (→)
Method (↓)
True
color-blind LTR
color-aware LTR
DELTR
FA*IR pre p∗
FA*IR pre p+
FA*IR pre p−
FIGR pre p∗
FIGR pre p+
FIGR pre p−
FA*IR post p∗
FA*IR post p+
FA*IR post p−
FIGR post p∗
FIGR post p+
FIGR post p−

k = 100
Grp. Ind.
0.1
1
0.13 0.17
0.1 0.17
0.18 0.16
0.12 0.16
0.23 0.14
0.11 0.17
0.27 0.14
0.35 0.13
0.27 0.14
0.15 0.16
0.25 0.15
0.11 0.17
0.3
0.13
0.35 0.13
0.31 0.13

k = 200
Grp. Ind.
0.14
1
0.14 0.46
0.12 0.46
0.22 0.49
0.14 0.47
0.27 0.39
0.12 0.45
0.3
0.38
0.35 0.35
0.31 0.36
0.17 0.42
0.26 0.45
0.12 0.46
0.3
0.46
0.35 0.44
0.31 0.52

k = 300
Grp.
Ind.
0.17
1
0.17 0.63
0.15 0.59
0.22
0.6
0.17 0.72
0.26 0.71
0.15 0.62
0.27 0.62
0.3
0.63
0.29 0.65
0.17 0.75
0.27
0.6
0.15
0.6
0.27 0.61
0.3
0.6
0.28
0.6

k = 400
Grp.
Ind.
0.19
1
0.18 0.88
0.16 0.86
0.21 0.85
0.18
0.9
0.23 0.96
0.17 0.88
0.23 0.95
0.24 0.92
0.23 0.91
0.18 0.92
0.24
0.88
0.16 0.86
0.24
0.87
0.24 0.89
0.22 0.85

k = 500
Grp. Ind.
0.17
1
0.17
1
0.16
1
0.18
1
0.16
1
0.19
1
0.16
1
0.19
1
0.19
1
0.19
1
0.16
1
0.2
1
0.16
1
0.2
1
0.2
1
0.18
1

Table 2: Individual and group fairness results for the ChileSAT dataset with female as the protected group. The
plots for these results are as shown in Figure 5 and Figure 6. The column Grp. shows the proportion of the
protected group items in the top-k ranks and the column Ind. shows min α in the ranks k − 99 to k. Note that
k
th block of size 100 shown in the plots.
k − 99 to k is the 100

19

